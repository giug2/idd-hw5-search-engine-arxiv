{
    "S3.T1": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 1: Operational modes and corresponding conditions in UniTok-Audio.",
        "body": "Mode\nTask Token\nConditions\n\n\nSR\nTSR{\\rm T_{SR}}\nDegraded Speech\n\n\nTSE\nTTSE{\\rm T_{TSE}}\nReference Speech, Mixture Speech\n\n\nrTSE\nTrTSE{\\rm T_{rTSE}}\nReference Speech, Mixture Speech\n\n\nVC\nTVC{\\rm T_{VC}}\nReference Speech, Source Speech\n\n\nLASS\nTLASS{\\rm T_{LASS}}\nCaption, Mixture Audio",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Mode</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Task Token</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Conditions</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">SR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><math alttext=\"{\\rm T_{SR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>SR</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{SR}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\">Degraded Speech</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">TSE</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{\\rm T_{TSE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>TSE</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{TSE}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Reference Speech, Mixture Speech</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">rTSE</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{\\rm T_{rTSE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{rTSE}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Reference Speech, Mixture Speech</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VC</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{\\rm T_{VC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>VC</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{VC}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Reference Speech, Source Speech</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">LASS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"{\\rm T_{LASS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>LASS</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{LASS}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">Caption, Mixture Audio</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "conditions",
            "lass",
            "caption",
            "speech",
            "ttserm",
            "tvcrm",
            "reference",
            "source",
            "tlass",
            "token",
            "operational",
            "rtse",
            "tse",
            "unitokaudio",
            "audio",
            "tvc",
            "degraded",
            "trtse",
            "tsrrm",
            "ttse",
            "trtserm",
            "mode",
            "mixture",
            "task",
            "tlassrm",
            "modes",
            "tsr",
            "corresponding"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Following our previous work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, we introduce special task tokens to distinguish between different operational modes.\nTo unify five tasks (i.e., SR, TSE, SS, VC, and LASS), we utilize five modes, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 AR Prediction of H-Codec Tokens &#8227; 3.2 Unified Multi-Task Generation &#8227; 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nEach mode corresponds to a special token and different task-specific condition types, which serve as a conditioning sequence\nfor the LM backbone to estimate the conditional probability density distribution of target discrete tokens.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative modeling has recently achieved remarkable success across text, image, and audio domains,\ndemonstrating powerful capabilities for unified representation learning.\nHowever, audio generation models still face challenges in terms of audio quality and generalization ability across tasks.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.\nTo address these issues, we propose <span class=\"ltx_text ltx_font_bold\">UniTok-Audio</span>, a scalable and extensible framework for unified audio generation tasks.\nSpecifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner;\n2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework;\n3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction.\nExperimental results demonstrate that UniTok-Audio achieves competitive performance\nin comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks:\nspeech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation.\nTo foster future research, we will open-source our codebase.\nThe demo page of our work can be found here: https://alibaba.github.io/unified-audio.\n</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "token",
                    "speech",
                    "task",
                    "unitokaudio",
                    "audio",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Leveraging the remarkable sequential generation capability of language model (LM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib67\" title=\"\">2017</a>)</cite>,\nrecent works have achieved significant improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib48\" title=\"\">2024</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib35\" title=\"\">2023</a>)</cite>,\npromoting the growing prevalence of artificial intelligence-generated content (AIGC).\nThese advances have inspired substantial research extending LMs to various audio tasks,\nwhich can be fundamentally categorized by the temporal relationship between input and output:\neither <span class=\"ltx_text ltx_font_italic\">time-aligned</span> (TA) or <span class=\"ltx_text ltx_font_italic\">non-time-aligned</span> (NTA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib82\" title=\"\">2025</a>)</cite>.\nThe former involves strict temporal correspondence between input and output signals, such as speech denoising,\nwhich aligns speech components in each frame between noisy and clean speech.\nWhile the latter dose not require point-wise temporal alignment, such as text-to-audio (TTA),\nwhich aims at semantic coherence between the holistical textual description and entire output soundscape.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study focuses on the TA tasks, especially which provides the input audio that temporally aligned with the output audio at the frame level,\nincluding: speech restoration (SR) that aims at restoring speech from the degraded recording with various distortions (e.g., noise, reverberation,and packet loss);\ntarget speaker extraction (TSE) that extracts target speech from mixture using assistive clues (e.g., voiceprint information from reference speech);\nspeech separation (SS) that aims to separate all existing speaker in the mixture;\nvoice conversion (VC) that transforms the timbre of source speech guided by reference speech of another speaker;\nlanguage-queried audio source separation (LASS) that aims at extracting target audio components from mixture, which are consistent with the given textual caption.\nNumerous generative models are developed for these tasks, while most of them are designed for single task with\ntask-specific architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "degraded",
                    "lass",
                    "caption",
                    "speech",
                    "task",
                    "tse",
                    "reference",
                    "mixture",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some studies aim to unify multiple tasks within a single framework, including AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>, UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.\nThese methods utilizes the LM backbone combined with discrete audio codec and\nexhibit remarkable generative ability, which benefit from the semantic understanding and contextual modeling capabilities of LMs.\nHowever, challenges still exist in terms of audio quality and generalization ability across tasks.\nFor instance, few unified models are capable of handling the SS task, as it generally requires customized\narchitecture to output multi-track speech.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Framework</span>: The framework unifies tasks by taking task-specific conditional information as the conditioning sequence of decoder-only LM,\nand the discrete token of target audio is predicted in an AR manner.\nWe utilize a special task token to distinguish different learning patterns of multiple tasks.\nNote that our model handles diverse tasks using a single set of shared weights, thereby eliminating the need for task-specific weight adaptation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "task",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">High-Fidelity Generation</span>: UniTok-Audio achieves high-fidelity generation quality\nin terms of SR, TSE, SS, VC, and LASS tasks, demonstrating strong competitiveness compared to\nstate-of-the-art (SOTA) task-specific or multi-task baselines.</p>\n\n",
                "matched_terms": [
                    "tse",
                    "unitokaudio",
                    "lass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the domain of TA audio tasks, early researches focus on discriminative modeling,\nwhich directly learns the mapping between input signal and target audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Williamson &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib80\" title=\"\">2017</a>; Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>.\nHowever, the lack of generative ability limits their generalization in unseen scenarios and\nthe performance in extreme situations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Welker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib78\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib71\" title=\"\">2020</a>)</cite>.\nMany studies integrate generative modeling into audio tasks in recent years.\nFor the SR task, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite> applies k-means to quantize noisy speech representations obtained\nby WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib4\" title=\"\">2022</a>)</cite> into discrete tokens,\nand then a Transformer-based speech LM maps the noisy tokens to clean tokens.\nFor the LASS task, FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>\nlearns linear flow trajectories from noise to target source features within the variational autoencoder (VAE) latent space,\nwhich are guided by the encoded text embeddings and the mixture audio.\nHowever, these models are designed for specific task, facing limited extensibility when migrating to more tasks.</p>\n\n",
                "matched_terms": [
                    "lass",
                    "speech",
                    "task",
                    "audio",
                    "mixture",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Creating an unified framework that can tackle diverse tasks\nstands as a critical research goal in the field of artificial intelligence.\nIn the unification of audio tasks, the approaches can be divided into two categories:\ndiscrete audio codec based method and continuous representation based method.\nThe former is based on the pre-trained audio codec, which encodes the waveform into discrete space and reconstructs\naudio signal from it.\nThe generative ability of AR modeling or masked generative modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib2\" title=\"\">2022</a>)</cite> is leveraged to\ngenerate discrete tokens of the target audio.\nFor instance, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite> tokenizes the target audio along with other condition modalities\nand then concatenates source-target pair as a single sequence, performing next-token prediction using LLM.\nMetis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite> adopts a two-stage generation framework using masked generative modeling,\nwhich first generates SSL tokens and then predicts acoustic tokens based the former.\nContinuous representation based methods typically adopt diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib20\" title=\"\">2020</a>)</cite> or flow matching techniques,\neliminating the inevitable quantitative loss in discrete codec.\nVoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib30\" title=\"\">2023</a>)</cite> performs flow matching on mel-spectrograms to unify tasks such as text-to-speech (TTS) and speech editing.\nUniFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>)</cite> utilizes VAE to learn a compact latent representation of raw audio,\ncoupled with a diffusion Transformer (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles &amp; Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib47\" title=\"\">2023</a>)</cite> that predicts latent updates.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to discrete audio codec based method, especially decoder-only AR models which can elegantly\nintegrate conditional information as a prefix sequence, continuous methods usually requires complex design\nto combines multimodal conditions, limiting the extensibility to more tasks.\nIn addition, discrete audio representation plays an important role in combining with LLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib62\" title=\"\">2025</a>)</cite>,\nbridging the natural language instructions and continuous waveform.\nTherefore, we develop a decoder-only AR LM-based framework (UniTok-Audio) to unify audio tasks.\nIt utilizes continuous conditional embeddings to maximize the preservation of semantic and acoustic information,\npredicting multi-layer codec tokens which reduce the quantization loss.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "conditions",
                    "unitokaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the development of LM, the research focus of codecs has gradually shifted\nfrom reducing data transmission costs toward the integration with LM, which ensures the high quality of generated audio.\nThis requires codecs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib37\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib10\" title=\"\">2024</a>)</cite> to preserve more semantic information that can be understood and modeled by LM.\nX-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib86\" title=\"\">2024a</a>)</cite> integrates the representations from the pre-trained SSL model to enhance semantic preservation,\nimproving both reconstruction quality and downstream TTS performance.\nSome studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib24\" title=\"\">2025</a>)</cite> explore single-layer codecs that are more suitable for autoregressive modeling in LM.\nX-Codec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite> utilizes finite scalar quantization (FSQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib41\" title=\"\">2024</a>)</cite> to perform single-layer quantization,\nenlarging the code space. BiCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite> generates a hybrid token stream combining semantic and global tokens,\nwhich are derived from a SSL model and a speaker verification model, respectively.\nHowever, single-layer codecs with a low frame rate still faces challenges in high-fidelity reconstruction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite>, e.g., speaker similarity.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In practice, downstream LMs are capable of generating multi-layer tokens in parallel&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib5\" title=\"\">2023</a>; Neekhara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib44\" title=\"\">2024</a>)</cite>,\nthereby relaxing the requirement for single-layer quantization.\nThis paradigm relies more heavily on the modeling capacity of LMs, raising the upper bound of the codec&#8217;s reconstruction capability.\nIn this context, the frame rate of codecs plays a critical role, which determines the number of time steps for inference.\nOur H-Codec benefits from the RVQ technique and SSL representations, achieving significant reconstruction quality in the domain of speech, music, and general audio.\nThe low frame rate ensures efficient generation when integrated with our UniTok-Audio framework.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "unitokaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F1\" title=\"In 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UniTok-Audio is a unified, autoregressive LM-based audio generation framework comprising four key components:\n(i) a novel dual-stream H-codec; (ii) a text encoder with adapter; (iii) an audio encoder with adapter; (iv) a decoder-only LM backbone.\nNext, we will introduce the architecture of H-Codec and the operational framework of UniTok-Audio in detail.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "unitokaudio",
                    "operational"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To unify various audio generation tasks within a single framework,\nwe extract task-specific conditional information as a conditioning sequence for the decoder-only AR backbone,\nwhich then predicts the corresponding H-Codec tokens of the target audio.\nSince continuous features, typically extracted from SSL models, contain richer audio details compared to discrete representations and are more adaptable to varying input conditions,\nwe extract continuous features to assemble the the task-conditioning sequence.\nSpecifically, we utilize T5-base<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/google/t5-v1_1-base</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib51\" title=\"\">2020</a>)</cite> as the\ntext encoder to extract embedding from audio caption.\nThe same HuBERT used in H-Codec is adopted to extract continuous features from audio waveforms.\nTwo linear layers serve as two adapters to map the text embedding and audio features into a representation space amenable to LM AR modeling, respectively.\nGiven text and audio embeddings as conditions,\nwe utilize LLaMA architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib64\" title=\"\">2023</a>)</cite> to predicts discrete tokens of target waveform in an AR manner.\nFinally, the H-Codec decoder reconstructs high-fidelity audio from the predicted token sequence.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "token",
                    "caption",
                    "corresponding",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SR Mode:</span>\nThe target audio is the clean speech corresponding to the degraded input speech. The conditional sequence of LM is formatted as\n<math alttext=\"\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>SR</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">I</mi><annotation encoding=\"application/x-tex\">{\\rm I}</annotation></semantics></math> denotes the start of input audio features,\n<math alttext=\"{\\rm E}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{i}</annotation></semantics></math> the input audio embeddings, and <math alttext=\"{\\rm S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">S</mi><annotation encoding=\"application/x-tex\">{\\rm S}</annotation></semantics></math> the start of codec tokens, respectively.\nThe output sequence is formulated as <math alttext=\"{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119952;</mi><mo>=</mo><mrow><mo>[</mo><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">E</mi><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm E}^{\\prime}_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m6\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">{\\rm E}^{\\prime}_{c}</annotation></semantics></math> indicates codec tokens with\ndelay pattern, and <math alttext=\"{\\rm E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">E</mi><annotation encoding=\"application/x-tex\">{\\rm E}</annotation></semantics></math> represents the end token. The trainable parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m8\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> in the model\nare optimized by minimizing the negative log-likelihood of the predicted outputs:</p>\n\n",
                "matched_terms": [
                    "degraded",
                    "token",
                    "speech",
                    "tsrrm",
                    "corresponding",
                    "mode",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TSE Mode:</span>\nThe target audio corresponds to the timbre-matched speech component in the input mixture audio that aligns with the reference audio.\nThe conditional sequence is formatted as <math alttext=\"\\left[{\\rm T_{TSE}},{\\rm R},{\\rm E}_{r},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>TSE</mi></msub><mo>,</mo><mi mathvariant=\"normal\">R</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>r</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{TSE}},{\\rm R},{\\rm E}_{r},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>,\nwhere <math alttext=\"{\\rm E}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{r}</annotation></semantics></math> and <math alttext=\"{\\rm R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">R</mi><annotation encoding=\"application/x-tex\">{\\rm R}</annotation></semantics></math> represent the features of reference speech and its start token, respectively.\nTherefore, the associated loss function is defined as</p>\n\n",
                "matched_terms": [
                    "audio",
                    "token",
                    "speech",
                    "tse",
                    "ttserm",
                    "mode",
                    "reference",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">rTSE Mode:</span>\nSince SS task requires generating multiple output tracks while our model only supports one-track output,\nwe include the rTSE mode during training,\nenabling the model to obtain multiple tracks through iterative inference.\nThis mode aims to extract the timbre-mismatched speech component in the mixture input when compared with\nthe reference speech. The loss function <math alttext=\"\\mathcal{L}_{\\rm rTSE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm rTSE}</annotation></semantics></math> keeps similar to that of the TSE mode,\nexcept that the task token has been replaced with <math alttext=\"{\\rm T_{rTSE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{rTSE}}</annotation></semantics></math>.\nWhen handling SS task (we only consider 2-speaker cases), we first apply the SR mode to extract the main speaker\nwith higher energy, and the other speaker is obtained by using the rTSE mode.</p>\n\n",
                "matched_terms": [
                    "trtse",
                    "token",
                    "speech",
                    "task",
                    "rtse",
                    "tse",
                    "mode",
                    "trtserm",
                    "reference",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VC Mode:</span>\nThe target signal is timbre-perturbed version of input source speech using the speaker characteristics\nof the reference speech, where the speech content remains unchanged.\nThe optimization object has a similar formulation with equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.3 Unifying Tasks with Operational Modes &#8227; 3.2 Unified Multi-Task Generation &#8227; 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "mode",
                    "reference",
                    "speech",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASS Mode:</span>\nThis mode aims at separating specific component that matches the given caption query from the input mixture audio.\nTherefore, the associated loss function is defined as</p>\n\n",
                "matched_terms": [
                    "lass",
                    "caption",
                    "mode",
                    "audio",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\rm E}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p6.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{t}</annotation></semantics></math> and <math alttext=\"{\\rm C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p6.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">C</mi><annotation encoding=\"application/x-tex\">{\\rm C}</annotation></semantics></math> denote the embedding of caption and its start token, respectively.</p>\n\n",
                "matched_terms": [
                    "caption",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span>\nWe utilize multi-domain data to train our codec, including speech, music, and audio.\nThe speech samples are sourced from the VoxBox dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite>, which comprises approximately 100k hours of speech\nand is composed of some publicly available speech datasets.\nFor the music domain, we utilize the FMA-full dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Defferrard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib7\" title=\"\">2017</a>)</cite> and the MUSDB18-HQ dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib52\" title=\"\">Rafii et&#160;al., </a>)</cite>,\ninvolving about 8k hours of data.\nFor the audio domain, we adopt AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib17\" title=\"\">2017</a>)</cite> and WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib40\" title=\"\">2024</a>)</cite>,\nincluding about 13k hours of recordings.\nWe evaluate the reconstruction quality on LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib45\" title=\"\">2015</a>)</cite> test-clean, MUSDB18-HQ test, and AudioSet eval sets for\nspeech, music, and audio domain, respectively.\nAll samples are resampled to 16k Hz.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics:</span>\nWe utilize several metrics to measure the reconstruction quality of speech,\nincluding the perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI),\nspeaker similarity (SPK-SIM) and UTMOS.\nThe loss on Mel-scale spectrum and STFT spectrum bettween the target audio and reconstructed audio\nare computed for general evaluation in the domain of speech, music, and audio.\nDetails about evaluation metrics of codec can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A2.SS1\" title=\"B.1 Codec Metrics &#8227; Appendix B Evaluation Metrics &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Reconstruction Performance:</span>\nAs reported in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T2\" title=\"In 4.1.2 Experimental Results &#8227; 4.1 H-Codec &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, our H-Codec achieves competitive performance at a frame rate of 50.\nSince multi-layer tokens can be predicted simultaneously within a single time step in downstream audio LM,\nwe argue that frame rate is more critical, as the number of time steps significantly affects computational cost.\nCompared to baselines wich support general audio, H-Codec exhibits better signal reconstruction quality\n(PESQ and STOI), speech naturalness (UTMOS), speaker consistency (SPK-SIM), and\nsemantic information preservation (WER). Note that some models achieve higher UTMOS than the ground truth,\nthis can be attributed to the generative ability of codec decoder,\nwhich generates plausible speech at the expense of inacurrate signal alignment.\nOur H-Codec reports UTMOS closely matches that of the ground truth,\nindicating the high fidelity of the reconstructed speech.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Reconstruction Performance:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T3\" title=\"In 4.1.2 Experimental Results &#8227; 4.1 H-Codec &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comprehensive comparison of audio codec models on speech, music, and general audio tasks.\nAll baselines supports general audio reconstruction.\nNotably, H-Codec achieves lowest Mel loss and STFT loss on all domain, illustrating the powerful multi-domain reconstruction ability.\nThis ensures the potential of H-Codec for extensive downstream tasks, including speech, music, and audio generation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Datasets:</span>\nFor the training of speech tasks,\nwe adopt clean speech samples from the VoxBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite> dataset, including approximately 3.8k hours of data from\nLibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib45\" title=\"\">2015</a>)</cite>, MLS_English&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib49\" title=\"\">2020</a>)</cite> and Emilia_ZH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib18\" title=\"\">2024</a>)</cite> subset.\nThe noise corpus comprises approximately 460 hours of data from the DNS Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib53\" title=\"\">2020</a>)</cite>,\nFSD50K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib14\" title=\"\">2022</a>)</cite>, WHAM!&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wichern et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib79\" title=\"\">2019</a>)</cite>, DESED&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turpault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib65\" title=\"\">2019</a>)</cite>, DEMAND&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Thiemann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib63\" title=\"\">2013</a>)</cite>, MUSAN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib58\" title=\"\">2015</a>)</cite>,\nDISCO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Furnon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib15\" title=\"\">2021</a>)</cite>, MUSDB18-HQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib52\" title=\"\">Rafii et&#160;al., </a>)</cite>, and TUT Urban Acoustic Scenes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib42\" title=\"\">2018</a>)</cite>.\nWe include 60k room impulse response (RIR) samples from SLR28&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib27\" title=\"\">2017</a>)</cite> to simulate reverberation.\nFor the audio data, we include captioned audio samples from\nWavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib40\" title=\"\">2024</a>)</cite>, CLAP_FreeSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib81\" title=\"\">2023</a>)</cite>, VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib3\" title=\"\">2020</a>)</cite>, and Internal data, resulting in approximately 40k hours.\nThe simulation pipeline of training samples for all operational modes are\ndescribed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A1\" title=\"Appendix A Data Simulation &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "modes",
                    "operational"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "operational",
                    "task",
                    "modes",
                    "unitokaudio",
                    "audio",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics:</span>\nWe adopt multiple evaluation metrics to assess different aspects of the generated audio across tasks.\nFor speech tasks, we evaluate quality by DNSMOS (SIG, BAK, OVRL) and NISQA, speaker similarity by SIM,\nintelligibility by WER, and continuity by PLCMOS.\nFor the LASS task, we utilize FAD, CLAPScore, and CLAPScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> to measure the audio separation performance.\nDetails about evaluation metrics can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A2.SS2\" title=\"B.2 Audio Task Metrics &#8227; Appendix B Evaluation Metrics &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "lass",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T4\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the SR performance comparison on 2020 DNS Challenge test sets.\nIt is clear that generative models generally outperform discriminative ones.\nContinuous-domain generative approaches perform well on the &#8220;No Reverb&#8221; subset,\nhighlighting the potential of continuous methods in terms of generated signal quality.\nHowever, discrete-domain generative approaches can perform better under reverberant conditions, indicating\nthat discrete representations may simplify the modeling difficulty of reverberation components.\nOur UniTok-Audio achieves comparable performance among SOTA baselines,\nand the single-task versions with different audio encoders result in similar performance to UniTok-Audio<sub class=\"ltx_sub\">omni</sub>.\nIn addition, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T5\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the performance on packet loss concealment (PLC),\na subtask of SR aimed at recovering speech frames lost during transmission.\nUniTok-Audio surpasses baselines in terms of both signal quality and continuity,\nshowing powerful content understanding and generation capabilities of the framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "conditions",
                    "unitokaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T6\" title=\"In 4.2.2 TSE Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the performance comparison for TSE task.\nThe results indicate that generative methods achieve higher speech quality than discriminative approaches\nbut struggle with speaker similarity.\nThis can be attributed to the upper bound limitation of codecs&#8217; reconstruction fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>.\nOur UniTok-Audio maintains comparable performance compared to SOTA baselines,\ndemonstrating the feasibility of constructing a unified framework.</p>\n\n",
                "matched_terms": [
                    "tse",
                    "speech",
                    "unitokaudio",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T7\" title=\"In 4.2.3 SS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the performance comparison for SS task, showing that our model achieves superior performance than baselines.\nThis verifies the effectiveness of our iterative inference strategy in handling the SS task that requires multiple output tracks.\nNote that although the experiments are conducted with the 2-speaker configuration,\nour approach can be extended to scenarios with more sources when the target signal of rTSE mode is defined as all remaining speakers.\nThe single-task version is not reported since the inference phase of SS requires the cooperation of multiple modes.</p>\n\n",
                "matched_terms": [
                    "mode",
                    "task",
                    "rtse",
                    "modes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nFollowing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, we create test set for the VC task using VCTK&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib68\" title=\"\">2017</a>)</cite> dataset.\nWe randomly select 200 recordings from the dataset as source speech,\nand for each source sample, a sample from another speaker is picked as the reference speech.\nBaselines include HierSpeech++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib31\" title=\"\">2023</a>)</cite>, LM-VC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>,\nVevo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib92\" title=\"\">2024a</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "reference",
                    "speech",
                    "task",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nVC results are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T8\" title=\"In 4.2.4 VC Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, showing the superiority of UniTok-Audio in\nspeech quality, speaker similarity, and intelligibility.\nWe observe that UniTok-Audio<sub class=\"ltx_sub\">vc-wavlm</sub> outperforms UniTok-Audio<sub class=\"ltx_sub\">vc-hubert</sub>, indicating that\nWavLM performs better in extracting semantic information and speaker characteristics.\nThe performance degrades when extending to multiple tasks from single-task version,\nimplying the distinct pattern between VC and other tasks,\nwhere the former changes the property of the input signal rather than restoring or extracting certain components.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "unitokaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe adopt 2024 DCASE LASS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://dcase.community/challenge2024/task-language-queried-audio-source-separation</span></span></span> validation set\nto evaluate the LASS performance, which contains 3k synthetic mixtures mixed from 1k audio clips.\nBaselines include LASS-Net&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib38\" title=\"\">2022b</a>)</cite> and FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "lass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T9\" title=\"In 4.2.5 LASS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, UniTok-Audio achieves competitive performance in the LASS task,\nindicating effective exploitation of textual information.\nWe prove that the unified domain codec has potential to handle the LASS tasks.\nThe single-task version outperforms UniTok-Audio<sub class=\"ltx_sub\">omni</sub>,\nwhich can be attributed to the domain gap between speech and audio.</p>\n\n",
                "matched_terms": [
                    "lass",
                    "speech",
                    "task",
                    "unitokaudio",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniTok-Audio,\na framework that resembles multiple time-aligned audio tasks.\nWe uniify different learning patterns of multiple tasks in a single framework using a special task token,\nwhich indicates current operational mode of model.\nThis paper also introduces H-Codec, achieving high-fidelity reconstruction quality with dual-stream architecture\nthat quantize acoustic and semantic features simultaneously.\nBased on H-Codec, UniTok-Audio adopts continuous conditional embeddings to generates multi-layer discrete tokens in parallel.\nExtensive experiments demonstrate that UniTok-Audio achieves competitive performance across diverse tasks\nwith limited training data and moderate model size,\nhighlighting its potential as a foundation model for unified AR audio generation.</p>\n\n",
                "matched_terms": [
                    "token",
                    "operational",
                    "task",
                    "unitokaudio",
                    "mode",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A data simulation pipeline is designed to synthesis data pairs dynamically during training.\nConsidering various types of degradation in the SR task,\nwe apply multiple distortions to a speech sample with independent probabilities,\nwhere the distortion categories and corresponding configurations are shown in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A1.T10\" title=\"In Appendix A Data Simulation &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nThe distortion chain is also applied to the TSE and rTSE modes,\nexcept that the probability of interfering speaker is set to 1.0 and the SIR is uniformly sampled between\n-5 and 5 dB.\nFor the LASS mode, we mix the target audio with another randomly selected audio using a SIR ranges from -5 to 20 dB.\nFor the VC mode, we leverage a voice conversion model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/myshell-ai/OpenVoice</span></span></span> to perform timbre perturbation using randomly selected target speech and reference speech,\ngenerating 6k hours of fixed training dataset.\nThe perturbed sample is used as input to predict the target speech based on another speech of the target speaker.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "lass",
                    "speech",
                    "task",
                    "rtse",
                    "modes",
                    "tse",
                    "corresponding",
                    "mode",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DNSMOS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib54\" title=\"\">2022</a>)</cite>:\nDNSMOS is a neural network-based MOS estimator<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS</span></span></span> that correlates strongly with human quality ratings.\nIt comprises three components: 1) speech quality (<span class=\"ltx_text ltx_font_bold\">SIG</span>), 2) background noise quality (<span class=\"ltx_text ltx_font_bold\">BAK</span>), and 3) overall quality (<span class=\"ltx_text ltx_font_bold\">OVRL</span>).\nNote that for the VC task, DNSMOS scores are calculated by averaging three components.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NISQA</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mittag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib43\" title=\"\">2021</a>)</cite>:\nNISQA<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>https://github.com/gabrielmittag/NISQA</span></span></span> is a deep learning framework for speech quality prediction.\nWe report NISQA for the TSE and VC tasks.</p>\n\n",
                "matched_terms": [
                    "tse",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM</span>:\nFor the TSE task, we evaluate the speaker similarity using finetuned WavLM-base<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>https://huggingface.co/microsoft/wavlm-base-plus-sv</span></span></span> following <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>)</cite>.\nWhile for the VC task, speaker embeddings are computed using the WavLM TDNN<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span>https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "tse",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span>:\nWe utilize the whisper-large-v3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span>https://huggingface.co/openai/whisper-large-v3</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib50\" title=\"\">2023</a>)</cite>\nto obtain the transcriptions of converted speech in the VC task, thereby calculating WER with\nthe ground-truth text of source speech.</p>\n\n",
                "matched_terms": [
                    "source",
                    "speech",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T11\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a> reports the hyperparameter configurations of different UniTok-Audio versions.\nUniTok-Audio-S and UniTok-Audio-L denote the small and large version, respectively.\nThe VC performance in terms of different verisons are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T12\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, where\nall versions are trained for the single VC task using WavLM-based audio encoder.\nIt can be seen that increasing the model size consistently improves performance,\nin accordance with scaling laws.\nThis indicates the potential of UniTok-Audio to be extended to a larger model size.\nTo balance complexity and performance, we report the medium-sized verison in the main text.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "task",
                    "unitokaudio"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 2: Comparison between different codec models on LibriSpeech test-clean set,\nwhere FPS and BPS denotes frame per second and bitrate per second, respectively.\nNq represents the number of codebook layer.\nUnified indicates whether the model supports general audio or only speech.",
        "body": "Model\nUnified\nFPS\nNq\nBPS\n\nPESQ(\\uparrow)\n\nSTOI(\\uparrow)\n\nUTMOS(\\uparrow)\n\nSPK-SIM(\\uparrow)\n\nWER(\\downarrow)\n\n\nGround Truth\n-\n-\n-\n-\n4.64\n1.00\n4.09\n1.00\n2.43\n\n\nEncodec\n\n75\n8\n6000\n2.77\n0.94\n3.09\n0.89\n2.64\n\n\nX-Codec\n\n50\n4\n2000\n2.77\n0.87\n4.21\n0.72\n3.13\n\n\nWavTokenizer\n\n75\n1\n900\n2.39\n0.91\n4.00\n0.68\n5.43\n\n\nX-Codec2\n\n50\n1\n800\n2.43\n0.92\n4.13\n0.82\n3.53\n\n\nBiCodec\n\n50\n1\n650\n2.51\n0.92\n4.18\n0.80\n3.23\n\n\nDAC\n\n50\n4\n2000\n1.42\n0.84\n1.83\n0.60\n4.32\n\n\nX-Codec\n\n50\n4\n2000\n2.64\n0.92\n3.88\n0.77\n3.33\n\n\nUniCodec\n\n75\n1\n900\n2.56\n0.92\n4.00\n0.76\n4.23\n\n\nWavTokenizer\n\n40\n1\n480\n1.88\n0.87\n3.78\n0.57\n10.03\n\n\nH-Codec (ours)\n\n25+25\n4\n2000\n2.99\n0.94\n4.06\n0.84\n3.18",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Unified</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">FPS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Nq</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BPS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">PESQ</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">STOI</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">UTMOS</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">SPK-SIM</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">WER</span>(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Ground Truth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">4.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">1.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">4.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">1.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">2.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Encodec</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">6000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.64</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">X-Codec</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">50</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2000</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">2.77</span></td>\n<td class=\"ltx_td ltx_align_center\">0.87</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.21</span></td>\n<td class=\"ltx_td ltx_align_center\">0.72</td>\n<td class=\"ltx_td ltx_align_center\">3.13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">WavTokenizer</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">75</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">900</td>\n<td class=\"ltx_td ltx_align_center\">2.39</td>\n<td class=\"ltx_td ltx_align_center\">0.91</td>\n<td class=\"ltx_td ltx_align_center\">4.00</td>\n<td class=\"ltx_td ltx_align_center\">0.68</td>\n<td class=\"ltx_td ltx_align_center\">5.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">X-Codec2</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">50</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">800</td>\n<td class=\"ltx_td ltx_align_center\">2.43</td>\n<td class=\"ltx_td ltx_align_center\">0.92</td>\n<td class=\"ltx_td ltx_align_center\">4.13</td>\n<td class=\"ltx_td ltx_align_center\">0.82</td>\n<td class=\"ltx_td ltx_align_center\">3.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">BiCodec</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">50</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">650</td>\n<td class=\"ltx_td ltx_align_center\">2.51</td>\n<td class=\"ltx_td ltx_align_center\">0.92</td>\n<td class=\"ltx_td ltx_align_center\">4.18</td>\n<td class=\"ltx_td ltx_align_center\">0.80</td>\n<td class=\"ltx_td ltx_align_center\">3.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">DAC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.32</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">X-Codec</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">50</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2000</td>\n<td class=\"ltx_td ltx_align_center\">2.64</td>\n<td class=\"ltx_td ltx_align_center\">0.92</td>\n<td class=\"ltx_td ltx_align_center\">3.88</td>\n<td class=\"ltx_td ltx_align_center\">0.77</td>\n<td class=\"ltx_td ltx_align_center\">3.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">UniCodec</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">75</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">900</td>\n<td class=\"ltx_td ltx_align_center\">2.56</td>\n<td class=\"ltx_td ltx_align_center\">0.92</td>\n<td class=\"ltx_td ltx_align_center\">4.00</td>\n<td class=\"ltx_td ltx_align_center\">0.76</td>\n<td class=\"ltx_td ltx_align_center\">4.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">WavTokenizer</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">40</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">480</td>\n<td class=\"ltx_td ltx_align_center\">1.88</td>\n<td class=\"ltx_td ltx_align_center\">0.87</td>\n<td class=\"ltx_td ltx_align_center\">3.78</td>\n<td class=\"ltx_td ltx_align_center\">0.57</td>\n<td class=\"ltx_td ltx_align_center\">10.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">H-Codec (ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">25+25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">2000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.18</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "frame",
            "codec",
            "respectively",
            "xcodec",
            "speech",
            "fps",
            "unified",
            "where",
            "stoiuparrow",
            "denotes",
            "spksimuparrow",
            "wavtokenizer",
            "only",
            "encodec",
            "unicodec",
            "different",
            "supports",
            "whether",
            "general",
            "truth",
            "bps",
            "audio",
            "bicodec",
            "indicates",
            "layer",
            "werdownarrow",
            "number",
            "comparison",
            "hcodec",
            "testclean",
            "between",
            "xcodec2",
            "second",
            "codebook",
            "pesquparrow",
            "ours",
            "librispeech",
            "ground",
            "models",
            "set",
            "dac",
            "utmosuparrow",
            "bitrate",
            "model",
            "represents"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Reconstruction Performance:</span>\nAs reported in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T2\" title=\"In 4.1.2 Experimental Results &#8227; 4.1 H-Codec &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, our H-Codec achieves competitive performance at a frame rate of 50.\nSince multi-layer tokens can be predicted simultaneously within a single time step in downstream audio LM,\nwe argue that frame rate is more critical, as the number of time steps significantly affects computational cost.\nCompared to baselines wich support general audio, H-Codec exhibits better signal reconstruction quality\n(PESQ and STOI), speech naturalness (UTMOS), speaker consistency (SPK-SIM), and\nsemantic information preservation (WER). Note that some models achieve higher UTMOS than the ground truth,\nthis can be attributed to the generative ability of codec decoder,\nwhich generates plausible speech at the expense of inacurrate signal alignment.\nOur H-Codec reports UTMOS closely matches that of the ground truth,\nindicating the high fidelity of the reconstructed speech.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative modeling has recently achieved remarkable success across text, image, and audio domains,\ndemonstrating powerful capabilities for unified representation learning.\nHowever, audio generation models still face challenges in terms of audio quality and generalization ability across tasks.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.\nTo address these issues, we propose <span class=\"ltx_text ltx_font_bold\">UniTok-Audio</span>, a scalable and extensible framework for unified audio generation tasks.\nSpecifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner;\n2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework;\n3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction.\nExperimental results demonstrate that UniTok-Audio achieves competitive performance\nin comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks:\nspeech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation.\nTo foster future research, we will open-source our codebase.\nThe demo page of our work can be found here: https://alibaba.github.io/unified-audio.\n</p>\n\n",
                "matched_terms": [
                    "codec",
                    "models",
                    "speech",
                    "different",
                    "unified",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Leveraging the remarkable sequential generation capability of language model (LM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib67\" title=\"\">2017</a>)</cite>,\nrecent works have achieved significant improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib48\" title=\"\">2024</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib35\" title=\"\">2023</a>)</cite>,\npromoting the growing prevalence of artificial intelligence-generated content (AIGC).\nThese advances have inspired substantial research extending LMs to various audio tasks,\nwhich can be fundamentally categorized by the temporal relationship between input and output:\neither <span class=\"ltx_text ltx_font_italic\">time-aligned</span> (TA) or <span class=\"ltx_text ltx_font_italic\">non-time-aligned</span> (NTA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib82\" title=\"\">2025</a>)</cite>.\nThe former involves strict temporal correspondence between input and output signals, such as speech denoising,\nwhich aligns speech components in each frame between noisy and clean speech.\nWhile the latter dose not require point-wise temporal alignment, such as text-to-audio (TTA),\nwhich aims at semantic coherence between the holistical textual description and entire output soundscape.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "speech",
                    "between",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study focuses on the TA tasks, especially which provides the input audio that temporally aligned with the output audio at the frame level,\nincluding: speech restoration (SR) that aims at restoring speech from the degraded recording with various distortions (e.g., noise, reverberation,and packet loss);\ntarget speaker extraction (TSE) that extracts target speech from mixture using assistive clues (e.g., voiceprint information from reference speech);\nspeech separation (SS) that aims to separate all existing speaker in the mixture;\nvoice conversion (VC) that transforms the timbre of source speech guided by reference speech of another speaker;\nlanguage-queried audio source separation (LASS) that aims at extracting target audio components from mixture, which are consistent with the given textual caption.\nNumerous generative models are developed for these tasks, while most of them are designed for single task with\ntask-specific architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "audio",
                    "speech",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some studies aim to unify multiple tasks within a single framework, including AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>, UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.\nThese methods utilizes the LM backbone combined with discrete audio codec and\nexhibit remarkable generative ability, which benefit from the semantic understanding and contextual modeling capabilities of LMs.\nHowever, challenges still exist in terms of audio quality and generalization ability across tasks.\nFor instance, few unified models are capable of handling the SS task, as it generally requires customized\narchitecture to output multi-track speech.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "models",
                    "speech",
                    "unified",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve audio generation quality, some works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib30\" title=\"\">2023</a>; Vyas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib69\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib82\" title=\"\">2025</a>)</cite>\nadopt generative paradigms in continuous space, such as flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib35\" title=\"\">2023</a>)</cite>,\nwhich eliminates the dependence on discrete codecs.\nHowever, the flowchart of model needs to be carefully designed to support different tasks, increasing the\ndifficulty when extending to more tasks.\nAdditionally, considering the trend of combining audio generation capabilities with large language models (LLM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib62\" title=\"\">2025</a>)</cite>,\ndeveloping audio generation models based on discrete codec has greater potential.\nThis highlights the need for improving the ability of audio codec,\nwhich directly affects the generation quality of audio models.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "models",
                    "different",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Framework</span>: The framework unifies tasks by taking task-specific conditional information as the conditioning sequence of decoder-only LM,\nand the discrete token of target audio is predicted in an AR manner.\nWe utilize a special task token to distinguish different learning patterns of multiple tasks.\nNote that our model handles diverse tasks using a single set of shared weights, thereby eliminating the need for task-specific weight adaptation.</p>\n\n",
                "matched_terms": [
                    "set",
                    "different",
                    "unified",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">New Tokenization</span>: We present <span class=\"ltx_text ltx_font_bold\">H-codec</span>,\nwhich integrates self-supervised learning (SSL) representation within the audio tokenization and reconstruction process.\nThe features from waveform and SSL model are individually quantized, resulting dual-stream (acoustic and semantic) codec tokens.\nH-Codec achieves remarkable audio reconstruction quality with a low frame rate,\nimproving both the efficiency and performance of downstream audio generation.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "codec",
                    "hcodec",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the domain of TA audio tasks, early researches focus on discriminative modeling,\nwhich directly learns the mapping between input signal and target audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Williamson &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib80\" title=\"\">2017</a>; Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>.\nHowever, the lack of generative ability limits their generalization in unseen scenarios and\nthe performance in extreme situations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Welker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib78\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib71\" title=\"\">2020</a>)</cite>.\nMany studies integrate generative modeling into audio tasks in recent years.\nFor the SR task, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite> applies k-means to quantize noisy speech representations obtained\nby WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib4\" title=\"\">2022</a>)</cite> into discrete tokens,\nand then a Transformer-based speech LM maps the noisy tokens to clean tokens.\nFor the LASS task, FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>\nlearns linear flow trajectories from noise to target source features within the variational autoencoder (VAE) latent space,\nwhich are guided by the encoded text embeddings and the mixture audio.\nHowever, these models are designed for specific task, facing limited extensibility when migrating to more tasks.</p>\n\n",
                "matched_terms": [
                    "between",
                    "audio",
                    "speech",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Creating an unified framework that can tackle diverse tasks\nstands as a critical research goal in the field of artificial intelligence.\nIn the unification of audio tasks, the approaches can be divided into two categories:\ndiscrete audio codec based method and continuous representation based method.\nThe former is based on the pre-trained audio codec, which encodes the waveform into discrete space and reconstructs\naudio signal from it.\nThe generative ability of AR modeling or masked generative modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib2\" title=\"\">2022</a>)</cite> is leveraged to\ngenerate discrete tokens of the target audio.\nFor instance, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite> tokenizes the target audio along with other condition modalities\nand then concatenates source-target pair as a single sequence, performing next-token prediction using LLM.\nMetis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite> adopts a two-stage generation framework using masked generative modeling,\nwhich first generates SSL tokens and then predicts acoustic tokens based the former.\nContinuous representation based methods typically adopt diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib20\" title=\"\">2020</a>)</cite> or flow matching techniques,\neliminating the inevitable quantitative loss in discrete codec.\nVoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib30\" title=\"\">2023</a>)</cite> performs flow matching on mel-spectrograms to unify tasks such as text-to-speech (TTS) and speech editing.\nUniFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>)</cite> utilizes VAE to learn a compact latent representation of raw audio,\ncoupled with a diffusion Transformer (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles &amp; Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib47\" title=\"\">2023</a>)</cite> that predicts latent updates.</p>\n\n",
                "matched_terms": [
                    "unified",
                    "audio",
                    "speech",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to discrete audio codec based method, especially decoder-only AR models which can elegantly\nintegrate conditional information as a prefix sequence, continuous methods usually requires complex design\nto combines multimodal conditions, limiting the extensibility to more tasks.\nIn addition, discrete audio representation plays an important role in combining with LLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib62\" title=\"\">2025</a>)</cite>,\nbridging the natural language instructions and continuous waveform.\nTherefore, we develop a decoder-only AR LM-based framework (UniTok-Audio) to unify audio tasks.\nIt utilizes continuous conditional embeddings to maximize the preservation of semantic and acoustic information,\npredicting multi-layer codec tokens which reduce the quantization loss.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "codec",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Neural audio codecs utilize neural networks to obtain highly compressed discrete representations of audio waveforms and\naim to reconstruct high-fidelity signal form discrete tokens.\nFor instance, SoundStream&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib90\" title=\"\">2021</a>)</cite> utilizes residual vector quantization (RVQ) where\neach quantizer refines the residuals left by the previous one,\nobtaining parallel multi-layer tokens and achieving remarkable reconstruction quality.\nMany works including Encodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib9\" title=\"\">2022b</a>)</cite> and DAC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib29\" title=\"\">2023</a>)</cite> follow this paradigm to improve performance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "where",
                    "encodec",
                    "dac"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the development of LM, the research focus of codecs has gradually shifted\nfrom reducing data transmission costs toward the integration with LM, which ensures the high quality of generated audio.\nThis requires codecs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib37\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib10\" title=\"\">2024</a>)</cite> to preserve more semantic information that can be understood and modeled by LM.\nX-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib86\" title=\"\">2024a</a>)</cite> integrates the representations from the pre-trained SSL model to enhance semantic preservation,\nimproving both reconstruction quality and downstream TTS performance.\nSome studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib24\" title=\"\">2025</a>)</cite> explore single-layer codecs that are more suitable for autoregressive modeling in LM.\nX-Codec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite> utilizes finite scalar quantization (FSQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib41\" title=\"\">2024</a>)</cite> to perform single-layer quantization,\nenlarging the code space. BiCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite> generates a hybrid token stream combining semantic and global tokens,\nwhich are derived from a SSL model and a speaker verification model, respectively.\nHowever, single-layer codecs with a low frame rate still faces challenges in high-fidelity reconstruction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite>, e.g., speaker similarity.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "respectively",
                    "xcodec",
                    "audio",
                    "model",
                    "xcodec2",
                    "bicodec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In practice, downstream LMs are capable of generating multi-layer tokens in parallel&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib5\" title=\"\">2023</a>; Neekhara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib44\" title=\"\">2024</a>)</cite>,\nthereby relaxing the requirement for single-layer quantization.\nThis paradigm relies more heavily on the modeling capacity of LMs, raising the upper bound of the codec&#8217;s reconstruction capability.\nIn this context, the frame rate of codecs plays a critical role, which determines the number of time steps for inference.\nOur H-Codec benefits from the RVQ technique and SSL representations, achieving significant reconstruction quality in the domain of speech, music, and general audio.\nThe low frame rate ensures efficient generation when integrated with our UniTok-Audio framework.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "number",
                    "speech",
                    "general",
                    "hcodec",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F1\" title=\"In 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UniTok-Audio is a unified, autoregressive LM-based audio generation framework comprising four key components:\n(i) a novel dual-stream H-codec; (ii) a text encoder with adapter; (iii) an audio encoder with adapter; (iv) a decoder-only LM backbone.\nNext, we will introduce the architecture of H-Codec and the operational framework of UniTok-Audio in detail.</p>\n\n",
                "matched_terms": [
                    "unified",
                    "audio",
                    "hcodec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve the audio generation quality, we propose H-Codec to discretize audio and reconstruct waveform from discrete tokens.\nAs illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F2\" title=\"In 3.1.3 Optimization Strategy &#8227; 3.1 H-Codec &#8227; 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, the architecture of H-Codec follows the common paradigm of audio tokenizers, including an acoustic encoder, a quantizer module, and an acoustic decoder.\nInspired by X-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib86\" title=\"\">2024a</a>)</cite>, we incorporate pretrained models to facilitate the preservation of semantic information.\nHowever, unlike X-Codec, which fuses acoustic and semantic information and then quantizes the combined representation using a single codebook,\nwe employ separate codebooks to quantize the two types of features independently, leading to dual-stream codec tokens.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "xcodec",
                    "models",
                    "hcodec",
                    "audio",
                    "codebook"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the encoding stage, the raw waveform <math alttext=\"\\bm{x}\\in\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119961;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{x}\\in\\mathbb{R}^{n}</annotation></semantics></math> is fed into the acoustic encoder to extract frame-level acoustic features, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> represents the number of waveform samples.\nThe architecture of the acoustic encoder follows Encodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib9\" title=\"\">2022b</a>)</cite>.\nA 4-layer RVQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib90\" title=\"\">2021</a>)</cite> is utilized to quantize features, resulting in the quantized features with a frame rate of 25 Hz.\nSynchronously, a pre-trained HuBERT<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://huggingface.co/bosonai/hubert_base</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib21\" title=\"\">2021</a>)</cite> extracts SSL features by averaging outputs from all transformer layers\nand the quantized semantic feature is obtained by applying the semantic encoder and RVQ quantizer.\nNote that HuBERT is trained on general audio, thus the codec has the potential to handle general audio as well.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "frame",
                    "codec",
                    "number",
                    "encodec",
                    "general",
                    "where",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The types of discriminators and the composition of the loss functions follow the configuration used in WavTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>)</cite>.\nWe employ a multi-period discriminator (MPD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib28\" title=\"\">2020</a>)</cite>, a multi-resolution discriminator (MRD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib22\" title=\"\">2021</a>)</cite>, and a sub-band complex STFT discriminator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib90\" title=\"\">2021</a>)</cite> to improve the naturalness and fidelity of reconstructed audio,\nand the training loss <math alttext=\"\\mathcal{L}_{dis}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{dis}</annotation></semantics></math> conforms to the hinge loss formulation suggested by&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib90\" title=\"\">2021</a>)</cite>. The training loss for the generator of H-Codec include:\ncommitment loss for quantizer <math alttext=\"\\mathcal{L}_{commit}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{commit}</annotation></semantics></math>, mel-spectrum reconstruction loss <math alttext=\"\\mathcal{L}_{mel}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{mel}</annotation></semantics></math>, adversarial loss <math alttext=\"\\mathcal{L}_{adv}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{adv}</annotation></semantics></math>, feature matching loss <math alttext=\"\\mathcal{L}_{fm}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{fm}</annotation></semantics></math>, and an auxiliary mean squared error (MSE) loss on SSL feature <math alttext=\"\\mathcal{L}_{aux}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{aux}</annotation></semantics></math>.\nThe composite training loss of the generator is obtained by</p>\n\n",
                "matched_terms": [
                    "audio",
                    "hcodec",
                    "wavtokenizer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\lambda_{commit}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{commit}</annotation></semantics></math>, <math alttext=\"\\lambda_{mel}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{mel}</annotation></semantics></math>, <math alttext=\"\\lambda_{adv}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m9\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{adv}</annotation></semantics></math>, <math alttext=\"\\lambda_{fm}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m10\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{fm}</annotation></semantics></math>, and <math alttext=\"\\lambda_{aux}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m11\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{aux}</annotation></semantics></math> are hyper-parameters to scale different loss components.\nAdditionally, the perceptual loss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Parker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib46\" title=\"\">2024</a>)</cite> is utilized during the final steps of the training process,\nwhich further enhances the reconstruction quality.</p>\n\n",
                "matched_terms": [
                    "where",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To unify various audio generation tasks within a single framework,\nwe extract task-specific conditional information as a conditioning sequence for the decoder-only AR backbone,\nwhich then predicts the corresponding H-Codec tokens of the target audio.\nSince continuous features, typically extracted from SSL models, contain richer audio details compared to discrete representations and are more adaptable to varying input conditions,\nwe extract continuous features to assemble the the task-conditioning sequence.\nSpecifically, we utilize T5-base<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/google/t5-v1_1-base</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib51\" title=\"\">2020</a>)</cite> as the\ntext encoder to extract embedding from audio caption.\nThe same HuBERT used in H-Codec is adopted to extract continuous features from audio waveforms.\nTwo linear layers serve as two adapters to map the text embedding and audio features into a representation space amenable to LM AR modeling, respectively.\nGiven text and audio embeddings as conditions,\nwe utilize LLaMA architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib64\" title=\"\">2023</a>)</cite> to predicts discrete tokens of target waveform in an AR manner.\nFinally, the H-Codec decoder reconstructs high-fidelity audio from the predicted token sequence.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "hcodec",
                    "respectively",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate multi-layer codec tokens into AR prediction, an existing method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib70\" title=\"\">2023a</a>)</cite> applies two-stage strategy:\n(i) model the tokens of the first layer in an AR manner; (ii) then, predict the tokens of remaining layers using a NAR post-network.\nHowever, this method causes additional complexity to the system. In addition, flattening all tokens into one layer leads to unbearable computational\ncost, while predicting tokens from all layers in parallel within one step deteriorates the performance.\nTherefore, we adopt the delay pattern&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib5\" title=\"\">2023</a>)</cite> to arrange our tokens for the trade-off between performance and computational cost.\nSpecifically, the 4-layer acoustic and semantic tokens produced by H-Codec are first interleaved sequentially across time steps, resulting in <math alttext=\"{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">E</mi><mi>c</mi></msub><mo>&#8712;</mo><msup><mi>&#8484;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}</annotation></semantics></math>\nwith a frame rate of 50 Hz, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> indicates the number of frames. Before feeding the tokens into the LM backbone,\ndifferent shifts are applied across layers and special pad tokens occupy empty positions, as shown in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F1\" title=\"In 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn the LM backbone, 4 embedding layers handle 4-layer tokens respectively, and the embeddings of each layer are added up as the input of transformer layers.\nThere are 4 output heads to predict the 4-layer logits of next time step. The delay pattern allows generating high-layer tokens conditioned by low-layer tokens,\nwhich improves prediction accuracy.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "layer",
                    "codec",
                    "respectively",
                    "number",
                    "different",
                    "hcodec",
                    "between",
                    "where",
                    "model",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following our previous work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, we introduce special task tokens to distinguish between different operational modes.\nTo unify five tasks (i.e., SR, TSE, SS, VC, and LASS), we utilize five modes, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 AR Prediction of H-Codec Tokens &#8227; 3.2 Unified Multi-Task Generation &#8227; 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nEach mode corresponds to a special token and different task-specific condition types, which serve as a conditioning sequence\nfor the LM backbone to estimate the conditional probability density distribution of target discrete tokens.</p>\n\n",
                "matched_terms": [
                    "between",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SR Mode:</span>\nThe target audio is the clean speech corresponding to the degraded input speech. The conditional sequence of LM is formatted as\n<math alttext=\"\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>SR</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">I</mi><annotation encoding=\"application/x-tex\">{\\rm I}</annotation></semantics></math> denotes the start of input audio features,\n<math alttext=\"{\\rm E}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{i}</annotation></semantics></math> the input audio embeddings, and <math alttext=\"{\\rm S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">S</mi><annotation encoding=\"application/x-tex\">{\\rm S}</annotation></semantics></math> the start of codec tokens, respectively.\nThe output sequence is formulated as <math alttext=\"{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119952;</mi><mo>=</mo><mrow><mo>[</mo><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">E</mi><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm E}^{\\prime}_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m6\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">{\\rm E}^{\\prime}_{c}</annotation></semantics></math> indicates codec tokens with\ndelay pattern, and <math alttext=\"{\\rm E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">E</mi><annotation encoding=\"application/x-tex\">{\\rm E}</annotation></semantics></math> represents the end token. The trainable parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m8\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> in the model\nare optimized by minimizing the negative log-likelihood of the predicted outputs:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "codec",
                    "denotes",
                    "respectively",
                    "speech",
                    "where",
                    "model",
                    "represents",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"o^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m9\" intent=\":literal\"><semantics><msubsup><mi>o</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">o^{i}_{t}</annotation></semantics></math> indicates the output token at <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m10\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-th step and <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m11\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th layer, and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m12\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the length of output sequence, respectively.</p>\n\n",
                "matched_terms": [
                    "indicates",
                    "layer",
                    "where",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TSE Mode:</span>\nThe target audio corresponds to the timbre-matched speech component in the input mixture audio that aligns with the reference audio.\nThe conditional sequence is formatted as <math alttext=\"\\left[{\\rm T_{TSE}},{\\rm R},{\\rm E}_{r},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>TSE</mi></msub><mo>,</mo><mi mathvariant=\"normal\">R</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>r</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{TSE}},{\\rm R},{\\rm E}_{r},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>,\nwhere <math alttext=\"{\\rm E}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{r}</annotation></semantics></math> and <math alttext=\"{\\rm R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">R</mi><annotation encoding=\"application/x-tex\">{\\rm R}</annotation></semantics></math> represent the features of reference speech and its start token, respectively.\nTherefore, the associated loss function is defined as</p>\n\n",
                "matched_terms": [
                    "audio",
                    "where",
                    "speech",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">rTSE Mode:</span>\nSince SS task requires generating multiple output tracks while our model only supports one-track output,\nwe include the rTSE mode during training,\nenabling the model to obtain multiple tracks through iterative inference.\nThis mode aims to extract the timbre-mismatched speech component in the mixture input when compared with\nthe reference speech. The loss function <math alttext=\"\\mathcal{L}_{\\rm rTSE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm rTSE}</annotation></semantics></math> keeps similar to that of the TSE mode,\nexcept that the task token has been replaced with <math alttext=\"{\\rm T_{rTSE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{rTSE}}</annotation></semantics></math>.\nWhen handling SS task (we only consider 2-speaker cases), we first apply the SR mode to extract the main speaker\nwith higher energy, and the other speaker is obtained by using the rTSE mode.</p>\n\n",
                "matched_terms": [
                    "supports",
                    "only",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">VC Mode:</span>\nThe target signal is timbre-perturbed version of input source speech using the speaker characteristics\nof the reference speech, where the speech content remains unchanged.\nThe optimization object has a similar formulation with equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.E3\" title=\"Equation 3 &#8227; 3.2.3 Unifying Tasks with Operational Modes &#8227; 3.2 Unified Multi-Task Generation &#8227; 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "where",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\rm E}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p6.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{t}</annotation></semantics></math> and <math alttext=\"{\\rm C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p6.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">C</mi><annotation encoding=\"application/x-tex\">{\\rm C}</annotation></semantics></math> denote the embedding of caption and its start token, respectively.</p>\n\n",
                "matched_terms": [
                    "where",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span>\nWe utilize multi-domain data to train our codec, including speech, music, and audio.\nThe speech samples are sourced from the VoxBox dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite>, which comprises approximately 100k hours of speech\nand is composed of some publicly available speech datasets.\nFor the music domain, we utilize the FMA-full dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Defferrard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib7\" title=\"\">2017</a>)</cite> and the MUSDB18-HQ dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib52\" title=\"\">Rafii et&#160;al., </a>)</cite>,\ninvolving about 8k hours of data.\nFor the audio domain, we adopt AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib17\" title=\"\">2017</a>)</cite> and WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib40\" title=\"\">2024</a>)</cite>,\nincluding about 13k hours of recordings.\nWe evaluate the reconstruction quality on LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib45\" title=\"\">2015</a>)</cite> test-clean, MUSDB18-HQ test, and AudioSet eval sets for\nspeech, music, and audio domain, respectively.\nAll samples are resampled to 16k Hz.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "respectively",
                    "librispeech",
                    "speech",
                    "testclean",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThe total downsampling ratio in H-Codec is set to 640 to obtain the frame rate of 25 Hz in both acoustic and semantic branch.\nIn the 4-layer RVQ, we utilize a codebook size of 1024 for each layer with the codebook dimension set to 512.\nDuring training, we randomly crop 5-second segments from audio samples.\nThe network is optimized using the AdamW optimizer with an initial learning rate of\n<math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, which is decayed based on a cosine scheduler.\nIn total, we train for 600k steps, and the perceptual loss is activated at final 100k steps.</p>\n\n",
                "matched_terms": [
                    "frame",
                    "layer",
                    "set",
                    "hcodec",
                    "audio",
                    "codebook"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics:</span>\nWe utilize several metrics to measure the reconstruction quality of speech,\nincluding the perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI),\nspeaker similarity (SPK-SIM) and UTMOS.\nThe loss on Mel-scale spectrum and STFT spectrum bettween the target audio and reconstructed audio\nare computed for general evaluation in the domain of speech, music, and audio.\nDetails about evaluation metrics of codec can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A2.SS1\" title=\"B.1 Codec Metrics &#8227; Appendix B Evaluation Metrics &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "general",
                    "audio",
                    "speech",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines:</span>\nWe compare our codec against some state-of-the-art (SOTA) baselines,\nincluding DAC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib29\" title=\"\">2023</a>)</cite>, Encodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib8\" title=\"\">2022a</a>)</cite>, X-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib87\" title=\"\">2024b</a>)</cite>,\nX-Codec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite>, BiCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite>, WavTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>)</cite>,\nand UniCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib24\" title=\"\">2025</a>)</cite>.\nAll results are obtained using their official checkpoints.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "wavtokenizer",
                    "xcodec",
                    "encodec",
                    "dac",
                    "unicodec",
                    "xcodec2",
                    "bicodec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Reconstruction Performance:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T3\" title=\"In 4.1.2 Experimental Results &#8227; 4.1 H-Codec &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comprehensive comparison of audio codec models on speech, music, and general audio tasks.\nAll baselines supports general audio reconstruction.\nNotably, H-Codec achieves lowest Mel loss and STFT loss on all domain, illustrating the powerful multi-domain reconstruction ability.\nThis ensures the potential of H-Codec for extensive downstream tasks, including speech, music, and audio generation.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "models",
                    "speech",
                    "comparison",
                    "supports",
                    "general",
                    "hcodec",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Datasets:</span>\nFor the training of speech tasks,\nwe adopt clean speech samples from the VoxBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite> dataset, including approximately 3.8k hours of data from\nLibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib45\" title=\"\">2015</a>)</cite>, MLS_English&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib49\" title=\"\">2020</a>)</cite> and Emilia_ZH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib18\" title=\"\">2024</a>)</cite> subset.\nThe noise corpus comprises approximately 460 hours of data from the DNS Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib53\" title=\"\">2020</a>)</cite>,\nFSD50K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib14\" title=\"\">2022</a>)</cite>, WHAM!&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wichern et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib79\" title=\"\">2019</a>)</cite>, DESED&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turpault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib65\" title=\"\">2019</a>)</cite>, DEMAND&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Thiemann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib63\" title=\"\">2013</a>)</cite>, MUSAN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib58\" title=\"\">2015</a>)</cite>,\nDISCO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Furnon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib15\" title=\"\">2021</a>)</cite>, MUSDB18-HQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib52\" title=\"\">Rafii et&#160;al., </a>)</cite>, and TUT Urban Acoustic Scenes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib42\" title=\"\">2018</a>)</cite>.\nWe include 60k room impulse response (RIR) samples from SLR28&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib27\" title=\"\">2017</a>)</cite> to simulate reverberation.\nFor the audio data, we include captioned audio samples from\nWavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib40\" title=\"\">2024</a>)</cite>, CLAP_FreeSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib81\" title=\"\">2023</a>)</cite>, VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib3\" title=\"\">2020</a>)</cite>, and Internal data, resulting in approximately 40k hours.\nThe simulation pipeline of training samples for all operational modes are\ndescribed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A1\" title=\"Appendix A Data Simulation &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "set",
                    "where",
                    "different",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics:</span>\nWe adopt multiple evaluation metrics to assess different aspects of the generated audio across tasks.\nFor speech tasks, we evaluate quality by DNSMOS (SIG, BAK, OVRL) and NISQA, speaker similarity by SIM,\nintelligibility by WER, and continuity by PLCMOS.\nFor the LASS task, we utilize FAD, CLAPScore, and CLAPScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> to measure the audio separation performance.\nDetails about evaluation metrics can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A2.SS2\" title=\"B.2 Audio Task Metrics &#8227; Appendix B Evaluation Metrics &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate speech restoration performance on the synthetic test sets of 2020 DNS Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib53\" title=\"\">2020</a>)</cite> (including &#8220;With Reverb&#8221; and &#8220;No Reverb&#8221;) and\n2022 PLC Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diener et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib11\" title=\"\">2022</a>)</cite> blind test set.\nBaselines include Conv-TasNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>, DEMUCS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib13\" title=\"\">2019</a>)</cite>, FRCRN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib94\" title=\"\">2022</a>)</cite>, FlowSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>)</cite>,\nUniFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>)</cite>, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite>, MaskSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib34\" title=\"\">2024</a>)</cite>, AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nGenSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib85\" title=\"\">2025</a>)</cite>, Metis-SE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>,\nUniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, KuaishouNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib33\" title=\"\">2022</a>)</cite>, LPCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Valin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib66\" title=\"\">2022</a>)</cite>, PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib36\" title=\"\">2022a</a>)</cite>,\nand BS-PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib93\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T4\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the SR performance comparison on 2020 DNS Challenge test sets.\nIt is clear that generative models generally outperform discriminative ones.\nContinuous-domain generative approaches perform well on the &#8220;No Reverb&#8221; subset,\nhighlighting the potential of continuous methods in terms of generated signal quality.\nHowever, discrete-domain generative approaches can perform better under reverberant conditions, indicating\nthat discrete representations may simplify the modeling difficulty of reverberation components.\nOur UniTok-Audio achieves comparable performance among SOTA baselines,\nand the single-task versions with different audio encoders result in similar performance to UniTok-Audio<sub class=\"ltx_sub\">omni</sub>.\nIn addition, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T5\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the performance on packet loss concealment (PLC),\na subtask of SR aimed at recovering speech frames lost during transmission.\nUniTok-Audio surpasses baselines in terms of both signal quality and continuity,\nshowing powerful content understanding and generation capabilities of the framework.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "comparison",
                    "different",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T6\" title=\"In 4.2.2 TSE Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the performance comparison for TSE task.\nThe results indicate that generative methods achieve higher speech quality than discriminative approaches\nbut struggle with speaker similarity.\nThis can be attributed to the upper bound limitation of codecs&#8217; reconstruction fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>.\nOur UniTok-Audio maintains comparable performance compared to SOTA baselines,\ndemonstrating the feasibility of constructing a unified framework.</p>\n\n",
                "matched_terms": [
                    "unified",
                    "speech",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate SS performance on Libri2Mix noisy test set and WSJ0-2mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib19\" title=\"\">2016</a>)</cite> test set,\nwhere the former additionally evaluates the denoising ability of models.\nBaselines include Sepformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Subakan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib59\" title=\"\">2021</a>)</cite>, Mossformer2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib95\" title=\"\">2024</a>)</cite>, and LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "where",
                    "set",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T7\" title=\"In 4.2.3 SS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the performance comparison for SS task, showing that our model achieves superior performance than baselines.\nThis verifies the effectiveness of our iterative inference strategy in handling the SS task that requires multiple output tracks.\nNote that although the experiments are conducted with the 2-speaker configuration,\nour approach can be extended to scenarios with more sources when the target signal of rTSE mode is defined as all remaining speakers.\nThe single-task version is not reported since the inference phase of SS requires the cooperation of multiple modes.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nFollowing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, we create test set for the VC task using VCTK&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib68\" title=\"\">2017</a>)</cite> dataset.\nWe randomly select 200 recordings from the dataset as source speech,\nand for each source sample, a sample from another speaker is picked as the reference speech.\nBaselines include HierSpeech++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib31\" title=\"\">2023</a>)</cite>, LM-VC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>,\nVevo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib92\" title=\"\">2024a</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nVC results are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T8\" title=\"In 4.2.4 VC Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, showing the superiority of UniTok-Audio in\nspeech quality, speaker similarity, and intelligibility.\nWe observe that UniTok-Audio<sub class=\"ltx_sub\">vc-wavlm</sub> outperforms UniTok-Audio<sub class=\"ltx_sub\">vc-hubert</sub>, indicating that\nWavLM performs better in extracting semantic information and speaker characteristics.\nThe performance degrades when extending to multiple tasks from single-task version,\nimplying the distinct pattern between VC and other tasks,\nwhere the former changes the property of the input signal rather than restoring or extracting certain components.</p>\n\n",
                "matched_terms": [
                    "between",
                    "where",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe adopt 2024 DCASE LASS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://dcase.community/challenge2024/task-language-queried-audio-source-separation</span></span></span> validation set\nto evaluate the LASS performance, which contains 3k synthetic mixtures mixed from 1k audio clips.\nBaselines include LASS-Net&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib38\" title=\"\">2022b</a>)</cite> and FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T9\" title=\"In 4.2.5 LASS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, UniTok-Audio achieves competitive performance in the LASS task,\nindicating effective exploitation of textual information.\nWe prove that the unified domain codec has potential to handle the LASS tasks.\nThe single-task version outperforms UniTok-Audio<sub class=\"ltx_sub\">omni</sub>,\nwhich can be attributed to the domain gap between speech and audio.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "speech",
                    "unified",
                    "between",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniTok-Audio,\na framework that resembles multiple time-aligned audio tasks.\nWe uniify different learning patterns of multiple tasks in a single framework using a special task token,\nwhich indicates current operational mode of model.\nThis paper also introduces H-Codec, achieving high-fidelity reconstruction quality with dual-stream architecture\nthat quantize acoustic and semantic features simultaneously.\nBased on H-Codec, UniTok-Audio adopts continuous conditional embeddings to generates multi-layer discrete tokens in parallel.\nExtensive experiments demonstrate that UniTok-Audio achieves competitive performance across diverse tasks\nwith limited training data and moderate model size,\nhighlighting its potential as a foundation model for unified AR audio generation.</p>\n\n",
                "matched_terms": [
                    "different",
                    "hcodec",
                    "unified",
                    "audio",
                    "model",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A data simulation pipeline is designed to synthesis data pairs dynamically during training.\nConsidering various types of degradation in the SR task,\nwe apply multiple distortions to a speech sample with independent probabilities,\nwhere the distortion categories and corresponding configurations are shown in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A1.T10\" title=\"In Appendix A Data Simulation &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nThe distortion chain is also applied to the TSE and rTSE modes,\nexcept that the probability of interfering speaker is set to 1.0 and the SIR is uniformly sampled between\n-5 and 5 dB.\nFor the LASS mode, we mix the target audio with another randomly selected audio using a SIR ranges from -5 to 20 dB.\nFor the VC mode, we leverage a voice conversion model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/myshell-ai/OpenVoice</span></span></span> to perform timbre perturbation using randomly selected target speech and reference speech,\ngenerating 6k hours of fixed training dataset.\nThe perturbed sample is used as input to predict the target speech based on another speech of the target speaker.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "set",
                    "where",
                    "between",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">STOI</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Andersen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib1\" title=\"\">2017</a>)</cite>:\nThe short-time objective intelligibility (STOI) evaluates the intelligibility of speech signals, ranging from 0 to 1.\nThe higher STOI score indicates a higher intelligibility and better preservation of the speech content.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span>:\nWord Error Rate (WER) measures the intelligibility of the generated speech by using the automatic speech recognition (ASR) model.\nWe utilize a HuBERT-based ASR system<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>https://huggingface.co/facebook/hubert-large-ls960-ft</span></span></span> to calculate WER.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SPK-SIM</span>:\nA WavLM-based speaker verification model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</span></span></span>\nis used to calculate the speaker similarity between the reconstructed speech and target speech.</p>\n\n",
                "matched_terms": [
                    "between",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">STFT Loss &amp; Mel Loss</span>:\nWe calculate the L1 loss between the magnitude spectrum of the reconstructed speech and target speech,\nwhere the STFT is performed using a Hann window with a length of 1024 and a shift of 256.\nFor the Mel Loss, 100 mel filters are utilized.</p>\n\n",
                "matched_terms": [
                    "between",
                    "where",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FAD</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib26\" title=\"\">2018</a>)</cite>:\nFr&#233;chet Audio Distance (FAD)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span>https://github.com/gudgud96/frechet-audio-distance</span></span></span> measures the quality of generated audio by comparing the statistics of deep features between real and synthesized audio.\nLower FAD value indicates higher fidelity and better distributional alignment.</p>\n\n",
                "matched_terms": [
                    "between",
                    "audio",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CLAPScore &amp; CLAPScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">A</span></sub></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib81\" title=\"\">2023</a>)</cite>:\nCLAPScore measures text-audio similarity using joint embeddings from a contrastive language-audio pretraining (CLAP) model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span>https://github.com/LittleFlyingSheep/CLAPScore_for_LASS</span></span></span>.\nWhile CLAPScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> evaluates the similarity between the output audio and the target audio.</p>\n\n",
                "matched_terms": [
                    "between",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T11\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a> reports the hyperparameter configurations of different UniTok-Audio versions.\nUniTok-Audio-S and UniTok-Audio-L denote the small and large version, respectively.\nThe VC performance in terms of different verisons are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T12\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, where\nall versions are trained for the single VC task using WavLM-based audio encoder.\nIt can be seen that increasing the model size consistently improves performance,\nin accordance with scaling laws.\nThis indicates the potential of UniTok-Audio to be extended to a larger model size.\nTo balance complexity and performance, we report the medium-sized verison in the main text.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "where",
                    "different",
                    "audio",
                    "model",
                    "indicates"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 3: Comparison between different codec models on speech (LibriSpeech test-clean),\nmusic (MUSDB18-HQ test), and audio (AudioSet eval) domain in terms of Mel loss and STFT loss.",
        "body": "Model\nBPS\nSpeech\nMusic\nAudio\n\n\n\nMel loss(\\downarrow)\n\nSTFT loss (\\downarrow)\n\nMel loss(\\downarrow)\n\nSTFT loss (\\downarrow)\n\nMel loss(\\downarrow)\n\nSTFT loss (\\downarrow)\n\n\nDAC\n2000\n0.6436\n0.1667\n0.8443\n0.2308\n1.9054\n0.5164\n\n\nX-Codec\n2000\n0.4225\n0.1161\n0.6403\n0.1804\n1.5073\n0.4193\n\n\nUniCodec\n900\n0.4147\n0.1201\n0.6488\n0.1999\n1.5403\n0.4760\n\n\nWavTokenizer\n480\n0.5143\n0.1364\n0.8174\n0.2270\n1.8912\n0.5201\n\n\nH-Codec (ours)\n2000\n0.3394\n0.1033\n0.5158\n0.1667\n1.2512\n0.4070",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">BPS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Music</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Audio</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Mel loss</span>(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">STFT loss</span> (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Mel loss</span>(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">STFT loss</span> (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Mel loss</span>(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">STFT loss</span> (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">DAC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">2000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">0.6436</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">0.1667</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">0.8443</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">0.2308</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">1.9054</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">0.5164</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">X-Codec</td>\n<td class=\"ltx_td ltx_align_center\">2000</td>\n<td class=\"ltx_td ltx_align_center\">0.4225</td>\n<td class=\"ltx_td ltx_align_center\">0.1161</td>\n<td class=\"ltx_td ltx_align_center\">0.6403</td>\n<td class=\"ltx_td ltx_align_center\">0.1804</td>\n<td class=\"ltx_td ltx_align_center\">1.5073</td>\n<td class=\"ltx_td ltx_align_center\">0.4193</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">UniCodec</td>\n<td class=\"ltx_td ltx_align_center\">900</td>\n<td class=\"ltx_td ltx_align_center\">0.4147</td>\n<td class=\"ltx_td ltx_align_center\">0.1201</td>\n<td class=\"ltx_td ltx_align_center\">0.6488</td>\n<td class=\"ltx_td ltx_align_center\">0.1999</td>\n<td class=\"ltx_td ltx_align_center\">1.5403</td>\n<td class=\"ltx_td ltx_align_center\">0.4760</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">WavTokenizer</td>\n<td class=\"ltx_td ltx_align_center\">480</td>\n<td class=\"ltx_td ltx_align_center\">0.5143</td>\n<td class=\"ltx_td ltx_align_center\">0.1364</td>\n<td class=\"ltx_td ltx_align_center\">0.8174</td>\n<td class=\"ltx_td ltx_align_center\">0.2270</td>\n<td class=\"ltx_td ltx_align_center\">1.8912</td>\n<td class=\"ltx_td ltx_align_center\">0.5201</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">H-Codec (ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">2000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.3394</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.1033</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.5158</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.1667</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.2512</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.4070</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "codec",
            "xcodec",
            "speech",
            "lossdownarrow",
            "stft",
            "wavtokenizer",
            "domain",
            "unicodec",
            "different",
            "bps",
            "audio",
            "music",
            "downarrow",
            "comparison",
            "hcodec",
            "mel",
            "testclean",
            "eval",
            "test",
            "ours",
            "librispeech",
            "musdb18hq",
            "models",
            "audioset",
            "loss",
            "terms",
            "dac",
            "between",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Reconstruction Performance:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T3\" title=\"In 4.1.2 Experimental Results &#8227; 4.1 H-Codec &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comprehensive comparison of audio codec models on speech, music, and general audio tasks.\nAll baselines supports general audio reconstruction.\nNotably, H-Codec achieves lowest Mel loss and STFT loss on all domain, illustrating the powerful multi-domain reconstruction ability.\nThis ensures the potential of H-Codec for extensive downstream tasks, including speech, music, and audio generation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative modeling has recently achieved remarkable success across text, image, and audio domains,\ndemonstrating powerful capabilities for unified representation learning.\nHowever, audio generation models still face challenges in terms of audio quality and generalization ability across tasks.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.\nTo address these issues, we propose <span class=\"ltx_text ltx_font_bold\">UniTok-Audio</span>, a scalable and extensible framework for unified audio generation tasks.\nSpecifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner;\n2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework;\n3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction.\nExperimental results demonstrate that UniTok-Audio achieves competitive performance\nin comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks:\nspeech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation.\nTo foster future research, we will open-source our codebase.\nThe demo page of our work can be found here: https://alibaba.github.io/unified-audio.\n</p>\n\n",
                "matched_terms": [
                    "codec",
                    "models",
                    "speech",
                    "terms",
                    "different",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Leveraging the remarkable sequential generation capability of language model (LM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib67\" title=\"\">2017</a>)</cite>,\nrecent works have achieved significant improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib48\" title=\"\">2024</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib35\" title=\"\">2023</a>)</cite>,\npromoting the growing prevalence of artificial intelligence-generated content (AIGC).\nThese advances have inspired substantial research extending LMs to various audio tasks,\nwhich can be fundamentally categorized by the temporal relationship between input and output:\neither <span class=\"ltx_text ltx_font_italic\">time-aligned</span> (TA) or <span class=\"ltx_text ltx_font_italic\">non-time-aligned</span> (NTA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib82\" title=\"\">2025</a>)</cite>.\nThe former involves strict temporal correspondence between input and output signals, such as speech denoising,\nwhich aligns speech components in each frame between noisy and clean speech.\nWhile the latter dose not require point-wise temporal alignment, such as text-to-audio (TTA),\nwhich aims at semantic coherence between the holistical textual description and entire output soundscape.</p>\n\n",
                "matched_terms": [
                    "between",
                    "audio",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study focuses on the TA tasks, especially which provides the input audio that temporally aligned with the output audio at the frame level,\nincluding: speech restoration (SR) that aims at restoring speech from the degraded recording with various distortions (e.g., noise, reverberation,and packet loss);\ntarget speaker extraction (TSE) that extracts target speech from mixture using assistive clues (e.g., voiceprint information from reference speech);\nspeech separation (SS) that aims to separate all existing speaker in the mixture;\nvoice conversion (VC) that transforms the timbre of source speech guided by reference speech of another speaker;\nlanguage-queried audio source separation (LASS) that aims at extracting target audio components from mixture, which are consistent with the given textual caption.\nNumerous generative models are developed for these tasks, while most of them are designed for single task with\ntask-specific architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "loss",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some studies aim to unify multiple tasks within a single framework, including AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>, UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.\nThese methods utilizes the LM backbone combined with discrete audio codec and\nexhibit remarkable generative ability, which benefit from the semantic understanding and contextual modeling capabilities of LMs.\nHowever, challenges still exist in terms of audio quality and generalization ability across tasks.\nFor instance, few unified models are capable of handling the SS task, as it generally requires customized\narchitecture to output multi-track speech.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "models",
                    "speech",
                    "terms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve audio generation quality, some works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib30\" title=\"\">2023</a>; Vyas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib69\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib82\" title=\"\">2025</a>)</cite>\nadopt generative paradigms in continuous space, such as flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib35\" title=\"\">2023</a>)</cite>,\nwhich eliminates the dependence on discrete codecs.\nHowever, the flowchart of model needs to be carefully designed to support different tasks, increasing the\ndifficulty when extending to more tasks.\nAdditionally, considering the trend of combining audio generation capabilities with large language models (LLM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib62\" title=\"\">2025</a>)</cite>,\ndeveloping audio generation models based on discrete codec has greater potential.\nThis highlights the need for improving the ability of audio codec,\nwhich directly affects the generation quality of audio models.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "models",
                    "different",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Framework</span>: The framework unifies tasks by taking task-specific conditional information as the conditioning sequence of decoder-only LM,\nand the discrete token of target audio is predicted in an AR manner.\nWe utilize a special task token to distinguish different learning patterns of multiple tasks.\nNote that our model handles diverse tasks using a single set of shared weights, thereby eliminating the need for task-specific weight adaptation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">New Tokenization</span>: We present <span class=\"ltx_text ltx_font_bold\">H-codec</span>,\nwhich integrates self-supervised learning (SSL) representation within the audio tokenization and reconstruction process.\nThe features from waveform and SSL model are individually quantized, resulting dual-stream (acoustic and semantic) codec tokens.\nH-Codec achieves remarkable audio reconstruction quality with a low frame rate,\nimproving both the efficiency and performance of downstream audio generation.</p>\n\n",
                "matched_terms": [
                    "hcodec",
                    "audio",
                    "model",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the domain of TA audio tasks, early researches focus on discriminative modeling,\nwhich directly learns the mapping between input signal and target audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Williamson &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib80\" title=\"\">2017</a>; Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>.\nHowever, the lack of generative ability limits their generalization in unseen scenarios and\nthe performance in extreme situations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Welker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib78\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib71\" title=\"\">2020</a>)</cite>.\nMany studies integrate generative modeling into audio tasks in recent years.\nFor the SR task, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite> applies k-means to quantize noisy speech representations obtained\nby WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib4\" title=\"\">2022</a>)</cite> into discrete tokens,\nand then a Transformer-based speech LM maps the noisy tokens to clean tokens.\nFor the LASS task, FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>\nlearns linear flow trajectories from noise to target source features within the variational autoencoder (VAE) latent space,\nwhich are guided by the encoded text embeddings and the mixture audio.\nHowever, these models are designed for specific task, facing limited extensibility when migrating to more tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "domain",
                    "between",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Creating an unified framework that can tackle diverse tasks\nstands as a critical research goal in the field of artificial intelligence.\nIn the unification of audio tasks, the approaches can be divided into two categories:\ndiscrete audio codec based method and continuous representation based method.\nThe former is based on the pre-trained audio codec, which encodes the waveform into discrete space and reconstructs\naudio signal from it.\nThe generative ability of AR modeling or masked generative modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib2\" title=\"\">2022</a>)</cite> is leveraged to\ngenerate discrete tokens of the target audio.\nFor instance, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite> tokenizes the target audio along with other condition modalities\nand then concatenates source-target pair as a single sequence, performing next-token prediction using LLM.\nMetis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite> adopts a two-stage generation framework using masked generative modeling,\nwhich first generates SSL tokens and then predicts acoustic tokens based the former.\nContinuous representation based methods typically adopt diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib20\" title=\"\">2020</a>)</cite> or flow matching techniques,\neliminating the inevitable quantitative loss in discrete codec.\nVoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib30\" title=\"\">2023</a>)</cite> performs flow matching on mel-spectrograms to unify tasks such as text-to-speech (TTS) and speech editing.\nUniFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>)</cite> utilizes VAE to learn a compact latent representation of raw audio,\ncoupled with a diffusion Transformer (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles &amp; Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib47\" title=\"\">2023</a>)</cite> that predicts latent updates.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "codec",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to discrete audio codec based method, especially decoder-only AR models which can elegantly\nintegrate conditional information as a prefix sequence, continuous methods usually requires complex design\nto combines multimodal conditions, limiting the extensibility to more tasks.\nIn addition, discrete audio representation plays an important role in combining with LLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib62\" title=\"\">2025</a>)</cite>,\nbridging the natural language instructions and continuous waveform.\nTherefore, we develop a decoder-only AR LM-based framework (UniTok-Audio) to unify audio tasks.\nIt utilizes continuous conditional embeddings to maximize the preservation of semantic and acoustic information,\npredicting multi-layer codec tokens which reduce the quantization loss.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "loss",
                    "codec",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Neural audio codecs utilize neural networks to obtain highly compressed discrete representations of audio waveforms and\naim to reconstruct high-fidelity signal form discrete tokens.\nFor instance, SoundStream&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib90\" title=\"\">2021</a>)</cite> utilizes residual vector quantization (RVQ) where\neach quantizer refines the residuals left by the previous one,\nobtaining parallel multi-layer tokens and achieving remarkable reconstruction quality.\nMany works including Encodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib9\" title=\"\">2022b</a>)</cite> and DAC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib29\" title=\"\">2023</a>)</cite> follow this paradigm to improve performance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dac"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the development of LM, the research focus of codecs has gradually shifted\nfrom reducing data transmission costs toward the integration with LM, which ensures the high quality of generated audio.\nThis requires codecs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib37\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib10\" title=\"\">2024</a>)</cite> to preserve more semantic information that can be understood and modeled by LM.\nX-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib86\" title=\"\">2024a</a>)</cite> integrates the representations from the pre-trained SSL model to enhance semantic preservation,\nimproving both reconstruction quality and downstream TTS performance.\nSome studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib24\" title=\"\">2025</a>)</cite> explore single-layer codecs that are more suitable for autoregressive modeling in LM.\nX-Codec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite> utilizes finite scalar quantization (FSQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib41\" title=\"\">2024</a>)</cite> to perform single-layer quantization,\nenlarging the code space. BiCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite> generates a hybrid token stream combining semantic and global tokens,\nwhich are derived from a SSL model and a speaker verification model, respectively.\nHowever, single-layer codecs with a low frame rate still faces challenges in high-fidelity reconstruction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite>, e.g., speaker similarity.</p>\n\n",
                "matched_terms": [
                    "xcodec",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In practice, downstream LMs are capable of generating multi-layer tokens in parallel&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib5\" title=\"\">2023</a>; Neekhara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib44\" title=\"\">2024</a>)</cite>,\nthereby relaxing the requirement for single-layer quantization.\nThis paradigm relies more heavily on the modeling capacity of LMs, raising the upper bound of the codec&#8217;s reconstruction capability.\nIn this context, the frame rate of codecs plays a critical role, which determines the number of time steps for inference.\nOur H-Codec benefits from the RVQ technique and SSL representations, achieving significant reconstruction quality in the domain of speech, music, and general audio.\nThe low frame rate ensures efficient generation when integrated with our UniTok-Audio framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "domain",
                    "hcodec",
                    "audio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F1\" title=\"In 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>, UniTok-Audio is a unified, autoregressive LM-based audio generation framework comprising four key components:\n(i) a novel dual-stream H-codec; (ii) a text encoder with adapter; (iii) an audio encoder with adapter; (iv) a decoder-only LM backbone.\nNext, we will introduce the architecture of H-Codec and the operational framework of UniTok-Audio in detail.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "hcodec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve the audio generation quality, we propose H-Codec to discretize audio and reconstruct waveform from discrete tokens.\nAs illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F2\" title=\"In 3.1.3 Optimization Strategy &#8227; 3.1 H-Codec &#8227; 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, the architecture of H-Codec follows the common paradigm of audio tokenizers, including an acoustic encoder, a quantizer module, and an acoustic decoder.\nInspired by X-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib86\" title=\"\">2024a</a>)</cite>, we incorporate pretrained models to facilitate the preservation of semantic information.\nHowever, unlike X-Codec, which fuses acoustic and semantic information and then quantizes the combined representation using a single codebook,\nwe employ separate codebooks to quantize the two types of features independently, leading to dual-stream codec tokens.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "xcodec",
                    "models",
                    "hcodec",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the encoding stage, the raw waveform <math alttext=\"\\bm{x}\\in\\mathbb{R}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119961;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>n</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\bm{x}\\in\\mathbb{R}^{n}</annotation></semantics></math> is fed into the acoustic encoder to extract frame-level acoustic features, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> represents the number of waveform samples.\nThe architecture of the acoustic encoder follows Encodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib9\" title=\"\">2022b</a>)</cite>.\nA 4-layer RVQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib90\" title=\"\">2021</a>)</cite> is utilized to quantize features, resulting in the quantized features with a frame rate of 25 Hz.\nSynchronously, a pre-trained HuBERT<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://huggingface.co/bosonai/hubert_base</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib21\" title=\"\">2021</a>)</cite> extracts SSL features by averaging outputs from all transformer layers\nand the quantized semantic feature is obtained by applying the semantic encoder and RVQ quantizer.\nNote that HuBERT is trained on general audio, thus the codec has the potential to handle general audio as well.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The types of discriminators and the composition of the loss functions follow the configuration used in WavTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>)</cite>.\nWe employ a multi-period discriminator (MPD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib28\" title=\"\">2020</a>)</cite>, a multi-resolution discriminator (MRD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib22\" title=\"\">2021</a>)</cite>, and a sub-band complex STFT discriminator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib90\" title=\"\">2021</a>)</cite> to improve the naturalness and fidelity of reconstructed audio,\nand the training loss <math alttext=\"\\mathcal{L}_{dis}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{dis}</annotation></semantics></math> conforms to the hinge loss formulation suggested by&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeghidour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib90\" title=\"\">2021</a>)</cite>. The training loss for the generator of H-Codec include:\ncommitment loss for quantizer <math alttext=\"\\mathcal{L}_{commit}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{commit}</annotation></semantics></math>, mel-spectrum reconstruction loss <math alttext=\"\\mathcal{L}_{mel}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{mel}</annotation></semantics></math>, adversarial loss <math alttext=\"\\mathcal{L}_{adv}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{adv}</annotation></semantics></math>, feature matching loss <math alttext=\"\\mathcal{L}_{fm}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{fm}</annotation></semantics></math>, and an auxiliary mean squared error (MSE) loss on SSL feature <math alttext=\"\\mathcal{L}_{aux}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{aux}</annotation></semantics></math>.\nThe composite training loss of the generator is obtained by</p>\n\n",
                "matched_terms": [
                    "wavtokenizer",
                    "stft",
                    "loss",
                    "hcodec",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\lambda_{commit}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{commit}</annotation></semantics></math>, <math alttext=\"\\lambda_{mel}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{mel}</annotation></semantics></math>, <math alttext=\"\\lambda_{adv}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m9\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{adv}</annotation></semantics></math>, <math alttext=\"\\lambda_{fm}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m10\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{fm}</annotation></semantics></math>, and <math alttext=\"\\lambda_{aux}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m11\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\lambda_{aux}</annotation></semantics></math> are hyper-parameters to scale different loss components.\nAdditionally, the perceptual loss&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Parker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib46\" title=\"\">2024</a>)</cite> is utilized during the final steps of the training process,\nwhich further enhances the reconstruction quality.</p>\n\n",
                "matched_terms": [
                    "loss",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To unify various audio generation tasks within a single framework,\nwe extract task-specific conditional information as a conditioning sequence for the decoder-only AR backbone,\nwhich then predicts the corresponding H-Codec tokens of the target audio.\nSince continuous features, typically extracted from SSL models, contain richer audio details compared to discrete representations and are more adaptable to varying input conditions,\nwe extract continuous features to assemble the the task-conditioning sequence.\nSpecifically, we utilize T5-base<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/google/t5-v1_1-base</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib51\" title=\"\">2020</a>)</cite> as the\ntext encoder to extract embedding from audio caption.\nThe same HuBERT used in H-Codec is adopted to extract continuous features from audio waveforms.\nTwo linear layers serve as two adapters to map the text embedding and audio features into a representation space amenable to LM AR modeling, respectively.\nGiven text and audio embeddings as conditions,\nwe utilize LLaMA architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib64\" title=\"\">2023</a>)</cite> to predicts discrete tokens of target waveform in an AR manner.\nFinally, the H-Codec decoder reconstructs high-fidelity audio from the predicted token sequence.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "hcodec",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate multi-layer codec tokens into AR prediction, an existing method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib70\" title=\"\">2023a</a>)</cite> applies two-stage strategy:\n(i) model the tokens of the first layer in an AR manner; (ii) then, predict the tokens of remaining layers using a NAR post-network.\nHowever, this method causes additional complexity to the system. In addition, flattening all tokens into one layer leads to unbearable computational\ncost, while predicting tokens from all layers in parallel within one step deteriorates the performance.\nTherefore, we adopt the delay pattern&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib5\" title=\"\">2023</a>)</cite> to arrange our tokens for the trade-off between performance and computational cost.\nSpecifically, the 4-layer acoustic and semantic tokens produced by H-Codec are first interleaved sequentially across time steps, resulting in <math alttext=\"{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">E</mi><mi>c</mi></msub><mo>&#8712;</mo><msup><mi>&#8484;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}</annotation></semantics></math>\nwith a frame rate of 50 Hz, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> indicates the number of frames. Before feeding the tokens into the LM backbone,\ndifferent shifts are applied across layers and special pad tokens occupy empty positions, as shown in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F1\" title=\"In 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn the LM backbone, 4 embedding layers handle 4-layer tokens respectively, and the embeddings of each layer are added up as the input of transformer layers.\nThere are 4 output heads to predict the 4-layer logits of next time step. The delay pattern allows generating high-layer tokens conditioned by low-layer tokens,\nwhich improves prediction accuracy.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "different",
                    "hcodec",
                    "between",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following our previous work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, we introduce special task tokens to distinguish between different operational modes.\nTo unify five tasks (i.e., SR, TSE, SS, VC, and LASS), we utilize five modes, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.T1\" title=\"Table 1 &#8227; 3.2.2 AR Prediction of H-Codec Tokens &#8227; 3.2 Unified Multi-Task Generation &#8227; 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nEach mode corresponds to a special token and different task-specific condition types, which serve as a conditioning sequence\nfor the LM backbone to estimate the conditional probability density distribution of target discrete tokens.</p>\n\n",
                "matched_terms": [
                    "between",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SR Mode:</span>\nThe target audio is the clean speech corresponding to the degraded input speech. The conditional sequence of LM is formatted as\n<math alttext=\"\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>SR</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">I</mi><annotation encoding=\"application/x-tex\">{\\rm I}</annotation></semantics></math> denotes the start of input audio features,\n<math alttext=\"{\\rm E}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{i}</annotation></semantics></math> the input audio embeddings, and <math alttext=\"{\\rm S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">S</mi><annotation encoding=\"application/x-tex\">{\\rm S}</annotation></semantics></math> the start of codec tokens, respectively.\nThe output sequence is formulated as <math alttext=\"{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119952;</mi><mo>=</mo><mrow><mo>[</mo><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">E</mi><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm E}^{\\prime}_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m6\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">{\\rm E}^{\\prime}_{c}</annotation></semantics></math> indicates codec tokens with\ndelay pattern, and <math alttext=\"{\\rm E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">E</mi><annotation encoding=\"application/x-tex\">{\\rm E}</annotation></semantics></math> represents the end token. The trainable parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m8\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> in the model\nare optimized by minimizing the negative log-likelihood of the predicted outputs:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "model",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TSE Mode:</span>\nThe target audio corresponds to the timbre-matched speech component in the input mixture audio that aligns with the reference audio.\nThe conditional sequence is formatted as <math alttext=\"\\left[{\\rm T_{TSE}},{\\rm R},{\\rm E}_{r},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>TSE</mi></msub><mo>,</mo><mi mathvariant=\"normal\">R</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>r</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{TSE}},{\\rm R},{\\rm E}_{r},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>,\nwhere <math alttext=\"{\\rm E}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{r}</annotation></semantics></math> and <math alttext=\"{\\rm R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">R</mi><annotation encoding=\"application/x-tex\">{\\rm R}</annotation></semantics></math> represent the features of reference speech and its start token, respectively.\nTherefore, the associated loss function is defined as</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">rTSE Mode:</span>\nSince SS task requires generating multiple output tracks while our model only supports one-track output,\nwe include the rTSE mode during training,\nenabling the model to obtain multiple tracks through iterative inference.\nThis mode aims to extract the timbre-mismatched speech component in the mixture input when compared with\nthe reference speech. The loss function <math alttext=\"\\mathcal{L}_{\\rm rTSE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm rTSE}</annotation></semantics></math> keeps similar to that of the TSE mode,\nexcept that the task token has been replaced with <math alttext=\"{\\rm T_{rTSE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{rTSE}}</annotation></semantics></math>.\nWhen handling SS task (we only consider 2-speaker cases), we first apply the SR mode to extract the main speaker\nwith higher energy, and the other speaker is obtained by using the rTSE mode.</p>\n\n",
                "matched_terms": [
                    "loss",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASS Mode:</span>\nThis mode aims at separating specific component that matches the given caption query from the input mixture audio.\nTherefore, the associated loss function is defined as</p>\n\n",
                "matched_terms": [
                    "audio",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span>\nWe utilize multi-domain data to train our codec, including speech, music, and audio.\nThe speech samples are sourced from the VoxBox dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite>, which comprises approximately 100k hours of speech\nand is composed of some publicly available speech datasets.\nFor the music domain, we utilize the FMA-full dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Defferrard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib7\" title=\"\">2017</a>)</cite> and the MUSDB18-HQ dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib52\" title=\"\">Rafii et&#160;al., </a>)</cite>,\ninvolving about 8k hours of data.\nFor the audio domain, we adopt AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib17\" title=\"\">2017</a>)</cite> and WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib40\" title=\"\">2024</a>)</cite>,\nincluding about 13k hours of recordings.\nWe evaluate the reconstruction quality on LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib45\" title=\"\">2015</a>)</cite> test-clean, MUSDB18-HQ test, and AudioSet eval sets for\nspeech, music, and audio domain, respectively.\nAll samples are resampled to 16k Hz.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "librispeech",
                    "musdb18hq",
                    "speech",
                    "domain",
                    "eval",
                    "audioset",
                    "testclean",
                    "audio",
                    "test",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThe total downsampling ratio in H-Codec is set to 640 to obtain the frame rate of 25 Hz in both acoustic and semantic branch.\nIn the 4-layer RVQ, we utilize a codebook size of 1024 for each layer with the codebook dimension set to 512.\nDuring training, we randomly crop 5-second segments from audio samples.\nThe network is optimized using the AdamW optimizer with an initial learning rate of\n<math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math>, which is decayed based on a cosine scheduler.\nIn total, we train for 600k steps, and the perceptual loss is activated at final 100k steps.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "loss",
                    "hcodec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics:</span>\nWe utilize several metrics to measure the reconstruction quality of speech,\nincluding the perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI),\nspeaker similarity (SPK-SIM) and UTMOS.\nThe loss on Mel-scale spectrum and STFT spectrum bettween the target audio and reconstructed audio\nare computed for general evaluation in the domain of speech, music, and audio.\nDetails about evaluation metrics of codec can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A2.SS1\" title=\"B.1 Codec Metrics &#8227; Appendix B Evaluation Metrics &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "stft",
                    "loss",
                    "speech",
                    "domain",
                    "audio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines:</span>\nWe compare our codec against some state-of-the-art (SOTA) baselines,\nincluding DAC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib29\" title=\"\">2023</a>)</cite>, Encodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib8\" title=\"\">2022a</a>)</cite>, X-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib87\" title=\"\">2024b</a>)</cite>,\nX-Codec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite>, BiCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite>, WavTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>)</cite>,\nand UniCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib24\" title=\"\">2025</a>)</cite>.\nAll results are obtained using their official checkpoints.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "wavtokenizer",
                    "xcodec",
                    "dac",
                    "unicodec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Reconstruction Performance:</span>\nAs reported in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T2\" title=\"In 4.1.2 Experimental Results &#8227; 4.1 H-Codec &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, our H-Codec achieves competitive performance at a frame rate of 50.\nSince multi-layer tokens can be predicted simultaneously within a single time step in downstream audio LM,\nwe argue that frame rate is more critical, as the number of time steps significantly affects computational cost.\nCompared to baselines wich support general audio, H-Codec exhibits better signal reconstruction quality\n(PESQ and STOI), speech naturalness (UTMOS), speaker consistency (SPK-SIM), and\nsemantic information preservation (WER). Note that some models achieve higher UTMOS than the ground truth,\nthis can be attributed to the generative ability of codec decoder,\nwhich generates plausible speech at the expense of inacurrate signal alignment.\nOur H-Codec reports UTMOS closely matches that of the ground truth,\nindicating the high fidelity of the reconstructed speech.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "models",
                    "speech",
                    "hcodec",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Datasets:</span>\nFor the training of speech tasks,\nwe adopt clean speech samples from the VoxBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite> dataset, including approximately 3.8k hours of data from\nLibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib45\" title=\"\">2015</a>)</cite>, MLS_English&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib49\" title=\"\">2020</a>)</cite> and Emilia_ZH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib18\" title=\"\">2024</a>)</cite> subset.\nThe noise corpus comprises approximately 460 hours of data from the DNS Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib53\" title=\"\">2020</a>)</cite>,\nFSD50K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib14\" title=\"\">2022</a>)</cite>, WHAM!&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wichern et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib79\" title=\"\">2019</a>)</cite>, DESED&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turpault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib65\" title=\"\">2019</a>)</cite>, DEMAND&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Thiemann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib63\" title=\"\">2013</a>)</cite>, MUSAN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib58\" title=\"\">2015</a>)</cite>,\nDISCO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Furnon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib15\" title=\"\">2021</a>)</cite>, MUSDB18-HQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib52\" title=\"\">Rafii et&#160;al., </a>)</cite>, and TUT Urban Acoustic Scenes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib42\" title=\"\">2018</a>)</cite>.\nWe include 60k room impulse response (RIR) samples from SLR28&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib27\" title=\"\">2017</a>)</cite> to simulate reverberation.\nFor the audio data, we include captioned audio samples from\nWavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib40\" title=\"\">2024</a>)</cite>, CLAP_FreeSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib81\" title=\"\">2023</a>)</cite>, VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib3\" title=\"\">2020</a>)</cite>, and Internal data, resulting in approximately 40k hours.\nThe simulation pipeline of training samples for all operational modes are\ndescribed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A1\" title=\"Appendix A Data Simulation &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "musdb18hq",
                    "audio",
                    "speech",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "models",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics:</span>\nWe adopt multiple evaluation metrics to assess different aspects of the generated audio across tasks.\nFor speech tasks, we evaluate quality by DNSMOS (SIG, BAK, OVRL) and NISQA, speaker similarity by SIM,\nintelligibility by WER, and continuity by PLCMOS.\nFor the LASS task, we utilize FAD, CLAPScore, and CLAPScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> to measure the audio separation performance.\nDetails about evaluation metrics can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A2.SS2\" title=\"B.2 Audio Task Metrics &#8227; Appendix B Evaluation Metrics &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate speech restoration performance on the synthetic test sets of 2020 DNS Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib53\" title=\"\">2020</a>)</cite> (including &#8220;With Reverb&#8221; and &#8220;No Reverb&#8221;) and\n2022 PLC Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diener et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib11\" title=\"\">2022</a>)</cite> blind test set.\nBaselines include Conv-TasNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>, DEMUCS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib13\" title=\"\">2019</a>)</cite>, FRCRN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib94\" title=\"\">2022</a>)</cite>, FlowSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>)</cite>,\nUniFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>)</cite>, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite>, MaskSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib34\" title=\"\">2024</a>)</cite>, AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nGenSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib85\" title=\"\">2025</a>)</cite>, Metis-SE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>,\nUniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, KuaishouNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib33\" title=\"\">2022</a>)</cite>, LPCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Valin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib66\" title=\"\">2022</a>)</cite>, PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib36\" title=\"\">2022a</a>)</cite>,\nand BS-PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib93\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T4\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the SR performance comparison on 2020 DNS Challenge test sets.\nIt is clear that generative models generally outperform discriminative ones.\nContinuous-domain generative approaches perform well on the &#8220;No Reverb&#8221; subset,\nhighlighting the potential of continuous methods in terms of generated signal quality.\nHowever, discrete-domain generative approaches can perform better under reverberant conditions, indicating\nthat discrete representations may simplify the modeling difficulty of reverberation components.\nOur UniTok-Audio achieves comparable performance among SOTA baselines,\nand the single-task versions with different audio encoders result in similar performance to UniTok-Audio<sub class=\"ltx_sub\">omni</sub>.\nIn addition, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T5\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the performance on packet loss concealment (PLC),\na subtask of SR aimed at recovering speech frames lost during transmission.\nUniTok-Audio surpasses baselines in terms of both signal quality and continuity,\nshowing powerful content understanding and generation capabilities of the framework.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "loss",
                    "comparison",
                    "terms",
                    "different",
                    "audio",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T6\" title=\"In 4.2.2 TSE Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the performance comparison for TSE task.\nThe results indicate that generative methods achieve higher speech quality than discriminative approaches\nbut struggle with speaker similarity.\nThis can be attributed to the upper bound limitation of codecs&#8217; reconstruction fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>.\nOur UniTok-Audio maintains comparable performance compared to SOTA baselines,\ndemonstrating the feasibility of constructing a unified framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate SS performance on Libri2Mix noisy test set and WSJ0-2mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib19\" title=\"\">2016</a>)</cite> test set,\nwhere the former additionally evaluates the denoising ability of models.\nBaselines include Sepformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Subakan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib59\" title=\"\">2021</a>)</cite>, Mossformer2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib95\" title=\"\">2024</a>)</cite>, and LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T7\" title=\"In 4.2.3 SS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the performance comparison for SS task, showing that our model achieves superior performance than baselines.\nThis verifies the effectiveness of our iterative inference strategy in handling the SS task that requires multiple output tracks.\nNote that although the experiments are conducted with the 2-speaker configuration,\nour approach can be extended to scenarios with more sources when the target signal of rTSE mode is defined as all remaining speakers.\nThe single-task version is not reported since the inference phase of SS requires the cooperation of multiple modes.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nFollowing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, we create test set for the VC task using VCTK&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib68\" title=\"\">2017</a>)</cite> dataset.\nWe randomly select 200 recordings from the dataset as source speech,\nand for each source sample, a sample from another speaker is picked as the reference speech.\nBaselines include HierSpeech++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib31\" title=\"\">2023</a>)</cite>, LM-VC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>,\nVevo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib92\" title=\"\">2024a</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nVC results are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T8\" title=\"In 4.2.4 VC Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, showing the superiority of UniTok-Audio in\nspeech quality, speaker similarity, and intelligibility.\nWe observe that UniTok-Audio<sub class=\"ltx_sub\">vc-wavlm</sub> outperforms UniTok-Audio<sub class=\"ltx_sub\">vc-hubert</sub>, indicating that\nWavLM performs better in extracting semantic information and speaker characteristics.\nThe performance degrades when extending to multiple tasks from single-task version,\nimplying the distinct pattern between VC and other tasks,\nwhere the former changes the property of the input signal rather than restoring or extracting certain components.</p>\n\n",
                "matched_terms": [
                    "between",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T9\" title=\"In 4.2.5 LASS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, UniTok-Audio achieves competitive performance in the LASS task,\nindicating effective exploitation of textual information.\nWe prove that the unified domain codec has potential to handle the LASS tasks.\nThe single-task version outperforms UniTok-Audio<sub class=\"ltx_sub\">omni</sub>,\nwhich can be attributed to the domain gap between speech and audio.</p>\n\n",
                "matched_terms": [
                    "codec",
                    "speech",
                    "domain",
                    "between",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniTok-Audio,\na framework that resembles multiple time-aligned audio tasks.\nWe uniify different learning patterns of multiple tasks in a single framework using a special task token,\nwhich indicates current operational mode of model.\nThis paper also introduces H-Codec, achieving high-fidelity reconstruction quality with dual-stream architecture\nthat quantize acoustic and semantic features simultaneously.\nBased on H-Codec, UniTok-Audio adopts continuous conditional embeddings to generates multi-layer discrete tokens in parallel.\nExtensive experiments demonstrate that UniTok-Audio achieves competitive performance across diverse tasks\nwith limited training data and moderate model size,\nhighlighting its potential as a foundation model for unified AR audio generation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "hcodec",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A data simulation pipeline is designed to synthesis data pairs dynamically during training.\nConsidering various types of degradation in the SR task,\nwe apply multiple distortions to a speech sample with independent probabilities,\nwhere the distortion categories and corresponding configurations are shown in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A1.T10\" title=\"In Appendix A Data Simulation &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nThe distortion chain is also applied to the TSE and rTSE modes,\nexcept that the probability of interfering speaker is set to 1.0 and the SIR is uniformly sampled between\n-5 and 5 dB.\nFor the LASS mode, we mix the target audio with another randomly selected audio using a SIR ranges from -5 to 20 dB.\nFor the VC mode, we leverage a voice conversion model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/myshell-ai/OpenVoice</span></span></span> to perform timbre perturbation using randomly selected target speech and reference speech,\ngenerating 6k hours of fixed training dataset.\nThe perturbed sample is used as input to predict the target speech based on another speech of the target speaker.</p>\n\n",
                "matched_terms": [
                    "between",
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span>:\nWord Error Rate (WER) measures the intelligibility of the generated speech by using the automatic speech recognition (ASR) model.\nWe utilize a HuBERT-based ASR system<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>https://huggingface.co/facebook/hubert-large-ls960-ft</span></span></span> to calculate WER.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SPK-SIM</span>:\nA WavLM-based speaker verification model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification</span></span></span>\nis used to calculate the speaker similarity between the reconstructed speech and target speech.</p>\n\n",
                "matched_terms": [
                    "between",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">STFT Loss &amp; Mel Loss</span>:\nWe calculate the L1 loss between the magnitude spectrum of the reconstructed speech and target speech,\nwhere the STFT is performed using a Hann window with a length of 1024 and a shift of 256.\nFor the Mel Loss, 100 mel filters are utilized.</p>\n\n",
                "matched_terms": [
                    "stft",
                    "loss",
                    "speech",
                    "mel",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FAD</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib26\" title=\"\">2018</a>)</cite>:\nFr&#233;chet Audio Distance (FAD)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span>https://github.com/gudgud96/frechet-audio-distance</span></span></span> measures the quality of generated audio by comparing the statistics of deep features between real and synthesized audio.\nLower FAD value indicates higher fidelity and better distributional alignment.</p>\n\n",
                "matched_terms": [
                    "between",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CLAPScore &amp; CLAPScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">A</span></sub></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib81\" title=\"\">2023</a>)</cite>:\nCLAPScore measures text-audio similarity using joint embeddings from a contrastive language-audio pretraining (CLAP) model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote16\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">16</sup><span class=\"ltx_tag ltx_tag_note\">16</span>https://github.com/LittleFlyingSheep/CLAPScore_for_LASS</span></span></span>.\nWhile CLAPScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> evaluates the similarity between the output audio and the target audio.</p>\n\n",
                "matched_terms": [
                    "between",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T11\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a> reports the hyperparameter configurations of different UniTok-Audio versions.\nUniTok-Audio-S and UniTok-Audio-L denote the small and large version, respectively.\nThe VC performance in terms of different verisons are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T12\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, where\nall versions are trained for the single VC task using WavLM-based audio encoder.\nIt can be seen that increasing the model size consistently improves performance,\nin accordance with scaling laws.\nThis indicates the potential of UniTok-Audio to be extended to a larger model size.\nTo balance complexity and performance, we report the medium-sized verison in the main text.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "terms",
                    "different"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 4: DNSMOS scores on the Interspeech 2020 DNS Challenge blind test set.\nD represents discriminative approaches.\nGc and Gd denote generative methods in the continuous domain and discrete domain, respectively.\nNo Reverb subset contains only noise while With Reverb subset additionally involves reverberation.",
        "body": "Model\nType\nWith Reverb\nNo Reverb\n\n\n\nSIG(\\uparrow)\n\nBAK(\\uparrow)\n\nOVRL(\\uparrow)\n\nSIG(\\uparrow)\n\nBAK(\\uparrow)\n\nOVRL(\\uparrow)\n\n\nNoisy\n-\n1.76\n1.50\n1.39\n3.39\n2.62\n2.48\n\n\nConv-TasNet\nD\n2.42\n2.71\n2.01\n3.09\n3.34\n3.00\n\n\nDEMUCS\nD\n2.86\n3.90\n2.55\n3.58\n4.15\n3.35\n\n\nFRCRN\nD\n2.93\n2.92\n2.28\n3.58\n4.13\n3.34\n\n\nFlowSE\nGc\n\n3.60\n4.10\n3.33\n3.69\n4.20\n3.45\n\n\nUniFlow\nGc\n\n3.59\n4.12\n3.32\n3.72\n4.21\n3.48\n\n\nSELM\nGd\n\n3.16\n3.58\n2.70\n3.51\n4.10\n3.26\n\n\nMaskSR\nGd\n\n3.53\n4.07\n3.25\n3.59\n4.12\n3.34\n\n\nAnyEnhance\nGd\n\n3.50\n4.04\n3.20\n3.64\n4.18\n3.42\n\n\nGenSE\nGd\n\n3.49\n3.73\n3.19\n3.65\n4.18\n3.43\n\n\nMetis-SE\nGd\n\n3.68\n4.14\n3.44\n3.64\n4.17\n3.43\n\n\nLLaSE-G1\nGd\n\n3.59\n4.10\n3.33\n3.66\n4.17\n3.42\n\n\nUniSE\nGd\n\n3.67\n4.10\n3.40\n3.67\n4.14\n3.43\n\n\nUniTok-Audiosr-hubert\n\nGd\n\n3.67\n4.11\n3.40\n3.66\n4.15\n3.41\n\n\nUniTok-Audiosr-wavlm\n\nGd\n\n3.67\n4.10\n3.40\n3.66\n4.14\n3.42\n\n\nUniTok-Audioomni\n\nGd\n\n3.67\n4.12\n3.42\n3.66\n4.15\n3.44",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">With Reverb</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">No Reverb</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">SIG</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">BAK</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">OVRL</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">SIG</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">BAK</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">OVRL</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Noisy</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Conv-TasNet</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">D</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">DEMUCS</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">D</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.86</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.55</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">FRCRN</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">D</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.34</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">FlowSE</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">c</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">UniFlow</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">c</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.72</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.21</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.48</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">SELM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.26</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MaskSR</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.34</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">AnyEnhance</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.42</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">GenSE</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.49</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Metis-SE</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.14</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.44</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LLaSE-G1</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.42</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">UniSE</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.41</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.42</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">UniTok-Audio<sub class=\"ltx_sub\">omni</sub>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.44</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "selm",
            "gense",
            "respectively",
            "metisse",
            "no",
            "approaches",
            "type",
            "noisy",
            "uniflow",
            "unitokaudiosrhubert",
            "reverb",
            "unitokaudioomni",
            "flowse",
            "dnsmos",
            "discriminative",
            "ovrluparrow",
            "unitokaudiosrwavlm",
            "gd",
            "blind",
            "only",
            "methods",
            "domain",
            "continuous",
            "masksr",
            "generative",
            "scores",
            "while",
            "unise",
            "demucs",
            "involves",
            "gc",
            "with",
            "test",
            "frcrn",
            "discrete",
            "dns",
            "contains",
            "noise",
            "anyenhance",
            "bakuparrow",
            "set",
            "model",
            "siguparrow",
            "additionally",
            "d",
            "subset",
            "denote",
            "interspeech",
            "llaseg1",
            "reverb",
            "reverberation",
            "challenge",
            "convtasnet",
            "represents"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T4\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the SR performance comparison on 2020 DNS Challenge test sets.\nIt is clear that generative models generally outperform discriminative ones.\nContinuous-domain generative approaches perform well on the &#8220;No Reverb&#8221; subset,\nhighlighting the potential of continuous methods in terms of generated signal quality.\nHowever, discrete-domain generative approaches can perform better under reverberant conditions, indicating\nthat discrete representations may simplify the modeling difficulty of reverberation components.\nOur UniTok-Audio achieves comparable performance among SOTA baselines,\nand the single-task versions with different audio encoders result in similar performance to UniTok-Audio<sub class=\"ltx_sub\">omni</sub>.\nIn addition, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T5\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the performance on packet loss concealment (PLC),\na subtask of SR aimed at recovering speech frames lost during transmission.\nUniTok-Audio surpasses baselines in terms of both signal quality and continuity,\nshowing powerful content understanding and generation capabilities of the framework.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative modeling has recently achieved remarkable success across text, image, and audio domains,\ndemonstrating powerful capabilities for unified representation learning.\nHowever, audio generation models still face challenges in terms of audio quality and generalization ability across tasks.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.\nTo address these issues, we propose <span class=\"ltx_text ltx_font_bold\">UniTok-Audio</span>, a scalable and extensible framework for unified audio generation tasks.\nSpecifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner;\n2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework;\n3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction.\nExperimental results demonstrate that UniTok-Audio achieves competitive performance\nin comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks:\nspeech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation.\nTo foster future research, we will open-source our codebase.\nThe demo page of our work can be found here: https://alibaba.github.io/unified-audio.\n</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "discrete",
                    "generative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Leveraging the remarkable sequential generation capability of language model (LM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib67\" title=\"\">2017</a>)</cite>,\nrecent works have achieved significant improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib48\" title=\"\">2024</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib35\" title=\"\">2023</a>)</cite>,\npromoting the growing prevalence of artificial intelligence-generated content (AIGC).\nThese advances have inspired substantial research extending LMs to various audio tasks,\nwhich can be fundamentally categorized by the temporal relationship between input and output:\neither <span class=\"ltx_text ltx_font_italic\">time-aligned</span> (TA) or <span class=\"ltx_text ltx_font_italic\">non-time-aligned</span> (NTA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib82\" title=\"\">2025</a>)</cite>.\nThe former involves strict temporal correspondence between input and output signals, such as speech denoising,\nwhich aligns speech components in each frame between noisy and clean speech.\nWhile the latter dose not require point-wise temporal alignment, such as text-to-audio (TTA),\nwhich aims at semantic coherence between the holistical textual description and entire output soundscape.</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "model",
                    "involves",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study focuses on the TA tasks, especially which provides the input audio that temporally aligned with the output audio at the frame level,\nincluding: speech restoration (SR) that aims at restoring speech from the degraded recording with various distortions (e.g., noise, reverberation,and packet loss);\ntarget speaker extraction (TSE) that extracts target speech from mixture using assistive clues (e.g., voiceprint information from reference speech);\nspeech separation (SS) that aims to separate all existing speaker in the mixture;\nvoice conversion (VC) that transforms the timbre of source speech guided by reference speech of another speaker;\nlanguage-queried audio source separation (LASS) that aims at extracting target audio components from mixture, which are consistent with the given textual caption.\nNumerous generative models are developed for these tasks, while most of them are designed for single task with\ntask-specific architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.</p>\n\n",
                "matched_terms": [
                    "generative",
                    "while",
                    "noise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some studies aim to unify multiple tasks within a single framework, including AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>, UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.\nThese methods utilizes the LM backbone combined with discrete audio codec and\nexhibit remarkable generative ability, which benefit from the semantic understanding and contextual modeling capabilities of LMs.\nHowever, challenges still exist in terms of audio quality and generalization ability across tasks.\nFor instance, few unified models are capable of handling the SS task, as it generally requires customized\narchitecture to output multi-track speech.</p>\n\n",
                "matched_terms": [
                    "discrete",
                    "unise",
                    "anyenhance",
                    "methods",
                    "llaseg1",
                    "generative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve audio generation quality, some works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib30\" title=\"\">2023</a>; Vyas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib69\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib82\" title=\"\">2025</a>)</cite>\nadopt generative paradigms in continuous space, such as flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib35\" title=\"\">2023</a>)</cite>,\nwhich eliminates the dependence on discrete codecs.\nHowever, the flowchart of model needs to be carefully designed to support different tasks, increasing the\ndifficulty when extending to more tasks.\nAdditionally, considering the trend of combining audio generation capabilities with large language models (LLM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib62\" title=\"\">2025</a>)</cite>,\ndeveloping audio generation models based on discrete codec has greater potential.\nThis highlights the need for improving the ability of audio codec,\nwhich directly affects the generation quality of audio models.</p>\n\n",
                "matched_terms": [
                    "discrete",
                    "additionally",
                    "continuous",
                    "generative",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Framework</span>: The framework unifies tasks by taking task-specific conditional information as the conditioning sequence of decoder-only LM,\nand the discrete token of target audio is predicted in an AR manner.\nWe utilize a special task token to distinguish different learning patterns of multiple tasks.\nNote that our model handles diverse tasks using a single set of shared weights, thereby eliminating the need for task-specific weight adaptation.</p>\n\n",
                "matched_terms": [
                    "discrete",
                    "set",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the domain of TA audio tasks, early researches focus on discriminative modeling,\nwhich directly learns the mapping between input signal and target audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Williamson &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib80\" title=\"\">2017</a>; Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>.\nHowever, the lack of generative ability limits their generalization in unseen scenarios and\nthe performance in extreme situations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Welker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib78\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib71\" title=\"\">2020</a>)</cite>.\nMany studies integrate generative modeling into audio tasks in recent years.\nFor the SR task, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite> applies k-means to quantize noisy speech representations obtained\nby WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib4\" title=\"\">2022</a>)</cite> into discrete tokens,\nand then a Transformer-based speech LM maps the noisy tokens to clean tokens.\nFor the LASS task, FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>\nlearns linear flow trajectories from noise to target source features within the variational autoencoder (VAE) latent space,\nwhich are guided by the encoded text embeddings and the mixture audio.\nHowever, these models are designed for specific task, facing limited extensibility when migrating to more tasks.</p>\n\n",
                "matched_terms": [
                    "selm",
                    "discrete",
                    "noise",
                    "noisy",
                    "domain",
                    "generative",
                    "discriminative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Creating an unified framework that can tackle diverse tasks\nstands as a critical research goal in the field of artificial intelligence.\nIn the unification of audio tasks, the approaches can be divided into two categories:\ndiscrete audio codec based method and continuous representation based method.\nThe former is based on the pre-trained audio codec, which encodes the waveform into discrete space and reconstructs\naudio signal from it.\nThe generative ability of AR modeling or masked generative modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib2\" title=\"\">2022</a>)</cite> is leveraged to\ngenerate discrete tokens of the target audio.\nFor instance, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite> tokenizes the target audio along with other condition modalities\nand then concatenates source-target pair as a single sequence, performing next-token prediction using LLM.\nMetis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite> adopts a two-stage generation framework using masked generative modeling,\nwhich first generates SSL tokens and then predicts acoustic tokens based the former.\nContinuous representation based methods typically adopt diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib20\" title=\"\">2020</a>)</cite> or flow matching techniques,\neliminating the inevitable quantitative loss in discrete codec.\nVoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib30\" title=\"\">2023</a>)</cite> performs flow matching on mel-spectrograms to unify tasks such as text-to-speech (TTS) and speech editing.\nUniFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>)</cite> utilizes VAE to learn a compact latent representation of raw audio,\ncoupled with a diffusion Transformer (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles &amp; Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib47\" title=\"\">2023</a>)</cite> that predicts latent updates.</p>\n\n",
                "matched_terms": [
                    "discrete",
                    "approaches",
                    "methods",
                    "uniflow",
                    "continuous",
                    "generative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to discrete audio codec based method, especially decoder-only AR models which can elegantly\nintegrate conditional information as a prefix sequence, continuous methods usually requires complex design\nto combines multimodal conditions, limiting the extensibility to more tasks.\nIn addition, discrete audio representation plays an important role in combining with LLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib62\" title=\"\">2025</a>)</cite>,\nbridging the natural language instructions and continuous waveform.\nTherefore, we develop a decoder-only AR LM-based framework (UniTok-Audio) to unify audio tasks.\nIt utilizes continuous conditional embeddings to maximize the preservation of semantic and acoustic information,\npredicting multi-layer codec tokens which reduce the quantization loss.</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "discrete",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the development of LM, the research focus of codecs has gradually shifted\nfrom reducing data transmission costs toward the integration with LM, which ensures the high quality of generated audio.\nThis requires codecs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib37\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib10\" title=\"\">2024</a>)</cite> to preserve more semantic information that can be understood and modeled by LM.\nX-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib86\" title=\"\">2024a</a>)</cite> integrates the representations from the pre-trained SSL model to enhance semantic preservation,\nimproving both reconstruction quality and downstream TTS performance.\nSome studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib24\" title=\"\">2025</a>)</cite> explore single-layer codecs that are more suitable for autoregressive modeling in LM.\nX-Codec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite> utilizes finite scalar quantization (FSQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib41\" title=\"\">2024</a>)</cite> to perform single-layer quantization,\nenlarging the code space. BiCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite> generates a hybrid token stream combining semantic and global tokens,\nwhich are derived from a SSL model and a speaker verification model, respectively.\nHowever, single-layer codecs with a low frame rate still faces challenges in high-fidelity reconstruction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite>, e.g., speaker similarity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To unify various audio generation tasks within a single framework,\nwe extract task-specific conditional information as a conditioning sequence for the decoder-only AR backbone,\nwhich then predicts the corresponding H-Codec tokens of the target audio.\nSince continuous features, typically extracted from SSL models, contain richer audio details compared to discrete representations and are more adaptable to varying input conditions,\nwe extract continuous features to assemble the the task-conditioning sequence.\nSpecifically, we utilize T5-base<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/google/t5-v1_1-base</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib51\" title=\"\">2020</a>)</cite> as the\ntext encoder to extract embedding from audio caption.\nThe same HuBERT used in H-Codec is adopted to extract continuous features from audio waveforms.\nTwo linear layers serve as two adapters to map the text embedding and audio features into a representation space amenable to LM AR modeling, respectively.\nGiven text and audio embeddings as conditions,\nwe utilize LLaMA architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib64\" title=\"\">2023</a>)</cite> to predicts discrete tokens of target waveform in an AR manner.\nFinally, the H-Codec decoder reconstructs high-fidelity audio from the predicted token sequence.</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "discrete",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate multi-layer codec tokens into AR prediction, an existing method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib70\" title=\"\">2023a</a>)</cite> applies two-stage strategy:\n(i) model the tokens of the first layer in an AR manner; (ii) then, predict the tokens of remaining layers using a NAR post-network.\nHowever, this method causes additional complexity to the system. In addition, flattening all tokens into one layer leads to unbearable computational\ncost, while predicting tokens from all layers in parallel within one step deteriorates the performance.\nTherefore, we adopt the delay pattern&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib5\" title=\"\">2023</a>)</cite> to arrange our tokens for the trade-off between performance and computational cost.\nSpecifically, the 4-layer acoustic and semantic tokens produced by H-Codec are first interleaved sequentially across time steps, resulting in <math alttext=\"{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">E</mi><mi>c</mi></msub><mo>&#8712;</mo><msup><mi>&#8484;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}</annotation></semantics></math>\nwith a frame rate of 50 Hz, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> indicates the number of frames. Before feeding the tokens into the LM backbone,\ndifferent shifts are applied across layers and special pad tokens occupy empty positions, as shown in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F1\" title=\"In 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn the LM backbone, 4 embedding layers handle 4-layer tokens respectively, and the embeddings of each layer are added up as the input of transformer layers.\nThere are 4 output heads to predict the 4-layer logits of next time step. The delay pattern allows generating high-layer tokens conditioned by low-layer tokens,\nwhich improves prediction accuracy.</p>\n\n",
                "matched_terms": [
                    "while",
                    "model",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SR Mode:</span>\nThe target audio is the clean speech corresponding to the degraded input speech. The conditional sequence of LM is formatted as\n<math alttext=\"\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>SR</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">I</mi><annotation encoding=\"application/x-tex\">{\\rm I}</annotation></semantics></math> denotes the start of input audio features,\n<math alttext=\"{\\rm E}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{i}</annotation></semantics></math> the input audio embeddings, and <math alttext=\"{\\rm S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">S</mi><annotation encoding=\"application/x-tex\">{\\rm S}</annotation></semantics></math> the start of codec tokens, respectively.\nThe output sequence is formulated as <math alttext=\"{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119952;</mi><mo>=</mo><mrow><mo>[</mo><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">E</mi><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm E}^{\\prime}_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m6\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">{\\rm E}^{\\prime}_{c}</annotation></semantics></math> indicates codec tokens with\ndelay pattern, and <math alttext=\"{\\rm E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">E</mi><annotation encoding=\"application/x-tex\">{\\rm E}</annotation></semantics></math> represents the end token. The trainable parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m8\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> in the model\nare optimized by minimizing the negative log-likelihood of the predicted outputs:</p>\n\n",
                "matched_terms": [
                    "model",
                    "represents",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">rTSE Mode:</span>\nSince SS task requires generating multiple output tracks while our model only supports one-track output,\nwe include the rTSE mode during training,\nenabling the model to obtain multiple tracks through iterative inference.\nThis mode aims to extract the timbre-mismatched speech component in the mixture input when compared with\nthe reference speech. The loss function <math alttext=\"\\mathcal{L}_{\\rm rTSE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm rTSE}</annotation></semantics></math> keeps similar to that of the TSE mode,\nexcept that the task token has been replaced with <math alttext=\"{\\rm T_{rTSE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{rTSE}}</annotation></semantics></math>.\nWhen handling SS task (we only consider 2-speaker cases), we first apply the SR mode to extract the main speaker\nwith higher energy, and the other speaker is obtained by using the rTSE mode.</p>\n\n",
                "matched_terms": [
                    "only",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\rm E}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p6.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{t}</annotation></semantics></math> and <math alttext=\"{\\rm C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p6.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">C</mi><annotation encoding=\"application/x-tex\">{\\rm C}</annotation></semantics></math> denote the embedding of caption and its start token, respectively.</p>\n\n",
                "matched_terms": [
                    "denote",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span>\nWe utilize multi-domain data to train our codec, including speech, music, and audio.\nThe speech samples are sourced from the VoxBox dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite>, which comprises approximately 100k hours of speech\nand is composed of some publicly available speech datasets.\nFor the music domain, we utilize the FMA-full dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Defferrard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib7\" title=\"\">2017</a>)</cite> and the MUSDB18-HQ dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib52\" title=\"\">Rafii et&#160;al., </a>)</cite>,\ninvolving about 8k hours of data.\nFor the audio domain, we adopt AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib17\" title=\"\">2017</a>)</cite> and WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib40\" title=\"\">2024</a>)</cite>,\nincluding about 13k hours of recordings.\nWe evaluate the reconstruction quality on LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib45\" title=\"\">2015</a>)</cite> test-clean, MUSDB18-HQ test, and AudioSet eval sets for\nspeech, music, and audio domain, respectively.\nAll samples are resampled to 16k Hz.</p>\n\n",
                "matched_terms": [
                    "test",
                    "domain",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Datasets:</span>\nFor the training of speech tasks,\nwe adopt clean speech samples from the VoxBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite> dataset, including approximately 3.8k hours of data from\nLibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib45\" title=\"\">2015</a>)</cite>, MLS_English&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib49\" title=\"\">2020</a>)</cite> and Emilia_ZH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib18\" title=\"\">2024</a>)</cite> subset.\nThe noise corpus comprises approximately 460 hours of data from the DNS Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib53\" title=\"\">2020</a>)</cite>,\nFSD50K&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fonseca et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib14\" title=\"\">2022</a>)</cite>, WHAM!&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wichern et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib79\" title=\"\">2019</a>)</cite>, DESED&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Turpault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib65\" title=\"\">2019</a>)</cite>, DEMAND&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Thiemann et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib63\" title=\"\">2013</a>)</cite>, MUSAN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Snyder et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib58\" title=\"\">2015</a>)</cite>,\nDISCO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Furnon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib15\" title=\"\">2021</a>)</cite>, MUSDB18-HQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib52\" title=\"\">Rafii et&#160;al., </a>)</cite>, and TUT Urban Acoustic Scenes&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mesaros et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib42\" title=\"\">2018</a>)</cite>.\nWe include 60k room impulse response (RIR) samples from SLR28&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib27\" title=\"\">2017</a>)</cite> to simulate reverberation.\nFor the audio data, we include captioned audio samples from\nWavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib40\" title=\"\">2024</a>)</cite>, CLAP_FreeSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib81\" title=\"\">2023</a>)</cite>, VGGSound&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib3\" title=\"\">2020</a>)</cite>, and Internal data, resulting in approximately 40k hours.\nThe simulation pipeline of training samples for all operational modes are\ndescribed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A1\" title=\"Appendix A Data Simulation &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "dns",
                    "noise",
                    "subset",
                    "reverberation",
                    "challenge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "unitokaudiosrwavlm",
                    "set",
                    "unitokaudiosrhubert",
                    "unitokaudioomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate speech restoration performance on the synthetic test sets of 2020 DNS Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib53\" title=\"\">2020</a>)</cite> (including &#8220;With Reverb&#8221; and &#8220;No Reverb&#8221;) and\n2022 PLC Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diener et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib11\" title=\"\">2022</a>)</cite> blind test set.\nBaselines include Conv-TasNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>, DEMUCS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib13\" title=\"\">2019</a>)</cite>, FRCRN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib94\" title=\"\">2022</a>)</cite>, FlowSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>)</cite>,\nUniFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>)</cite>, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite>, MaskSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib34\" title=\"\">2024</a>)</cite>, AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nGenSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib85\" title=\"\">2025</a>)</cite>, Metis-SE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>,\nUniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, KuaishouNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib33\" title=\"\">2022</a>)</cite>, LPCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Valin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib66\" title=\"\">2022</a>)</cite>, PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib36\" title=\"\">2022a</a>)</cite>,\nand BS-PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib93\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "selm",
                    "gense",
                    "metisse",
                    "no",
                    "uniflow",
                    "flowse",
                    "blind",
                    "masksr",
                    "unise",
                    "demucs",
                    "with",
                    "test",
                    "frcrn",
                    "dns",
                    "anyenhance",
                    "set",
                    "llaseg1",
                    "reverb",
                    "challenge",
                    "convtasnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nThe performance of TSE is evaluated on the Libri2Mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cosentino et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib6\" title=\"\">2020</a>)</cite> clean test set.\nBaselines include Spex+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ge et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib16\" title=\"\">2020</a>)</cite>, WeSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib72\" title=\"\">2024a</a>)</cite>, TSELM-L&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib60\" title=\"\">2024</a>)</cite>,\nAnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>,\nMetis-TSE &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, LauraTSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>)</cite>, and UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "unise",
                    "anyenhance",
                    "set",
                    "llaseg1",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T6\" title=\"In 4.2.2 TSE Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the performance comparison for TSE task.\nThe results indicate that generative methods achieve higher speech quality than discriminative approaches\nbut struggle with speaker similarity.\nThis can be attributed to the upper bound limitation of codecs&#8217; reconstruction fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>.\nOur UniTok-Audio maintains comparable performance compared to SOTA baselines,\ndemonstrating the feasibility of constructing a unified framework.</p>\n\n",
                "matched_terms": [
                    "approaches",
                    "methods",
                    "generative",
                    "discriminative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate SS performance on Libri2Mix noisy test set and WSJ0-2mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib19\" title=\"\">2016</a>)</cite> test set,\nwhere the former additionally evaluates the denoising ability of models.\nBaselines include Sepformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Subakan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib59\" title=\"\">2021</a>)</cite>, Mossformer2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib95\" title=\"\">2024</a>)</cite>, and LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "set",
                    "llaseg1",
                    "additionally",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nFollowing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, we create test set for the VC task using VCTK&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib68\" title=\"\">2017</a>)</cite> dataset.\nWe randomly select 200 recordings from the dataset as source speech,\nand for each source sample, a sample from another speaker is picked as the reference speech.\nBaselines include HierSpeech++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib31\" title=\"\">2023</a>)</cite>, LM-VC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>,\nVevo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib92\" title=\"\">2024a</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe adopt 2024 DCASE LASS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://dcase.community/challenge2024/task-language-queried-audio-source-separation</span></span></span> validation set\nto evaluate the LASS performance, which contains 3k synthetic mixtures mixed from 1k audio clips.\nBaselines include LASS-Net&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib38\" title=\"\">2022b</a>)</cite> and FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "contains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T9\" title=\"In 4.2.5 LASS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, UniTok-Audio achieves competitive performance in the LASS task,\nindicating effective exploitation of textual information.\nWe prove that the unified domain codec has potential to handle the LASS tasks.\nThe single-task version outperforms UniTok-Audio<sub class=\"ltx_sub\">omni</sub>,\nwhich can be attributed to the domain gap between speech and audio.</p>\n\n",
                "matched_terms": [
                    "unitokaudioomni",
                    "domain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniTok-Audio,\na framework that resembles multiple time-aligned audio tasks.\nWe uniify different learning patterns of multiple tasks in a single framework using a special task token,\nwhich indicates current operational mode of model.\nThis paper also introduces H-Codec, achieving high-fidelity reconstruction quality with dual-stream architecture\nthat quantize acoustic and semantic features simultaneously.\nBased on H-Codec, UniTok-Audio adopts continuous conditional embeddings to generates multi-layer discrete tokens in parallel.\nExtensive experiments demonstrate that UniTok-Audio achieves competitive performance across diverse tasks\nwith limited training data and moderate model size,\nhighlighting its potential as a foundation model for unified AR audio generation.</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "discrete",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DNSMOS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib54\" title=\"\">2022</a>)</cite>:\nDNSMOS is a neural network-based MOS estimator<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS</span></span></span> that correlates strongly with human quality ratings.\nIt comprises three components: 1) speech quality (<span class=\"ltx_text ltx_font_bold\">SIG</span>), 2) background noise quality (<span class=\"ltx_text ltx_font_bold\">BAK</span>), and 3) overall quality (<span class=\"ltx_text ltx_font_bold\">OVRL</span>).\nNote that for the VC task, DNSMOS scores are calculated by averaging three components.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "scores",
                    "dnsmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T11\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a> reports the hyperparameter configurations of different UniTok-Audio versions.\nUniTok-Audio-S and UniTok-Audio-L denote the small and large version, respectively.\nThe VC performance in terms of different verisons are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T12\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, where\nall versions are trained for the single VC task using WavLM-based audio encoder.\nIt can be seen that increasing the model size consistently improves performance,\nin accordance with scaling laws.\nThis indicates the potential of UniTok-Audio to be extended to a larger model size.\nTo balance complexity and performance, we report the medium-sized verison in the main text.</p>\n\n",
                "matched_terms": [
                    "denote",
                    "model",
                    "respectively"
                ]
            }
        ]
    },
    "S4.T5": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 5: DNSMOS OVRL and PLCMOS scores on 2022 ICASSP PLC challenge blind test set.",
        "body": "Model\nType\n\nOVRL(\\uparrow)\n\nPLCMOS(\\uparrow)\n\n\nNoisy\n-\n2.56\n2.90\n\n\nKuaishouNet(Li etal., 2022)\n\nD\n-\n4.27\n\n\nLPCNet(Valin etal., 2022)\n\nD\n3.09\n3.74\n\n\nPLCNet(Liu etal., 2022a)\n\nD\n-\n3.83\n\n\nBS-PLCNet(Zhang etal., 2024b)\n\nD\n3.20\n4.29\n\n\nLLaSE-G1(Kang etal., 2025)\n\nGd\n\n3.03\n3.68\n\n\nUniTok-Audiosr-hubert\n\nGd\n\n3.30\n4.55\n\n\nUniTok-Audiosr-wavlm\n\nGd\n\n3.33\n4.55\n\n\nUniTok-Audioomni\n\nGd\n\n3.35\n4.58",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">OVRL</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">PLCMOS</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Noisy</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">2.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">2.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">KuaishouNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib33\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">D</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">4.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">LPCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Valin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib66\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">D</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3.09</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3.74</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib36\" title=\"\">2022a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">D</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3.83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">BS-PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib93\" title=\"\">2024b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">D</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">4.29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3.03</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">4.55</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">4.55</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">UniTok-Audio<sub class=\"ltx_sub\">omni</sub>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.58</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "plcnet",
            "bsplcnet",
            "noisy",
            "unitokaudiosrhubert",
            "kuaishounet",
            "unitokaudioomni",
            "dnsmos",
            "icassp",
            "plc",
            "ovrluparrow",
            "unitokaudiosrwavlm",
            "blind",
            "2024b",
            "valin",
            "lpcnet",
            "scores",
            "zhang",
            "2022a",
            "kang",
            "test",
            "plcmos",
            "plcmosuparrow",
            "set",
            "challenge",
            "llaseg1",
            "ovrl",
            "liu",
            "model",
            "type"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T4\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the SR performance comparison on 2020 DNS Challenge test sets.\nIt is clear that generative models generally outperform discriminative ones.\nContinuous-domain generative approaches perform well on the &#8220;No Reverb&#8221; subset,\nhighlighting the potential of continuous methods in terms of generated signal quality.\nHowever, discrete-domain generative approaches can perform better under reverberant conditions, indicating\nthat discrete representations may simplify the modeling difficulty of reverberation components.\nOur UniTok-Audio achieves comparable performance among SOTA baselines,\nand the single-task versions with different audio encoders result in similar performance to UniTok-Audio<sub class=\"ltx_sub\">omni</sub>.\nIn addition, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T5\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the performance on packet loss concealment (PLC),\na subtask of SR aimed at recovering speech frames lost during transmission.\nUniTok-Audio surpasses baselines in terms of both signal quality and continuity,\nshowing powerful content understanding and generation capabilities of the framework.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Leveraging the remarkable sequential generation capability of language model (LM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib67\" title=\"\">2017</a>)</cite>,\nrecent works have achieved significant improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib48\" title=\"\">2024</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib35\" title=\"\">2023</a>)</cite>,\npromoting the growing prevalence of artificial intelligence-generated content (AIGC).\nThese advances have inspired substantial research extending LMs to various audio tasks,\nwhich can be fundamentally categorized by the temporal relationship between input and output:\neither <span class=\"ltx_text ltx_font_italic\">time-aligned</span> (TA) or <span class=\"ltx_text ltx_font_italic\">non-time-aligned</span> (NTA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib82\" title=\"\">2025</a>)</cite>.\nThe former involves strict temporal correspondence between input and output signals, such as speech denoising,\nwhich aligns speech components in each frame between noisy and clean speech.\nWhile the latter dose not require point-wise temporal alignment, such as text-to-audio (TTA),\nwhich aims at semantic coherence between the holistical textual description and entire output soundscape.</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some studies aim to unify multiple tasks within a single framework, including AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>, UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.\nThese methods utilizes the LM backbone combined with discrete audio codec and\nexhibit remarkable generative ability, which benefit from the semantic understanding and contextual modeling capabilities of LMs.\nHowever, challenges still exist in terms of audio quality and generalization ability across tasks.\nFor instance, few unified models are capable of handling the SS task, as it generally requires customized\narchitecture to output multi-track speech.</p>\n\n",
                "matched_terms": [
                    "zhang",
                    "kang",
                    "llaseg1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Framework</span>: The framework unifies tasks by taking task-specific conditional information as the conditioning sequence of decoder-only LM,\nand the discrete token of target audio is predicted in an AR manner.\nWe utilize a special task token to distinguish different learning patterns of multiple tasks.\nNote that our model handles diverse tasks using a single set of shared weights, thereby eliminating the need for task-specific weight adaptation.</p>\n\n",
                "matched_terms": [
                    "set",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the domain of TA audio tasks, early researches focus on discriminative modeling,\nwhich directly learns the mapping between input signal and target audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Williamson &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib80\" title=\"\">2017</a>; Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>.\nHowever, the lack of generative ability limits their generalization in unseen scenarios and\nthe performance in extreme situations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Welker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib78\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib71\" title=\"\">2020</a>)</cite>.\nMany studies integrate generative modeling into audio tasks in recent years.\nFor the SR task, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite> applies k-means to quantize noisy speech representations obtained\nby WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib4\" title=\"\">2022</a>)</cite> into discrete tokens,\nand then a Transformer-based speech LM maps the noisy tokens to clean tokens.\nFor the LASS task, FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>\nlearns linear flow trajectories from noise to target source features within the variational autoencoder (VAE) latent space,\nwhich are guided by the encoded text embeddings and the mixture audio.\nHowever, these models are designed for specific task, facing limited extensibility when migrating to more tasks.</p>\n\n",
                "matched_terms": [
                    "2024b",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the development of LM, the research focus of codecs has gradually shifted\nfrom reducing data transmission costs toward the integration with LM, which ensures the high quality of generated audio.\nThis requires codecs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib37\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib10\" title=\"\">2024</a>)</cite> to preserve more semantic information that can be understood and modeled by LM.\nX-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib86\" title=\"\">2024a</a>)</cite> integrates the representations from the pre-trained SSL model to enhance semantic preservation,\nimproving both reconstruction quality and downstream TTS performance.\nSome studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib24\" title=\"\">2025</a>)</cite> explore single-layer codecs that are more suitable for autoregressive modeling in LM.\nX-Codec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite> utilizes finite scalar quantization (FSQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib41\" title=\"\">2024</a>)</cite> to perform single-layer quantization,\nenlarging the code space. BiCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite> generates a hybrid token stream combining semantic and global tokens,\nwhich are derived from a SSL model and a speaker verification model, respectively.\nHowever, single-layer codecs with a low frame rate still faces challenges in high-fidelity reconstruction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite>, e.g., speaker similarity.</p>\n\n",
                "matched_terms": [
                    "liu",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines:</span>\nWe compare our codec against some state-of-the-art (SOTA) baselines,\nincluding DAC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib29\" title=\"\">2023</a>)</cite>, Encodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib8\" title=\"\">2022a</a>)</cite>, X-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib87\" title=\"\">2024b</a>)</cite>,\nX-Codec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite>, BiCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite>, WavTokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>)</cite>,\nand UniCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib24\" title=\"\">2025</a>)</cite>.\nAll results are obtained using their official checkpoints.</p>\n\n",
                "matched_terms": [
                    "2024b",
                    "2022a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "unitokaudiosrwavlm",
                    "set",
                    "unitokaudiosrhubert",
                    "unitokaudioomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics:</span>\nWe adopt multiple evaluation metrics to assess different aspects of the generated audio across tasks.\nFor speech tasks, we evaluate quality by DNSMOS (SIG, BAK, OVRL) and NISQA, speaker similarity by SIM,\nintelligibility by WER, and continuity by PLCMOS.\nFor the LASS task, we utilize FAD, CLAPScore, and CLAPScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> to measure the audio separation performance.\nDetails about evaluation metrics can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A2.SS2\" title=\"B.2 Audio Task Metrics &#8227; Appendix B Evaluation Metrics &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "ovrl",
                    "plcmos",
                    "dnsmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate speech restoration performance on the synthetic test sets of 2020 DNS Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib53\" title=\"\">2020</a>)</cite> (including &#8220;With Reverb&#8221; and &#8220;No Reverb&#8221;) and\n2022 PLC Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diener et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib11\" title=\"\">2022</a>)</cite> blind test set.\nBaselines include Conv-TasNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>, DEMUCS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib13\" title=\"\">2019</a>)</cite>, FRCRN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib94\" title=\"\">2022</a>)</cite>, FlowSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>)</cite>,\nUniFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>)</cite>, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite>, MaskSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib34\" title=\"\">2024</a>)</cite>, AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nGenSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib85\" title=\"\">2025</a>)</cite>, Metis-SE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>,\nUniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, KuaishouNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib33\" title=\"\">2022</a>)</cite>, LPCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Valin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib66\" title=\"\">2022</a>)</cite>, PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib36\" title=\"\">2022a</a>)</cite>,\nand BS-PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib93\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "plcnet",
                    "bsplcnet",
                    "blind",
                    "2024b",
                    "set",
                    "llaseg1",
                    "zhang",
                    "kuaishounet",
                    "valin",
                    "lpcnet",
                    "liu",
                    "2022a",
                    "challenge",
                    "plc",
                    "kang",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nThe performance of TSE is evaluated on the Libri2Mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cosentino et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib6\" title=\"\">2020</a>)</cite> clean test set.\nBaselines include Spex+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ge et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib16\" title=\"\">2020</a>)</cite>, WeSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib72\" title=\"\">2024a</a>)</cite>, TSELM-L&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib60\" title=\"\">2024</a>)</cite>,\nAnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>,\nMetis-TSE &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, LauraTSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>)</cite>, and UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "llaseg1",
                    "zhang",
                    "kang",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate SS performance on Libri2Mix noisy test set and WSJ0-2mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib19\" title=\"\">2016</a>)</cite> test set,\nwhere the former additionally evaluates the denoising ability of models.\nBaselines include Sepformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Subakan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib59\" title=\"\">2021</a>)</cite>, Mossformer2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib95\" title=\"\">2024</a>)</cite>, and LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "set",
                    "llaseg1",
                    "kang",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nFollowing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, we create test set for the VC task using VCTK&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib68\" title=\"\">2017</a>)</cite> dataset.\nWe randomly select 200 recordings from the dataset as source speech,\nand for each source sample, a sample from another speaker is picked as the reference speech.\nBaselines include HierSpeech++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib31\" title=\"\">2023</a>)</cite>, LM-VC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>,\nVevo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib92\" title=\"\">2024a</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "zhang",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe adopt 2024 DCASE LASS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://dcase.community/challenge2024/task-language-queried-audio-source-separation</span></span></span> validation set\nto evaluate the LASS performance, which contains 3k synthetic mixtures mixed from 1k audio clips.\nBaselines include LASS-Net&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib38\" title=\"\">2022b</a>)</cite> and FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "liu",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DNSMOS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib54\" title=\"\">2022</a>)</cite>:\nDNSMOS is a neural network-based MOS estimator<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS</span></span></span> that correlates strongly with human quality ratings.\nIt comprises three components: 1) speech quality (<span class=\"ltx_text ltx_font_bold\">SIG</span>), 2) background noise quality (<span class=\"ltx_text ltx_font_bold\">BAK</span>), and 3) overall quality (<span class=\"ltx_text ltx_font_bold\">OVRL</span>).\nNote that for the VC task, DNSMOS scores are calculated by averaging three components.</p>\n\n",
                "matched_terms": [
                    "ovrl",
                    "scores",
                    "dnsmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PLCMOS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diener et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib12\" title=\"\">2023</a>)</cite>:\nA metric<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span>https://github.com/microsoft/PLC-Challenge/tree/main/PLCMOS</span></span></span> designed to evaluate the quality of speech enhanced by PLC algorithms,\noutputting a single score ranging from 1 to 5 (higher is better).</p>\n\n",
                "matched_terms": [
                    "plc",
                    "plcmos"
                ]
            }
        ]
    },
    "S4.T6": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 6: TSE results on Libri2Mix clean test set.",
        "body": "Model\nType\n\nSIG(\\uparrow)\n\nBAK(\\uparrow)\n\nOVRL(\\uparrow)\n\nNISQA(\\uparrow)\n\nSIM(\\uparrow)\n\n\nMixture\n-\n3.38\n3.10\n2.65\n2.45\n0.85\n\n\nSpex+\nD\n3.38\n3.77\n3.00\n3.03\n0.96\n\n\nWeSep\nD\n3.56\n3.93\n3.23\n4.04\n0.99\n\n\nTSELM-L\nGd\n\n3.55\n4.08\n3.23\n4.03\n0.91\n\n\nAnyEnhance\nGd\n\n3.64\n4.07\n3.35\n4.28\n0.91\n\n\nLLaSE-G1\nGd\n\n3.53\n4.01\n3.22\n3.89\n0.92\n\n\nMetis-TSE\nGd\n\n3.65\n4.08\n3.34\n4.36\n-\n\n\nLauraTSE\nGd\n\n3.61\n4.08\n3.34\n4.33\n0.97\n\n\nUniSE\nGd\n\n3.62\n4.06\n3.33\n4.00\n0.95\n\n\nUniTok-Audiotse-hubert\n\nGd\n\n3.58\n4.03\n3.31\n3.97\n0.95\n\n\nUniTok-Audiotse-wavlm\n\nGd\n\n3.60\n4.04\n3.32\n3.99\n0.95\n\n\nUniTok-Audioomni\n\nGd\n\n3.62\n4.05\n3.32\n4.00\n0.95",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">SIG</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">BAK</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">OVRL</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">NISQA</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Mixture</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Spex+</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">D</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.96</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">WeSep</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">D</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.99</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">TSELM-L</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.55</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.03</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">AnyEnhance</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.35</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">LLaSE-G1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.89</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.92</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Metis-TSE</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.34</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.36</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">LauraTSE</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.34</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.97</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniSE</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.62</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.06</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.95</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniTok-Audio<sub class=\"ltx_sub\">tse-hubert</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.95</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniTok-Audio<sub class=\"ltx_sub\">tse-wavlm</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.99</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.95</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniTok-Audio<sub class=\"ltx_sub\">omni</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.95</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "lauratse",
            "unitokaudioomni",
            "tselml",
            "wesep",
            "ovrluparrow",
            "clean",
            "metistse",
            "tse",
            "results",
            "unitokaudiotsehubert",
            "nisqauparrow",
            "spex",
            "unise",
            "libri2mix",
            "unitokaudiotsewavlm",
            "simuparrow",
            "mixture",
            "test",
            "anyenhance",
            "bakuparrow",
            "set",
            "siguparrow",
            "llaseg1",
            "model",
            "type"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T6\" title=\"In 4.2.2 TSE Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the performance comparison for TSE task.\nThe results indicate that generative methods achieve higher speech quality than discriminative approaches\nbut struggle with speaker similarity.\nThis can be attributed to the upper bound limitation of codecs&#8217; reconstruction fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>.\nOur UniTok-Audio maintains comparable performance compared to SOTA baselines,\ndemonstrating the feasibility of constructing a unified framework.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Leveraging the remarkable sequential generation capability of language model (LM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib67\" title=\"\">2017</a>)</cite>,\nrecent works have achieved significant improvements in generation quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib48\" title=\"\">2024</a>; Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib35\" title=\"\">2023</a>)</cite>,\npromoting the growing prevalence of artificial intelligence-generated content (AIGC).\nThese advances have inspired substantial research extending LMs to various audio tasks,\nwhich can be fundamentally categorized by the temporal relationship between input and output:\neither <span class=\"ltx_text ltx_font_italic\">time-aligned</span> (TA) or <span class=\"ltx_text ltx_font_italic\">non-time-aligned</span> (NTA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib82\" title=\"\">2025</a>)</cite>.\nThe former involves strict temporal correspondence between input and output signals, such as speech denoising,\nwhich aligns speech components in each frame between noisy and clean speech.\nWhile the latter dose not require point-wise temporal alignment, such as text-to-audio (TTA),\nwhich aims at semantic coherence between the holistical textual description and entire output soundscape.</p>\n\n",
                "matched_terms": [
                    "model",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study focuses on the TA tasks, especially which provides the input audio that temporally aligned with the output audio at the frame level,\nincluding: speech restoration (SR) that aims at restoring speech from the degraded recording with various distortions (e.g., noise, reverberation,and packet loss);\ntarget speaker extraction (TSE) that extracts target speech from mixture using assistive clues (e.g., voiceprint information from reference speech);\nspeech separation (SS) that aims to separate all existing speaker in the mixture;\nvoice conversion (VC) that transforms the timbre of source speech guided by reference speech of another speaker;\nlanguage-queried audio source separation (LASS) that aims at extracting target audio components from mixture, which are consistent with the given textual caption.\nNumerous generative models are developed for these tasks, while most of them are designed for single task with\ntask-specific architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.</p>\n\n",
                "matched_terms": [
                    "tse",
                    "results",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some studies aim to unify multiple tasks within a single framework, including AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>, UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.\nThese methods utilizes the LM backbone combined with discrete audio codec and\nexhibit remarkable generative ability, which benefit from the semantic understanding and contextual modeling capabilities of LMs.\nHowever, challenges still exist in terms of audio quality and generalization ability across tasks.\nFor instance, few unified models are capable of handling the SS task, as it generally requires customized\narchitecture to output multi-track speech.</p>\n\n",
                "matched_terms": [
                    "unise",
                    "anyenhance",
                    "llaseg1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Framework</span>: The framework unifies tasks by taking task-specific conditional information as the conditioning sequence of decoder-only LM,\nand the discrete token of target audio is predicted in an AR manner.\nWe utilize a special task token to distinguish different learning patterns of multiple tasks.\nNote that our model handles diverse tasks using a single set of shared weights, thereby eliminating the need for task-specific weight adaptation.</p>\n\n",
                "matched_terms": [
                    "set",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the domain of TA audio tasks, early researches focus on discriminative modeling,\nwhich directly learns the mapping between input signal and target audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Williamson &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib80\" title=\"\">2017</a>; Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>.\nHowever, the lack of generative ability limits their generalization in unseen scenarios and\nthe performance in extreme situations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Welker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib78\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib71\" title=\"\">2020</a>)</cite>.\nMany studies integrate generative modeling into audio tasks in recent years.\nFor the SR task, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite> applies k-means to quantize noisy speech representations obtained\nby WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib4\" title=\"\">2022</a>)</cite> into discrete tokens,\nand then a Transformer-based speech LM maps the noisy tokens to clean tokens.\nFor the LASS task, FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>\nlearns linear flow trajectories from noise to target source features within the variational autoencoder (VAE) latent space,\nwhich are guided by the encoded text embeddings and the mixture audio.\nHowever, these models are designed for specific task, facing limited extensibility when migrating to more tasks.</p>\n\n",
                "matched_terms": [
                    "clean",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SR Mode:</span>\nThe target audio is the clean speech corresponding to the degraded input speech. The conditional sequence of LM is formatted as\n<math alttext=\"\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>SR</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">I</mi><annotation encoding=\"application/x-tex\">{\\rm I}</annotation></semantics></math> denotes the start of input audio features,\n<math alttext=\"{\\rm E}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{i}</annotation></semantics></math> the input audio embeddings, and <math alttext=\"{\\rm S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">S</mi><annotation encoding=\"application/x-tex\">{\\rm S}</annotation></semantics></math> the start of codec tokens, respectively.\nThe output sequence is formulated as <math alttext=\"{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119952;</mi><mo>=</mo><mrow><mo>[</mo><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">E</mi><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm E}^{\\prime}_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m6\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">{\\rm E}^{\\prime}_{c}</annotation></semantics></math> indicates codec tokens with\ndelay pattern, and <math alttext=\"{\\rm E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">E</mi><annotation encoding=\"application/x-tex\">{\\rm E}</annotation></semantics></math> represents the end token. The trainable parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m8\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> in the model\nare optimized by minimizing the negative log-likelihood of the predicted outputs:</p>\n\n",
                "matched_terms": [
                    "model",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TSE Mode:</span>\nThe target audio corresponds to the timbre-matched speech component in the input mixture audio that aligns with the reference audio.\nThe conditional sequence is formatted as <math alttext=\"\\left[{\\rm T_{TSE}},{\\rm R},{\\rm E}_{r},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>TSE</mi></msub><mo>,</mo><mi mathvariant=\"normal\">R</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>r</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{TSE}},{\\rm R},{\\rm E}_{r},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>,\nwhere <math alttext=\"{\\rm E}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{r}</annotation></semantics></math> and <math alttext=\"{\\rm R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">R</mi><annotation encoding=\"application/x-tex\">{\\rm R}</annotation></semantics></math> represent the features of reference speech and its start token, respectively.\nTherefore, the associated loss function is defined as</p>\n\n",
                "matched_terms": [
                    "tse",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">rTSE Mode:</span>\nSince SS task requires generating multiple output tracks while our model only supports one-track output,\nwe include the rTSE mode during training,\nenabling the model to obtain multiple tracks through iterative inference.\nThis mode aims to extract the timbre-mismatched speech component in the mixture input when compared with\nthe reference speech. The loss function <math alttext=\"\\mathcal{L}_{\\rm rTSE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm rTSE}</annotation></semantics></math> keeps similar to that of the TSE mode,\nexcept that the task token has been replaced with <math alttext=\"{\\rm T_{rTSE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{rTSE}}</annotation></semantics></math>.\nWhen handling SS task (we only consider 2-speaker cases), we first apply the SR mode to extract the main speaker\nwith higher energy, and the other speaker is obtained by using the rTSE mode.</p>\n\n",
                "matched_terms": [
                    "tse",
                    "model",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "results",
                    "unitokaudioomni",
                    "set",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate speech restoration performance on the synthetic test sets of 2020 DNS Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib53\" title=\"\">2020</a>)</cite> (including &#8220;With Reverb&#8221; and &#8220;No Reverb&#8221;) and\n2022 PLC Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diener et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib11\" title=\"\">2022</a>)</cite> blind test set.\nBaselines include Conv-TasNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>, DEMUCS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib13\" title=\"\">2019</a>)</cite>, FRCRN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib94\" title=\"\">2022</a>)</cite>, FlowSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>)</cite>,\nUniFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>)</cite>, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite>, MaskSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib34\" title=\"\">2024</a>)</cite>, AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nGenSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib85\" title=\"\">2025</a>)</cite>, Metis-SE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>,\nUniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, KuaishouNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib33\" title=\"\">2022</a>)</cite>, LPCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Valin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib66\" title=\"\">2022</a>)</cite>, PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib36\" title=\"\">2022a</a>)</cite>,\nand BS-PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib93\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "unise",
                    "anyenhance",
                    "set",
                    "llaseg1",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T4\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the SR performance comparison on 2020 DNS Challenge test sets.\nIt is clear that generative models generally outperform discriminative ones.\nContinuous-domain generative approaches perform well on the &#8220;No Reverb&#8221; subset,\nhighlighting the potential of continuous methods in terms of generated signal quality.\nHowever, discrete-domain generative approaches can perform better under reverberant conditions, indicating\nthat discrete representations may simplify the modeling difficulty of reverberation components.\nOur UniTok-Audio achieves comparable performance among SOTA baselines,\nand the single-task versions with different audio encoders result in similar performance to UniTok-Audio<sub class=\"ltx_sub\">omni</sub>.\nIn addition, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T5\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the performance on packet loss concealment (PLC),\na subtask of SR aimed at recovering speech frames lost during transmission.\nUniTok-Audio surpasses baselines in terms of both signal quality and continuity,\nshowing powerful content understanding and generation capabilities of the framework.</p>\n\n",
                "matched_terms": [
                    "results",
                    "unitokaudioomni",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nThe performance of TSE is evaluated on the Libri2Mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cosentino et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib6\" title=\"\">2020</a>)</cite> clean test set.\nBaselines include Spex+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ge et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib16\" title=\"\">2020</a>)</cite>, WeSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib72\" title=\"\">2024a</a>)</cite>, TSELM-L&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib60\" title=\"\">2024</a>)</cite>,\nAnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>,\nMetis-TSE &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, LauraTSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>)</cite>, and UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "lauratse",
                    "spex",
                    "clean",
                    "metistse",
                    "unise",
                    "anyenhance",
                    "set",
                    "libri2mix",
                    "llaseg1",
                    "tse",
                    "tselml",
                    "wesep",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate SS performance on Libri2Mix noisy test set and WSJ0-2mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib19\" title=\"\">2016</a>)</cite> test set,\nwhere the former additionally evaluates the denoising ability of models.\nBaselines include Sepformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Subakan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib59\" title=\"\">2021</a>)</cite>, Mossformer2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib95\" title=\"\">2024</a>)</cite>, and LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "llaseg1",
                    "set",
                    "libri2mix",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T7\" title=\"In 4.2.3 SS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the performance comparison for SS task, showing that our model achieves superior performance than baselines.\nThis verifies the effectiveness of our iterative inference strategy in handling the SS task that requires multiple output tracks.\nNote that although the experiments are conducted with the 2-speaker configuration,\nour approach can be extended to scenarios with more sources when the target signal of rTSE mode is defined as all remaining speakers.\nThe single-task version is not reported since the inference phase of SS requires the cooperation of multiple modes.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nFollowing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, we create test set for the VC task using VCTK&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib68\" title=\"\">2017</a>)</cite> dataset.\nWe randomly select 200 recordings from the dataset as source speech,\nand for each source sample, a sample from another speaker is picked as the reference speech.\nBaselines include HierSpeech++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib31\" title=\"\">2023</a>)</cite>, LM-VC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>,\nVevo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib92\" title=\"\">2024a</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T9\" title=\"In 4.2.5 LASS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, UniTok-Audio achieves competitive performance in the LASS task,\nindicating effective exploitation of textual information.\nWe prove that the unified domain codec has potential to handle the LASS tasks.\nThe single-task version outperforms UniTok-Audio<sub class=\"ltx_sub\">omni</sub>,\nwhich can be attributed to the domain gap between speech and audio.</p>\n\n",
                "matched_terms": [
                    "results",
                    "unitokaudioomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A data simulation pipeline is designed to synthesis data pairs dynamically during training.\nConsidering various types of degradation in the SR task,\nwe apply multiple distortions to a speech sample with independent probabilities,\nwhere the distortion categories and corresponding configurations are shown in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A1.T10\" title=\"In Appendix A Data Simulation &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nThe distortion chain is also applied to the TSE and rTSE modes,\nexcept that the probability of interfering speaker is set to 1.0 and the SIR is uniformly sampled between\n-5 and 5 dB.\nFor the LASS mode, we mix the target audio with another randomly selected audio using a SIR ranges from -5 to 20 dB.\nFor the VC mode, we leverage a voice conversion model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/myshell-ai/OpenVoice</span></span></span> to perform timbre perturbation using randomly selected target speech and reference speech,\ngenerating 6k hours of fixed training dataset.\nThe perturbed sample is used as input to predict the target speech based on another speech of the target speaker.</p>\n\n",
                "matched_terms": [
                    "tse",
                    "set"
                ]
            }
        ]
    },
    "S4.T7": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 7: SS results on Libri2Mix and WSJ0-2mix test sets.",
        "body": "Model\nType\nLibri2Mix\nWSJ0-2mix\n\n\n\nSIG(\\uparrow)\n\nBAK(\\uparrow)\n\nOVRL(\\uparrow)\n\nSIG(\\uparrow)\n\nBAK(\\uparrow)\n\nOVRL(\\uparrow)\n\n\nMixture\n-\n2.33\n1.66\n1.64\n3.42\n3.20\n2.76\n\n\nSepformer(Subakan etal., 2021)\n\nD\n3.33\n3.88\n3.02\n3.43\n3.96\n3.14\n\n\nMossformer2(Zhao etal., 2024)\n\nD\n3.44\n3.94\n3.11\n3.50\n4.05\n3.23\n\n\nLLaSE-G1(Kang etal., 2025)\n\nGd\n\n3.48\n3.83\n3.11\n3.52\n3.92\n3.19\n\n\nUniTok-Audioomni\n\nGd\n\n3.56\n4.04\n3.25\n3.57\n3.96\n3.26",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Libri2Mix</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">WSJ0-2mix</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIG</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">BAK</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">OVRL</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SIG</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">BAK</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">OVRL</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Mixture</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">2.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">1.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">1.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">3.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">3.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">2.76</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Sepformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Subakan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib59\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">D</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Mossformer2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib95\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">D</td>\n<td class=\"ltx_td ltx_align_center\">3.44</td>\n<td class=\"ltx_td ltx_align_center\">3.94</td>\n<td class=\"ltx_td ltx_align_center\">3.11</td>\n<td class=\"ltx_td ltx_align_center\">3.50</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.05</span></td>\n<td class=\"ltx_td ltx_align_center\">3.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\">3.48</td>\n<td class=\"ltx_td ltx_align_center\">3.83</td>\n<td class=\"ltx_td ltx_align_center\">3.11</td>\n<td class=\"ltx_td ltx_align_center\">3.52</td>\n<td class=\"ltx_td ltx_align_center\">3.92</td>\n<td class=\"ltx_td ltx_align_center\">3.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">UniTok-Audio<sub class=\"ltx_sub\">omni</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">3.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.26</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "sets",
            "ovrluparrow",
            "bakuparrow",
            "model",
            "siguparrow",
            "libri2mix",
            "mixture",
            "sepformer",
            "results",
            "mossformer2",
            "zhao",
            "llaseg1",
            "unitokaudioomni",
            "subakan",
            "wsj02mix",
            "kang",
            "type",
            "test"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T7\" title=\"In 4.2.3 SS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the performance comparison for SS task, showing that our model achieves superior performance than baselines.\nThis verifies the effectiveness of our iterative inference strategy in handling the SS task that requires multiple output tracks.\nNote that although the experiments are conducted with the 2-speaker configuration,\nour approach can be extended to scenarios with more sources when the target signal of rTSE mode is defined as all remaining speakers.\nThe single-task version is not reported since the inference phase of SS requires the cooperation of multiple modes.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This study focuses on the TA tasks, especially which provides the input audio that temporally aligned with the output audio at the frame level,\nincluding: speech restoration (SR) that aims at restoring speech from the degraded recording with various distortions (e.g., noise, reverberation,and packet loss);\ntarget speaker extraction (TSE) that extracts target speech from mixture using assistive clues (e.g., voiceprint information from reference speech);\nspeech separation (SS) that aims to separate all existing speaker in the mixture;\nvoice conversion (VC) that transforms the timbre of source speech guided by reference speech of another speaker;\nlanguage-queried audio source separation (LASS) that aims at extracting target audio components from mixture, which are consistent with the given textual caption.\nNumerous generative models are developed for these tasks, while most of them are designed for single task with\ntask-specific architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.</p>\n\n",
                "matched_terms": [
                    "results",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some studies aim to unify multiple tasks within a single framework, including AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>, UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.\nThese methods utilizes the LM backbone combined with discrete audio codec and\nexhibit remarkable generative ability, which benefit from the semantic understanding and contextual modeling capabilities of LMs.\nHowever, challenges still exist in terms of audio quality and generalization ability across tasks.\nFor instance, few unified models are capable of handling the SS task, as it generally requires customized\narchitecture to output multi-track speech.</p>\n\n",
                "matched_terms": [
                    "kang",
                    "llaseg1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">rTSE Mode:</span>\nSince SS task requires generating multiple output tracks while our model only supports one-track output,\nwe include the rTSE mode during training,\nenabling the model to obtain multiple tracks through iterative inference.\nThis mode aims to extract the timbre-mismatched speech component in the mixture input when compared with\nthe reference speech. The loss function <math alttext=\"\\mathcal{L}_{\\rm rTSE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm rTSE}</annotation></semantics></math> keeps similar to that of the TSE mode,\nexcept that the task token has been replaced with <math alttext=\"{\\rm T_{rTSE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{rTSE}}</annotation></semantics></math>.\nWhen handling SS task (we only consider 2-speaker cases), we first apply the SR mode to extract the main speaker\nwith higher energy, and the other speaker is obtained by using the rTSE mode.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span>\nWe utilize multi-domain data to train our codec, including speech, music, and audio.\nThe speech samples are sourced from the VoxBox dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite>, which comprises approximately 100k hours of speech\nand is composed of some publicly available speech datasets.\nFor the music domain, we utilize the FMA-full dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Defferrard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib7\" title=\"\">2017</a>)</cite> and the MUSDB18-HQ dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib52\" title=\"\">Rafii et&#160;al., </a>)</cite>,\ninvolving about 8k hours of data.\nFor the audio domain, we adopt AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib17\" title=\"\">2017</a>)</cite> and WavCaps&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib40\" title=\"\">2024</a>)</cite>,\nincluding about 13k hours of recordings.\nWe evaluate the reconstruction quality on LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib45\" title=\"\">2015</a>)</cite> test-clean, MUSDB18-HQ test, and AudioSet eval sets for\nspeech, music, and audio domain, respectively.\nAll samples are resampled to 16k Hz.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "results",
                    "unitokaudioomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate speech restoration performance on the synthetic test sets of 2020 DNS Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib53\" title=\"\">2020</a>)</cite> (including &#8220;With Reverb&#8221; and &#8220;No Reverb&#8221;) and\n2022 PLC Challenge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Diener et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib11\" title=\"\">2022</a>)</cite> blind test set.\nBaselines include Conv-TasNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>, DEMUCS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib13\" title=\"\">2019</a>)</cite>, FRCRN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib94\" title=\"\">2022</a>)</cite>, FlowSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>)</cite>,\nUniFlow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>)</cite>, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite>, MaskSR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib34\" title=\"\">2024</a>)</cite>, AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nGenSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib85\" title=\"\">2025</a>)</cite>, Metis-SE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>,\nUniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, KuaishouNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib33\" title=\"\">2022</a>)</cite>, LPCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Valin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib66\" title=\"\">2022</a>)</cite>, PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib36\" title=\"\">2022a</a>)</cite>,\nand BS-PLCNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib93\" title=\"\">2024b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "llaseg1",
                    "zhao",
                    "kang",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T4\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the SR performance comparison on 2020 DNS Challenge test sets.\nIt is clear that generative models generally outperform discriminative ones.\nContinuous-domain generative approaches perform well on the &#8220;No Reverb&#8221; subset,\nhighlighting the potential of continuous methods in terms of generated signal quality.\nHowever, discrete-domain generative approaches can perform better under reverberant conditions, indicating\nthat discrete representations may simplify the modeling difficulty of reverberation components.\nOur UniTok-Audio achieves comparable performance among SOTA baselines,\nand the single-task versions with different audio encoders result in similar performance to UniTok-Audio<sub class=\"ltx_sub\">omni</sub>.\nIn addition, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T5\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the performance on packet loss concealment (PLC),\na subtask of SR aimed at recovering speech frames lost during transmission.\nUniTok-Audio surpasses baselines in terms of both signal quality and continuity,\nshowing powerful content understanding and generation capabilities of the framework.</p>\n\n",
                "matched_terms": [
                    "results",
                    "sets",
                    "test",
                    "unitokaudioomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nThe performance of TSE is evaluated on the Libri2Mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cosentino et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib6\" title=\"\">2020</a>)</cite> clean test set.\nBaselines include Spex+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ge et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib16\" title=\"\">2020</a>)</cite>, WeSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib72\" title=\"\">2024a</a>)</cite>, TSELM-L&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib60\" title=\"\">2024</a>)</cite>,\nAnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>,\nMetis-TSE &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, LauraTSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>)</cite>, and UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "test",
                    "kang",
                    "libri2mix",
                    "llaseg1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe evaluate SS performance on Libri2Mix noisy test set and WSJ0-2mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hershey et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib19\" title=\"\">2016</a>)</cite> test set,\nwhere the former additionally evaluates the denoising ability of models.\nBaselines include Sepformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Subakan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib59\" title=\"\">2021</a>)</cite>, Mossformer2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib95\" title=\"\">2024</a>)</cite>, and LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "llaseg1",
                    "libri2mix",
                    "sepformer",
                    "zhao",
                    "mossformer2",
                    "subakan",
                    "wsj02mix",
                    "kang",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T9\" title=\"In 4.2.5 LASS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, UniTok-Audio achieves competitive performance in the LASS task,\nindicating effective exploitation of textual information.\nWe prove that the unified domain codec has potential to handle the LASS tasks.\nThe single-task version outperforms UniTok-Audio<sub class=\"ltx_sub\">omni</sub>,\nwhich can be attributed to the domain gap between speech and audio.</p>\n\n",
                "matched_terms": [
                    "results",
                    "unitokaudioomni"
                ]
            }
        ]
    },
    "S4.T8": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 8: Performance comparison on the VC task.",
        "body": "Model\nType\n\nWER(\\downarrow)\n\nSIM(\\uparrow)\n\nDNSMOS(\\uparrow)\n\nNISQA(\\uparrow)\n\n\nHierSpeech++\nGc\n\n4.87\n0.38\n3.40\n3.79\n\n\nLM-VC\nGd\n\n8.35\n0.29\n3.46\n3.93\n\n\nUniAudio\nGd\n\n9.00\n0.25\n3.47\n4.28\n\n\nVevo\nGc\n\n3.48\n0.38\n3.47\n4.30\n\n\nMetis-VC\nGd\n\n4.49\n0.50\n3.48\n4.46\n\n\nUniTok-Audiovc-hubert\n\nGd\n\n4.15\n0.48\n3.42\n4.43\n\n\nUniTok-Audiovc-wavlm\n\nGd\n\n3.02\n0.51\n3.46\n4.46\n\n\nUniTok-Audioomni\n\nGd\n\n4.23\n0.50\n3.51\n4.51",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">WER</span>(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">DNSMOS</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">NISQA</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">HierSpeech++</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">G<sub class=\"ltx_sub\">c</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">4.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">0.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">3.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">3.79</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LM-VC</td>\n<td class=\"ltx_td ltx_align_center\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.35</td>\n<td class=\"ltx_td ltx_align_center\">0.29</td>\n<td class=\"ltx_td ltx_align_center\">3.46</td>\n<td class=\"ltx_td ltx_align_center\">3.93</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">UniAudio</td>\n<td class=\"ltx_td ltx_align_center\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\">9.00</td>\n<td class=\"ltx_td ltx_align_center\">0.25</td>\n<td class=\"ltx_td ltx_align_center\">3.47</td>\n<td class=\"ltx_td ltx_align_center\">4.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Vevo</td>\n<td class=\"ltx_td ltx_align_center\">G<sub class=\"ltx_sub\">c</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\">3.48</td>\n<td class=\"ltx_td ltx_align_center\">0.38</td>\n<td class=\"ltx_td ltx_align_center\">3.47</td>\n<td class=\"ltx_td ltx_align_center\">4.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Metis-VC</td>\n<td class=\"ltx_td ltx_align_center\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\">4.49</td>\n<td class=\"ltx_td ltx_align_center\">0.50</td>\n<td class=\"ltx_td ltx_align_center\">3.48</td>\n<td class=\"ltx_td ltx_align_center\">4.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">UniTok-Audio<sub class=\"ltx_sub\">vc-hubert</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">UniTok-Audio<sub class=\"ltx_sub\">vc-wavlm</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.02</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.51</span></td>\n<td class=\"ltx_td ltx_align_center\">3.46</td>\n<td class=\"ltx_td ltx_align_center\">4.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">UniTok-Audio<sub class=\"ltx_sub\">omni</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.51</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "werdownarrow",
            "dnsmosuparrow",
            "uniaudio",
            "performance",
            "task",
            "vevo",
            "comparison",
            "unitokaudioomni",
            "nisqauparrow",
            "unitokaudiovcwavlm",
            "hierspeech",
            "lmvc",
            "simuparrow",
            "metisvc",
            "unitokaudiovchubert",
            "model",
            "type"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nVC results are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T8\" title=\"In 4.2.4 VC Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, showing the superiority of UniTok-Audio in\nspeech quality, speaker similarity, and intelligibility.\nWe observe that UniTok-Audio<sub class=\"ltx_sub\">vc-wavlm</sub> outperforms UniTok-Audio<sub class=\"ltx_sub\">vc-hubert</sub>, indicating that\nWavLM performs better in extracting semantic information and speaker characteristics.\nThe performance degrades when extending to multiple tasks from single-task version,\nimplying the distinct pattern between VC and other tasks,\nwhere the former changes the property of the input signal rather than restoring or extracting certain components.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative modeling has recently achieved remarkable success across text, image, and audio domains,\ndemonstrating powerful capabilities for unified representation learning.\nHowever, audio generation models still face challenges in terms of audio quality and generalization ability across tasks.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.\nTo address these issues, we propose <span class=\"ltx_text ltx_font_bold\">UniTok-Audio</span>, a scalable and extensible framework for unified audio generation tasks.\nSpecifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner;\n2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework;\n3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction.\nExperimental results demonstrate that UniTok-Audio achieves competitive performance\nin comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks:\nspeech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation.\nTo foster future research, we will open-source our codebase.\nThe demo page of our work can be found here: https://alibaba.github.io/unified-audio.\n</p>\n\n",
                "matched_terms": [
                    "task",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study focuses on the TA tasks, especially which provides the input audio that temporally aligned with the output audio at the frame level,\nincluding: speech restoration (SR) that aims at restoring speech from the degraded recording with various distortions (e.g., noise, reverberation,and packet loss);\ntarget speaker extraction (TSE) that extracts target speech from mixture using assistive clues (e.g., voiceprint information from reference speech);\nspeech separation (SS) that aims to separate all existing speaker in the mixture;\nvoice conversion (VC) that transforms the timbre of source speech guided by reference speech of another speaker;\nlanguage-queried audio source separation (LASS) that aims at extracting target audio components from mixture, which are consistent with the given textual caption.\nNumerous generative models are developed for these tasks, while most of them are designed for single task with\ntask-specific architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.</p>\n\n",
                "matched_terms": [
                    "task",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some studies aim to unify multiple tasks within a single framework, including AnyEnhance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib91\" title=\"\">2025</a>)</cite>,\nUniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>, LLaSE-G1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib25\" title=\"\">2025</a>)</cite>, UniSE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.\nThese methods utilizes the LM backbone combined with discrete audio codec and\nexhibit remarkable generative ability, which benefit from the semantic understanding and contextual modeling capabilities of LMs.\nHowever, challenges still exist in terms of audio quality and generalization ability across tasks.\nFor instance, few unified models are capable of handling the SS task, as it generally requires customized\narchitecture to output multi-track speech.</p>\n\n",
                "matched_terms": [
                    "uniaudio",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Framework</span>: The framework unifies tasks by taking task-specific conditional information as the conditioning sequence of decoder-only LM,\nand the discrete token of target audio is predicted in an AR manner.\nWe utilize a special task token to distinguish different learning patterns of multiple tasks.\nNote that our model handles diverse tasks using a single set of shared weights, thereby eliminating the need for task-specific weight adaptation.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">New Tokenization</span>: We present <span class=\"ltx_text ltx_font_bold\">H-codec</span>,\nwhich integrates self-supervised learning (SSL) representation within the audio tokenization and reconstruction process.\nThe features from waveform and SSL model are individually quantized, resulting dual-stream (acoustic and semantic) codec tokens.\nH-Codec achieves remarkable audio reconstruction quality with a low frame rate,\nimproving both the efficiency and performance of downstream audio generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the domain of TA audio tasks, early researches focus on discriminative modeling,\nwhich directly learns the mapping between input signal and target audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Williamson &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib80\" title=\"\">2017</a>; Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>.\nHowever, the lack of generative ability limits their generalization in unseen scenarios and\nthe performance in extreme situations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Welker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib78\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib71\" title=\"\">2020</a>)</cite>.\nMany studies integrate generative modeling into audio tasks in recent years.\nFor the SR task, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite> applies k-means to quantize noisy speech representations obtained\nby WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib4\" title=\"\">2022</a>)</cite> into discrete tokens,\nand then a Transformer-based speech LM maps the noisy tokens to clean tokens.\nFor the LASS task, FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>\nlearns linear flow trajectories from noise to target source features within the variational autoencoder (VAE) latent space,\nwhich are guided by the encoded text embeddings and the mixture audio.\nHowever, these models are designed for specific task, facing limited extensibility when migrating to more tasks.</p>\n\n",
                "matched_terms": [
                    "task",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the development of LM, the research focus of codecs has gradually shifted\nfrom reducing data transmission costs toward the integration with LM, which ensures the high quality of generated audio.\nThis requires codecs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib37\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib10\" title=\"\">2024</a>)</cite> to preserve more semantic information that can be understood and modeled by LM.\nX-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib86\" title=\"\">2024a</a>)</cite> integrates the representations from the pre-trained SSL model to enhance semantic preservation,\nimproving both reconstruction quality and downstream TTS performance.\nSome studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib24\" title=\"\">2025</a>)</cite> explore single-layer codecs that are more suitable for autoregressive modeling in LM.\nX-Codec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite> utilizes finite scalar quantization (FSQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib41\" title=\"\">2024</a>)</cite> to perform single-layer quantization,\nenlarging the code space. BiCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite> generates a hybrid token stream combining semantic and global tokens,\nwhich are derived from a SSL model and a speaker verification model, respectively.\nHowever, single-layer codecs with a low frame rate still faces challenges in high-fidelity reconstruction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite>, e.g., speaker similarity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate multi-layer codec tokens into AR prediction, an existing method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib70\" title=\"\">2023a</a>)</cite> applies two-stage strategy:\n(i) model the tokens of the first layer in an AR manner; (ii) then, predict the tokens of remaining layers using a NAR post-network.\nHowever, this method causes additional complexity to the system. In addition, flattening all tokens into one layer leads to unbearable computational\ncost, while predicting tokens from all layers in parallel within one step deteriorates the performance.\nTherefore, we adopt the delay pattern&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib5\" title=\"\">2023</a>)</cite> to arrange our tokens for the trade-off between performance and computational cost.\nSpecifically, the 4-layer acoustic and semantic tokens produced by H-Codec are first interleaved sequentially across time steps, resulting in <math alttext=\"{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">E</mi><mi>c</mi></msub><mo>&#8712;</mo><msup><mi>&#8484;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}</annotation></semantics></math>\nwith a frame rate of 50 Hz, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> indicates the number of frames. Before feeding the tokens into the LM backbone,\ndifferent shifts are applied across layers and special pad tokens occupy empty positions, as shown in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F1\" title=\"In 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn the LM backbone, 4 embedding layers handle 4-layer tokens respectively, and the embeddings of each layer are added up as the input of transformer layers.\nThere are 4 output heads to predict the 4-layer logits of next time step. The delay pattern allows generating high-layer tokens conditioned by low-layer tokens,\nwhich improves prediction accuracy.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">rTSE Mode:</span>\nSince SS task requires generating multiple output tracks while our model only supports one-track output,\nwe include the rTSE mode during training,\nenabling the model to obtain multiple tracks through iterative inference.\nThis mode aims to extract the timbre-mismatched speech component in the mixture input when compared with\nthe reference speech. The loss function <math alttext=\"\\mathcal{L}_{\\rm rTSE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm rTSE}</annotation></semantics></math> keeps similar to that of the TSE mode,\nexcept that the task token has been replaced with <math alttext=\"{\\rm T_{rTSE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{rTSE}}</annotation></semantics></math>.\nWhen handling SS task (we only consider 2-speaker cases), we first apply the SR mode to extract the main speaker\nwith higher energy, and the other speaker is obtained by using the rTSE mode.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Reconstruction Performance:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T3\" title=\"In 4.1.2 Experimental Results &#8227; 4.1 H-Codec &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comprehensive comparison of audio codec models on speech, music, and general audio tasks.\nAll baselines supports general audio reconstruction.\nNotably, H-Codec achieves lowest Mel loss and STFT loss on all domain, illustrating the powerful multi-domain reconstruction ability.\nThis ensures the potential of H-Codec for extensive downstream tasks, including speech, music, and audio generation.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "unitokaudioomni",
                    "task",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics:</span>\nWe adopt multiple evaluation metrics to assess different aspects of the generated audio across tasks.\nFor speech tasks, we evaluate quality by DNSMOS (SIG, BAK, OVRL) and NISQA, speaker similarity by SIM,\nintelligibility by WER, and continuity by PLCMOS.\nFor the LASS task, we utilize FAD, CLAPScore, and CLAPScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> to measure the audio separation performance.\nDetails about evaluation metrics can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A2.SS2\" title=\"B.2 Audio Task Metrics &#8227; Appendix B Evaluation Metrics &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T4\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the SR performance comparison on 2020 DNS Challenge test sets.\nIt is clear that generative models generally outperform discriminative ones.\nContinuous-domain generative approaches perform well on the &#8220;No Reverb&#8221; subset,\nhighlighting the potential of continuous methods in terms of generated signal quality.\nHowever, discrete-domain generative approaches can perform better under reverberant conditions, indicating\nthat discrete representations may simplify the modeling difficulty of reverberation components.\nOur UniTok-Audio achieves comparable performance among SOTA baselines,\nand the single-task versions with different audio encoders result in similar performance to UniTok-Audio<sub class=\"ltx_sub\">omni</sub>.\nIn addition, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T5\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the performance on packet loss concealment (PLC),\na subtask of SR aimed at recovering speech frames lost during transmission.\nUniTok-Audio surpasses baselines in terms of both signal quality and continuity,\nshowing powerful content understanding and generation capabilities of the framework.</p>\n\n",
                "matched_terms": [
                    "unitokaudioomni",
                    "comparison",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T6\" title=\"In 4.2.2 TSE Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the performance comparison for TSE task.\nThe results indicate that generative methods achieve higher speech quality than discriminative approaches\nbut struggle with speaker similarity.\nThis can be attributed to the upper bound limitation of codecs&#8217; reconstruction fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>.\nOur UniTok-Audio maintains comparable performance compared to SOTA baselines,\ndemonstrating the feasibility of constructing a unified framework.</p>\n\n",
                "matched_terms": [
                    "task",
                    "comparison",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T7\" title=\"In 4.2.3 SS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the performance comparison for SS task, showing that our model achieves superior performance than baselines.\nThis verifies the effectiveness of our iterative inference strategy in handling the SS task that requires multiple output tracks.\nNote that although the experiments are conducted with the 2-speaker configuration,\nour approach can be extended to scenarios with more sources when the target signal of rTSE mode is defined as all remaining speakers.\nThe single-task version is not reported since the inference phase of SS requires the cooperation of multiple modes.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model",
                    "comparison",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nFollowing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>, we create test set for the VC task using VCTK&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib68\" title=\"\">2017</a>)</cite> dataset.\nWe randomly select 200 recordings from the dataset as source speech,\nand for each source sample, a sample from another speaker is picked as the reference speech.\nBaselines include HierSpeech++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib31\" title=\"\">2023</a>)</cite>, LM-VC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>, UniAudio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib84\" title=\"\">2024</a>)</cite>,\nVevo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib92\" title=\"\">2024a</a>)</cite>, and Metis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib74\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "uniaudio",
                    "task",
                    "vevo",
                    "hierspeech",
                    "lmvc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T9\" title=\"In 4.2.5 LASS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, UniTok-Audio achieves competitive performance in the LASS task,\nindicating effective exploitation of textual information.\nWe prove that the unified domain codec has potential to handle the LASS tasks.\nThe single-task version outperforms UniTok-Audio<sub class=\"ltx_sub\">omni</sub>,\nwhich can be attributed to the domain gap between speech and audio.</p>\n\n",
                "matched_terms": [
                    "unitokaudioomni",
                    "task",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniTok-Audio,\na framework that resembles multiple time-aligned audio tasks.\nWe uniify different learning patterns of multiple tasks in a single framework using a special task token,\nwhich indicates current operational mode of model.\nThis paper also introduces H-Codec, achieving high-fidelity reconstruction quality with dual-stream architecture\nthat quantize acoustic and semantic features simultaneously.\nBased on H-Codec, UniTok-Audio adopts continuous conditional embeddings to generates multi-layer discrete tokens in parallel.\nExtensive experiments demonstrate that UniTok-Audio achieves competitive performance across diverse tasks\nwith limited training data and moderate model size,\nhighlighting its potential as a foundation model for unified AR audio generation.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T11\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a> reports the hyperparameter configurations of different UniTok-Audio versions.\nUniTok-Audio-S and UniTok-Audio-L denote the small and large version, respectively.\nThe VC performance in terms of different verisons are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T12\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, where\nall versions are trained for the single VC task using WavLM-based audio encoder.\nIt can be seen that increasing the model size consistently improves performance,\nin accordance with scaling laws.\nThis indicates the potential of UniTok-Audio to be extended to a larger model size.\nTo balance complexity and performance, we report the medium-sized verison in the main text.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model",
                    "performance"
                ]
            }
        ]
    },
    "S4.T9": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 9: LASS results on 2024 DCASE LASS validation set.",
        "body": "Model\nType\n\nFAD(\\downarrow)\n\nCLAPScore(\\uparrow)\n\nCLAPScoreA(\\uparrow)\n\n\nMixture\n-\n-\n23.83\n60.39\n\n\nLASS-Net\nD\n2.57\n23.04\n65.14\n\n\nFlowSep\nGc\n\n0.50\n20.00\n63.47\n\n\nUniTok-Audiolass-hubert\n\nGd\n\n0.68\n28.85\n65.56\n\n\nUniTok-Audioomni\n\nGd\n\n1.48\n26.21\n61.21",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">FAD</span>(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">CLAPScore</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">CLAPScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">A</span></sub></span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Mixture</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">23.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">60.39</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LASS-Net</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">D</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FlowSep</td>\n<td class=\"ltx_td ltx_align_center\">G<sub class=\"ltx_sub\">c</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.50</span></td>\n<td class=\"ltx_td ltx_align_center\">20.00</td>\n<td class=\"ltx_td ltx_align_center\">63.47</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">UniTok-Audio<sub class=\"ltx_sub\">lass-hubert</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">28.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">65.56</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">UniTok-Audio<sub class=\"ltx_sub\">omni</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">G<sub class=\"ltx_sub\">d</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">26.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">61.21</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "validation",
            "lass",
            "clapscoreauparrow",
            "set",
            "unitokaudiolasshubert",
            "clapscoreuparrow",
            "dcase",
            "results",
            "lassnet",
            "faddownarrow",
            "unitokaudioomni",
            "flowsep",
            "model",
            "type",
            "mixture"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T9\" title=\"In 4.2.5 LASS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, UniTok-Audio achieves competitive performance in the LASS task,\nindicating effective exploitation of textual information.\nWe prove that the unified domain codec has potential to handle the LASS tasks.\nThe single-task version outperforms UniTok-Audio<sub class=\"ltx_sub\">omni</sub>,\nwhich can be attributed to the domain gap between speech and audio.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This study focuses on the TA tasks, especially which provides the input audio that temporally aligned with the output audio at the frame level,\nincluding: speech restoration (SR) that aims at restoring speech from the degraded recording with various distortions (e.g., noise, reverberation,and packet loss);\ntarget speaker extraction (TSE) that extracts target speech from mixture using assistive clues (e.g., voiceprint information from reference speech);\nspeech separation (SS) that aims to separate all existing speaker in the mixture;\nvoice conversion (VC) that transforms the timbre of source speech guided by reference speech of another speaker;\nlanguage-queried audio source separation (LASS) that aims at extracting target audio components from mixture, which are consistent with the given textual caption.\nNumerous generative models are developed for these tasks, while most of them are designed for single task with\ntask-specific architectures&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib32\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib61\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib75\" title=\"\">2023b</a>)</cite>.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.</p>\n\n",
                "matched_terms": [
                    "results",
                    "mixture",
                    "lass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Framework</span>: The framework unifies tasks by taking task-specific conditional information as the conditioning sequence of decoder-only LM,\nand the discrete token of target audio is predicted in an AR manner.\nWe utilize a special task token to distinguish different learning patterns of multiple tasks.\nNote that our model handles diverse tasks using a single set of shared weights, thereby eliminating the need for task-specific weight adaptation.</p>\n\n",
                "matched_terms": [
                    "set",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the domain of TA audio tasks, early researches focus on discriminative modeling,\nwhich directly learns the mapping between input signal and target audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Williamson &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib80\" title=\"\">2017</a>; Luo &amp; Mesgarani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib39\" title=\"\">2019</a>)</cite>.\nHowever, the lack of generative ability limits their generalization in unseen scenarios and\nthe performance in extreme situations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Welker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib78\" title=\"\">2022</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib71\" title=\"\">2020</a>)</cite>.\nMany studies integrate generative modeling into audio tasks in recent years.\nFor the SR task, SELM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib76\" title=\"\">2024b</a>)</cite> applies k-means to quantize noisy speech representations obtained\nby WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib4\" title=\"\">2022</a>)</cite> into discrete tokens,\nand then a Transformer-based speech LM maps the noisy tokens to clean tokens.\nFor the LASS task, FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>\nlearns linear flow trajectories from noise to target source features within the variational autoencoder (VAE) latent space,\nwhich are guided by the encoded text embeddings and the mixture audio.\nHowever, these models are designed for specific task, facing limited extensibility when migrating to more tasks.</p>\n\n",
                "matched_terms": [
                    "flowsep",
                    "mixture",
                    "lass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">rTSE Mode:</span>\nSince SS task requires generating multiple output tracks while our model only supports one-track output,\nwe include the rTSE mode during training,\nenabling the model to obtain multiple tracks through iterative inference.\nThis mode aims to extract the timbre-mismatched speech component in the mixture input when compared with\nthe reference speech. The loss function <math alttext=\"\\mathcal{L}_{\\rm rTSE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm rTSE}</annotation></semantics></math> keeps similar to that of the TSE mode,\nexcept that the task token has been replaced with <math alttext=\"{\\rm T_{rTSE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p4.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">T</mi><mi>rTSE</mi></msub><annotation encoding=\"application/x-tex\">{\\rm T_{rTSE}}</annotation></semantics></math>.\nWhen handling SS task (we only consider 2-speaker cases), we first apply the SR mode to extract the main speaker\nwith higher energy, and the other speaker is obtained by using the rTSE mode.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mixture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LASS Mode:</span>\nThis mode aims at separating specific component that matches the given caption query from the input mixture audio.\nTherefore, the associated loss function is defined as</p>\n\n",
                "matched_terms": [
                    "mixture",
                    "lass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "results",
                    "unitokaudioomni",
                    "set",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T4\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the SR performance comparison on 2020 DNS Challenge test sets.\nIt is clear that generative models generally outperform discriminative ones.\nContinuous-domain generative approaches perform well on the &#8220;No Reverb&#8221; subset,\nhighlighting the potential of continuous methods in terms of generated signal quality.\nHowever, discrete-domain generative approaches can perform better under reverberant conditions, indicating\nthat discrete representations may simplify the modeling difficulty of reverberation components.\nOur UniTok-Audio achieves comparable performance among SOTA baselines,\nand the single-task versions with different audio encoders result in similar performance to UniTok-Audio<sub class=\"ltx_sub\">omni</sub>.\nIn addition, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T5\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the performance on packet loss concealment (PLC),\na subtask of SR aimed at recovering speech frames lost during transmission.\nUniTok-Audio surpasses baselines in terms of both signal quality and continuity,\nshowing powerful content understanding and generation capabilities of the framework.</p>\n\n",
                "matched_terms": [
                    "results",
                    "unitokaudioomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T7\" title=\"In 4.2.3 SS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the performance comparison for SS task, showing that our model achieves superior performance than baselines.\nThis verifies the effectiveness of our iterative inference strategy in handling the SS task that requires multiple output tracks.\nNote that although the experiments are conducted with the 2-speaker configuration,\nour approach can be extended to scenarios with more sources when the target signal of rTSE mode is defined as all remaining speakers.\nThe single-task version is not reported since the inference phase of SS requires the cooperation of multiple modes.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Configuration:</span>\nWe adopt 2024 DCASE LASS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://dcase.community/challenge2024/task-language-queried-audio-source-separation</span></span></span> validation set\nto evaluate the LASS performance, which contains 3k synthetic mixtures mixed from 1k audio clips.\nBaselines include LASS-Net&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib38\" title=\"\">2022b</a>)</cite> and FlowSep&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib89\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "validation",
                    "lass",
                    "set",
                    "dcase",
                    "lassnet",
                    "flowsep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A data simulation pipeline is designed to synthesis data pairs dynamically during training.\nConsidering various types of degradation in the SR task,\nwe apply multiple distortions to a speech sample with independent probabilities,\nwhere the distortion categories and corresponding configurations are shown in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A1.T10\" title=\"In Appendix A Data Simulation &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nThe distortion chain is also applied to the TSE and rTSE modes,\nexcept that the probability of interfering speaker is set to 1.0 and the SIR is uniformly sampled between\n-5 and 5 dB.\nFor the LASS mode, we mix the target audio with another randomly selected audio using a SIR ranges from -5 to 20 dB.\nFor the VC mode, we leverage a voice conversion model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/myshell-ai/OpenVoice</span></span></span> to perform timbre perturbation using randomly selected target speech and reference speech,\ngenerating 6k hours of fixed training dataset.\nThe perturbed sample is used as input to predict the target speech based on another speech of the target speaker.</p>\n\n",
                "matched_terms": [
                    "set",
                    "lass"
                ]
            }
        ]
    },
    "A1.T10": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 10: Distortion categories and corresponding configurations,\nwhere SNR and SIR denote the signal-to-noise ratio and signal-to-interference ratio, respectively.",
        "body": "Min_quantile \\sim Uniform([0.0, 0.1])\n\n\nMax_quantile \\sim Uniform([0.9, 1.0])",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Min_quantile <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T10.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math> Uniform([0.0, 0.1])</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Max_quantile <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T10.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math> Uniform([0.9, 1.0])</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "configurations",
            "maxquantile",
            "uniform09",
            "respectively",
            "signaltointerference",
            "ratio",
            "snr",
            "distortion",
            "signaltonoise",
            "minquantile",
            "denote",
            "corresponding",
            "uniform00",
            "categories",
            "where",
            "sim",
            "sir"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">A data simulation pipeline is designed to synthesis data pairs dynamically during training.\nConsidering various types of degradation in the SR task,\nwe apply multiple distortions to a speech sample with independent probabilities,\nwhere the distortion categories and corresponding configurations are shown in &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A1.T10\" title=\"In Appendix A Data Simulation &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nThe distortion chain is also applied to the TSE and rTSE modes,\nexcept that the probability of interfering speaker is set to 1.0 and the SIR is uniformly sampled between\n-5 and 5 dB.\nFor the LASS mode, we mix the target audio with another randomly selected audio using a SIR ranges from -5 to 20 dB.\nFor the VC mode, we leverage a voice conversion model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/myshell-ai/OpenVoice</span></span></span> to perform timbre perturbation using randomly selected target speech and reference speech,\ngenerating 6k hours of fixed training dataset.\nThe perturbed sample is used as input to predict the target speech based on another speech of the target speaker.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To unify various audio generation tasks within a single framework,\nwe extract task-specific conditional information as a conditioning sequence for the decoder-only AR backbone,\nwhich then predicts the corresponding H-Codec tokens of the target audio.\nSince continuous features, typically extracted from SSL models, contain richer audio details compared to discrete representations and are more adaptable to varying input conditions,\nwe extract continuous features to assemble the the task-conditioning sequence.\nSpecifically, we utilize T5-base<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://huggingface.co/google/t5-v1_1-base</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib51\" title=\"\">2020</a>)</cite> as the\ntext encoder to extract embedding from audio caption.\nThe same HuBERT used in H-Codec is adopted to extract continuous features from audio waveforms.\nTwo linear layers serve as two adapters to map the text embedding and audio features into a representation space amenable to LM AR modeling, respectively.\nGiven text and audio embeddings as conditions,\nwe utilize LLaMA architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib64\" title=\"\">2023</a>)</cite> to predicts discrete tokens of target waveform in an AR manner.\nFinally, the H-Codec decoder reconstructs high-fidelity audio from the predicted token sequence.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate multi-layer codec tokens into AR prediction, an existing method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib70\" title=\"\">2023a</a>)</cite> applies two-stage strategy:\n(i) model the tokens of the first layer in an AR manner; (ii) then, predict the tokens of remaining layers using a NAR post-network.\nHowever, this method causes additional complexity to the system. In addition, flattening all tokens into one layer leads to unbearable computational\ncost, while predicting tokens from all layers in parallel within one step deteriorates the performance.\nTherefore, we adopt the delay pattern&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib5\" title=\"\">2023</a>)</cite> to arrange our tokens for the trade-off between performance and computational cost.\nSpecifically, the 4-layer acoustic and semantic tokens produced by H-Codec are first interleaved sequentially across time steps, resulting in <math alttext=\"{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">E</mi><mi>c</mi></msub><mo>&#8712;</mo><msup><mi>&#8484;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}</annotation></semantics></math>\nwith a frame rate of 50 Hz, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> indicates the number of frames. Before feeding the tokens into the LM backbone,\ndifferent shifts are applied across layers and special pad tokens occupy empty positions, as shown in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F1\" title=\"In 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn the LM backbone, 4 embedding layers handle 4-layer tokens respectively, and the embeddings of each layer are added up as the input of transformer layers.\nThere are 4 output heads to predict the 4-layer logits of next time step. The delay pattern allows generating high-layer tokens conditioned by low-layer tokens,\nwhich improves prediction accuracy.</p>\n\n",
                "matched_terms": [
                    "where",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SR Mode:</span>\nThe target audio is the clean speech corresponding to the degraded input speech. The conditional sequence of LM is formatted as\n<math alttext=\"\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>SR</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">I</mi><annotation encoding=\"application/x-tex\">{\\rm I}</annotation></semantics></math> denotes the start of input audio features,\n<math alttext=\"{\\rm E}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{i}</annotation></semantics></math> the input audio embeddings, and <math alttext=\"{\\rm S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">S</mi><annotation encoding=\"application/x-tex\">{\\rm S}</annotation></semantics></math> the start of codec tokens, respectively.\nThe output sequence is formulated as <math alttext=\"{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119952;</mi><mo>=</mo><mrow><mo>[</mo><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">E</mi><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm E}^{\\prime}_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m6\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">{\\rm E}^{\\prime}_{c}</annotation></semantics></math> indicates codec tokens with\ndelay pattern, and <math alttext=\"{\\rm E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">E</mi><annotation encoding=\"application/x-tex\">{\\rm E}</annotation></semantics></math> represents the end token. The trainable parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m8\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> in the model\nare optimized by minimizing the negative log-likelihood of the predicted outputs:</p>\n\n",
                "matched_terms": [
                    "where",
                    "corresponding",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"o^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m9\" intent=\":literal\"><semantics><msubsup><mi>o</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">o^{i}_{t}</annotation></semantics></math> indicates the output token at <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m10\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>-th step and <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m11\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th layer, and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m12\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the length of output sequence, respectively.</p>\n\n",
                "matched_terms": [
                    "where",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TSE Mode:</span>\nThe target audio corresponds to the timbre-matched speech component in the input mixture audio that aligns with the reference audio.\nThe conditional sequence is formatted as <math alttext=\"\\left[{\\rm T_{TSE}},{\\rm R},{\\rm E}_{r},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>TSE</mi></msub><mo>,</mo><mi mathvariant=\"normal\">R</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>r</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{TSE}},{\\rm R},{\\rm E}_{r},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>,\nwhere <math alttext=\"{\\rm E}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m2\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{r}</annotation></semantics></math> and <math alttext=\"{\\rm R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">R</mi><annotation encoding=\"application/x-tex\">{\\rm R}</annotation></semantics></math> represent the features of reference speech and its start token, respectively.\nTherefore, the associated loss function is defined as</p>\n\n",
                "matched_terms": [
                    "where",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\rm E}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p6.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{t}</annotation></semantics></math> and <math alttext=\"{\\rm C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p6.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">C</mi><annotation encoding=\"application/x-tex\">{\\rm C}</annotation></semantics></math> denote the embedding of caption and its start token, respectively.</p>\n\n",
                "matched_terms": [
                    "denote",
                    "where",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T11\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a> reports the hyperparameter configurations of different UniTok-Audio versions.\nUniTok-Audio-S and UniTok-Audio-L denote the small and large version, respectively.\nThe VC performance in terms of different verisons are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T12\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, where\nall versions are trained for the single VC task using WavLM-based audio encoder.\nIt can be seen that increasing the model size consistently improves performance,\nin accordance with scaling laws.\nThis indicates the potential of UniTok-Audio to be extended to a larger model size.\nTo balance complexity and performance, we report the medium-sized verison in the main text.</p>\n\n",
                "matched_terms": [
                    "denote",
                    "configurations",
                    "respectively",
                    "where"
                ]
            }
        ]
    },
    "A3.T11": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 11: Model configurations of different UniTok-Audio versions.",
        "body": "Model Size\nDepth\nEmbed Size\nNum Heads\nTrainable Params\n\n\nUniTok-Audio-S\n8\n768\n8\n109M\n\n\nUniTok-Audio\n16\n1024\n16\n481M\n\n\nUniTok-Audio-L\n44\n1024\n32\n1.3B",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Model Size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Depth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Embed Size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Num Heads</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Trainable Params</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">UniTok-Audio-S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">768</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">109M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">UniTok-Audio</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_center\">1024</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_center\">481M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">UniTok-Audio-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1024</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.3B</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "unitokaudiol",
            "configurations",
            "trainable",
            "13b",
            "size",
            "versions",
            "different",
            "num",
            "depth",
            "unitokaudios",
            "unitokaudio",
            "params",
            "109m",
            "heads",
            "481m",
            "model",
            "embed"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T11\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a> reports the hyperparameter configurations of different UniTok-Audio versions.\nUniTok-Audio-S and UniTok-Audio-L denote the small and large version, respectively.\nThe VC performance in terms of different verisons are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T12\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, where\nall versions are trained for the single VC task using WavLM-based audio encoder.\nIt can be seen that increasing the model size consistently improves performance,\nin accordance with scaling laws.\nThis indicates the potential of UniTok-Audio to be extended to a larger model size.\nTo balance complexity and performance, we report the medium-sized verison in the main text.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative modeling has recently achieved remarkable success across text, image, and audio domains,\ndemonstrating powerful capabilities for unified representation learning.\nHowever, audio generation models still face challenges in terms of audio quality and generalization ability across tasks.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.\nTo address these issues, we propose <span class=\"ltx_text ltx_font_bold\">UniTok-Audio</span>, a scalable and extensible framework for unified audio generation tasks.\nSpecifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner;\n2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework;\n3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction.\nExperimental results demonstrate that UniTok-Audio achieves competitive performance\nin comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks:\nspeech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation.\nTo foster future research, we will open-source our codebase.\nThe demo page of our work can be found here: https://alibaba.github.io/unified-audio.\n</p>\n\n",
                "matched_terms": [
                    "unitokaudio",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve audio generation quality, some works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib30\" title=\"\">2023</a>; Vyas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib69\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib82\" title=\"\">2025</a>)</cite>\nadopt generative paradigms in continuous space, such as flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib35\" title=\"\">2023</a>)</cite>,\nwhich eliminates the dependence on discrete codecs.\nHowever, the flowchart of model needs to be carefully designed to support different tasks, increasing the\ndifficulty when extending to more tasks.\nAdditionally, considering the trend of combining audio generation capabilities with large language models (LLM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib62\" title=\"\">2025</a>)</cite>,\ndeveloping audio generation models based on discrete codec has greater potential.\nThis highlights the need for improving the ability of audio codec,\nwhich directly affects the generation quality of audio models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Framework</span>: The framework unifies tasks by taking task-specific conditional information as the conditioning sequence of decoder-only LM,\nand the discrete token of target audio is predicted in an AR manner.\nWe utilize a special task token to distinguish different learning patterns of multiple tasks.\nNote that our model handles diverse tasks using a single set of shared weights, thereby eliminating the need for task-specific weight adaptation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate multi-layer codec tokens into AR prediction, an existing method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib70\" title=\"\">2023a</a>)</cite> applies two-stage strategy:\n(i) model the tokens of the first layer in an AR manner; (ii) then, predict the tokens of remaining layers using a NAR post-network.\nHowever, this method causes additional complexity to the system. In addition, flattening all tokens into one layer leads to unbearable computational\ncost, while predicting tokens from all layers in parallel within one step deteriorates the performance.\nTherefore, we adopt the delay pattern&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib5\" title=\"\">2023</a>)</cite> to arrange our tokens for the trade-off between performance and computational cost.\nSpecifically, the 4-layer acoustic and semantic tokens produced by H-Codec are first interleaved sequentially across time steps, resulting in <math alttext=\"{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">E</mi><mi>c</mi></msub><mo>&#8712;</mo><msup><mi>&#8484;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}</annotation></semantics></math>\nwith a frame rate of 50 Hz, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> indicates the number of frames. Before feeding the tokens into the LM backbone,\ndifferent shifts are applied across layers and special pad tokens occupy empty positions, as shown in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F1\" title=\"In 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn the LM backbone, 4 embedding layers handle 4-layer tokens respectively, and the embeddings of each layer are added up as the input of transformer layers.\nThere are 4 output heads to predict the 4-layer logits of next time step. The delay pattern allows generating high-layer tokens conditioned by low-layer tokens,\nwhich improves prediction accuracy.</p>\n\n",
                "matched_terms": [
                    "heads",
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SR Mode:</span>\nThe target audio is the clean speech corresponding to the degraded input speech. The conditional sequence of LM is formatted as\n<math alttext=\"\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mo>[</mo><msub><mi mathvariant=\"normal\">T</mi><mi>SR</mi></msub><mo>,</mo><mi mathvariant=\"normal\">I</mi><mo>,</mo><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant=\"normal\">S</mi><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">\\left[{\\rm T_{SR}},{\\rm I},{\\rm E}_{i},{\\rm S}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm I}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">I</mi><annotation encoding=\"application/x-tex\">{\\rm I}</annotation></semantics></math> denotes the start of input audio features,\n<math alttext=\"{\\rm E}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">E</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">{\\rm E}_{i}</annotation></semantics></math> the input audio embeddings, and <math alttext=\"{\\rm S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">S</mi><annotation encoding=\"application/x-tex\">{\\rm S}</annotation></semantics></math> the start of codec tokens, respectively.\nThe output sequence is formulated as <math alttext=\"{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119952;</mi><mo>=</mo><mrow><mo>[</mo><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">E</mi><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{o}}=\\left[{\\rm E}^{\\prime}_{c},{\\rm E}\\right]</annotation></semantics></math>, where <math alttext=\"{\\rm E}^{\\prime}_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m6\" intent=\":literal\"><semantics><msubsup><mi mathvariant=\"normal\">E</mi><mi>c</mi><mo>&#8242;</mo></msubsup><annotation encoding=\"application/x-tex\">{\\rm E}^{\\prime}_{c}</annotation></semantics></math> indicates codec tokens with\ndelay pattern, and <math alttext=\"{\\rm E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">E</mi><annotation encoding=\"application/x-tex\">{\\rm E}</annotation></semantics></math> represents the end token. The trainable parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m8\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> in the model\nare optimized by minimizing the negative log-likelihood of the predicted outputs:</p>\n\n",
                "matched_terms": [
                    "model",
                    "trainable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "trainable",
                    "size",
                    "model",
                    "different",
                    "unitokaudio",
                    "heads",
                    "481m"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T4\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the SR performance comparison on 2020 DNS Challenge test sets.\nIt is clear that generative models generally outperform discriminative ones.\nContinuous-domain generative approaches perform well on the &#8220;No Reverb&#8221; subset,\nhighlighting the potential of continuous methods in terms of generated signal quality.\nHowever, discrete-domain generative approaches can perform better under reverberant conditions, indicating\nthat discrete representations may simplify the modeling difficulty of reverberation components.\nOur UniTok-Audio achieves comparable performance among SOTA baselines,\nand the single-task versions with different audio encoders result in similar performance to UniTok-Audio<sub class=\"ltx_sub\">omni</sub>.\nIn addition, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T5\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the performance on packet loss concealment (PLC),\na subtask of SR aimed at recovering speech frames lost during transmission.\nUniTok-Audio surpasses baselines in terms of both signal quality and continuity,\nshowing powerful content understanding and generation capabilities of the framework.</p>\n\n",
                "matched_terms": [
                    "versions",
                    "unitokaudio",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniTok-Audio,\na framework that resembles multiple time-aligned audio tasks.\nWe uniify different learning patterns of multiple tasks in a single framework using a special task token,\nwhich indicates current operational mode of model.\nThis paper also introduces H-Codec, achieving high-fidelity reconstruction quality with dual-stream architecture\nthat quantize acoustic and semantic features simultaneously.\nBased on H-Codec, UniTok-Audio adopts continuous conditional embeddings to generates multi-layer discrete tokens in parallel.\nExtensive experiments demonstrate that UniTok-Audio achieves competitive performance across diverse tasks\nwith limited training data and moderate model size,\nhighlighting its potential as a foundation model for unified AR audio generation.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model",
                    "unitokaudio",
                    "different"
                ]
            }
        ]
    },
    "A3.T12": {
        "source_file": "UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens",
        "caption": "Table 12: VC performance across different model sizes.",
        "body": "Model\n\nWER(\\downarrow)\n\nSIM(\\uparrow)\n\nDNSMOS(\\uparrow)\n\nNISQA(\\uparrow)\n\n\nUniTok-Audio-S\n5.38\n0.42\n3.41\n4.30\n\n\nUniTok-Audio\n3.02\n0.51\n3.46\n4.46\n\n\nUniTok-Audio-L\n2.10\n0.61\n3.61\n4.54",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">WER</span>(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">SIM</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T12.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">DNSMOS</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T12.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">NISQA</span>(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T12.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">UniTok-Audio-S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">5.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">0.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">3.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">4.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">UniTok-Audio</td>\n<td class=\"ltx_td ltx_align_center\">3.02</td>\n<td class=\"ltx_td ltx_align_center\">0.51</td>\n<td class=\"ltx_td ltx_align_center\">3.46</td>\n<td class=\"ltx_td ltx_align_center\">4.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">UniTok-Audio-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">2.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">4.54</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "werdownarrow",
            "dnsmosuparrow",
            "sizes",
            "across",
            "unitokaudiol",
            "performance",
            "different",
            "nisqauparrow",
            "unitokaudios",
            "unitokaudio",
            "model",
            "simuparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T11\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">11</span></a> reports the hyperparameter configurations of different UniTok-Audio versions.\nUniTok-Audio-S and UniTok-Audio-L denote the small and large version, respectively.\nThe VC performance in terms of different verisons are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3.T12\" title=\"In Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, where\nall versions are trained for the single VC task using WavLM-based audio encoder.\nIt can be seen that increasing the model size consistently improves performance,\nin accordance with scaling laws.\nThis indicates the potential of UniTok-Audio to be extended to a larger model size.\nTo balance complexity and performance, we report the medium-sized verison in the main text.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generative modeling has recently achieved remarkable success across text, image, and audio domains,\ndemonstrating powerful capabilities for unified representation learning.\nHowever, audio generation models still face challenges in terms of audio quality and generalization ability across tasks.\nThis fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility.\nTo address these issues, we propose <span class=\"ltx_text ltx_font_bold\">UniTok-Audio</span>, a scalable and extensible framework for unified audio generation tasks.\nSpecifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner;\n2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework;\n3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction.\nExperimental results demonstrate that UniTok-Audio achieves competitive performance\nin comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks:\nspeech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation.\nTo foster future research, we will open-source our codebase.\nThe demo page of our work can be found here: https://alibaba.github.io/unified-audio.\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "unitokaudio",
                    "performance",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve audio generation quality, some works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib30\" title=\"\">2023</a>; Vyas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib69\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib77\" title=\"\">2025c</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib82\" title=\"\">2025</a>)</cite>\nadopt generative paradigms in continuous space, such as flow matching&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib35\" title=\"\">2023</a>)</cite>,\nwhich eliminates the dependence on discrete codecs.\nHowever, the flowchart of model needs to be carefully designed to support different tasks, increasing the\ndifficulty when extending to more tasks.\nAdditionally, considering the trend of combining audio generation capabilities with large language models (LLM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib62\" title=\"\">2025</a>)</cite>,\ndeveloping audio generation models based on discrete codec has greater potential.\nThis highlights the need for improving the ability of audio codec,\nwhich directly affects the generation quality of audio models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Framework</span>: The framework unifies tasks by taking task-specific conditional information as the conditioning sequence of decoder-only LM,\nand the discrete token of target audio is predicted in an AR manner.\nWe utilize a special task token to distinguish different learning patterns of multiple tasks.\nNote that our model handles diverse tasks using a single set of shared weights, thereby eliminating the need for task-specific weight adaptation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">New Tokenization</span>: We present <span class=\"ltx_text ltx_font_bold\">H-codec</span>,\nwhich integrates self-supervised learning (SSL) representation within the audio tokenization and reconstruction process.\nThe features from waveform and SSL model are individually quantized, resulting dual-stream (acoustic and semantic) codec tokens.\nH-Codec achieves remarkable audio reconstruction quality with a low frame rate,\nimproving both the efficiency and performance of downstream audio generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the development of LM, the research focus of codecs has gradually shifted\nfrom reducing data transmission costs toward the integration with LM, which ensures the high quality of generated audio.\nThis requires codecs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib37\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib10\" title=\"\">2024</a>)</cite> to preserve more semantic information that can be understood and modeled by LM.\nX-Codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib86\" title=\"\">2024a</a>)</cite> integrates the representations from the pre-trained SSL model to enhance semantic preservation,\nimproving both reconstruction quality and downstream TTS performance.\nSome studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib23\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib24\" title=\"\">2025</a>)</cite> explore single-layer codecs that are more suitable for autoregressive modeling in LM.\nX-Codec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite> utilizes finite scalar quantization (FSQ)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib41\" title=\"\">2024</a>)</cite> to perform single-layer quantization,\nenlarging the code space. BiCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib73\" title=\"\">2025a</a>)</cite> generates a hybrid token stream combining semantic and global tokens,\nwhich are derived from a SSL model and a speaker verification model, respectively.\nHowever, single-layer codecs with a low frame rate still faces challenges in high-fidelity reconstruction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib88\" title=\"\">2025</a>)</cite>, e.g., speaker similarity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To incorporate multi-layer codec tokens into AR prediction, an existing method&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib70\" title=\"\">2023a</a>)</cite> applies two-stage strategy:\n(i) model the tokens of the first layer in an AR manner; (ii) then, predict the tokens of remaining layers using a NAR post-network.\nHowever, this method causes additional complexity to the system. In addition, flattening all tokens into one layer leads to unbearable computational\ncost, while predicting tokens from all layers in parallel within one step deteriorates the performance.\nTherefore, we adopt the delay pattern&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib5\" title=\"\">2023</a>)</cite> to arrange our tokens for the trade-off between performance and computational cost.\nSpecifically, the 4-layer acoustic and semantic tokens produced by H-Codec are first interleaved sequentially across time steps, resulting in <math alttext=\"{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">E</mi><mi>c</mi></msub><mo>&#8712;</mo><msup><mi>&#8484;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">{\\rm E}_{c}\\in\\mathbb{Z}^{T\\times 4}</annotation></semantics></math>\nwith a frame rate of 50 Hz, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> indicates the number of frames. Before feeding the tokens into the LM backbone,\ndifferent shifts are applied across layers and special pad tokens occupy empty positions, as shown in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S3.F1\" title=\"In 3 UniTok-Audio &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn the LM backbone, 4 embedding layers handle 4-layer tokens respectively, and the embeddings of each layer are added up as the input of transformer layers.\nThere are 4 output heads to predict the 4-layer logits of next time step. The delay pattern allows generating high-layer tokens conditioned by low-layer tokens,\nwhich improves prediction accuracy.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "performance",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details:</span>\nThere are 16 layers with 16 attention heads and a hidden dimension of 1024 in the LLaMA-based LM backbone,\nresulting in 481M trainable parameters.\nWe also explore different model size configurations in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A3\" title=\"Appendix C Model Size vs. Performance &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nOur model is trained using AdamW optimizer with 30 epochs,\nwhere the learning rate reaches a peak of 0.001 after 4000 warm-up steps and reduces at a decay factor of 0.98 in each epoch.\nThe lengths of reference audio and input signal are set to 5 seconds for both training and inference phases.\nWe train the multi-task version (UniTok-Audio<sub class=\"ltx_sub\">omni</sub>) and single-task version of UniTok-Audio for performance evaluation.\nFor the former, one of the five operational modes is randomly selected for every batch during training.\nFor the latter, we report results of models trained within single task.\nWe also attempt to adopt WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://huggingface.co/microsoft/wavlm-base-plus</span></span></span> as the audio encoder for the single-task version.\nSubscripts are used to distinguish different models (e.g., HuBERT-based and WavLM-based single-task verisons for SR are denoted as UniTok-Audio<sub class=\"ltx_sub\">sr-hubert</sub> and UniTok-Audio<sub class=\"ltx_sub\">sr-wavlm</sub>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "unitokaudio",
                    "performance",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics:</span>\nWe adopt multiple evaluation metrics to assess different aspects of the generated audio across tasks.\nFor speech tasks, we evaluate quality by DNSMOS (SIG, BAK, OVRL) and NISQA, speaker similarity by SIM,\nintelligibility by WER, and continuity by PLCMOS.\nFor the LASS task, we utilize FAD, CLAPScore, and CLAPScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">A</span></sub> to measure the audio separation performance.\nDetails about evaluation metrics can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#A2.SS2\" title=\"B.2 Audio Task Metrics &#8227; Appendix B Evaluation Metrics &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T4\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the SR performance comparison on 2020 DNS Challenge test sets.\nIt is clear that generative models generally outperform discriminative ones.\nContinuous-domain generative approaches perform well on the &#8220;No Reverb&#8221; subset,\nhighlighting the potential of continuous methods in terms of generated signal quality.\nHowever, discrete-domain generative approaches can perform better under reverberant conditions, indicating\nthat discrete representations may simplify the modeling difficulty of reverberation components.\nOur UniTok-Audio achieves comparable performance among SOTA baselines,\nand the single-task versions with different audio encoders result in similar performance to UniTok-Audio<sub class=\"ltx_sub\">omni</sub>.\nIn addition, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T5\" title=\"In 4.2.1 SR Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> reports the performance on packet loss concealment (PLC),\na subtask of SR aimed at recovering speech frames lost during transmission.\nUniTok-Audio surpasses baselines in terms of both signal quality and continuity,\nshowing powerful content understanding and generation capabilities of the framework.</p>\n\n",
                "matched_terms": [
                    "unitokaudio",
                    "performance",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T6\" title=\"In 4.2.2 TSE Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the performance comparison for TSE task.\nThe results indicate that generative methods achieve higher speech quality than discriminative approaches\nbut struggle with speaker similarity.\nThis can be attributed to the upper bound limitation of codecs&#8217; reconstruction fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#bib.bib83\" title=\"\">2025</a>)</cite>.\nOur UniTok-Audio maintains comparable performance compared to SOTA baselines,\ndemonstrating the feasibility of constructing a unified framework.</p>\n\n",
                "matched_terms": [
                    "unitokaudio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T7\" title=\"In 4.2.3 SS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">7</span></a> reports the performance comparison for SS task, showing that our model achieves superior performance than baselines.\nThis verifies the effectiveness of our iterative inference strategy in handling the SS task that requires multiple output tracks.\nNote that although the experiments are conducted with the 2-speaker configuration,\nour approach can be extended to scenarios with more sources when the target signal of rTSE mode is defined as all remaining speakers.\nThe single-task version is not reported since the inference phase of SS requires the cooperation of multiple modes.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nVC results are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T8\" title=\"In 4.2.4 VC Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, showing the superiority of UniTok-Audio in\nspeech quality, speaker similarity, and intelligibility.\nWe observe that UniTok-Audio<sub class=\"ltx_sub\">vc-wavlm</sub> outperforms UniTok-Audio<sub class=\"ltx_sub\">vc-hubert</sub>, indicating that\nWavLM performs better in extracting semantic information and speaker characteristics.\nThe performance degrades when extending to multiple tasks from single-task version,\nimplying the distinct pattern between VC and other tasks,\nwhere the former changes the property of the input signal rather than restoring or extracting certain components.</p>\n\n",
                "matched_terms": [
                    "unitokaudio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Results:</span>\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26372v1#S4.T9\" title=\"In 4.2.5 LASS Performance &#8227; 4.2 UniTok-Audio &#8227; 4 Experiments &#8227; UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, UniTok-Audio achieves competitive performance in the LASS task,\nindicating effective exploitation of textual information.\nWe prove that the unified domain codec has potential to handle the LASS tasks.\nThe single-task version outperforms UniTok-Audio<sub class=\"ltx_sub\">omni</sub>,\nwhich can be attributed to the domain gap between speech and audio.</p>\n\n",
                "matched_terms": [
                    "unitokaudio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose UniTok-Audio,\na framework that resembles multiple time-aligned audio tasks.\nWe uniify different learning patterns of multiple tasks in a single framework using a special task token,\nwhich indicates current operational mode of model.\nThis paper also introduces H-Codec, achieving high-fidelity reconstruction quality with dual-stream architecture\nthat quantize acoustic and semantic features simultaneously.\nBased on H-Codec, UniTok-Audio adopts continuous conditional embeddings to generates multi-layer discrete tokens in parallel.\nExtensive experiments demonstrate that UniTok-Audio achieves competitive performance across diverse tasks\nwith limited training data and moderate model size,\nhighlighting its potential as a foundation model for unified AR audio generation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "different",
                    "unitokaudio",
                    "model"
                ]
            }
        ]
    }
}