{
    "S3.T1": {
        "caption": "Table 1: The S-MOS and M-MOS of different context models",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Narration</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">M-MOS</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Dialogue</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">S-MOS</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Chapter</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">M-MOS</span></span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Baseline</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.91&#177;0.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.84&#177;0.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.88&#177;0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">MOSS-TTSD</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.78&#177;0.11</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.67&#177;0.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.72&#177;0.09</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Proposed</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Infer-ctx</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.06&#177;0.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.93&#177;0.06</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.13&#177;0.09</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Infer-inst</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">/</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.96&#177;0.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">/</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Infer-ctx&amp;inst</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">/</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.11&#177;0.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.25&#177;0.11</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "dialogue",
            "367±007",
            "393±006",
            "smos",
            "inferctx",
            "mmos",
            "inferinst",
            "372±009",
            "context",
            "inferctxinst",
            "narration",
            "388±007",
            "378±011",
            "384±009",
            "406±008",
            "391±007",
            "cosyvoice2",
            "chapter",
            "411±006",
            "mossttsd",
            "proposed",
            "413±009",
            "model",
            "425±011",
            "396±008",
            "baseline",
            "different"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Existing text-to-speech systems predominantly focus on single-sentence synthesis and lack adequate contextual modeling as well as fine-grained performance control capabilities for generating coherent multicast audiobooks. To address these limitations, we propose a context-aware and emotion controllable speech synthesis framework specifically engineered for multicast audiobooks with three key innovations: a context mechanism for contextual consistency, a disentanglement paradigm to decouple style control from speech prompts for semantic consistency, and self-distillation to boost emotional expressiveness and instruction controllability. Experimental results show superior performance across the generation of narration, dialogue, and the whole chapter, significantly outperforming existing baselines. Ablation studies are conducted to validate the effectiveness of our proposed methods. Demo samples can be found in <a class=\"ltx_ref ltx_href\" href=\"https://everest-ai.github.io/\" style=\"--ltx-fg-color:#0000FF;\" title=\"\">https://everest-ai.github.io/</a>.</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "dialogue",
                    "context",
                    "narration",
                    "chapter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In response, researchers have recently developed automated solutions for high-quality audiobook generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib5\" title=\"\">5</a>]</cite>. AudioStory <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib2\" title=\"\">2</a>]</cite> employs an LLM to process instruction inputs, decomposes long audio into structured subtasks, and generates short clips sequentially. MultiActor-Audiobook <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib1\" title=\"\">1</a>]</cite> utilizes a Transformer-based multimodal model to capture character traits, uses LLMs for emotional guidance, and synthesizes speech at the sentence level. Dopamine Audiobook <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib3\" title=\"\">3</a>]</cite> and MM-StoryAgent <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib4\" title=\"\">4</a>]</cite> adopt multi-agent pipelines for story-based generation, but integrate existing TTS systems such as CosyVoice <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib6\" title=\"\">6</a>]</cite> rather than proposing new synthesis methods. Similarly, Shaja et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib5\" title=\"\">5</a>]</cite> proposed a system enhancing immersion via spatial audio, which also relies on established TTS backbones. A key limitation across these methods is the lack of explicit inter-sentence modeling, resulting in inadequate contextual consistency.</p>\n\n",
                "matched_terms": [
                    "model",
                    "proposed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several industry approaches have incorporated context modeling for long-form speech. MoonCast <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib7\" title=\"\">7</a>]</cite> targets podcast generation with long-context modeling and colloquial scripts; MOSS-TTSD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib8\" title=\"\">8</a>]</cite> achieves state-of-the-art long-segment quality via efficient codecs and data pipelines; CoVoMix <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib9\" title=\"\">9</a>]</cite> and koel-TTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib10\" title=\"\">10</a>]</cite> focus on conversational expressiveness. Despite these advances, such systems remain largely tailored to podcasts, modeling long context in an simplistic manner that lacks fine-grained controllability&#8212;a critical requirement for audiobooks, which demand precise narrative flow and expressive multi-character portrayal.</p>\n\n",
                "matched_terms": [
                    "mossttsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main architecture of our proposed speech generation model shown in Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S2.F1\" title=\"Figure 1 &#8227; 2 METHODOLOGY &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b) leverages CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib6\" title=\"\">6</a>]</cite>. We preserve its text&amp;speech tokenizer and flow-matching modules, but notably substitute the HiFi-GAN vocoder with BigVGAN to enhance audio fidelity and robustness. Our work mainly focuses on exploring how to better utilize contextual information and enhance controllability in the training process of large language models.\nFor auto-regressive LLMs, the organization of sequential structures is of critical importance. In our system, it is constructed as follows:</p>\n\n",
                "matched_terms": [
                    "models",
                    "cosyvoice2",
                    "model",
                    "proposed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets and Training:</span> There were three stages in our training. First, 1 million hours audiobook data was used to finetune CosyVoice2 for audiobook domain adaptation. Second, a 100K hours context-aware dataset and 500 hours recordings labeled with instructions were employed to further finetune the model. It is worthy to note that each sentence in context-aware dataset was annotated with timestamp, speaker IDs and position information in the whole chapter. Third, to enhance the expressiveness and controllability of instructions, a data augmentation dataset of 5k hours was constructed based on the model after two-stage training. During the training, the model was optimized with AdamW Optimizer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib15\" title=\"\">15</a>]</cite> by setting learning rate to 1e-5 for first two stages but 1e-6 for the last stage. 64 NVIDIA A800 GPUs are employed to\ntrain the model with batch size of 384 for 720K, 300K and 10K steps respectively for three stages.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "chapter",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate on audiobook scenarios, three test sets were constructed: a narrative test set <span class=\"ltx_text ltx_font_italic\">Test-NAR</span> consists of 100 paragraphs with each paragraph over 240 sentences. A dialogue test set <span class=\"ltx_text ltx_font_italic\">Test-DIA</span> composed of 570 dialogue sentences. 15 chapters varying from 2000 to 4000 sentences were used to construct a chapter test set <span class=\"ltx_text ltx_font_italic\">Test-CHAP</span> including both narration and dialogue parts.</p>\n\n",
                "matched_terms": [
                    "chapter",
                    "dialogue",
                    "narration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Evaluation: </span>For comprehensive evaluation of our system, we used two subjective assessment methods<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib16\" title=\"\">16</a>]</cite>: Single-sentence Mean Opinion Score (S-MOS) for dialogue samples, and Multi-sentence Mean Opinion Score (M-MOS) for narration and long-chapter samples. We randomly sampled 20 paragraphs (from <span class=\"ltx_text ltx_font_italic\">Test-NAR</span>), 60 sentences (from <span class=\"ltx_text ltx_font_italic\">Test-DIA</span>) and 10 chapters (from <span class=\"ltx_text ltx_font_italic\">Test-CHAP</span>) for subjective ratings, with 50 Chinese native speakers recruited to score both S-MOS and M-MOS. We further compared it with two baselines via ABX tests: single-sentence (S-ABX) for dialogue, and multi-sentence (M-ABX) for narration and chapters.</p>\n\n",
                "matched_terms": [
                    "mmos",
                    "model",
                    "dialogue",
                    "narration",
                    "smos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides subjective evaluations, we use objective metrics, PER<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib17\" title=\"\">17</a>]</cite> and SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, to guarantee the stability of the model. Our proposed model was evaluated and compared under the following three configurations:\n<span class=\"ltx_text ltx_font_bold\">Infer-ctx</span>: inference using only the additional context sequence.\n<span class=\"ltx_text ltx_font_bold\">Infer-inst</span>: inference using only the additional instruction sequence. <span class=\"ltx_text ltx_font_bold\">Infer-ctx&amp;inst</span>: inference incorporating both context and instruction sequences.</p>\n\n",
                "matched_terms": [
                    "inferinst",
                    "proposed",
                    "model",
                    "context",
                    "inferctxinst",
                    "inferctx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results from Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:t_mos_overall</span> and Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.F2\" title=\"Figure 2 &#8227; 3.2 Main Results &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> show that the proposed system largely exceeds both baseline models for all test setups. By comparing the results between the narration and dialogue tests, the results of the dialogue tests are more advantageous than those of the narration tests for both MOS and ABX tests.</p>\n\n",
                "matched_terms": [
                    "models",
                    "proposed",
                    "dialogue",
                    "narration",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is also worthy to mention that the speech generation strategy of <span class=\"ltx_text ltx_font_italic\">Infer-Ctx&amp;Inst</span> does benefit from combining context-aware narration generation and instructed dialogue generation. From Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.F2\" title=\"Figure 2 &#8227; 3.2 Main Results &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <span class=\"ltx_text ltx_font_italic\">Ctx&amp;Inst</span> achieves the most preferable result compared to just <span class=\"ltx_text ltx_font_italic\">Infer-Ctx</span> and <span class=\"ltx_text ltx_font_italic\">Infer-inst</span>, which is consistent with the results in Table <span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:t_mos_overall</span>.</p>\n\n",
                "matched_terms": [
                    "inferinst",
                    "dialogue",
                    "inferctxinst",
                    "narration",
                    "inferctx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate key components of our framework, four ablation experiments were conducted. Experiments 1&#8211;3 focus on context awareness, using a 100 k-hour audiobook chapter dataset for training, Test-DIA for evaluation, and SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, PER<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib17\" title=\"\">17</a>]</cite> and S-MOS as unified metrics. Experiment 4 focuses on instruction, adopting dedicated training data&#8212;including 500 hours of high-quality emotional data and 5,300 hours of augmented data&#8212;and tailored assessment methods.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "context",
                    "chapter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Decoupled vs Non-Decoupled Models:</span> We compared two strategies for the context-aware module: the non-decoupled model, where audio prompts and targets are identical, and the decoupled model, in which distinct prompts and targets are selected via a specific strategy. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Ablation Analysis &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> show the non-decoupled model achieves excessively high SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, indicating over-similar timbre, prosody, and emotion&#8212;and thus impractical for audiobook due to insufficient character diversity.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Decoupling Threshold:</span> Audio clips within chapters were clustered using different thresholds to generate distinct speaker IDs. A lower threshold reduced SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, with occasional timbre discontinuities but yielded slightly higher S-MOS than higher thresholds. Conversely, an excessively high threshold is hypothesized to increase SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, approaching non-decoupled model performance, while risking lower S-MOS, based on tested threshold trends.</p>\n\n",
                "matched_terms": [
                    "different",
                    "smos",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Contextual Text Input:</span>\nWe evaluated the influence of contextual text on the context-aware module by feeding the Text-Speech Language Model with two input types: target text alone, or target text plus its preceding and subsequent sentence. The context-augmented model achieved a higher S-MOS, with listening tests confirming improved coherence. Next, we present an example.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Grained Emotional Control: </span> We used the Chinese emotional speech test set from CV3-Eval<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib18\" title=\"\">18</a>]</cite> to evaluate controllability, modifying the original instructions into three types of emotional states: &#8220;high-intensity single emotion&#8221;, &#8220;low-intensity single emotion&#8221;, and &#8220;mixed emotion&#8221;. Three metrics are employed for emotional evaluation: emotion classification F1-score<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib19\" title=\"\">19</a>]</cite> for single emotions; S-MOS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib16\" title=\"\">16</a>]</cite> and SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib16\" title=\"\">16</a>]</cite> for mixed emotions. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Ablation Analysis &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> show our model exhibits stronger H-L (discriminability between &#8220;high-intensity&#8221; and &#8220;low-intensity&#8221;) emotion control on &#8220;Text-Unrelated&#8221; test set and outperform baseline models<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib6\" title=\"\">6</a>]</cite> in high-intensity emotion control. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Ablation Analysis &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that our model also outperforms the baseline model<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib6\" title=\"\">6</a>]</cite> in terms of mixed emotion performance.</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper proposes a controllable, context-aware TTS framework for multicast audiobooks, with 3 innovations: a context mechanism for contextual consistency, a disentanglement paradigm to decouple style control from speech prompts for semantic consistency, and self-distillation to boost emotional expressiveness and controllability.\nExperiments show the framework outperforms baselines in narration, dialogue, and chapter generation, with ablation studies validating its key components.In future, we can expand chapter data or select specific data to mitigate data sparsity, and explore reinforcement learning for performance improvement.</p>\n\n",
                "matched_terms": [
                    "chapter",
                    "dialogue",
                    "context",
                    "narration"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Performance Comparison of Context Strategy",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S-MOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Non-Decoupled</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.45&#177;0.09</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Decoupled-0.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.19</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.86&#177;0.06</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Decoupled-0.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.79</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.84</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.82&#177;0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Decoupled-0.8 + context</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.93&#177;0.06</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "382±007",
            "nondecoupled",
            "345±009",
            "model",
            "context",
            "strategy",
            "decoupled068",
            "393±006",
            "386±006",
            "smos",
            "comparison",
            "decoupled08",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Decoupled vs Non-Decoupled Models:</span> We compared two strategies for the context-aware module: the non-decoupled model, where audio prompts and targets are identical, and the decoupled model, in which distinct prompts and targets are selected via a specific strategy. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Ablation Analysis &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> show the non-decoupled model achieves excessively high SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, indicating over-similar timbre, prosody, and emotion&#8212;and thus impractical for audiobook due to insufficient character diversity.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Existing text-to-speech systems predominantly focus on single-sentence synthesis and lack adequate contextual modeling as well as fine-grained performance control capabilities for generating coherent multicast audiobooks. To address these limitations, we propose a context-aware and emotion controllable speech synthesis framework specifically engineered for multicast audiobooks with three key innovations: a context mechanism for contextual consistency, a disentanglement paradigm to decouple style control from speech prompts for semantic consistency, and self-distillation to boost emotional expressiveness and instruction controllability. Experimental results show superior performance across the generation of narration, dialogue, and the whole chapter, significantly outperforming existing baselines. Ablation studies are conducted to validate the effectiveness of our proposed methods. Demo samples can be found in <a class=\"ltx_ref ltx_href\" href=\"https://everest-ai.github.io/\" style=\"--ltx-fg-color:#0000FF;\" title=\"\">https://everest-ai.github.io/</a>.</p>\n\n",
                "matched_terms": [
                    "context",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic Consistency Enhancement:</span> To improve alignment between synthesized audio and text semantics while mitigating excessive prosodic interference from prompt audio, we adopt a decoupled training strategy. Rather than the traditional coupled &#8220;prompt-target&#8221; training model, we employ independent modeling that focuses on the adaptive relationship between target audio prosody and text semantics. This approach reduces cross-audio prosodic interference at the data level and enhances consistency between synthesized audio and current text semantics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotional Intensity Enhancement:</span> To alleviate the scarcity of high-intensity emotional samples, we employ a self-distillation strategy consisting of three key steps:\nFirst, samples with varying intensity levels are synthesized using a pre-trained emotional TTS model;\nSecond, the generated samples are filtered through PER, speaker similarity and pitch to guarantee quality;\nThird, the intensity distribution is balanced via targeted data augmentation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Evaluation: </span>For comprehensive evaluation of our system, we used two subjective assessment methods<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib16\" title=\"\">16</a>]</cite>: Single-sentence Mean Opinion Score (S-MOS) for dialogue samples, and Multi-sentence Mean Opinion Score (M-MOS) for narration and long-chapter samples. We randomly sampled 20 paragraphs (from <span class=\"ltx_text ltx_font_italic\">Test-NAR</span>), 60 sentences (from <span class=\"ltx_text ltx_font_italic\">Test-DIA</span>) and 10 chapters (from <span class=\"ltx_text ltx_font_italic\">Test-CHAP</span>) for subjective ratings, with 50 Chinese native speakers recruited to score both S-MOS and M-MOS. We further compared it with two baselines via ABX tests: single-sentence (S-ABX) for dialogue, and multi-sentence (M-ABX) for narration and chapters.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides subjective evaluations, we use objective metrics, PER<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib17\" title=\"\">17</a>]</cite> and SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, to guarantee the stability of the model. Our proposed model was evaluated and compared under the following three configurations:\n<span class=\"ltx_text ltx_font_bold\">Infer-ctx</span>: inference using only the additional context sequence.\n<span class=\"ltx_text ltx_font_bold\">Infer-inst</span>: inference using only the additional instruction sequence. <span class=\"ltx_text ltx_font_bold\">Infer-ctx&amp;inst</span>: inference incorporating both context and instruction sequences.</p>\n\n",
                "matched_terms": [
                    "model",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate key components of our framework, four ablation experiments were conducted. Experiments 1&#8211;3 focus on context awareness, using a 100 k-hour audiobook chapter dataset for training, Test-DIA for evaluation, and SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, PER<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib17\" title=\"\">17</a>]</cite> and S-MOS as unified metrics. Experiment 4 focuses on instruction, adopting dedicated training data&#8212;including 500 hours of high-quality emotional data and 5,300 hours of augmented data&#8212;and tailored assessment methods.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Decoupling Threshold:</span> Audio clips within chapters were clustered using different thresholds to generate distinct speaker IDs. A lower threshold reduced SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, with occasional timbre discontinuities but yielded slightly higher S-MOS than higher thresholds. Conversely, an excessively high threshold is hypothesized to increase SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, approaching non-decoupled model performance, while risking lower S-MOS, based on tested threshold trends.</p>\n\n",
                "matched_terms": [
                    "model",
                    "smos",
                    "nondecoupled",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Contextual Text Input:</span>\nWe evaluated the influence of contextual text on the context-aware module by feeding the Text-Speech Language Model with two input types: target text alone, or target text plus its preceding and subsequent sentence. The context-augmented model achieved a higher S-MOS, with listening tests confirming improved coherence. Next, we present an example.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Grained Emotional Control: </span> We used the Chinese emotional speech test set from CV3-Eval<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib18\" title=\"\">18</a>]</cite> to evaluate controllability, modifying the original instructions into three types of emotional states: &#8220;high-intensity single emotion&#8221;, &#8220;low-intensity single emotion&#8221;, and &#8220;mixed emotion&#8221;. Three metrics are employed for emotional evaluation: emotion classification F1-score<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib19\" title=\"\">19</a>]</cite> for single emotions; S-MOS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib16\" title=\"\">16</a>]</cite> and SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib16\" title=\"\">16</a>]</cite> for mixed emotions. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Ablation Analysis &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> show our model exhibits stronger H-L (discriminability between &#8220;high-intensity&#8221; and &#8220;low-intensity&#8221;) emotion control on &#8220;Text-Unrelated&#8221; test set and outperform baseline models<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib6\" title=\"\">6</a>]</cite> in high-intensity emotion control. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Ablation Analysis &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that our model also outperforms the baseline model<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib6\" title=\"\">6</a>]</cite> in terms of mixed emotion performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper proposes a controllable, context-aware TTS framework for multicast audiobooks, with 3 innovations: a context mechanism for contextual consistency, a disentanglement paradigm to decouple style control from speech prompts for semantic consistency, and self-distillation to boost emotional expressiveness and controllability.\nExperiments show the framework outperforms baselines in narration, dialogue, and chapter generation, with ablation studies validating its key components.In future, we can expand chapter data or select specific data to mitigate data sparsity, and explore reinforcement learning for performance improvement.</p>\n\n",
                "matched_terms": [
                    "context",
                    "performance"
                ]
            }
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Comparison of F1 scores For Single Emotion",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text-Related</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text-Unrelated</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">angry</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">happy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">sad</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">angry</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">happy</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">sad</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2-H</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.53</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2-L</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.77</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.38</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Infer-inst-H</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.72</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.98</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.31</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.54</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.65</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Infer-inst-L</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.39</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.32</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<math alttext=\"\\Delta_{\\text{CosyVoice2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi><mtext mathsize=\"0.900em\">CosyVoice2</mtext></msub><annotation encoding=\"application/x-tex\">\\Delta_{\\text{CosyVoice2}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">(H-L)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-0.06</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<math alttext=\"\\Delta_{\\text{Infer-inst}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi><mtext mathsize=\"0.900em\">Infer-inst</mtext></msub><annotation encoding=\"application/x-tex\">\\Delta_{\\text{Infer-inst}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">(H-L)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.33</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "inferinstl",
            "textunrelated",
            "textrelated",
            "happy",
            "model",
            "δcosyvoice2deltatextcosyvoice2hl",
            "sad",
            "cosyvoice2h",
            "scores",
            "cosyvoice2l",
            "inferinsth",
            "δinferinstdeltatextinferinsthl",
            "angry",
            "single",
            "comparison",
            "emotion"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Grained Emotional Control: </span> We used the Chinese emotional speech test set from CV3-Eval<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib18\" title=\"\">18</a>]</cite> to evaluate controllability, modifying the original instructions into three types of emotional states: &#8220;high-intensity single emotion&#8221;, &#8220;low-intensity single emotion&#8221;, and &#8220;mixed emotion&#8221;. Three metrics are employed for emotional evaluation: emotion classification F1-score<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib19\" title=\"\">19</a>]</cite> for single emotions; S-MOS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib16\" title=\"\">16</a>]</cite> and SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib16\" title=\"\">16</a>]</cite> for mixed emotions. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Ablation Analysis &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> show our model exhibits stronger H-L (discriminability between &#8220;high-intensity&#8221; and &#8220;low-intensity&#8221;) emotion control on &#8220;Text-Unrelated&#8221; test set and outperform baseline models<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib6\" title=\"\">6</a>]</cite> in high-intensity emotion control. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Ablation Analysis &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that our model also outperforms the baseline model<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib6\" title=\"\">6</a>]</cite> in terms of mixed emotion performance.</p>\n\n"
        ],
        "contextual_paragraphs": []
    },
    "S3.T4": {
        "caption": "Table 4: Performance Comparison For Mixed Emotion",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text-Related</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text-Unrelated</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S-MOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SS</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S-MOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2-instruct</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.67&#177;0.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.75</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.35&#177;0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Infer-inst</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.08&#177;0.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.78</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.87&#177;0.09</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "textunrelated",
            "inferinst",
            "textrelated",
            "408±007",
            "model",
            "367±006",
            "335±007",
            "cosyvoice2instruct",
            "387±009",
            "performance",
            "smos",
            "mixed",
            "comparison",
            "emotion"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Grained Emotional Control: </span> We used the Chinese emotional speech test set from CV3-Eval<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib18\" title=\"\">18</a>]</cite> to evaluate controllability, modifying the original instructions into three types of emotional states: &#8220;high-intensity single emotion&#8221;, &#8220;low-intensity single emotion&#8221;, and &#8220;mixed emotion&#8221;. Three metrics are employed for emotional evaluation: emotion classification F1-score<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib19\" title=\"\">19</a>]</cite> for single emotions; S-MOS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib16\" title=\"\">16</a>]</cite> and SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib16\" title=\"\">16</a>]</cite> for mixed emotions. Results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Ablation Analysis &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> show our model exhibits stronger H-L (discriminability between &#8220;high-intensity&#8221; and &#8220;low-intensity&#8221;) emotion control on &#8220;Text-Unrelated&#8221; test set and outperform baseline models<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib6\" title=\"\">6</a>]</cite> in high-intensity emotion control. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Ablation Analysis &#8227; 3 Experimental Setup &#8227; audiobook-cc: Controllable Long-context Speech Generation for Multicast Audiobook\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that our model also outperforms the baseline model<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib6\" title=\"\">6</a>]</cite> in terms of mixed emotion performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Existing text-to-speech systems predominantly focus on single-sentence synthesis and lack adequate contextual modeling as well as fine-grained performance control capabilities for generating coherent multicast audiobooks. To address these limitations, we propose a context-aware and emotion controllable speech synthesis framework specifically engineered for multicast audiobooks with three key innovations: a context mechanism for contextual consistency, a disentanglement paradigm to decouple style control from speech prompts for semantic consistency, and self-distillation to boost emotional expressiveness and instruction controllability. Experimental results show superior performance across the generation of narration, dialogue, and the whole chapter, significantly outperforming existing baselines. Ablation studies are conducted to validate the effectiveness of our proposed methods. Demo samples can be found in <a class=\"ltx_ref ltx_href\" href=\"https://everest-ai.github.io/\" style=\"--ltx-fg-color:#0000FF;\" title=\"\">https://everest-ai.github.io/</a>.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Evaluation: </span>For comprehensive evaluation of our system, we used two subjective assessment methods<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib16\" title=\"\">16</a>]</cite>: Single-sentence Mean Opinion Score (S-MOS) for dialogue samples, and Multi-sentence Mean Opinion Score (M-MOS) for narration and long-chapter samples. We randomly sampled 20 paragraphs (from <span class=\"ltx_text ltx_font_italic\">Test-NAR</span>), 60 sentences (from <span class=\"ltx_text ltx_font_italic\">Test-DIA</span>) and 10 chapters (from <span class=\"ltx_text ltx_font_italic\">Test-CHAP</span>) for subjective ratings, with 50 Chinese native speakers recruited to score both S-MOS and M-MOS. We further compared it with two baselines via ABX tests: single-sentence (S-ABX) for dialogue, and multi-sentence (M-ABX) for narration and chapters.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides subjective evaluations, we use objective metrics, PER<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib17\" title=\"\">17</a>]</cite> and SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, to guarantee the stability of the model. Our proposed model was evaluated and compared under the following three configurations:\n<span class=\"ltx_text ltx_font_bold\">Infer-ctx</span>: inference using only the additional context sequence.\n<span class=\"ltx_text ltx_font_bold\">Infer-inst</span>: inference using only the additional instruction sequence. <span class=\"ltx_text ltx_font_bold\">Infer-ctx&amp;inst</span>: inference incorporating both context and instruction sequences.</p>\n\n",
                "matched_terms": [
                    "model",
                    "inferinst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Decoupling Threshold:</span> Audio clips within chapters were clustered using different thresholds to generate distinct speaker IDs. A lower threshold reduced SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, with occasional timbre discontinuities but yielded slightly higher S-MOS than higher thresholds. Conversely, an excessively high threshold is hypothesized to increase SS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17516v1#bib.bib14\" title=\"\">14</a>]</cite>, approaching non-decoupled model performance, while risking lower S-MOS, based on tested threshold trends.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Contextual Text Input:</span>\nWe evaluated the influence of contextual text on the context-aware module by feeding the Text-Speech Language Model with two input types: target text alone, or target text plus its preceding and subsequent sentence. The context-augmented model achieved a higher S-MOS, with listening tests confirming improved coherence. Next, we present an example.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "model"
                ]
            }
        ]
    }
}