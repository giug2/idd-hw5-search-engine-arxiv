{
    "S3.T1": {
        "source_file": "Modeling Turn-Taking with Semantically Informed Gestures",
        "caption": "Table 1: Distribution of semantic gesture types across turn-taking labels. For each turn type (hold or yield), the table shows the relative frequency (%) of each gesture type, deictic (de), discourse (di), iconic (ic), and metaphoric (me), and the number of turns containing at least one semantic gesture, along with the total number of turns in that label.",
        "body": "turns w sem gesture\n\n\n(total # of turns)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">turns w sem gesture</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">(total # of turns)</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "relative",
            "deictic",
            "discourse",
            "turntaking",
            "types",
            "turn",
            "shows",
            "each",
            "across",
            "one",
            "metaphoric",
            "hold",
            "yield",
            "label",
            "iconic",
            "frequency",
            "turns",
            "distribution",
            "labels",
            "containing",
            "number",
            "semantic",
            "along",
            "sem",
            "gesture",
            "least",
            "total"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To model turn-taking, we convert continuous multi-party conversations in DnD Gesture++ into a structured dataset of discrete prediction instances, following previous works <cite class=\"ltx_cite ltx_citemacro_cite\">Kelterer and Schuppler (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib14\" title=\"\">2025</a>); Jokinen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib13\" title=\"\">2013</a>); Ekstedt et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib5\" title=\"\">2023</a>)</cite>. We segment audio into Inter-Pausal Units (IPUs), continuous speech segments bounded by silence, with each IPU ending at a Transition Relevance Place (TRP) where a speaker change may occur. Turns are labeled <span class=\"ltx_text ltx_font_italic\">yield</span> if another participant speaks next, or <span class=\"ltx_text ltx_font_italic\">hold</span> if the same speaker continues, using a 200 ms silence threshold. Short IPUs are merged with adjacent utterances. This results in 12k turns (7k hold, 5k yield). We split the data into 70% train, 10% validation, and 20% test. Refer to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Gesture type annotation &#8227; 3 DnD Group Gesture++ &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for more details.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In conversation, humans use multimodal cues, such as speech, gestures, and gaze, to manage turn-taking. While linguistic and acoustic features are informative, gestures provide complementary cues for modeling these transitions. To study this, we introduce DnD Gesture++, an extension of the multi-party DnD Gesture corpus enriched with 2,663 semantic gesture annotations spanning iconic, metaphoric, deictic, and discourse types. Using this dataset, we model turn-taking prediction through a Mixture-of-Experts framework integrating text, audio, and gestures. Experiments show that incorporating semantically guided gestures yields consistent performance gains over baselines, demonstrating their complementary role in multimodal turn-taking.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "discourse",
                    "deictic",
                    "metaphoric",
                    "iconic",
                    "turntaking",
                    "types",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans use multimodal cues such as prosody, gaze, and gestures to signal intentions to hold or yield a turn <cite class=\"ltx_cite ltx_citemacro_cite\">Duncan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib2\" title=\"\">1979</a>); Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib32\" title=\"\">2021</a>)</cite>. In particular, semantic gestures serve distinct conversational functions <cite class=\"ltx_cite ltx_citemacro_cite\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>)</cite> and can be categorized into four types: iconic, metaphoric, deictic, and discourse. For example, deictic gestures can signal turn exchange, while metaphoric gestures emphasize utterance content <cite class=\"ltx_cite ltx_citemacro_cite\">Bavelas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib1\" title=\"\">1995</a>); Holler et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib11\" title=\"\">2018</a>)</cite>. Incorporating these gesture types could therefore improve computational models of turn-taking.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "discourse",
                    "deictic",
                    "metaphoric",
                    "hold",
                    "yield",
                    "iconic",
                    "turntaking",
                    "types",
                    "turn",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the role of semantic gestures in turn-taking prediction, we build upon the DnD Gesture dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib27\" title=\"\">2024</a>)</cite>, a multi-party conversational corpus containing synchronized 3D motion, audio, and transcripts from participants in a tabletop game. We manually annotated the six-hour corpus with gesture-type labels following <cite class=\"ltx_cite ltx_citemacro_citet\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>)</cite>. The resulting DnD Gesture++ corpus includes 2,663 gesture instances across six hours (444 labels/hour), forming the most densely annotated English dataset of its kind. We further reformat this data for turn-taking prediction task, labeling 12k turns as either <span class=\"ltx_text ltx_font_italic\">hold</span> or <span class=\"ltx_text ltx_font_italic\">yield</span>.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "turns",
                    "across",
                    "yield",
                    "hold",
                    "labels",
                    "turntaking",
                    "containing",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we model turn-taking using a Mixture-of-Experts (MoE) framework that fuses text, audio, and gesture modalities through a gating network. Gesture embeddings are semantically enriched using our annotations to better align motion cues with linguistic and acoustic information. Our experiments demonstrate consistent gains from incorporating gestures, particularly when gesture embeddings are semantically supervised. We further analyze the latent space and modality weights from the MoE framework to better understand the contribution of semantic gesture representations. Beyond turn-taking, the dense annotations in DnD Gesture++ provide a valuable resource for tasks such as co-speech gesture generation, enabling more semantically grounded gesture synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kucherenko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib18\" title=\"\">2021</a>); Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "turntaking",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The coordination of speaking turns in human dialogue is inherently multimodal <cite class=\"ltx_cite ltx_citemacro_cite\">Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib32\" title=\"\">2021</a>)</cite>. Speakers use non-verbal signals such as prosody, gaze, and hand gestures to project turn boundaries and manage conversational flow <cite class=\"ltx_cite ltx_citemacro_cite\">Duncan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib2\" title=\"\">1979</a>); Kendrick et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib16\" title=\"\">2023</a>)</cite>. Gestures serve key semantic functions: pragmatic or discourse-related gestures often signal a <span class=\"ltx_text ltx_font_italic\">yield</span>, while representational gestures like iconic forms indicate to <span class=\"ltx_text ltx_font_italic\">hold</span> the floor <cite class=\"ltx_cite ltx_citemacro_cite\">Bavelas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib1\" title=\"\">1995</a>); Holler et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib11\" title=\"\">2018</a>)</cite>. Moreover, they serve as reliable cues for turn-taking and help maintain conversational smoothness <cite class=\"ltx_cite ltx_citemacro_cite\">Holler et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib11\" title=\"\">2018</a>); Ter&#160;Bekke et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib34\" title=\"\">2024</a>); Kendrick et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib16\" title=\"\">2023</a>); Holler and Levinson (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib12\" title=\"\">2019</a>); Hofstetter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib10\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "turns",
                    "yield",
                    "hold",
                    "iconic",
                    "turntaking",
                    "turn",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early dialogue systems detected turn boundaries using fixed silence thresholds, while recent data-driven approaches use syntactic and pragmatic cues from dialogue transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ekstedt and Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib3\" title=\"\">2020</a>)</cite>. Multimodal approaches further integrate prosody <cite class=\"ltx_cite ltx_citemacro_cite\">Ekstedt and Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib4\" title=\"\">2022</a>)</cite>, face features <cite class=\"ltx_cite ltx_citemacro_cite\">Russell and Harte (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib30\" title=\"\">2025</a>); Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib20\" title=\"\">2025</a>); Kurata et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib19\" title=\"\">2023</a>)</cite> to enhance accuracy. However, the effect of semantic gestures remains underexplored in turn-taking models. Our work examines whether semantic gesture types enhance turn-taking prediction in language models.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "turntaking",
                    "types",
                    "turn",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Co-speech gestures are typically classified as rhythmic (beat) gestures, aligned with prosody, or semantic gestures, aligned with meaning <cite class=\"ltx_cite ltx_citemacro_cite\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>); Nyatsanga et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib28\" title=\"\">2023</a>)</cite>. Semantic gestures play a key role in communication by enhancing comprehension <cite class=\"ltx_cite ltx_citemacro_cite\">Holler et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib11\" title=\"\">2018</a>)</cite> and clarifying speaker intent <cite class=\"ltx_cite ltx_citemacro_cite\">Goldin-Meadow (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib7\" title=\"\">2014</a>)</cite>. They also provide valuable signals for tasks like multimodal reference resolution <cite class=\"ltx_cite ltx_citemacro_cite\">Ghaleb et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib6\" title=\"\">2025</a>)</cite>, discourse marker disambiguation <cite class=\"ltx_cite ltx_citemacro_cite\">Suresh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib33\" title=\"\">2025</a>)</cite>, and co-speech gesture synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib26\" title=\"\">2025</a>); Kucherenko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib18\" title=\"\">2021</a>); Ram et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib29\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "gesture",
                    "discourse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Semantic gestures are often classified by McNeill&#8217;s taxonomy <cite class=\"ltx_cite ltx_citemacro_cite\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>)</cite>, which reflects their communicative intent. The main types include iconic, metaphoric and deictic gestures. Discourse gestures that structure dialogue, such as signaling topic shifts, are also considered semantic <cite class=\"ltx_cite ltx_citemacro_cite\">Bavelas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib1\" title=\"\">1995</a>)</cite>. Refer to Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S3.SS1\" title=\"3.1 Gesture type annotation &#8227; 3 DnD Group Gesture++ &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> for details.\nDatasets like SAGA <cite class=\"ltx_cite ltx_citemacro_cite\">L&#252;cking et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib23\" title=\"\">2013</a>); Kucherenko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib18\" title=\"\">2021</a>)</cite> and BEAT <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib22\" title=\"\">2022</a>)</cite> include semantic type labels but are limited to scripted or monadic data <cite class=\"ltx_cite ltx_citemacro_cite\">L&#252;cking et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib23\" title=\"\">2013</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib22\" title=\"\">2022</a>)</cite>. The DnD Group Gesture Dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib27\" title=\"\">2024</a>)</cite> captures natural multiparty interaction but lacks such labels. Our work extends it with semantic type annotations, yielding the first multiparty dataset for studying gesture-informed turn-taking modeling.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "type",
                    "discourse",
                    "deictic",
                    "metaphoric",
                    "labels",
                    "iconic",
                    "turntaking",
                    "types",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The DnD Group Gesture Dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib27\" title=\"\">2024</a>)</cite>\ncaptures co-speech gestures in multi-party conversations during a Dungeons &amp; Dragons (DnD) roleplaying game. It includes full-body motion and fine-grained finger articulation from five English-speaking participants over four sessions (6 hours total).</p>\n\n",
                "matched_terms": [
                    "total",
                    "gesture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend the DnD Gesture <cite class=\"ltx_cite ltx_citemacro_cite\">Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib27\" title=\"\">2024</a>)</cite> with gesture type annotations based on McNeill&#8217;s framework&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib25\" title=\"\">2005</a>)</cite>, classifying gestures as iconic, metaphoric, deictic, or discourse&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Goldin-Meadow et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib8\" title=\"\">1993</a>); Kendon (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib15\" title=\"\">1995</a>); Khosrobeigi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib17\" title=\"\">2022</a>)</cite>.\nThe annotations include (# of samples), <span class=\"ltx_text ltx_font_bold\">iconic</span> (724) gestures, which depict concrete objects; <span class=\"ltx_text ltx_font_bold\">metaphoric</span> (151) gestures, which represent mental images of abstract concepts; <span class=\"ltx_text ltx_font_bold\">deictic</span> (1155) gestures, which involve referential gestures like pointing; and <span class=\"ltx_text ltx_font_bold\">discourse</span> (633) gestures, which are elicited by the structure of the spoken discourse. The average interrater agreement Cohen&#8217;s <math alttext=\"\\kappa\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#954;</mi><annotation encoding=\"application/x-tex\">\\kappa</annotation></semantics></math> is 0.52. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#A1.SS1\" title=\"A.1 Additional Annotation Details &#8227; Appendix A Appendix &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> for more details on annotation.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "type",
                    "discourse",
                    "deictic",
                    "metaphoric",
                    "iconic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We model turn-taking using text, audio, and gestures captured via motion data. Each modality is processed by a dedicated expert within a MoE framework, where the outputs of all experts are fused via a gating network that dynamically weighs each modality based on context. Let <math alttext=\"x_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">x_{m}</annotation></semantics></math> denote the input features for modality <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> and <math alttext=\"f_{m}(x_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{m}(x_{m})</annotation></semantics></math> its expert output. The gating network computes modality weights <math alttext=\"w_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">w_{m}</annotation></semantics></math> using softmax over concatenated weights. The fused representation is a weighted sum of expert outputs <math alttext=\"h_{\\text{fused}}=\\sum_{m=1}^{M}w_{m}\\cdot f_{m}(x_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>h</mi><mtext>fused</mtext></msub><mo rspace=\"0.111em\">=</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mrow><mrow><msub><mi>w</mi><mi>m</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msub><mi>f</mi><mi>m</mi></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">h_{\\text{fused}}=\\sum_{m=1}^{M}w_{m}\\cdot f_{m}(x_{m})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "turntaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, <math alttext=\"h_{fused}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{fused}</annotation></semantics></math> is passed through a linear classifier to predict <span class=\"ltx_text ltx_font_italic\">hold</span> or <span class=\"ltx_text ltx_font_italic\">yield</span>. Text and audio features are obtained from pretrained embeddings, sentence embeddings for text<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\" title=\"\">https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1</a></span></span></span>\nand Wav2Vec2 for audio<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/wav2vec2-large-960h\" title=\"\">https://huggingface.co/facebook/wav2vec2-large-960h</a></span></span></span> and projected via MLP layers.\nGestures are encoded following recent VQ-VAE based gesture tokenization approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib21\" title=\"\">2024</a>); Suresh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib33\" title=\"\">2025</a>)</cite>. Given a sequence of 3D upper body motion <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math>, the encoder network compresses it into a sequence of tokens. Each token <math alttext=\"\\mathbf{z}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{i}</annotation></semantics></math> is then quantized via a codebook&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Van Den&#160;Oord et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib35\" title=\"\">2017</a>)</cite> to produce <math alttext=\"\\mathbf{z}^{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>q</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{q}_{i}</annotation></semantics></math>. The sequence of quantized embeddings is then used to reconstruct the input motion <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math>. Consequently, the VQ-VAE is trained through an MSE loss on reconstructed motion <math alttext=\"\\mathbf{\\hat{x}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119857;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathbf{\\hat{x}}</annotation></semantics></math> and input motion <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m7\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math>. Since the base model lacks semantics, we use gesture type annotations to inject semantic information (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S4.F3\" title=\"Figure 3 &#8227; 4 Multimodal Modeling of Turn-Taking Prediction &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). For each turn, we obtain the gestures token sequences of codebook embeddings from the trained VQ-VAE which are then processed by a transformer encoder <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib36\" title=\"\">2017</a>)</cite> which forms the gesture expert. The output of the Transformer is mean pooled over the sequence length to produce a fixed-length representation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "gesture",
                    "type",
                    "yield",
                    "hold",
                    "turn",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work addresses two key research questions: (i) Does the inclusion of body movement features such as gestures improve turn-taking prediction beyond text and audio? (ii) Do semantically informed gesture representations offer advantages over raw motion features?</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "turntaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We systematically compare our Text+Audio+Gesture model against single- and dual-modality baselines. We then evaluate the impact of semantic gesture embeddings from a VQ-VAE versus raw motion features. We also benchmark our MoE-based fusion against existing multimodal turn-taking approaches using the same modality experts for fair comparison. Importantly, our main aim is to assess the effect of gesture representations on turn-taking beyond text and audio, and the MoE setup allows us to quantify the contribution of each modality in this task. Further implementation details can be found in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#A1.SS2\" title=\"A.2 Implementation Details &#8227; Appendix A Appendix &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "turntaking",
                    "gesture",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Model Comparisons &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, text performs best among single-modality experts, followed by audio. Gesture only models show lower performance, with little difference between non-semantic and semantic variants. All results are averaged over three random seeds, and mean scores are reported.\nMultimodal fusion outperforms single-modality models with Text+Audio+Gesture model achieving the highest overall macro-F1 compared to the Text+Audio variant (p &lt; 0.05). Notably, the semantically aligned gesture representation outperforms the non-semantic one (p = 0.05), showing that semantic supervision enhances multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "one",
                    "gesture",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When ablating with fusion techniques from prior multimodal turn-taking studies (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Model Comparisons &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), we observe per-class F1 imbalances, with the minority class particularly affected. In contrast, MoE-based fusion with semantically aligned gestures yields more balanced predictions across classes. Future work can explore fusion strategies to further improve alignment between text, audio, and gestures.</p>\n\n",
                "matched_terms": [
                    "turntaking",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Analysing gesture representations &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we visualize the gesture embeddings with and without semantic supervision. The semantically aligned embeddings form more distinct clusters corresponding to different gesture types, which facilitates better alignment with text and audio modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Analysing gesture representations &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> further shows the average weight contribution for each modality in the MoE framework for the test samples. Using semantically aligned gestures increases the weights for both gestures and text, likely due to improved cross-modal alignment, which in turn supports better overall task performance.</p>\n\n",
                "matched_terms": [
                    "each",
                    "gesture",
                    "shows",
                    "types",
                    "turn",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we investigate the role of semantic gestures in multimodal turn-taking prediction. We extended the multi-party DnD Gesture corpus with semantic annotations to create DnD Gesture++. Our results show that semantically guided gestures improve turn-taking modeling, especially when fused with text and audio. Beyond turn-taking, the corpus also supports other multiparty interaction tasks, such as co-speech gesture generation and listener backchannel prediction.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "turntaking",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this work is grounded in a naturalistic, game-based interaction setup, the specificity of the context may limit the generalisability of the findings. Future research should examine how semantic gestures are distributed in other interaction settings, such as dyadic conversations. Our primary goal was to examine how semantic gestures contribute to turn-taking when combined with the two most widely used modalities for this task, text and audio. Although we employ a MoE fusion framework, future work could investigate more advanced fusion mechanisms to better capture the complex interactions between modalities.</p>\n\n",
                "matched_terms": [
                    "turntaking",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset contains 4 recording sessions. To label the gesture type in each recording, the annotators utilize the speech from the group conversation and multi-view videos to identify who made the gesture and mark it with label for the duration of said gesture. Therefore, the annotators considered both linguistic and visual modalities to identify the gesture types. Annotation was performed in ELAN <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://archive.mpi.nl/tla/elan\" title=\"\">https://archive.mpi.nl/tla/elan</a></span></span></span> where each recording participant was assigned a separate track for labeling. For annotation, two English-speaking young students were recruited from the university. After that, authors randomly sampled the annotations and verified the annotations manually and cleaned conflicting or wrong annotations. To acquire the transcriptions we use,\nWhisperX <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/m-bain/whisperX\" title=\"\">https://github.com/m-bain/whisperX</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "gesture",
                    "type",
                    "label",
                    "types"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For obtaining gesture representations:\nIn order to train the VQ-VAE, we pass 3D upper body motion encoded as joint positions.\nThe input skeleton is normalized relative to pelvis (root) joint and translation is fixed to zero, such that the network only models hand and arm movements.\nWe utilize convolutional encoder and decoder networks based on ResNet backbone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib9\" title=\"\">2016</a>)</cite>.\nTo apply semantic alignment loss, we utilize the annotations by training using 4 annotated semantic classes i.e. iconic, deictic, metaphoric, discourse, and a \"none\" class consisting of beat and other non-semantic gestures.\nDuring training, cross entropy loss is applied on the linear classifier to identify gesture categories.\nThis loss ignores the \"none\" class.\nThe whole framework is trained for 120 epochs with following hyperparameters: number of residual blocks = 2 in both encoder and decoder, embedding dim = 256, codebook size = 256, learning rate = 3e-4, optimizer = AdamW, reconstruction loss weight = 1, semantic loss weight = 0.1, and batch size = 512.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "discourse",
                    "relative",
                    "deictic",
                    "metaphoric",
                    "iconic",
                    "number",
                    "semantic"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "Modeling Turn-Taking with Semantically Informed Gestures",
        "caption": "Table 2: Performance of multimodal variations for turn-taking prediction using text, audio, and gesture modalities with MoE based fusion modeling.",
        "body": "Text+Audio+Gesture\n\n\n(w/o sem)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">Text+Audio+Gesture</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">(w/o sem)</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "sem",
            "gesture",
            "prediction",
            "text",
            "based",
            "variations",
            "modeling",
            "textaudiogesture",
            "turntaking",
            "modalities",
            "moe",
            "fusion",
            "performance",
            "audio",
            "multimodal"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Model Comparisons &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, text performs best among single-modality experts, followed by audio. Gesture only models show lower performance, with little difference between non-semantic and semantic variants. All results are averaged over three random seeds, and mean scores are reported.\nMultimodal fusion outperforms single-modality models with Text+Audio+Gesture model achieving the highest overall macro-F1 compared to the Text+Audio variant (p &lt; 0.05). Notably, the semantically aligned gesture representation outperforms the non-semantic one (p = 0.05), showing that semantic supervision enhances multimodal fusion.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In conversation, humans use multimodal cues, such as speech, gestures, and gaze, to manage turn-taking. While linguistic and acoustic features are informative, gestures provide complementary cues for modeling these transitions. To study this, we introduce DnD Gesture++, an extension of the multi-party DnD Gesture corpus enriched with 2,663 semantic gesture annotations spanning iconic, metaphoric, deictic, and discourse types. Using this dataset, we model turn-taking prediction through a Mixture-of-Experts framework integrating text, audio, and gestures. Experiments show that incorporating semantically guided gestures yields consistent performance gains over baselines, demonstrating their complementary role in multimodal turn-taking.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "prediction",
                    "text",
                    "modeling",
                    "turntaking",
                    "performance",
                    "audio",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Modeling Turn-Taking with Semantically Informed Gestures</span>\n</p>\n\n",
                "matched_terms": [
                    "turntaking",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans use multimodal cues such as prosody, gaze, and gestures to signal intentions to hold or yield a turn <cite class=\"ltx_cite ltx_citemacro_cite\">Duncan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib2\" title=\"\">1979</a>); Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib32\" title=\"\">2021</a>)</cite>. In particular, semantic gestures serve distinct conversational functions <cite class=\"ltx_cite ltx_citemacro_cite\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>)</cite> and can be categorized into four types: iconic, metaphoric, deictic, and discourse. For example, deictic gestures can signal turn exchange, while metaphoric gestures emphasize utterance content <cite class=\"ltx_cite ltx_citemacro_cite\">Bavelas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib1\" title=\"\">1995</a>); Holler et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib11\" title=\"\">2018</a>)</cite>. Incorporating these gesture types could therefore improve computational models of turn-taking.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "turntaking",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the role of semantic gestures in turn-taking prediction, we build upon the DnD Gesture dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib27\" title=\"\">2024</a>)</cite>, a multi-party conversational corpus containing synchronized 3D motion, audio, and transcripts from participants in a tabletop game. We manually annotated the six-hour corpus with gesture-type labels following <cite class=\"ltx_cite ltx_citemacro_citet\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>)</cite>. The resulting DnD Gesture++ corpus includes 2,663 gesture instances across six hours (444 labels/hour), forming the most densely annotated English dataset of its kind. We further reformat this data for turn-taking prediction task, labeling 12k turns as either <span class=\"ltx_text ltx_font_italic\">hold</span> or <span class=\"ltx_text ltx_font_italic\">yield</span>.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "turntaking",
                    "audio",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we model turn-taking using a Mixture-of-Experts (MoE) framework that fuses text, audio, and gesture modalities through a gating network. Gesture embeddings are semantically enriched using our annotations to better align motion cues with linguistic and acoustic information. Our experiments demonstrate consistent gains from incorporating gestures, particularly when gesture embeddings are semantically supervised. We further analyze the latent space and modality weights from the MoE framework to better understand the contribution of semantic gesture representations. Beyond turn-taking, the dense annotations in DnD Gesture++ provide a valuable resource for tasks such as co-speech gesture generation, enabling more semantically grounded gesture synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kucherenko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib18\" title=\"\">2021</a>); Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "text",
                    "turntaking",
                    "modalities",
                    "moe",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The coordination of speaking turns in human dialogue is inherently multimodal <cite class=\"ltx_cite ltx_citemacro_cite\">Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib32\" title=\"\">2021</a>)</cite>. Speakers use non-verbal signals such as prosody, gaze, and hand gestures to project turn boundaries and manage conversational flow <cite class=\"ltx_cite ltx_citemacro_cite\">Duncan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib2\" title=\"\">1979</a>); Kendrick et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib16\" title=\"\">2023</a>)</cite>. Gestures serve key semantic functions: pragmatic or discourse-related gestures often signal a <span class=\"ltx_text ltx_font_italic\">yield</span>, while representational gestures like iconic forms indicate to <span class=\"ltx_text ltx_font_italic\">hold</span> the floor <cite class=\"ltx_cite ltx_citemacro_cite\">Bavelas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib1\" title=\"\">1995</a>); Holler et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib11\" title=\"\">2018</a>)</cite>. Moreover, they serve as reliable cues for turn-taking and help maintain conversational smoothness <cite class=\"ltx_cite ltx_citemacro_cite\">Holler et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib11\" title=\"\">2018</a>); Ter&#160;Bekke et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib34\" title=\"\">2024</a>); Kendrick et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib16\" title=\"\">2023</a>); Holler and Levinson (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib12\" title=\"\">2019</a>); Hofstetter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib10\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "turntaking",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early dialogue systems detected turn boundaries using fixed silence thresholds, while recent data-driven approaches use syntactic and pragmatic cues from dialogue transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ekstedt and Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib3\" title=\"\">2020</a>)</cite>. Multimodal approaches further integrate prosody <cite class=\"ltx_cite ltx_citemacro_cite\">Ekstedt and Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib4\" title=\"\">2022</a>)</cite>, face features <cite class=\"ltx_cite ltx_citemacro_cite\">Russell and Harte (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib30\" title=\"\">2025</a>); Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib20\" title=\"\">2025</a>); Kurata et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib19\" title=\"\">2023</a>)</cite> to enhance accuracy. However, the effect of semantic gestures remains underexplored in turn-taking models. Our work examines whether semantic gesture types enhance turn-taking prediction in language models.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "turntaking",
                    "multimodal",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Co-speech gestures are typically classified as rhythmic (beat) gestures, aligned with prosody, or semantic gestures, aligned with meaning <cite class=\"ltx_cite ltx_citemacro_cite\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>); Nyatsanga et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib28\" title=\"\">2023</a>)</cite>. Semantic gestures play a key role in communication by enhancing comprehension <cite class=\"ltx_cite ltx_citemacro_cite\">Holler et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib11\" title=\"\">2018</a>)</cite> and clarifying speaker intent <cite class=\"ltx_cite ltx_citemacro_cite\">Goldin-Meadow (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib7\" title=\"\">2014</a>)</cite>. They also provide valuable signals for tasks like multimodal reference resolution <cite class=\"ltx_cite ltx_citemacro_cite\">Ghaleb et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib6\" title=\"\">2025</a>)</cite>, discourse marker disambiguation <cite class=\"ltx_cite ltx_citemacro_cite\">Suresh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib33\" title=\"\">2025</a>)</cite>, and co-speech gesture synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib26\" title=\"\">2025</a>); Kucherenko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib18\" title=\"\">2021</a>); Ram et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib29\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Semantic gestures are often classified by McNeill&#8217;s taxonomy <cite class=\"ltx_cite ltx_citemacro_cite\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>)</cite>, which reflects their communicative intent. The main types include iconic, metaphoric and deictic gestures. Discourse gestures that structure dialogue, such as signaling topic shifts, are also considered semantic <cite class=\"ltx_cite ltx_citemacro_cite\">Bavelas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib1\" title=\"\">1995</a>)</cite>. Refer to Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S3.SS1\" title=\"3.1 Gesture type annotation &#8227; 3 DnD Group Gesture++ &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> for details.\nDatasets like SAGA <cite class=\"ltx_cite ltx_citemacro_cite\">L&#252;cking et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib23\" title=\"\">2013</a>); Kucherenko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib18\" title=\"\">2021</a>)</cite> and BEAT <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib22\" title=\"\">2022</a>)</cite> include semantic type labels but are limited to scripted or monadic data <cite class=\"ltx_cite ltx_citemacro_cite\">L&#252;cking et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib23\" title=\"\">2013</a>); Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib22\" title=\"\">2022</a>)</cite>. The DnD Group Gesture Dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib27\" title=\"\">2024</a>)</cite> captures natural multiparty interaction but lacks such labels. Our work extends it with semantic type annotations, yielding the first multiparty dataset for studying gesture-informed turn-taking modeling.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "turntaking",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend the DnD Gesture <cite class=\"ltx_cite ltx_citemacro_cite\">Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib27\" title=\"\">2024</a>)</cite> with gesture type annotations based on McNeill&#8217;s framework&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib25\" title=\"\">2005</a>)</cite>, classifying gestures as iconic, metaphoric, deictic, or discourse&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Goldin-Meadow et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib8\" title=\"\">1993</a>); Kendon (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib15\" title=\"\">1995</a>); Khosrobeigi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib17\" title=\"\">2022</a>)</cite>.\nThe annotations include (# of samples), <span class=\"ltx_text ltx_font_bold\">iconic</span> (724) gestures, which depict concrete objects; <span class=\"ltx_text ltx_font_bold\">metaphoric</span> (151) gestures, which represent mental images of abstract concepts; <span class=\"ltx_text ltx_font_bold\">deictic</span> (1155) gestures, which involve referential gestures like pointing; and <span class=\"ltx_text ltx_font_bold\">discourse</span> (633) gestures, which are elicited by the structure of the spoken discourse. The average interrater agreement Cohen&#8217;s <math alttext=\"\\kappa\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#954;</mi><annotation encoding=\"application/x-tex\">\\kappa</annotation></semantics></math> is 0.52. See Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#A1.SS1\" title=\"A.1 Additional Annotation Details &#8227; Appendix A Appendix &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> for more details on annotation.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To model turn-taking, we convert continuous multi-party conversations in DnD Gesture++ into a structured dataset of discrete prediction instances, following previous works <cite class=\"ltx_cite ltx_citemacro_cite\">Kelterer and Schuppler (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib14\" title=\"\">2025</a>); Jokinen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib13\" title=\"\">2013</a>); Ekstedt et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib5\" title=\"\">2023</a>)</cite>. We segment audio into Inter-Pausal Units (IPUs), continuous speech segments bounded by silence, with each IPU ending at a Transition Relevance Place (TRP) where a speaker change may occur. Turns are labeled <span class=\"ltx_text ltx_font_italic\">yield</span> if another participant speaks next, or <span class=\"ltx_text ltx_font_italic\">hold</span> if the same speaker continues, using a 200 ms silence threshold. Short IPUs are merged with adjacent utterances. This results in 12k turns (7k hold, 5k yield). We split the data into 70% train, 10% validation, and 20% test. Refer to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Gesture type annotation &#8227; 3 DnD Group Gesture++ &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for more details.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "turntaking",
                    "audio",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We model turn-taking using text, audio, and gestures captured via motion data. Each modality is processed by a dedicated expert within a MoE framework, where the outputs of all experts are fused via a gating network that dynamically weighs each modality based on context. Let <math alttext=\"x_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">x_{m}</annotation></semantics></math> denote the input features for modality <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> and <math alttext=\"f_{m}(x_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{m}(x_{m})</annotation></semantics></math> its expert output. The gating network computes modality weights <math alttext=\"w_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">w_{m}</annotation></semantics></math> using softmax over concatenated weights. The fused representation is a weighted sum of expert outputs <math alttext=\"h_{\\text{fused}}=\\sum_{m=1}^{M}w_{m}\\cdot f_{m}(x_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>h</mi><mtext>fused</mtext></msub><mo rspace=\"0.111em\">=</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mrow><mrow><msub><mi>w</mi><mi>m</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msub><mi>f</mi><mi>m</mi></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">h_{\\text{fused}}=\\sum_{m=1}^{M}w_{m}\\cdot f_{m}(x_{m})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "moe",
                    "turntaking",
                    "audio",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, <math alttext=\"h_{fused}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{fused}</annotation></semantics></math> is passed through a linear classifier to predict <span class=\"ltx_text ltx_font_italic\">hold</span> or <span class=\"ltx_text ltx_font_italic\">yield</span>. Text and audio features are obtained from pretrained embeddings, sentence embeddings for text<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\" title=\"\">https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1</a></span></span></span>\nand Wav2Vec2 for audio<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/wav2vec2-large-960h\" title=\"\">https://huggingface.co/facebook/wav2vec2-large-960h</a></span></span></span> and projected via MLP layers.\nGestures are encoded following recent VQ-VAE based gesture tokenization approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib21\" title=\"\">2024</a>); Suresh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib33\" title=\"\">2025</a>)</cite>. Given a sequence of 3D upper body motion <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math>, the encoder network compresses it into a sequence of tokens. Each token <math alttext=\"\\mathbf{z}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{i}</annotation></semantics></math> is then quantized via a codebook&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Van Den&#160;Oord et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib35\" title=\"\">2017</a>)</cite> to produce <math alttext=\"\\mathbf{z}^{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>q</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{q}_{i}</annotation></semantics></math>. The sequence of quantized embeddings is then used to reconstruct the input motion <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math>. Consequently, the VQ-VAE is trained through an MSE loss on reconstructed motion <math alttext=\"\\mathbf{\\hat{x}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119857;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathbf{\\hat{x}}</annotation></semantics></math> and input motion <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m7\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math>. Since the base model lacks semantics, we use gesture type annotations to inject semantic information (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S4.F3\" title=\"Figure 3 &#8227; 4 Multimodal Modeling of Turn-Taking Prediction &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). For each turn, we obtain the gestures token sequences of codebook embeddings from the trained VQ-VAE which are then processed by a transformer encoder <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib36\" title=\"\">2017</a>)</cite> which forms the gesture expert. The output of the Transformer is mean pooled over the sequence length to produce a fixed-length representation.</p>\n\n",
                "matched_terms": [
                    "text",
                    "gesture",
                    "audio",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work addresses two key research questions: (i) Does the inclusion of body movement features such as gestures improve turn-taking prediction beyond text and audio? (ii) Do semantically informed gesture representations offer advantages over raw motion features?</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "prediction",
                    "text",
                    "turntaking",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We systematically compare our Text+Audio+Gesture model against single- and dual-modality baselines. We then evaluate the impact of semantic gesture embeddings from a VQ-VAE versus raw motion features. We also benchmark our MoE-based fusion against existing multimodal turn-taking approaches using the same modality experts for fair comparison. Importantly, our main aim is to assess the effect of gesture representations on turn-taking beyond text and audio, and the MoE setup allows us to quantify the contribution of each modality in this task. Further implementation details can be found in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#A1.SS2\" title=\"A.2 Implementation Details &#8227; Appendix A Appendix &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "text",
                    "textaudiogesture",
                    "turntaking",
                    "moe",
                    "fusion",
                    "audio",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When ablating with fusion techniques from prior multimodal turn-taking studies (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Model Comparisons &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), we observe per-class F1 imbalances, with the minority class particularly affected. In contrast, MoE-based fusion with semantically aligned gestures yields more balanced predictions across classes. Future work can explore fusion strategies to further improve alignment between text, audio, and gestures.</p>\n\n",
                "matched_terms": [
                    "text",
                    "turntaking",
                    "fusion",
                    "audio",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Analysing gesture representations &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we visualize the gesture embeddings with and without semantic supervision. The semantically aligned embeddings form more distinct clusters corresponding to different gesture types, which facilitates better alignment with text and audio modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Analysing gesture representations &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> further shows the average weight contribution for each modality in the MoE framework for the test samples. Using semantically aligned gestures increases the weights for both gestures and text, likely due to improved cross-modal alignment, which in turn supports better overall task performance.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "text",
                    "modalities",
                    "moe",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we investigate the role of semantic gestures in multimodal turn-taking prediction. We extended the multi-party DnD Gesture corpus with semantic annotations to create DnD Gesture++. Our results show that semantically guided gestures improve turn-taking modeling, especially when fused with text and audio. Beyond turn-taking, the corpus also supports other multiparty interaction tasks, such as co-speech gesture generation and listener backchannel prediction.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "prediction",
                    "text",
                    "modeling",
                    "turntaking",
                    "audio",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this work is grounded in a naturalistic, game-based interaction setup, the specificity of the context may limit the generalisability of the findings. Future research should examine how semantic gestures are distributed in other interaction settings, such as dyadic conversations. Our primary goal was to examine how semantic gestures contribute to turn-taking when combined with the two most widely used modalities for this task, text and audio. Although we employ a MoE fusion framework, future work could investigate more advanced fusion mechanisms to better capture the complex interactions between modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "moe",
                    "turntaking",
                    "modalities",
                    "fusion",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dataset contains 4 recording sessions. To label the gesture type in each recording, the annotators utilize the speech from the group conversation and multi-view videos to identify who made the gesture and mark it with label for the duration of said gesture. Therefore, the annotators considered both linguistic and visual modalities to identify the gesture types. Annotation was performed in ELAN <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://archive.mpi.nl/tla/elan\" title=\"\">https://archive.mpi.nl/tla/elan</a></span></span></span> where each recording participant was assigned a separate track for labeling. For annotation, two English-speaking young students were recruited from the university. After that, authors randomly sampled the annotations and verified the annotations manually and cleaned conflicting or wrong annotations. To acquire the transcriptions we use,\nWhisperX <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/m-bain/whisperX\" title=\"\">https://github.com/m-bain/whisperX</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For obtaining gesture representations:\nIn order to train the VQ-VAE, we pass 3D upper body motion encoded as joint positions.\nThe input skeleton is normalized relative to pelvis (root) joint and translation is fixed to zero, such that the network only models hand and arm movements.\nWe utilize convolutional encoder and decoder networks based on ResNet backbone&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib9\" title=\"\">2016</a>)</cite>.\nTo apply semantic alignment loss, we utilize the annotations by training using 4 annotated semantic classes i.e. iconic, deictic, metaphoric, discourse, and a \"none\" class consisting of beat and other non-semantic gestures.\nDuring training, cross entropy loss is applied on the linear classifier to identify gesture categories.\nThis loss ignores the \"none\" class.\nThe whole framework is trained for 120 epochs with following hyperparameters: number of residual blocks = 2 in both encoder and decoder, embedding dim = 256, codebook size = 256, learning rate = 3e-4, optimizer = AdamW, reconstruction loss weight = 1, semantic loss weight = 0.1, and batch size = 512.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For gesture transformer encoder, we use a 1-layer transformer with a feedforward dimension of 128 and 4 attention heads. The final hidden states are mean-pooled to obtain the gesture expert representation. MoE training is conducted on a single NVIDIA V100 GPU with a batch size of 32 for 20 epochs. The learning rate is selected via hyperparameter tuning from 1e-4, 5e-5, 1e-5, with 5e-5 yielding the best performance.</p>\n\n",
                "matched_terms": [
                    "gesture",
                    "performance",
                    "moe"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Modeling Turn-Taking with Semantically Informed Gestures",
        "caption": "Table 3: Ablation study on fusion techniques: F1 scores of the Text+Audio+Gestures model evaluated with different existing multimodal fusion methods for turn-taking prediction.",
        "body": "Overall\nhold\nyield\n\n\n\n\nConcatFusion Kurata etal. (2023)\n\n68.2\n78.5\n58.0\n\n\nLMF Lin etal. (2025)\n\n67.9\n77.2\n58.7\n\n\nMoE (Ours)\n69.9\n76.7\n63.1",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Overall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">hold</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">yield</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">ConcatFusion <cite class=\"ltx_cite ltx_citemacro_cite\">Kurata et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib19\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">78.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LMF <cite class=\"ltx_cite ltx_citemacro_cite\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib20\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">67.9</td>\n<td class=\"ltx_td ltx_align_center\">77.2</td>\n<td class=\"ltx_td ltx_align_center\">58.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">MoE (Ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">69.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">76.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">63.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "overall",
            "lmf",
            "concatfusion",
            "ablation",
            "ours",
            "techniques",
            "turntaking",
            "evaluated",
            "prediction",
            "hold",
            "yield",
            "lin",
            "moe",
            "methods",
            "existing",
            "model",
            "kurata",
            "fusion",
            "study",
            "different",
            "scores",
            "textaudiogestures",
            "multimodal"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">When ablating with fusion techniques from prior multimodal turn-taking studies (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Model Comparisons &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), we observe per-class F1 imbalances, with the minority class particularly affected. In contrast, MoE-based fusion with semantically aligned gestures yields more balanced predictions across classes. Future work can explore fusion strategies to further improve alignment between text, audio, and gestures.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In conversation, humans use multimodal cues, such as speech, gestures, and gaze, to manage turn-taking. While linguistic and acoustic features are informative, gestures provide complementary cues for modeling these transitions. To study this, we introduce DnD Gesture++, an extension of the multi-party DnD Gesture corpus enriched with 2,663 semantic gesture annotations spanning iconic, metaphoric, deictic, and discourse types. Using this dataset, we model turn-taking prediction through a Mixture-of-Experts framework integrating text, audio, and gestures. Experiments show that incorporating semantically guided gestures yields consistent performance gains over baselines, demonstrating their complementary role in multimodal turn-taking.</p>\n\n",
                "matched_terms": [
                    "model",
                    "study",
                    "prediction",
                    "turntaking",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans use multimodal cues such as prosody, gaze, and gestures to signal intentions to hold or yield a turn <cite class=\"ltx_cite ltx_citemacro_cite\">Duncan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib2\" title=\"\">1979</a>); Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib32\" title=\"\">2021</a>)</cite>. In particular, semantic gestures serve distinct conversational functions <cite class=\"ltx_cite ltx_citemacro_cite\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>)</cite> and can be categorized into four types: iconic, metaphoric, deictic, and discourse. For example, deictic gestures can signal turn exchange, while metaphoric gestures emphasize utterance content <cite class=\"ltx_cite ltx_citemacro_cite\">Bavelas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib1\" title=\"\">1995</a>); Holler et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib11\" title=\"\">2018</a>)</cite>. Incorporating these gesture types could therefore improve computational models of turn-taking.</p>\n\n",
                "matched_terms": [
                    "yield",
                    "hold",
                    "turntaking",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the role of semantic gestures in turn-taking prediction, we build upon the DnD Gesture dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib27\" title=\"\">2024</a>)</cite>, a multi-party conversational corpus containing synchronized 3D motion, audio, and transcripts from participants in a tabletop game. We manually annotated the six-hour corpus with gesture-type labels following <cite class=\"ltx_cite ltx_citemacro_citet\">McNeill (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib24\" title=\"\">1992</a>)</cite>. The resulting DnD Gesture++ corpus includes 2,663 gesture instances across six hours (444 labels/hour), forming the most densely annotated English dataset of its kind. We further reformat this data for turn-taking prediction task, labeling 12k turns as either <span class=\"ltx_text ltx_font_italic\">hold</span> or <span class=\"ltx_text ltx_font_italic\">yield</span>.</p>\n\n",
                "matched_terms": [
                    "yield",
                    "hold",
                    "turntaking",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we model turn-taking using a Mixture-of-Experts (MoE) framework that fuses text, audio, and gesture modalities through a gating network. Gesture embeddings are semantically enriched using our annotations to better align motion cues with linguistic and acoustic information. Our experiments demonstrate consistent gains from incorporating gestures, particularly when gesture embeddings are semantically supervised. We further analyze the latent space and modality weights from the MoE framework to better understand the contribution of semantic gesture representations. Beyond turn-taking, the dense annotations in DnD Gesture++ provide a valuable resource for tasks such as co-speech gesture generation, enabling more semantically grounded gesture synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Kucherenko et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib18\" title=\"\">2021</a>); Mughal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib26\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "moe",
                    "turntaking",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The coordination of speaking turns in human dialogue is inherently multimodal <cite class=\"ltx_cite ltx_citemacro_cite\">Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib32\" title=\"\">2021</a>)</cite>. Speakers use non-verbal signals such as prosody, gaze, and hand gestures to project turn boundaries and manage conversational flow <cite class=\"ltx_cite ltx_citemacro_cite\">Duncan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib2\" title=\"\">1979</a>); Kendrick et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib16\" title=\"\">2023</a>)</cite>. Gestures serve key semantic functions: pragmatic or discourse-related gestures often signal a <span class=\"ltx_text ltx_font_italic\">yield</span>, while representational gestures like iconic forms indicate to <span class=\"ltx_text ltx_font_italic\">hold</span> the floor <cite class=\"ltx_cite ltx_citemacro_cite\">Bavelas et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib1\" title=\"\">1995</a>); Holler et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib11\" title=\"\">2018</a>)</cite>. Moreover, they serve as reliable cues for turn-taking and help maintain conversational smoothness <cite class=\"ltx_cite ltx_citemacro_cite\">Holler et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib11\" title=\"\">2018</a>); Ter&#160;Bekke et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib34\" title=\"\">2024</a>); Kendrick et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib16\" title=\"\">2023</a>); Holler and Levinson (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib12\" title=\"\">2019</a>); Hofstetter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib10\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "yield",
                    "hold",
                    "turntaking",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early dialogue systems detected turn boundaries using fixed silence thresholds, while recent data-driven approaches use syntactic and pragmatic cues from dialogue transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ekstedt and Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib3\" title=\"\">2020</a>)</cite>. Multimodal approaches further integrate prosody <cite class=\"ltx_cite ltx_citemacro_cite\">Ekstedt and Skantze (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib4\" title=\"\">2022</a>)</cite>, face features <cite class=\"ltx_cite ltx_citemacro_cite\">Russell and Harte (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib30\" title=\"\">2025</a>); Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib20\" title=\"\">2025</a>); Kurata et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib19\" title=\"\">2023</a>)</cite> to enhance accuracy. However, the effect of semantic gestures remains underexplored in turn-taking models. Our work examines whether semantic gesture types enhance turn-taking prediction in language models.</p>\n\n",
                "matched_terms": [
                    "prediction",
                    "lin",
                    "kurata",
                    "turntaking",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To model turn-taking, we convert continuous multi-party conversations in DnD Gesture++ into a structured dataset of discrete prediction instances, following previous works <cite class=\"ltx_cite ltx_citemacro_cite\">Kelterer and Schuppler (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib14\" title=\"\">2025</a>); Jokinen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib13\" title=\"\">2013</a>); Ekstedt et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib5\" title=\"\">2023</a>)</cite>. We segment audio into Inter-Pausal Units (IPUs), continuous speech segments bounded by silence, with each IPU ending at a Transition Relevance Place (TRP) where a speaker change may occur. Turns are labeled <span class=\"ltx_text ltx_font_italic\">yield</span> if another participant speaks next, or <span class=\"ltx_text ltx_font_italic\">hold</span> if the same speaker continues, using a 200 ms silence threshold. Short IPUs are merged with adjacent utterances. This results in 12k turns (7k hold, 5k yield). We split the data into 70% train, 10% validation, and 20% test. Refer to Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Gesture type annotation &#8227; 3 DnD Group Gesture++ &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for more details.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prediction",
                    "yield",
                    "hold",
                    "turntaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We model turn-taking using text, audio, and gestures captured via motion data. Each modality is processed by a dedicated expert within a MoE framework, where the outputs of all experts are fused via a gating network that dynamically weighs each modality based on context. Let <math alttext=\"x_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">x_{m}</annotation></semantics></math> denote the input features for modality <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> and <math alttext=\"f_{m}(x_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{m}(x_{m})</annotation></semantics></math> its expert output. The gating network computes modality weights <math alttext=\"w_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m4\" intent=\":literal\"><semantics><msub><mi>w</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">w_{m}</annotation></semantics></math> using softmax over concatenated weights. The fused representation is a weighted sum of expert outputs <math alttext=\"h_{\\text{fused}}=\\sum_{m=1}^{M}w_{m}\\cdot f_{m}(x_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>h</mi><mtext>fused</mtext></msub><mo rspace=\"0.111em\">=</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mrow><mrow><msub><mi>w</mi><mi>m</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msub><mi>f</mi><mi>m</mi></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">h_{\\text{fused}}=\\sum_{m=1}^{M}w_{m}\\cdot f_{m}(x_{m})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "moe",
                    "turntaking",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, <math alttext=\"h_{fused}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{fused}</annotation></semantics></math> is passed through a linear classifier to predict <span class=\"ltx_text ltx_font_italic\">hold</span> or <span class=\"ltx_text ltx_font_italic\">yield</span>. Text and audio features are obtained from pretrained embeddings, sentence embeddings for text<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\" title=\"\">https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1</a></span></span></span>\nand Wav2Vec2 for audio<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/wav2vec2-large-960h\" title=\"\">https://huggingface.co/facebook/wav2vec2-large-960h</a></span></span></span> and projected via MLP layers.\nGestures are encoded following recent VQ-VAE based gesture tokenization approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib21\" title=\"\">2024</a>); Suresh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib33\" title=\"\">2025</a>)</cite>. Given a sequence of 3D upper body motion <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math>, the encoder network compresses it into a sequence of tokens. Each token <math alttext=\"\\mathbf{z}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119859;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{z}_{i}</annotation></semantics></math> is then quantized via a codebook&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Van Den&#160;Oord et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib35\" title=\"\">2017</a>)</cite> to produce <math alttext=\"\\mathbf{z}^{q}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119859;</mi><mi>i</mi><mi>q</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{q}_{i}</annotation></semantics></math>. The sequence of quantized embeddings is then used to reconstruct the input motion <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math>. Consequently, the VQ-VAE is trained through an MSE loss on reconstructed motion <math alttext=\"\\mathbf{\\hat{x}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119857;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathbf{\\hat{x}}</annotation></semantics></math> and input motion <math alttext=\"\\mathbf{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m7\" intent=\":literal\"><semantics><mi>&#119857;</mi><annotation encoding=\"application/x-tex\">\\mathbf{x}</annotation></semantics></math>. Since the base model lacks semantics, we use gesture type annotations to inject semantic information (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S4.F3\" title=\"Figure 3 &#8227; 4 Multimodal Modeling of Turn-Taking Prediction &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). For each turn, we obtain the gestures token sequences of codebook embeddings from the trained VQ-VAE which are then processed by a transformer encoder <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#bib.bib36\" title=\"\">2017</a>)</cite> which forms the gesture expert. The output of the Transformer is mean pooled over the sequence length to produce a fixed-length representation.</p>\n\n",
                "matched_terms": [
                    "yield",
                    "hold",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work addresses two key research questions: (i) Does the inclusion of body movement features such as gestures improve turn-taking prediction beyond text and audio? (ii) Do semantically informed gesture representations offer advantages over raw motion features?</p>\n\n",
                "matched_terms": [
                    "turntaking",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We systematically compare our Text+Audio+Gesture model against single- and dual-modality baselines. We then evaluate the impact of semantic gesture embeddings from a VQ-VAE versus raw motion features. We also benchmark our MoE-based fusion against existing multimodal turn-taking approaches using the same modality experts for fair comparison. Importantly, our main aim is to assess the effect of gesture representations on turn-taking beyond text and audio, and the MoE setup allows us to quantify the contribution of each modality in this task. Further implementation details can be found in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#A1.SS2\" title=\"A.2 Implementation Details &#8227; Appendix A Appendix &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "existing",
                    "model",
                    "moe",
                    "turntaking",
                    "fusion",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Model Comparisons &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, text performs best among single-modality experts, followed by audio. Gesture only models show lower performance, with little difference between non-semantic and semantic variants. All results are averaged over three random seeds, and mean scores are reported.\nMultimodal fusion outperforms single-modality models with Text+Audio+Gesture model achieving the highest overall macro-F1 compared to the Text+Audio variant (p &lt; 0.05). Notably, the semantically aligned gesture representation outperforms the non-semantic one (p = 0.05), showing that semantic supervision enhances multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "model",
                    "scores",
                    "fusion",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Analysing gesture representations &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we visualize the gesture embeddings with and without semantic supervision. The semantically aligned embeddings form more distinct clusters corresponding to different gesture types, which facilitates better alignment with text and audio modalities. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19350v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Analysing gesture representations &#8227; 5 Experiments &#8227; Modeling Turn-Taking with Semantically Informed Gestures\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> further shows the average weight contribution for each modality in the MoE framework for the test samples. Using semantically aligned gestures increases the weights for both gestures and text, likely due to improved cross-modal alignment, which in turn supports better overall task performance.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "moe",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we investigate the role of semantic gestures in multimodal turn-taking prediction. We extended the multi-party DnD Gesture corpus with semantic annotations to create DnD Gesture++. Our results show that semantically guided gestures improve turn-taking modeling, especially when fused with text and audio. Beyond turn-taking, the corpus also supports other multiparty interaction tasks, such as co-speech gesture generation and listener backchannel prediction.</p>\n\n",
                "matched_terms": [
                    "turntaking",
                    "multimodal",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this work is grounded in a naturalistic, game-based interaction setup, the specificity of the context may limit the generalisability of the findings. Future research should examine how semantic gestures are distributed in other interaction settings, such as dyadic conversations. Our primary goal was to examine how semantic gestures contribute to turn-taking when combined with the two most widely used modalities for this task, text and audio. Although we employ a MoE fusion framework, future work could investigate more advanced fusion mechanisms to better capture the complex interactions between modalities.</p>\n\n",
                "matched_terms": [
                    "moe",
                    "turntaking",
                    "fusion"
                ]
            }
        ]
    }
}