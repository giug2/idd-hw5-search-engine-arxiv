{
    "S5.T1": {
        "caption": "Table 1: Unit error rate (UER) for different EMG feature representations when predicting dis‚Äã(‚Ñã)HuBERT\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}} units. The dataset and preprocessing details are described in section¬†3. Lower UER is better.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">Model input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\">UER</span> (<math alttext=\"\\%\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.800em\">%</mo><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo></mrow><annotation encoding=\"application/x-tex\">\\%\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">64.18 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.68</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m6\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">62.96 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.41</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\"><math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m8\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">58.7 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.49</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "unit",
            "feature",
            "details",
            "when",
            "‚Üìdownarrow",
            "error",
            "rate",
            "input",
            "described",
            "predicting",
            "units",
            "lower",
            "dis‚Äã‚Ñãhuberttextttdismathcalhtextschubert",
            "ùîª‚Äã‚Ñ∞mathbbdmathcale",
            "emg",
            "vec‚Äã‚Ñ∞textttvecmathcale",
            "model",
            "preprocessing",
            "better",
            "uer",
            "¬±pm",
            "vec‚Äã‚Ñ¨textttvecmathcalb",
            "representations",
            "dataset",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We present the results for <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> decoding in table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T1\" title=\"Table 1 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, or <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m4\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math> were provided as input to the TDS network, which was trained to predict the corresponding <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m5\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units. For example, for the sentence <math alttext=\"{}_{\\textsc{t-start}}&lt;\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.SS3.p3.m6\" intent=\":literal\"><semantics><mmultiscripts><mo>&lt;</mo><mprescripts/><mtext class=\"ltx_font_smallcaps\">t-start</mtext><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\textsc{t-start}}&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"--ltx-fg-color:#0000FF;\">It Was Paid For<math alttext=\"&gt;_{\\textsc{t-end}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m7\" intent=\":literal\"><semantics><msub><mo mathcolor=\"#000000\" style=\"--ltx-fg-color:#000000;\">&gt;</mo><mtext class=\"ltx_font_smallcaps\" mathcolor=\"#000000\" style=\"--ltx-fg-color:#000000;\">t-end</mtext></msub><annotation encoding=\"application/x-tex\">&gt;_{\\textsc{t-end}}</annotation></semantics></math></span> with target <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m8\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-fg-color:#004D00;\">71-12-71-12-4-12-4-40-93-86-13-58-32-1-99-&#8230;</span>, the TDS model is trained to learn the mapping from <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m9\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m10\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, or <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m11\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math> to <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m12\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units using the CTC loss. During inference, the model outputs probabilities for all 100 <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m13\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units at each time step, and we decode these outputs using greedy search. For instance, the decoded sequence might be <span class=\"ltx_text ltx_font_smallcaps\" style=\"--ltx-fg-color:#BF0040;\">71-12-57-4-54-40-93-86-13-58-16-14-76-6-36-&#8230;</span>. We compute the unit error rate (UER) as the Levenshtein distance between the target and predicted <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m14\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> unit sequences, normalized by the length of the target sequence.</p>\n\n",
            "<p class=\"ltx_p\">As shown in tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T1\" title=\"Table 1 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T2\" title=\"Table 2 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math> outperforms <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>. <math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math> was computed using 31 linearly spaced frequency bins, and for any given time frame, both <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m4\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math> and <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math> have 991 dimensions. Notably, even <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, which has only 31 dimensions (i.e., a dimensionality lower by roughly the square root of the others), performs better than <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m7\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>. This finding is consistent with the linear mapping results shown in figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 &#8459; can linearly map to &#120123;&#8290;(&#8496;) &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F4\" title=\"Figure 4 &#8227; 5.2 &#8459; can linearly map to &#120123;&#8290;(&#8496;) &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">As shown in tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T1\" title=\"Table 1 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T2\" title=\"Table 2 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the model decodes phoneme sequences more accurately than <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units. We are currently exploring phoneme-guided decoding strategies for <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units to further improve accuracy. In addition, we are developing methods and models to objectively assess the perceptual quality of the synthesized audio and to compute metrics such as word error rate (WER) and character error rate (CER).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present a neuromuscular speech interface that translates electromyographic (EMG) signals collected from orofacial muscles during speech articulation directly into audio. We show that self-supervised speech (SS) representations exhibit a strong linear relationship with the electrical power of muscle action potentials: SS features can be linearly mapped to EMG power with a correlation of <math alttext=\"r=0.85\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">r=0.85</annotation></semantics></math>. Moreover, EMG power vectors corresponding to different articulatory gestures form structured and separable clusters in feature space. This relationship: <span class=\"ltx_text ltx_font_smallcaps\">SS features</span> <math alttext=\"\\xrightarrow{\\texttt{linear mapping}}\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_monospace\">linear mapping</mtext></mover><annotation encoding=\"application/x-tex\">\\xrightarrow{\\texttt{linear mapping}}</annotation></semantics></math> <span class=\"ltx_text ltx_font_smallcaps\">EMG power</span> <math alttext=\"\\xrightarrow{\\texttt{gesture-specific clustering}}\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mover accent=\"true\"><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_monospace\">gesture-specific clustering</mtext></mover><annotation encoding=\"application/x-tex\">\\xrightarrow{\\texttt{gesture-specific clustering}}</annotation></semantics></math> <span class=\"ltx_text ltx_font_smallcaps\">articulatory movements</span>, highlights that SS models implicitly encode articulatory mechanisms. Leveraging this property, we directly map EMG signals to SS feature space and synthesize speech, enabling end-to-end EMG-to-speech generation without explicit articulatory models and vocoder training<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code will be made publicly available upon completion of the project.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "different",
                    "feature",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this article, we present a method for leveraging self-supervised speech (SS) models to convert electromyographic (EMG) signals collected during speech articulation directly into audio, without the need for explicitly training a vocoder. Our key insight arises from the observation that speech features derived from SS models can be linearly mapped to the electrical power of muscle action potentials. Because these action potential powers corresponding to different articulatory gestures form structured and separable clusters in feature space, it follows that SS models implicitly encode articulatory information. This relationship suggests that EMG power can serve as an effective intermediate representation for mapping muscle activity to speech features. We exploit this property to design a lightweight and interpretable EMG-to-audio conversion model that leverages EMG power representations in conjunction with SS models. Such an approach has the potential to enable efficient few-shot and zero-shot learning of EMG-to-audio mappings&#8212;an especially valuable property given the limited availability of EMG datasets and the frequent data distributional drift caused by factors such as electrode displacement, changes in skin moisture, and other recording variabilities.</p>\n\n",
                "matched_terms": [
                    "emg",
                    "model",
                    "feature",
                    "representations",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Converting non-speech signals into audio has been explored in several modalities, including lip movements-to-speech <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">kim2021lip</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">prajwal2020learning</span> </a></cite>, motor cortex neural signals-to-speech <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">wairagkar2025instantaneous</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">metzger2023high</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">littlejohn2025streaming</span> </a></cite>, and EMG-to-speech <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">gaddy2020digital</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">gaddy2021improved</span> </a></cite>. Most existing approaches in these domains <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">kim2021lip</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">prajwal2020learning</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">wairagkar2025instantaneous</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">gaddy2020digital</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">gaddy2021improved</span> </a></cite> assume that the alignment between the input signals (e.g., video or neural activity) and the corresponding audio is known. In contrast, we address a more challenging scenario similar to <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">metzger2023high</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">littlejohn2025streaming</span> </a></cite>, where the alignment between the neural activity (in our case, EMG) and speech is <span class=\"ltx_text ltx_font_italic\">unknown</span>. This setting requires the model not only to learn the mapping between EMG activity and audio but also to infer the underlying alignment from an exponential search space.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Work in <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">littlejohn2025streaming</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">metzger2023high</span> </a></cite> addresses this alignment-free setting by training an encoder that takes motor cortex neural signals as input and learns to map them to discrete HuBERT units <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">lakhotia-etal-2021-generative</span> </a></cite>, which are then passed to a pretrained vocoder (Tacotron <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">Tacotron</span> </a></cite>) following the pipeline in <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">lakhotia-etal-2021-generative</span> </a></cite>. We adopt a similar high-level pipeline for EMG-to-speech conversion. However, our approach explicitly leverages the <span class=\"ltx_text ltx_font_italic\">geometric structure</span> of EMG signals and their relationship to self-supervised (SS) speech representations to design an efficient encoder.</p>\n\n",
                "matched_terms": [
                    "input",
                    "units",
                    "representations",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these shortcomings, we create and open-source a large-vocabulary corpus (approximately 9 hours of EMG speech data) comprising over 6,800 unique words, articulated at a natural speaking rate of around 115 words per minute. Since data scarcity and signal distribution shifts remain core challenges in neural interface research, our approach focuses on understanding the intrinsic structure of EMG signals to guide encoder design grounded in articulatory mechanisms.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adapt the language corpora from <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">willett2023high</span> </a></cite>, who demonstrated a speech brain-computer interface by translating neural spikes from the motor cortex into speech. The dataset comprises an extensive English corpus containing approximately 6,800 unique words and 9660 sentences. The corpus includes sentences of varying lengths, with the subject articulating at a normal speaking rate, averaging 115 words per minute. The dataset is divided into training, validation, and test sets containing 7000, 1000, and 1660 sentences, respectively. Sentences in the test set are not included in either the training or validation sets.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The data collection environment was carefully controlled to eliminate AC electrical interference. EMG signals underwent minimal preprocessing. The signal from the <span class=\"ltx_text ltx_font_smallcaps\">reference</span> channel (electrode 32) was subtracted from all other EMG data channels. The resulting signals were then bandpass filtered using a third-order Butterworth filter between 80 and 1000&#160;Hz and segmented according to sentence start and end times based on synchronized timestamps.</p>\n\n",
                "matched_terms": [
                    "preprocessing",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EMG covariance matrices:</span> for an EMG signal <math alttext=\"E_{\\mathcal{V}\\times\\tau}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>&#964;</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{\\mathcal{V}\\times\\tau}</annotation></semantics></math> collected from <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> sensor nodes over a duration of <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> samples, we construct a symmetric positive definite (SPD) covariance matrix <math alttext=\"\\mathcal{E}_{\\mathcal{V}\\times\\mathcal{V}}=\\epsilon EE^{\\top}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi></mrow></msub><mo>=</mo><mrow><mi>&#1013;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>E</mi><mo>&#8868;</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{E}_{\\mathcal{V}\\times\\mathcal{V}}=\\epsilon EE^{\\top}</annotation></semantics></math>, where <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> is a scaling factor. We denote the diagonal of <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> as <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> and its lower triangular part as <math alttext=\"\\lfloor\\mathcal{E}\\rfloor\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#8970;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">&#8971;</mo></mrow><annotation encoding=\"application/x-tex\">\\lfloor\\mathcal{E}\\rfloor</annotation></semantics></math>. The vector <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> represents the muscle action potential power at each electrode <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> during the interval <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m11\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math>, while the off-diagonal elements capture the pairwise cross-channel covariance, reflecting the spatial co-activation structure across electrodes. A vectorized representation of <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m12\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> is denoted as <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m13\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, a column vector of dimension <math alttext=\"\\mathcal{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m14\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">\\mathcal{V}^{2}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "emg",
                    "vec‚Äã‚Ñ∞textttvecmathcale",
                    "lower",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EMG spectrograms:</span> for an EMG signal <math alttext=\"E_{\\mathcal{V}\\times\\tau}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>&#964;</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{\\mathcal{V}\\times\\tau}</annotation></semantics></math> collected from <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> sensor nodes at a sampling frequency <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math>, we compute the short-time Fourier transform (STFT) over successive time windows to obtain a power spectrogram representation <math alttext=\"\\mathcal{S}_{\\mathcal{V}\\times F\\times\\tau^{\\prime}}=\\bigl|\\mathrm{STFT}(E_{\\mathcal{V}\\times\\tau})\\bigr|^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>&#964;</mi><mo>&#8242;</mo></msup></mrow></msub><mo>=</mo><msup><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo><mrow><mi>STFT</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>&#964;</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo></mrow><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}_{\\mathcal{V}\\times F\\times\\tau^{\\prime}}=\\bigl|\\mathrm{STFT}(E_{\\mathcal{V}\\times\\tau})\\bigr|^{2}</annotation></semantics></math>, where <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> denotes the number of frequency bins and <math alttext=\"\\tau^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m6\" intent=\":literal\"><semantics><msup><mi>&#964;</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">\\tau^{\\prime}</annotation></semantics></math> the number of time frames. Each slice <math alttext=\"\\mathcal{S}_{\\mathcal{V}\\times F}^{(t)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m7\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{S}_{\\mathcal{V}\\times F}^{(t)}</annotation></semantics></math> captures the frequency-domain energy distribution of EMG activity across <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m8\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> electrodes at time frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m9\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. To reduce the spectral granularity, we bin the frequency axis into <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m10\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> frequency bands using <math alttext=\"\\mathcal{B}_{\\mathcal{V}\\times B\\times\\tau^{\\prime}}(b)=\\frac{1}{|F_{b}|}\\sum_{f\\in F_{b}}\\mathcal{S}_{\\mathcal{V}\\times f\\times\\tau^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m11\" intent=\":literal\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>B</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>&#964;</mi><mo>&#8242;</mo></msup></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>b</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><msub><mo>&#8721;</mo><mrow><mi>f</mi><mo>&#8712;</mo><msub><mi>F</mi><mi>b</mi></msub></mrow></msub><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>f</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>&#964;</mi><mo>&#8242;</mo></msup></mrow></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{B}_{\\mathcal{V}\\times B\\times\\tau^{\\prime}}(b)=\\frac{1}{|F_{b}|}\\sum_{f\\in F_{b}}\\mathcal{S}_{\\mathcal{V}\\times f\\times\\tau^{\\prime}}</annotation></semantics></math>, where <math alttext=\"F_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m12\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>b</mi></msub><annotation encoding=\"application/x-tex\">F_{b}</annotation></semantics></math> is the set of frequency bins assigned to band <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m13\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. The matrix <math alttext=\"\\mathcal{B}^{(t)}_{\\mathcal{V}\\times B}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m14\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>B</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{B}^{(t)}_{\\mathcal{V}\\times B}</annotation></semantics></math> thus represents the band power of muscle activity at each electrode across frequency bands during frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m15\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. In practice, we use either five log-spaced bands <math alttext=\"B_{1}=[80,125]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m16\" intent=\":literal\"><semantics><mrow><msub><mi>B</mi><mn>1</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>80</mn><mo>,</mo><mn>125</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B_{1}=[80,125]</annotation></semantics></math> Hz, <math alttext=\"B_{2}=[125,250]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m17\" intent=\":literal\"><semantics><mrow><msub><mi>B</mi><mn>2</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>125</mn><mo>,</mo><mn>250</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B_{2}=[125,250]</annotation></semantics></math> Hz, <math alttext=\"B_{3}=[250,375]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m18\" intent=\":literal\"><semantics><mrow><msub><mi>B</mi><mn>3</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>250</mn><mo>,</mo><mn>375</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B_{3}=[250,375]</annotation></semantics></math> Hz, <math alttext=\"B_{4}=[375,687.5]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m19\" intent=\":literal\"><semantics><mrow><msub><mi>B</mi><mn>4</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>375</mn><mo>,</mo><mn>687.5</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B_{4}=[375,687.5]</annotation></semantics></math> Hz, and <math alttext=\"B_{5}=[687.5,1000]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m20\" intent=\":literal\"><semantics><mrow><msub><mi>B</mi><mn>5</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>687.5</mn><mo>,</mo><mn>1000</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B_{5}=[687.5,1000]</annotation></semantics></math> Hz, following <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">Kaifosh2025Generic</span> </a></cite>, or 31 linearly spaced frequency bands between <math alttext=\"80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m21\" intent=\":literal\"><semantics><mn>80</mn><annotation encoding=\"application/x-tex\">80</annotation></semantics></math> and <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m22\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> Hz. A vectorized representation of <math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m23\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math> is denoted as <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m24\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>, a column vector of dimension <math alttext=\"\\mathcal{V}B\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m25\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}B</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "vec‚Äã‚Ñ¨textttvecmathcalb",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio features from SS models:</span> for a speech waveform <math alttext=\"a(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">a(t)</annotation></semantics></math>, we extract self-supervised (SS) representations by passing the signal through a pretrained model <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, yielding <math alttext=\"\\mathcal{H}=\\mathcal{S}(a(t))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}=\\mathcal{S}(a(t))</annotation></semantics></math>. The model <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math> can be instantiated as <span class=\"ltx_text ltx_font_smallcaps\">Wav2Vec 2.0</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">baevski2020wav2vec</span> </a></cite>, <span class=\"ltx_text ltx_font_smallcaps\">HuBERT</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">hsu2021hubert</span> </a></cite>, or <span class=\"ltx_text ltx_font_smallcaps\">WavLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">chen2022wavlm</span> </a></cite>. We denote the column vector of SS audio representations by <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> throughout the article.</p>\n\n",
                "matched_terms": [
                    "model",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct sequences of <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>, <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, and <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math>, which are emitted every 20&#160;ms and use a context length of 25&#160;ms. For temporal relation modeling, we employ a time depth separable convolutional network (TDS), as described below.</p>\n\n",
                "matched_terms": [
                    "described",
                    "vec‚Äã‚Ñ∞textttvecmathcale",
                    "vec‚Äã‚Ñ¨textttvecmathcalb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We collected data from 12 participants, each performing 13 distinct orofacial movements, with 10 repetitions per movement. The set of movements includes <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">cheeks &#8211; puff out<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">cheeks &#8211; suck in<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">jaw &#8211; dropdown<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">jaw &#8211; move backward<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m8\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">jaw &#8211; move forward<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m10\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">jaw &#8211; move left<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m12\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">jaw &#8211; move right<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m14\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">lips &#8211; pucker<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m16\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">lips &#8211; smile<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m18\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">lips &#8211; tuck as if blotting<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m20\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">tongue &#8211; back of lower teeth<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m22\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m23\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">tongue &#8211; back of upper teeth<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m24\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m25\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">tongue &#8211; roof of the mouth<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m26\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>. These movements were selected to span a broad range of articulatory gestures involved in natural speech production, encompassing mechanisms such as lip rounding, jaw positioning, and tongue placement, which are essential for producing different phonemes.</p>\n\n",
                "matched_terms": [
                    "different",
                    "lower"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each gesture is represented by an EMG signal matrix <math alttext=\"E_{22\\times 7500}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mn>22</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7500</mn></mrow></msub><annotation encoding=\"application/x-tex\">E_{22\\times 7500}</annotation></semantics></math>, where 22 denotes the number of electrode channels<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We used a subset of the 22 electrodes shown in figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S3.F1\" title=\"Figure 1 &#8227; 3 Data &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, excluding those on the right side of the neck, for this data. Each gesture was performed over a 1.5&#160;s interval, which corresponds to 7500 time steps at a sampling frequency of 5000&#160;Hz. This dataset was collected independently of the one described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S3\" title=\"3 Data &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Unless stated otherwise, all references to data throughout the manuscript refer to the dataset described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S3\" title=\"3 Data &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\n</span></span></span>. The corresponding symmetric positive definite (SPD) covariance matrix for each gesture is denoted as <math alttext=\"\\mathcal{E}_{22\\times 22}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mrow><mn>22</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>22</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{E}_{22\\times 22}</annotation></semantics></math>, and its diagonal <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> is a 22-dimensional vector representing the per-channel EMG power.</p>\n\n",
                "matched_terms": [
                    "described",
                    "emg",
                    "dataset",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The vectors <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> corresponding to different orofacial gestures naturally form distinct clusters, as shown in figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F2\" title=\"Figure 2 &#8227; 5.1 &#8496; and &#120123;&#8290;(&#8496;) encode articulatory information &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. We further quantified their discriminability using the unsupervised <span class=\"ltx_text ltx_font_italic\">k</span>-medoids clustering algorithm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">Kaufman1990PAM</span> </a></cite>, achieving a classification accuracy of 70.6% based on <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> (averaged across 12 subjects). When using the full covariance matrix <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> with the geodesic distance defined in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S4.E1\" title=\"In 4.1 Electromyography (EMG) &#8227; 4 Methods &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the <span class=\"ltx_text ltx_font_italic\">k</span>-medoids classification accuracy increased to 73.7%, both well above the random chance level of 10%.</p>\n\n",
                "matched_terms": [
                    "different",
                    "when",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that both <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p4.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> and <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> naturally encode discriminative articulatory information. While <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> alone is sufficient to distinguish between different orofacial movements, incorporating the full covariance structure in <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p4.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> leads to improved decoding accuracy.</p>\n\n",
                "matched_terms": [
                    "different",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that other widely used EMG features such as log-spectrograms&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">sivakumaremg2qwerty</span> </a></cite> or rectified time-domain signals&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">Halliday2010Rectification</span> </a></cite> cannot be directly probed to verify whether such a structured representation exists. When raw EMG signals <math alttext=\"E_{22\\times 7500}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m1\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mn>22</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7500</mn></mrow></msub><annotation encoding=\"application/x-tex\">E_{22\\times 7500}</annotation></semantics></math> are featurized using spectrograms or rectified signals, the temporal dimension may be reduced in granularity but is not collapsed into a single frame. In contrast, covariance-based representations aggregate the temporal information within a single frame, yielding fixed-dimensional features such as <math alttext=\"\\mathbb{D}(\\mathcal{E})\\in\\mathbb{R}^{22}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mn>22</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})\\in\\mathbb{R}^{22}</annotation></semantics></math> or <math alttext=\"\\mathcal{E}\\in\\mathbb{R}^{22\\times 22}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>22</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>22</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{E}\\in\\mathbb{R}^{22\\times 22}</annotation></semantics></math>. (We analyze <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m4\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> using the standard Euclidean distance, while <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> is compared using the specialized metric defined in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S4.E1\" title=\"In 4.1 Electromyography (EMG) &#8227; 4 Methods &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.) Consequently, there is no low-dimensional equivalent of <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> or <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m7\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> when using log-spectrogram or rectified features that captures articulatory structure in the same way.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "emg",
                    "when",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test whether there exists a linear mapping defined by a weight matrix <math alttext=\"W\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi>W</mi><annotation encoding=\"application/x-tex\">W</annotation></semantics></math> and bias <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> such that <math alttext=\"\\mathbb{D}(\\mathcal{E})\\approx W\\mathcal{H}+b\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8776;</mo><mrow><mrow><mi>W</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></mrow><mo>+</mo><mi>b</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})\\approx W\\mathcal{H}+b</annotation></semantics></math> with a high correlation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>We actually aim to probe whether <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> (<math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m2\" intent=\":literal\"><semantics><mn>768</mn><annotation encoding=\"application/x-tex\">768</annotation></semantics></math>&#8211;<math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m3\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math> dimensions) can map to <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m4\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math> (<math alttext=\"991\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m5\" intent=\":literal\"><semantics><mn>991</mn><annotation encoding=\"application/x-tex\">991</annotation></semantics></math> dimensions). However, the resulting <math alttext=\"\\sim 10^{6}\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m6\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><msup><mn>10</mn><mn>6</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sim 10^{6}</annotation></semantics></math>-parameter linear transformation would be severely ill-posed and dominated by noise without massive data and strong regularization. To make this analysis tractable, we use <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m7\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> as a proxy because it provides a compact, well-conditioned, and physically meaningful representation grounded in articulatory mechanisms, making it well suited for linear probing. Importantly, this substitution is justified because both <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m8\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> and <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m9\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> encode structured articulatory information, and the latter serves as a low-dimensional surrogate for the former, as shown in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.SS1\" title=\"5.1 &#8496; and &#120123;&#8290;(&#8496;) encode articulatory information &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "vec‚Äã‚Ñ∞textttvecmathcale",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the training set described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S3\" title=\"3 Data &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> to learn this mapping and evaluate it on the test set. We report the Pearson correlation between the predicted sequences <math alttext=\"\\mathbb{D}(\\mathcal{E}^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E}^{\\prime})</annotation></semantics></math> and the ground-truth <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> on the test set. The representations <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> are extracted using <span class=\"ltx_text ltx_font_smallcaps\">HuBERT</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">hsu2021hubert</span> </a></cite>, <span class=\"ltx_text ltx_font_smallcaps\">Wav2Vec 2.0</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">baevski2020wav2vec</span> </a></cite>, and <span class=\"ltx_text ltx_font_smallcaps\">WavLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">chen2022wavlm</span> </a></cite>. We evaluate <span class=\"ltx_text ltx_font_smallcaps\">base</span> models with a hidden dimension of 768 and 12 transformer layers, <span class=\"ltx_text ltx_font_smallcaps\">large</span> models with a hidden dimension of 1024 and 24 transformer layers, and <span class=\"ltx_text ltx_font_smallcaps\">fine-tuned (ft)</span> models that have been trained for automatic speech recognition (ASR).</p>\n\n",
                "matched_terms": [
                    "representations",
                    "described",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Correlation coefficients (<em class=\"ltx_emph ltx_font_italic\">r</em>) across models and layers are shown in figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 &#8459; can linearly map to &#120123;&#8290;(&#8496;) &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. We find that a simple linear model can predict <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> from <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> with a correlation as high as <math alttext=\"r=0.85\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">r=0.85</annotation></semantics></math>. The layer-wise trends across different models partially mirror the observations reported in <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">cho2023evidence</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">cho2024self</span> </a></cite> for electromagnetic articulography (EMA), where two local peaks were consistently observed across models. In our case, we observe two local peaks for <span class=\"ltx_text ltx_font_smallcaps\">Wav2Vec 2.0</span> models but only a single dominant peak for <span class=\"ltx_text ltx_font_smallcaps\">HuBERT</span> and <span class=\"ltx_text ltx_font_smallcaps\">WavLM</span> models. A sharp decline in correlation emerges in the upper layers of fine-tuned models, reflecting the growing influence of task-specific objectives. This effect is especially pronounced for <span class=\"ltx_text ltx_font_smallcaps\">Wav2Vec 2.0</span> compared to <span class=\"ltx_text ltx_font_smallcaps\">HuBERT</span> and <span class=\"ltx_text ltx_font_smallcaps\">WavLM</span>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, for the <span class=\"ltx_text ltx_font_smallcaps\">HuBERT-base</span> model, the peak correlation at layer&#160;6 aligns with the layer previously identified as optimal for discrete speech resynthesis and spoken language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">lakhotia-etal-2021-generative</span> </a></cite>. While prior work established this empirical result, the mechanistic basis for this peak remained unclear. Our analysis provides a principled interpretation: layer&#160;6 exhibits the strongest linear predictive power for <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, which encodes structured and discriminative articulatory information (i.e., different articulatory gestures such as tongue and jaw positions naturally form separable clusters). This tight alignment between articulatory structure and model representations offers a direct explanation for why layer&#160;6 is particularly effective for downstream speech resynthesis and language modeling. In short, the layer that best captures articulatory mechanisms is also the one that yields the strongest downstream performance, providing convergent evidence for its functional role.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "different",
                    "model",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also examined whether a similar linear mapping exists between EMG spectrogram features (<math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>) and <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math>. Frequency bands of <math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math> are obtained using five log-spaced frequency bins, as described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S4\" title=\"4 Methods &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. However, the resulting correlation coefficients are substantially lower, with a maximum correlation of approximately <math alttext=\"r=0.57\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.57</mn></mrow><annotation encoding=\"application/x-tex\">r=0.57</annotation></semantics></math> (figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F4\" title=\"Figure 4 &#8227; 5.2 &#8459; can linearly map to &#120123;&#8290;(&#8496;) &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). For comparison, we also computed correlations for linear mappings between <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math> (audio spectrograms) and <math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math> (<math alttext=\"r=0.37\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m7\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.37</mn></mrow><annotation encoding=\"application/x-tex\">r=0.37</annotation></semantics></math>) and between <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m8\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math> and <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m9\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> (<math alttext=\"r=0.61\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m10\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.61</mn></mrow><annotation encoding=\"application/x-tex\">r=0.61</annotation></semantics></math>), both of which are considerably lower than the correlation between <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m11\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> and <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m12\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "vec‚Äã‚Ñ¨textttvecmathcalb",
                    "lower",
                    "emg",
                    "described",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The above observations indicate that among the different EMG feature representations considered, <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m1\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> exhibits the strongest linear alignment with the self-supervised speech feature space <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math>. This strong correspondence suggests that <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> and <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> encode highly compatible representations, making them particularly well suited for EMG-to-audio learning. In contrast, EMG spectrogram features (<math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math>) and their alignment with audio features (<math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>) yield notably weaker correlations. These findings imply that, while <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m7\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math> and <math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m8\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math> share some structure, self-supervised representations provide a more robust and articulatorily grounded intermediate latent space. Consequently, pairing <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m9\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> with <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> offers the most effective pathway for EMG-to-audio mapping.</p>\n\n",
                "matched_terms": [
                    "emg",
                    "feature",
                    "representations",
                    "different",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The existence of a simple linear mapping from <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> to <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> is significant: it reveals that the self-supervised representations <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> inherently encode articulatory structure reflecting underlying muscle activations. This forward direction is well posed &#8212; <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> has moderate dimensionality (<math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m5\" intent=\":literal\"><semantics><mn>768</mn><annotation encoding=\"application/x-tex\">768</annotation></semantics></math>&#8211;<math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m6\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math>), <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> is low dimensional (<math alttext=\"31\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m8\" intent=\":literal\"><semantics><mn>31</mn><annotation encoding=\"application/x-tex\">31</annotation></semantics></math>), and the mapping can be stably estimated. In contrast, the inverse problem <math alttext=\"\\mathbb{D}(\\mathcal{E})\\rightarrow\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">&#8594;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})\\rightarrow\\mathcal{H}</annotation></semantics></math> is underdetermined, non-invertible in the linear case, and especially ill-posed when temporal alignments are unknown. Nonetheless, the existence of the forward mapping provides strong evidence that <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> encodes articulatory mechanisms, motivating structured nonlinear approaches for the inverse direction rather than expecting a trivial linear inversion.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "when",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this observation, we address the problem of predicting <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> from EMG features (<math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, or <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>) without explicit temporal alignments. Since linear inversion is ill-posed, we model this mapping using a nonlinear sequence-to-sequence architecture capable of capturing the structured dependencies in <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math>. Specifically, EMG features are fed into a TDS convolutional network (section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S4\" title=\"4 Methods &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), which predicts discrete units derived from <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math>. We use the 100-unit discrete representation from layer 6 of the <span class=\"ltx_text ltx_font_smallcaps\">HuBERT-base</span> model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">lakhotia-etal-2021-generative</span> </a></cite>, denoted <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math>. The model is trained with the connectionist temporal classification (CTC) loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">graves2006connectionist</span> </a></cite>, enabling alignment-free learning between EMG sequences and <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math>. Finally, the predicted <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m9\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> sequence is passed to a pretrained Tacotron vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">Tacotron</span> </a></cite> to generate audio waveforms. The end-to-end architecture is shown in figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "vec‚Äã‚Ñ∞textttvecmathcale",
                    "model",
                    "predicting",
                    "units",
                    "vec‚Äã‚Ñ¨textttvecmathcalb",
                    "emg",
                    "dis‚Äã‚Ñãhuberttextttdismathcalhtextschubert",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also present the results of phoneme-level decoding in table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T2\" title=\"Table 2 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. For the sentence <math alttext=\"{}_{\\textsc{t-start}}&lt;\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.SS3.p4.m1\" intent=\":literal\"><semantics><mmultiscripts><mo>&lt;</mo><mprescripts/><mtext class=\"ltx_font_smallcaps\">t-start</mtext><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\textsc{t-start}}&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"--ltx-fg-color:#0000FF;\">It Was Paid For<math alttext=\"&gt;_{\\textsc{t-end}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p4.m2\" intent=\":literal\"><semantics><msub><mo mathcolor=\"#000000\" style=\"--ltx-fg-color:#000000;\">&gt;</mo><mtext class=\"ltx_font_smallcaps\" mathcolor=\"#000000\" style=\"--ltx-fg-color:#000000;\">t-end</mtext></msub><annotation encoding=\"application/x-tex\">&gt;_{\\textsc{t-end}}</annotation></semantics></math></span> with the corresponding phonemic transcription <span class=\"ltx_text ltx_font_smallcaps\" style=\"--ltx-fg-color:#004D00;\">ih-t <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> w-aa-z <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> p-ey-d <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> f-ao-r</span>, the TDS model is trained to learn the mapping from <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p4.m6\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p4.m7\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, or <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p4.m8\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math> to phoneme sequences using the CTC loss. During inference, the model outputs probabilities for all 40 English phonemes at each time step, and the predictions are decoded using greedy search. For example, the decoded output might be <span class=\"ltx_text ltx_font_smallcaps\" style=\"--ltx-fg-color:#BF0040;\">ih-t <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> w-aa-z <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> p-ey-t <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> f-ao-r</span>. We compute the phoneme error rate (PER) as the Levenshtein distance between the target and decoded phoneme sequences, normalized by the length of the target sequence.</p>\n\n",
                "matched_terms": [
                    "vec‚Äã‚Ñ∞textttvecmathcale",
                    "rate",
                    "model",
                    "vec‚Äã‚Ñ¨textttvecmathcalb",
                    "error",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Phoneme error rate (PER) for different EMG feature representations when predicting phonemes. The dataset and preprocessing details are described in section¬†3. Lower PER is better.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">Model input</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\">PER</span> (<math alttext=\"\\%\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.800em\">%</mo><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo></mrow><annotation encoding=\"application/x-tex\">\\%\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">53.79 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 2.02</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">50.17 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.66</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\"><math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m6\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">41.42 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.77</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "feature",
            "details",
            "when",
            "‚Üìdownarrow",
            "phonemes",
            "error",
            "rate",
            "input",
            "described",
            "phoneme",
            "predicting",
            "lower",
            "ùîª‚Äã‚Ñ∞mathbbdmathcale",
            "emg",
            "vec‚Äã‚Ñ∞textttvecmathcale",
            "model",
            "preprocessing",
            "better",
            "¬±pm",
            "vec‚Äã‚Ñ¨textttvecmathcalb",
            "representations",
            "dataset",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We also present the results of phoneme-level decoding in table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T2\" title=\"Table 2 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. For the sentence <math alttext=\"{}_{\\textsc{t-start}}&lt;\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.SS3.p4.m1\" intent=\":literal\"><semantics><mmultiscripts><mo>&lt;</mo><mprescripts/><mtext class=\"ltx_font_smallcaps\">t-start</mtext><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\textsc{t-start}}&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"--ltx-fg-color:#0000FF;\">It Was Paid For<math alttext=\"&gt;_{\\textsc{t-end}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p4.m2\" intent=\":literal\"><semantics><msub><mo mathcolor=\"#000000\" style=\"--ltx-fg-color:#000000;\">&gt;</mo><mtext class=\"ltx_font_smallcaps\" mathcolor=\"#000000\" style=\"--ltx-fg-color:#000000;\">t-end</mtext></msub><annotation encoding=\"application/x-tex\">&gt;_{\\textsc{t-end}}</annotation></semantics></math></span> with the corresponding phonemic transcription <span class=\"ltx_text ltx_font_smallcaps\" style=\"--ltx-fg-color:#004D00;\">ih-t <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> w-aa-z <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> p-ey-d <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> f-ao-r</span>, the TDS model is trained to learn the mapping from <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p4.m6\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p4.m7\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, or <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p4.m8\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math> to phoneme sequences using the CTC loss. During inference, the model outputs probabilities for all 40 English phonemes at each time step, and the predictions are decoded using greedy search. For example, the decoded output might be <span class=\"ltx_text ltx_font_smallcaps\" style=\"--ltx-fg-color:#BF0040;\">ih-t <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> w-aa-z <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> p-ey-t <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:50%;\">space</span> f-ao-r</span>. We compute the phoneme error rate (PER) as the Levenshtein distance between the target and decoded phoneme sequences, normalized by the length of the target sequence.</p>\n\n",
            "<p class=\"ltx_p\">As shown in tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T1\" title=\"Table 1 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T2\" title=\"Table 2 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math> outperforms <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>. <math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math> was computed using 31 linearly spaced frequency bins, and for any given time frame, both <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m4\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math> and <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m5\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math> have 991 dimensions. Notably, even <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m6\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, which has only 31 dimensions (i.e., a dimensionality lower by roughly the square root of the others), performs better than <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p5.m7\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>. This finding is consistent with the linear mapping results shown in figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 &#8459; can linearly map to &#120123;&#8290;(&#8496;) &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F4\" title=\"Figure 4 &#8227; 5.2 &#8459; can linearly map to &#120123;&#8290;(&#8496;) &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">As shown in tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T1\" title=\"Table 1 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T2\" title=\"Table 2 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the model decodes phoneme sequences more accurately than <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units. We are currently exploring phoneme-guided decoding strategies for <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units to further improve accuracy. In addition, we are developing methods and models to objectively assess the perceptual quality of the synthesized audio and to compute metrics such as word error rate (WER) and character error rate (CER).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present a neuromuscular speech interface that translates electromyographic (EMG) signals collected from orofacial muscles during speech articulation directly into audio. We show that self-supervised speech (SS) representations exhibit a strong linear relationship with the electrical power of muscle action potentials: SS features can be linearly mapped to EMG power with a correlation of <math alttext=\"r=0.85\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">r=0.85</annotation></semantics></math>. Moreover, EMG power vectors corresponding to different articulatory gestures form structured and separable clusters in feature space. This relationship: <span class=\"ltx_text ltx_font_smallcaps\">SS features</span> <math alttext=\"\\xrightarrow{\\texttt{linear mapping}}\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_monospace\">linear mapping</mtext></mover><annotation encoding=\"application/x-tex\">\\xrightarrow{\\texttt{linear mapping}}</annotation></semantics></math> <span class=\"ltx_text ltx_font_smallcaps\">EMG power</span> <math alttext=\"\\xrightarrow{\\texttt{gesture-specific clustering}}\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mover accent=\"true\"><mo stretchy=\"false\">&#8594;</mo><mtext class=\"ltx_mathvariant_monospace\">gesture-specific clustering</mtext></mover><annotation encoding=\"application/x-tex\">\\xrightarrow{\\texttt{gesture-specific clustering}}</annotation></semantics></math> <span class=\"ltx_text ltx_font_smallcaps\">articulatory movements</span>, highlights that SS models implicitly encode articulatory mechanisms. Leveraging this property, we directly map EMG signals to SS feature space and synthesize speech, enabling end-to-end EMG-to-speech generation without explicit articulatory models and vocoder training<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code will be made publicly available upon completion of the project.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "different",
                    "feature",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this article, we present a method for leveraging self-supervised speech (SS) models to convert electromyographic (EMG) signals collected during speech articulation directly into audio, without the need for explicitly training a vocoder. Our key insight arises from the observation that speech features derived from SS models can be linearly mapped to the electrical power of muscle action potentials. Because these action potential powers corresponding to different articulatory gestures form structured and separable clusters in feature space, it follows that SS models implicitly encode articulatory information. This relationship suggests that EMG power can serve as an effective intermediate representation for mapping muscle activity to speech features. We exploit this property to design a lightweight and interpretable EMG-to-audio conversion model that leverages EMG power representations in conjunction with SS models. Such an approach has the potential to enable efficient few-shot and zero-shot learning of EMG-to-audio mappings&#8212;an especially valuable property given the limited availability of EMG datasets and the frequent data distributional drift caused by factors such as electrode displacement, changes in skin moisture, and other recording variabilities.</p>\n\n",
                "matched_terms": [
                    "emg",
                    "model",
                    "feature",
                    "representations",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Converting non-speech signals into audio has been explored in several modalities, including lip movements-to-speech <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">kim2021lip</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">prajwal2020learning</span> </a></cite>, motor cortex neural signals-to-speech <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">wairagkar2025instantaneous</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">metzger2023high</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">littlejohn2025streaming</span> </a></cite>, and EMG-to-speech <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">gaddy2020digital</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">gaddy2021improved</span> </a></cite>. Most existing approaches in these domains <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">kim2021lip</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">prajwal2020learning</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">wairagkar2025instantaneous</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">gaddy2020digital</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">gaddy2021improved</span> </a></cite> assume that the alignment between the input signals (e.g., video or neural activity) and the corresponding audio is known. In contrast, we address a more challenging scenario similar to <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">metzger2023high</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">littlejohn2025streaming</span> </a></cite>, where the alignment between the neural activity (in our case, EMG) and speech is <span class=\"ltx_text ltx_font_italic\">unknown</span>. This setting requires the model not only to learn the mapping between EMG activity and audio but also to infer the underlying alignment from an exponential search space.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Work in <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">littlejohn2025streaming</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">metzger2023high</span> </a></cite> addresses this alignment-free setting by training an encoder that takes motor cortex neural signals as input and learns to map them to discrete HuBERT units <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">lakhotia-etal-2021-generative</span> </a></cite>, which are then passed to a pretrained vocoder (Tacotron <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">Tacotron</span> </a></cite>) following the pipeline in <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">lakhotia-etal-2021-generative</span> </a></cite>. We adopt a similar high-level pipeline for EMG-to-speech conversion. However, our approach explicitly leverages the <span class=\"ltx_text ltx_font_italic\">geometric structure</span> of EMG signals and their relationship to self-supervised (SS) speech representations to design an efficient encoder.</p>\n\n",
                "matched_terms": [
                    "input",
                    "representations",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these shortcomings, we create and open-source a large-vocabulary corpus (approximately 9 hours of EMG speech data) comprising over 6,800 unique words, articulated at a natural speaking rate of around 115 words per minute. Since data scarcity and signal distribution shifts remain core challenges in neural interface research, our approach focuses on understanding the intrinsic structure of EMG signals to guide encoder design grounded in articulatory mechanisms.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adapt the language corpora from <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">willett2023high</span> </a></cite>, who demonstrated a speech brain-computer interface by translating neural spikes from the motor cortex into speech. The dataset comprises an extensive English corpus containing approximately 6,800 unique words and 9660 sentences. The corpus includes sentences of varying lengths, with the subject articulating at a normal speaking rate, averaging 115 words per minute. The dataset is divided into training, validation, and test sets containing 7000, 1000, and 1660 sentences, respectively. Sentences in the test set are not included in either the training or validation sets.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The data collection environment was carefully controlled to eliminate AC electrical interference. EMG signals underwent minimal preprocessing. The signal from the <span class=\"ltx_text ltx_font_smallcaps\">reference</span> channel (electrode 32) was subtracted from all other EMG data channels. The resulting signals were then bandpass filtered using a third-order Butterworth filter between 80 and 1000&#160;Hz and segmented according to sentence start and end times based on synchronized timestamps.</p>\n\n",
                "matched_terms": [
                    "preprocessing",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">EMG signals are collected by a set of sensors <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> and represented as functions of time <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>.\nA sequence of EMG signals <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> corresponding to articulated speech, associated with an audio signal <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> and phonemic content <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>, is represented as <math alttext=\"E=\\{\\mathbf{f}_{v}(t)\\}_{\\forall\\,v\\in\\mathcal{V}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo>=</mo><msub><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>&#119839;</mi><mi>v</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mrow><mo rspace=\"0.337em\">&#8704;</mo><mi>v</mi></mrow><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">E=\\{\\mathbf{f}_{v}(t)\\}_{\\forall\\,v\\in\\mathcal{V}}</annotation></semantics></math>.\nHere, <math alttext=\"\\mathbf{f}_{v}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#119839;</mi><mi>v</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{f}_{v}(t)</annotation></semantics></math> denotes the EMG signal captured at sensor node <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math> as a function of time.\nThe audio signal <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> encodes both phonemic (lexical) content and expressive aspects of speech such as volume, pitch, prosody, and intonation, while <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> represents only the phonemic content&#8212;a sequence of phonemes.\nFor example, the phonemic content <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> of the word <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m12\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\">friday<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m13\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span> is denoted by the phoneme sequence <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m14\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\">f-r-iy-d-ay<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m15\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "emg",
                    "phonemes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EMG covariance matrices:</span> for an EMG signal <math alttext=\"E_{\\mathcal{V}\\times\\tau}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>&#964;</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{\\mathcal{V}\\times\\tau}</annotation></semantics></math> collected from <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> sensor nodes over a duration of <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> samples, we construct a symmetric positive definite (SPD) covariance matrix <math alttext=\"\\mathcal{E}_{\\mathcal{V}\\times\\mathcal{V}}=\\epsilon EE^{\\top}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi></mrow></msub><mo>=</mo><mrow><mi>&#1013;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>E</mi><mo>&#8868;</mo></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{E}_{\\mathcal{V}\\times\\mathcal{V}}=\\epsilon EE^{\\top}</annotation></semantics></math>, where <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> is a scaling factor. We denote the diagonal of <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> as <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> and its lower triangular part as <math alttext=\"\\lfloor\\mathcal{E}\\rfloor\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#8970;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">&#8971;</mo></mrow><annotation encoding=\"application/x-tex\">\\lfloor\\mathcal{E}\\rfloor</annotation></semantics></math>. The vector <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> represents the muscle action potential power at each electrode <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> during the interval <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m11\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math>, while the off-diagonal elements capture the pairwise cross-channel covariance, reflecting the spatial co-activation structure across electrodes. A vectorized representation of <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m12\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> is denoted as <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m13\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, a column vector of dimension <math alttext=\"\\mathcal{V}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m14\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mn>2</mn></msup><annotation encoding=\"application/x-tex\">\\mathcal{V}^{2}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "emg",
                    "vec‚Äã‚Ñ∞textttvecmathcale",
                    "lower",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EMG spectrograms:</span> for an EMG signal <math alttext=\"E_{\\mathcal{V}\\times\\tau}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>&#964;</mi></mrow></msub><annotation encoding=\"application/x-tex\">E_{\\mathcal{V}\\times\\tau}</annotation></semantics></math> collected from <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> sensor nodes at a sampling frequency <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math>, we compute the short-time Fourier transform (STFT) over successive time windows to obtain a power spectrogram representation <math alttext=\"\\mathcal{S}_{\\mathcal{V}\\times F\\times\\tau^{\\prime}}=\\bigl|\\mathrm{STFT}(E_{\\mathcal{V}\\times\\tau})\\bigr|^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>&#964;</mi><mo>&#8242;</mo></msup></mrow></msub><mo>=</mo><msup><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo><mrow><mi>STFT</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>E</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>&#964;</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\" stretchy=\"true\">|</mo></mrow><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}_{\\mathcal{V}\\times F\\times\\tau^{\\prime}}=\\bigl|\\mathrm{STFT}(E_{\\mathcal{V}\\times\\tau})\\bigr|^{2}</annotation></semantics></math>, where <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> denotes the number of frequency bins and <math alttext=\"\\tau^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m6\" intent=\":literal\"><semantics><msup><mi>&#964;</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">\\tau^{\\prime}</annotation></semantics></math> the number of time frames. Each slice <math alttext=\"\\mathcal{S}_{\\mathcal{V}\\times F}^{(t)}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m7\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{S}_{\\mathcal{V}\\times F}^{(t)}</annotation></semantics></math> captures the frequency-domain energy distribution of EMG activity across <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m8\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> electrodes at time frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m9\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. To reduce the spectral granularity, we bin the frequency axis into <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m10\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> frequency bands using <math alttext=\"\\mathcal{B}_{\\mathcal{V}\\times B\\times\\tau^{\\prime}}(b)=\\frac{1}{|F_{b}|}\\sum_{f\\in F_{b}}\\mathcal{S}_{\\mathcal{V}\\times f\\times\\tau^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m11\" intent=\":literal\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>B</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>&#964;</mi><mo>&#8242;</mo></msup></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy=\"false\">|</mo><msub><mi>F</mi><mi>b</mi></msub><mo stretchy=\"false\">|</mo></mrow></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><msub><mo>&#8721;</mo><mrow><mi>f</mi><mo>&#8712;</mo><msub><mi>F</mi><mi>b</mi></msub></mrow></msub><msub><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>f</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>&#964;</mi><mo>&#8242;</mo></msup></mrow></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{B}_{\\mathcal{V}\\times B\\times\\tau^{\\prime}}(b)=\\frac{1}{|F_{b}|}\\sum_{f\\in F_{b}}\\mathcal{S}_{\\mathcal{V}\\times f\\times\\tau^{\\prime}}</annotation></semantics></math>, where <math alttext=\"F_{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m12\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>b</mi></msub><annotation encoding=\"application/x-tex\">F_{b}</annotation></semantics></math> is the set of frequency bins assigned to band <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m13\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. The matrix <math alttext=\"\\mathcal{B}^{(t)}_{\\mathcal{V}\\times B}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m14\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>B</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{B}^{(t)}_{\\mathcal{V}\\times B}</annotation></semantics></math> thus represents the band power of muscle activity at each electrode across frequency bands during frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m15\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. In practice, we use either five log-spaced bands <math alttext=\"B_{1}=[80,125]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m16\" intent=\":literal\"><semantics><mrow><msub><mi>B</mi><mn>1</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>80</mn><mo>,</mo><mn>125</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B_{1}=[80,125]</annotation></semantics></math> Hz, <math alttext=\"B_{2}=[125,250]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m17\" intent=\":literal\"><semantics><mrow><msub><mi>B</mi><mn>2</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>125</mn><mo>,</mo><mn>250</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B_{2}=[125,250]</annotation></semantics></math> Hz, <math alttext=\"B_{3}=[250,375]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m18\" intent=\":literal\"><semantics><mrow><msub><mi>B</mi><mn>3</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>250</mn><mo>,</mo><mn>375</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B_{3}=[250,375]</annotation></semantics></math> Hz, <math alttext=\"B_{4}=[375,687.5]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m19\" intent=\":literal\"><semantics><mrow><msub><mi>B</mi><mn>4</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>375</mn><mo>,</mo><mn>687.5</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B_{4}=[375,687.5]</annotation></semantics></math> Hz, and <math alttext=\"B_{5}=[687.5,1000]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m20\" intent=\":literal\"><semantics><mrow><msub><mi>B</mi><mn>5</mn></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mn>687.5</mn><mo>,</mo><mn>1000</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B_{5}=[687.5,1000]</annotation></semantics></math> Hz, following <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">Kaifosh2025Generic</span> </a></cite>, or 31 linearly spaced frequency bands between <math alttext=\"80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m21\" intent=\":literal\"><semantics><mn>80</mn><annotation encoding=\"application/x-tex\">80</annotation></semantics></math> and <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m22\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> Hz. A vectorized representation of <math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m23\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math> is denoted as <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m24\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>, a column vector of dimension <math alttext=\"\\mathcal{V}B\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m25\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{V}B</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "vec‚Äã‚Ñ¨textttvecmathcalb",
                    "emg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio features from SS models:</span> for a speech waveform <math alttext=\"a(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">a(t)</annotation></semantics></math>, we extract self-supervised (SS) representations by passing the signal through a pretrained model <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, yielding <math alttext=\"\\mathcal{H}=\\mathcal{S}(a(t))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{H}=\\mathcal{S}(a(t))</annotation></semantics></math>. The model <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math> can be instantiated as <span class=\"ltx_text ltx_font_smallcaps\">Wav2Vec 2.0</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">baevski2020wav2vec</span> </a></cite>, <span class=\"ltx_text ltx_font_smallcaps\">HuBERT</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">hsu2021hubert</span> </a></cite>, or <span class=\"ltx_text ltx_font_smallcaps\">WavLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">chen2022wavlm</span> </a></cite>. We denote the column vector of SS audio representations by <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> throughout the article.</p>\n\n",
                "matched_terms": [
                    "model",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct sequences of <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>, <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, and <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math>, which are emitted every 20&#160;ms and use a context length of 25&#160;ms. For temporal relation modeling, we employ a time depth separable convolutional network (TDS), as described below.</p>\n\n",
                "matched_terms": [
                    "described",
                    "vec‚Äã‚Ñ∞textttvecmathcale",
                    "vec‚Äã‚Ñ¨textttvecmathcalb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We collected data from 12 participants, each performing 13 distinct orofacial movements, with 10 repetitions per movement. The set of movements includes <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">cheeks &#8211; puff out<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">cheeks &#8211; suck in<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">jaw &#8211; dropdown<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">jaw &#8211; move backward<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m8\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">jaw &#8211; move forward<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m10\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">jaw &#8211; move left<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m12\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">jaw &#8211; move right<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m14\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">lips &#8211; pucker<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m16\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">lips &#8211; smile<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m18\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">lips &#8211; tuck as if blotting<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m20\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">tongue &#8211; back of lower teeth<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m22\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m23\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">tongue &#8211; back of upper teeth<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m24\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>, and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m25\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">tongue &#8211; roof of the mouth<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m26\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math></span>. These movements were selected to span a broad range of articulatory gestures involved in natural speech production, encompassing mechanisms such as lip rounding, jaw positioning, and tongue placement, which are essential for producing different phonemes.</p>\n\n",
                "matched_terms": [
                    "different",
                    "lower",
                    "phonemes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each gesture is represented by an EMG signal matrix <math alttext=\"E_{22\\times 7500}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mn>22</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7500</mn></mrow></msub><annotation encoding=\"application/x-tex\">E_{22\\times 7500}</annotation></semantics></math>, where 22 denotes the number of electrode channels<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We used a subset of the 22 electrodes shown in figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S3.F1\" title=\"Figure 1 &#8227; 3 Data &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, excluding those on the right side of the neck, for this data. Each gesture was performed over a 1.5&#160;s interval, which corresponds to 7500 time steps at a sampling frequency of 5000&#160;Hz. This dataset was collected independently of the one described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S3\" title=\"3 Data &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Unless stated otherwise, all references to data throughout the manuscript refer to the dataset described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S3\" title=\"3 Data &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\n</span></span></span>. The corresponding symmetric positive definite (SPD) covariance matrix for each gesture is denoted as <math alttext=\"\\mathcal{E}_{22\\times 22}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mrow><mn>22</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>22</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{E}_{22\\times 22}</annotation></semantics></math>, and its diagonal <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> is a 22-dimensional vector representing the per-channel EMG power.</p>\n\n",
                "matched_terms": [
                    "described",
                    "emg",
                    "dataset",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The vectors <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> corresponding to different orofacial gestures naturally form distinct clusters, as shown in figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F2\" title=\"Figure 2 &#8227; 5.1 &#8496; and &#120123;&#8290;(&#8496;) encode articulatory information &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. We further quantified their discriminability using the unsupervised <span class=\"ltx_text ltx_font_italic\">k</span>-medoids clustering algorithm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">Kaufman1990PAM</span> </a></cite>, achieving a classification accuracy of 70.6% based on <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> (averaged across 12 subjects). When using the full covariance matrix <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> with the geodesic distance defined in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S4.E1\" title=\"In 4.1 Electromyography (EMG) &#8227; 4 Methods &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the <span class=\"ltx_text ltx_font_italic\">k</span>-medoids classification accuracy increased to 73.7%, both well above the random chance level of 10%.</p>\n\n",
                "matched_terms": [
                    "different",
                    "when",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results demonstrate that both <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p4.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> and <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> naturally encode discriminative articulatory information. While <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> alone is sufficient to distinguish between different orofacial movements, incorporating the full covariance structure in <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p4.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> leads to improved decoding accuracy.</p>\n\n",
                "matched_terms": [
                    "different",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that other widely used EMG features such as log-spectrograms&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">sivakumaremg2qwerty</span> </a></cite> or rectified time-domain signals&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">Halliday2010Rectification</span> </a></cite> cannot be directly probed to verify whether such a structured representation exists. When raw EMG signals <math alttext=\"E_{22\\times 7500}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m1\" intent=\":literal\"><semantics><msub><mi>E</mi><mrow><mn>22</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7500</mn></mrow></msub><annotation encoding=\"application/x-tex\">E_{22\\times 7500}</annotation></semantics></math> are featurized using spectrograms or rectified signals, the temporal dimension may be reduced in granularity but is not collapsed into a single frame. In contrast, covariance-based representations aggregate the temporal information within a single frame, yielding fixed-dimensional features such as <math alttext=\"\\mathbb{D}(\\mathcal{E})\\in\\mathbb{R}^{22}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mn>22</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})\\in\\mathbb{R}^{22}</annotation></semantics></math> or <math alttext=\"\\mathcal{E}\\in\\mathbb{R}^{22\\times 22}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>22</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>22</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{E}\\in\\mathbb{R}^{22\\times 22}</annotation></semantics></math>. (We analyze <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m4\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> using the standard Euclidean distance, while <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> is compared using the specialized metric defined in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S4.E1\" title=\"In 4.1 Electromyography (EMG) &#8227; 4 Methods &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.) Consequently, there is no low-dimensional equivalent of <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> or <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m7\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> when using log-spectrogram or rectified features that captures articulatory structure in the same way.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "emg",
                    "when",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test whether there exists a linear mapping defined by a weight matrix <math alttext=\"W\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi>W</mi><annotation encoding=\"application/x-tex\">W</annotation></semantics></math> and bias <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> such that <math alttext=\"\\mathbb{D}(\\mathcal{E})\\approx W\\mathcal{H}+b\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8776;</mo><mrow><mrow><mi>W</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></mrow><mo>+</mo><mi>b</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})\\approx W\\mathcal{H}+b</annotation></semantics></math> with a high correlation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>We actually aim to probe whether <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> (<math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m2\" intent=\":literal\"><semantics><mn>768</mn><annotation encoding=\"application/x-tex\">768</annotation></semantics></math>&#8211;<math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m3\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math> dimensions) can map to <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m4\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math> (<math alttext=\"991\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m5\" intent=\":literal\"><semantics><mn>991</mn><annotation encoding=\"application/x-tex\">991</annotation></semantics></math> dimensions). However, the resulting <math alttext=\"\\sim 10^{6}\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m6\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><msup><mn>10</mn><mn>6</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sim 10^{6}</annotation></semantics></math>-parameter linear transformation would be severely ill-posed and dominated by noise without massive data and strong regularization. To make this analysis tractable, we use <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m7\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> as a proxy because it provides a compact, well-conditioned, and physically meaningful representation grounded in articulatory mechanisms, making it well suited for linear probing. Importantly, this substitution is justified because both <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m8\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math> and <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"footnote3.m9\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> encode structured articulatory information, and the latter serves as a low-dimensional surrogate for the former, as shown in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.SS1\" title=\"5.1 &#8496; and &#120123;&#8290;(&#8496;) encode articulatory information &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "vec‚Äã‚Ñ∞textttvecmathcale",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the training set described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S3\" title=\"3 Data &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> to learn this mapping and evaluate it on the test set. We report the Pearson correlation between the predicted sequences <math alttext=\"\\mathbb{D}(\\mathcal{E}^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E}^{\\prime})</annotation></semantics></math> and the ground-truth <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> on the test set. The representations <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> are extracted using <span class=\"ltx_text ltx_font_smallcaps\">HuBERT</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">hsu2021hubert</span> </a></cite>, <span class=\"ltx_text ltx_font_smallcaps\">Wav2Vec 2.0</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">baevski2020wav2vec</span> </a></cite>, and <span class=\"ltx_text ltx_font_smallcaps\">WavLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">chen2022wavlm</span> </a></cite>. We evaluate <span class=\"ltx_text ltx_font_smallcaps\">base</span> models with a hidden dimension of 768 and 12 transformer layers, <span class=\"ltx_text ltx_font_smallcaps\">large</span> models with a hidden dimension of 1024 and 24 transformer layers, and <span class=\"ltx_text ltx_font_smallcaps\">fine-tuned (ft)</span> models that have been trained for automatic speech recognition (ASR).</p>\n\n",
                "matched_terms": [
                    "representations",
                    "described",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Correlation coefficients (<em class=\"ltx_emph ltx_font_italic\">r</em>) across models and layers are shown in figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 &#8459; can linearly map to &#120123;&#8290;(&#8496;) &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. We find that a simple linear model can predict <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> from <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> with a correlation as high as <math alttext=\"r=0.85\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">r=0.85</annotation></semantics></math>. The layer-wise trends across different models partially mirror the observations reported in <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">cho2023evidence</span> </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">cho2024self</span> </a></cite> for electromagnetic articulography (EMA), where two local peaks were consistently observed across models. In our case, we observe two local peaks for <span class=\"ltx_text ltx_font_smallcaps\">Wav2Vec 2.0</span> models but only a single dominant peak for <span class=\"ltx_text ltx_font_smallcaps\">HuBERT</span> and <span class=\"ltx_text ltx_font_smallcaps\">WavLM</span> models. A sharp decline in correlation emerges in the upper layers of fine-tuned models, reflecting the growing influence of task-specific objectives. This effect is especially pronounced for <span class=\"ltx_text ltx_font_smallcaps\">Wav2Vec 2.0</span> compared to <span class=\"ltx_text ltx_font_smallcaps\">HuBERT</span> and <span class=\"ltx_text ltx_font_smallcaps\">WavLM</span>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, for the <span class=\"ltx_text ltx_font_smallcaps\">HuBERT-base</span> model, the peak correlation at layer&#160;6 aligns with the layer previously identified as optimal for discrete speech resynthesis and spoken language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">lakhotia-etal-2021-generative</span> </a></cite>. While prior work established this empirical result, the mechanistic basis for this peak remained unclear. Our analysis provides a principled interpretation: layer&#160;6 exhibits the strongest linear predictive power for <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, which encodes structured and discriminative articulatory information (i.e., different articulatory gestures such as tongue and jaw positions naturally form separable clusters). This tight alignment between articulatory structure and model representations offers a direct explanation for why layer&#160;6 is particularly effective for downstream speech resynthesis and language modeling. In short, the layer that best captures articulatory mechanisms is also the one that yields the strongest downstream performance, providing convergent evidence for its functional role.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "different",
                    "model",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also examined whether a similar linear mapping exists between EMG spectrogram features (<math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>) and <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math>. Frequency bands of <math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math> are obtained using five log-spaced frequency bins, as described in section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S4\" title=\"4 Methods &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. However, the resulting correlation coefficients are substantially lower, with a maximum correlation of approximately <math alttext=\"r=0.57\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.57</mn></mrow><annotation encoding=\"application/x-tex\">r=0.57</annotation></semantics></math> (figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F4\" title=\"Figure 4 &#8227; 5.2 &#8459; can linearly map to &#120123;&#8290;(&#8496;) &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). For comparison, we also computed correlations for linear mappings between <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math> (audio spectrograms) and <math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math> (<math alttext=\"r=0.37\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m7\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.37</mn></mrow><annotation encoding=\"application/x-tex\">r=0.37</annotation></semantics></math>) and between <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m8\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math> and <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m9\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> (<math alttext=\"r=0.61\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m10\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.61</mn></mrow><annotation encoding=\"application/x-tex\">r=0.61</annotation></semantics></math>), both of which are considerably lower than the correlation between <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m11\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> and <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m12\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "vec‚Äã‚Ñ¨textttvecmathcalb",
                    "lower",
                    "emg",
                    "described",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The above observations indicate that among the different EMG feature representations considered, <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m1\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> exhibits the strongest linear alignment with the self-supervised speech feature space <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math>. This strong correspondence suggests that <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> and <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> encode highly compatible representations, making them particularly well suited for EMG-to-audio learning. In contrast, EMG spectrogram features (<math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math>) and their alignment with audio features (<math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>) yield notably weaker correlations. These findings imply that, while <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m7\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math> and <math alttext=\"\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m8\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><annotation encoding=\"application/x-tex\">\\mathcal{B}</annotation></semantics></math> share some structure, self-supervised representations provide a more robust and articulatorily grounded intermediate latent space. Consequently, pairing <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m9\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> with <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p6.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> offers the most effective pathway for EMG-to-audio mapping.</p>\n\n",
                "matched_terms": [
                    "emg",
                    "feature",
                    "representations",
                    "different",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The existence of a simple linear mapping from <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> to <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> is significant: it reveals that the self-supervised representations <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> inherently encode articulatory structure reflecting underlying muscle activations. This forward direction is well posed &#8212; <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> has moderate dimensionality (<math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m5\" intent=\":literal\"><semantics><mn>768</mn><annotation encoding=\"application/x-tex\">768</annotation></semantics></math>&#8211;<math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m6\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math>), <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math> is low dimensional (<math alttext=\"31\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m8\" intent=\":literal\"><semantics><mn>31</mn><annotation encoding=\"application/x-tex\">31</annotation></semantics></math>), and the mapping can be stably estimated. In contrast, the inverse problem <math alttext=\"\\mathbb{D}(\\mathcal{E})\\rightarrow\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">&#8594;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})\\rightarrow\\mathcal{H}</annotation></semantics></math> is underdetermined, non-invertible in the linear case, and especially ill-posed when temporal alignments are unknown. Nonetheless, the existence of the forward mapping provides strong evidence that <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> encodes articulatory mechanisms, motivating structured nonlinear approaches for the inverse direction rather than expecting a trivial linear inversion.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "when",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this observation, we address the problem of predicting <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math> from EMG features (<math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, or <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math>) without explicit temporal alignments. Since linear inversion is ill-posed, we model this mapping using a nonlinear sequence-to-sequence architecture capable of capturing the structured dependencies in <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math>. Specifically, EMG features are fed into a TDS convolutional network (section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S4\" title=\"4 Methods &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), which predicts discrete units derived from <math alttext=\"\\mathcal{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><annotation encoding=\"application/x-tex\">\\mathcal{H}</annotation></semantics></math>. We use the 100-unit discrete representation from layer 6 of the <span class=\"ltx_text ltx_font_smallcaps\">HuBERT-base</span> model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">lakhotia-etal-2021-generative</span> </a></cite>, denoted <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math>. The model is trained with the connectionist temporal classification (CTC) loss&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">graves2006connectionist</span> </a></cite>, enabling alignment-free learning between EMG sequences and <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math>. Finally, the predicted <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m9\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> sequence is passed to a pretrained Tacotron vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">Tacotron</span> </a></cite> to generate audio waveforms. The end-to-end architecture is shown in figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "vec‚Äã‚Ñ∞textttvecmathcale",
                    "model",
                    "predicting",
                    "vec‚Äã‚Ñ¨textttvecmathcalb",
                    "emg",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the results for <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> decoding in table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23969v1#S5.T1\" title=\"Table 1 &#8227; 5.3 emg2speech synthesis &#8227; 5 Results &#8227; emg2speech: synthesizing speech from electromyography using self-supervised speech models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, or <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m4\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math> were provided as input to the TDS network, which was trained to predict the corresponding <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m5\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units. For example, for the sentence <math alttext=\"{}_{\\textsc{t-start}}&lt;\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S5.SS3.p3.m6\" intent=\":literal\"><semantics><mmultiscripts><mo>&lt;</mo><mprescripts/><mtext class=\"ltx_font_smallcaps\">t-start</mtext><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\textsc{t-start}}&lt;</annotation></semantics></math><span class=\"ltx_text ltx_font_smallcaps\" style=\"--ltx-fg-color:#0000FF;\">It Was Paid For<math alttext=\"&gt;_{\\textsc{t-end}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m7\" intent=\":literal\"><semantics><msub><mo mathcolor=\"#000000\" style=\"--ltx-fg-color:#000000;\">&gt;</mo><mtext class=\"ltx_font_smallcaps\" mathcolor=\"#000000\" style=\"--ltx-fg-color:#000000;\">t-end</mtext></msub><annotation encoding=\"application/x-tex\">&gt;_{\\textsc{t-end}}</annotation></semantics></math></span> with target <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m8\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;--ltx-fg-color:#004D00;\">71-12-71-12-4-12-4-40-93-86-13-58-32-1-99-&#8230;</span>, the TDS model is trained to learn the mapping from <math alttext=\"\\texttt{vec}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m9\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{E})</annotation></semantics></math>, <math alttext=\"\\mathbb{D}(\\mathcal{E})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m10\" intent=\":literal\"><semantics><mrow><mi>&#120123;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbb{D}(\\mathcal{E})</annotation></semantics></math>, or <math alttext=\"\\texttt{vec}(\\mathcal{B})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m11\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">vec</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{vec}(\\mathcal{B})</annotation></semantics></math> to <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m12\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units using the CTC loss. During inference, the model outputs probabilities for all 100 <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m13\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> units at each time step, and we decode these outputs using greedy search. For instance, the decoded sequence might be <span class=\"ltx_text ltx_font_smallcaps\" style=\"--ltx-fg-color:#BF0040;\">71-12-57-4-54-40-93-86-13-58-16-14-76-6-36-&#8230;</span>. We compute the unit error rate (UER) as the Levenshtein distance between the target and predicted <math alttext=\"\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p3.m14\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">dis</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi><mo stretchy=\"false\">)</mo></mrow><mtext class=\"ltx_font_smallcaps\">HuBERT</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{dis}(\\mathcal{H})_{\\textsc{HuBERT}}</annotation></semantics></math> unit sequences, normalized by the length of the target sequence.</p>\n\n",
                "matched_terms": [
                    "vec‚Äã‚Ñ∞textttvecmathcale",
                    "rate",
                    "model",
                    "vec‚Äã‚Ñ¨textttvecmathcalb",
                    "input",
                    "error",
                    "ùîª‚Äã‚Ñ∞mathbbdmathcale"
                ]
            }
        ]
    }
}