{
    "S5.T1.st1": {
        "source_file": "Towards Unsupervised Speech Recognition at the Syllable-Level",
        "caption": "(a) UASR results on LibriSpeech (clean subsets)",
        "body": "Model\nStudent\nToken\nMatched\nUnmatched\n\n\n\nCER (↓\\downarrow)\nCER (↓\\downarrow)\n\n\n\nG2P-based approach\n\n\n\nwav2vec-U* (Baevski et al., 2021)\n\nNone\nPhone\n-\n13.3\n\n\n\nwav2vec-U 2.0* (Liu et al., 2023)\n\nNone\nPhone\n-\n12.2\n\n\n\nREBORN* (Tseng et al., 2024)\n\nNone\nPhone\n-\n8.3\n\n\n\nG2P-free approach\n\n\n\nwav2vec-U (Baevski et al., 2021)\n\nNone\nChar.\n35.6\n43.3\n\n\n\nwav2vec 2.0\nChar.\n33.8\n42.1\n\n\n\nREBORN (Tseng et al., 2024)\n\nNone\nChar.\n37.8\n76.6\n\n\n\nJSTTI (forced align)\nNone\nChar\n81.4\n81.1\n\n\n\nJSTTI (Ni et al., 2025)\n\nNone\nWord\n49.5\n54.2\n\n\n\nPUSM (Sylber) (Wang et al., 2023c)\n\nNone\nSyllable\n35.5\n57.7\n\n\n\nwav2vec 2.0\nSyllable\n33.0\n54.7\n\n\n\nSylCipher (Ours, forced align)\nNone\nSyllable\n38.5\n46.4\n\n\n\nSylCipher (Ours, Sylber)\nNone\nSyllable\n43.5\n48.6\n\n\n\nSylCipher (Ours, Sylber+JE2E)\nNone\nSyllable\n39.2\n46.8\n\n\n\nSylCipher (Ours, Sylber+JE2E+PUSM)\nNone\nSyllable\n21.8\n35.9\n\n\n\nwav2vec 2.0\nSyllable\n17.5\n33.3",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Student</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Token</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Matched</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Unmatched</span></td>\n<td class=\"ltx_td ltx_border_tt\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">CER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.st1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">CER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.st1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"5\"><em class=\"ltx_emph ltx_font_italic\">G2P-based approach</em></th>\n<td class=\"ltx_td ltx_border_tt\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">wav2vec-U*&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Phone</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13.3</td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">wav2vec-U 2.0*&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib28\" title=\"\">2023</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Phone</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">12.2</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">REBORN*&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Phone</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">8.3</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"5\"><em class=\"ltx_emph ltx_font_italic\">G2P-free approach</em></th>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\">wav2vec-U&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Char.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.3</td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">wav2vec 2.0</th>\n<td class=\"ltx_td ltx_align_center\">Char.</td>\n<td class=\"ltx_td ltx_align_center\">33.8</td>\n<td class=\"ltx_td ltx_align_center\">42.1</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">REBORN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Char.</td>\n<td class=\"ltx_td ltx_align_center\">37.8</td>\n<td class=\"ltx_td ltx_align_center\">76.6</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">JSTTI (forced align)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Char</td>\n<td class=\"ltx_td ltx_align_center\">81.4</td>\n<td class=\"ltx_td ltx_align_center\">81.1</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">JSTTI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Word</td>\n<td class=\"ltx_td ltx_align_center\">49.5</td>\n<td class=\"ltx_td ltx_align_center\">54.2</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" rowspan=\"2\">PUSM (Sylber)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\">35.5</td>\n<td class=\"ltx_td ltx_align_center\">57.7</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">wav2vec 2.0</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\">33.0</td>\n<td class=\"ltx_td ltx_align_center\">54.7</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">SylCipher (Ours, forced align)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Syllable</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">46.4</td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SylCipher (Ours, Sylber)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\">43.5</td>\n<td class=\"ltx_td ltx_align_center\">48.6</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SylCipher (Ours, Sylber+JE2E)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\">39.2</td>\n<td class=\"ltx_td ltx_align_center\">46.8</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" rowspan=\"2\">SylCipher (Ours, Sylber+JE2E+PUSM)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">21.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">35.9</span></td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">wav2vec 2.0</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Syllable</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">17.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">33.3</span></td>\n<td class=\"ltx_td ltx_border_bb\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "forced",
            "pusm",
            "clean",
            "token",
            "char",
            "jstti",
            "results",
            "baevski",
            "none",
            "tseng",
            "sylberje2epusm",
            "sylcipher",
            "uasr",
            "reborn",
            "wav2vecu",
            "g2pfree",
            "wang",
            "student",
            "approach",
            "↓downarrow",
            "g2pbased",
            "subsets",
            "2023c",
            "phone",
            "word",
            "ours",
            "wav2vec",
            "align",
            "librispeech",
            "sylber",
            "sylberje2e",
            "matched",
            "syllable",
            "unmatched",
            "liu",
            "cer",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level modeling performs best under G2P-free setting.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T1.st1\" title=\"In Table 1 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a> summarizes results on LibriSpeech. Among baselines, PUSM performs best in the matched setting, while wav2vec-U is strongest in the unmatched setting, though its performance is limited by the lack of G2P. Unlike phonemized training, REBORN trained directly on raw characters performs worse than wav2vec-U, suggesting sensitivity to misalignment between speech and text. By contrast, SylCipher with all three stages (Sylber+JE2E+PUSM) achieves 21.8% CER (matched) and 35.9% (unmatched), outperforming all baselines. Compared to the best-in-average system wav2vec-U (35.6%/43.3%), SylCipher reduces by CER by 40% (matched) and 17% (unmatched) relative.\nBoth syllable-level models (PUSM and SylCipher) outperform word- or character-level models, confirming that speech-text alignment is the best at the syllable level. Word-level JSTTI performs worst due to poor rare-word coverage (<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span>s <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p6.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 17%), and adapting JSTTI to phone-level degrades further, likely because phone clusters are noisier than syllable or word clusters.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Training speech recognizers with unpaired speech and text &#8211; known as unsupervised speech recognition (UASR) &#8211; is a crucial step toward extending ASR to low-resource languages in the long-tail distribution and enabling multimodal learning from non-parallel data. However, existing approaches based on phones often rely on costly resources such as grapheme-to-phoneme converters (G2Ps) and struggle to generalize to languages with ambiguous phoneme boundaries due to training instability. In this paper, we address both challenges by introducing a syllable-level UASR framework based on masked language modeling, which avoids the need for G2P and the instability of GAN-based methods. Our approach achieves up to a 40% relative reduction in character error rate (CER) on LibriSpeech and generalizes effectively to Mandarin, a language that has remained particularly difficult for prior methods. Code will be released upon acceptance.\n\n</p>\n\n",
                "matched_terms": [
                    "cer",
                    "uasr",
                    "librispeech",
                    "approach"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A promising step toward language-universal assistants is to build speech recognizers from <em class=\"ltx_emph ltx_font_italic\">unpaired</em> speech and text, or <em class=\"ltx_emph ltx_font_italic\">unsupervised speech recognition</em> (UASR)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib20\" title=\"\">2012</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib28\" title=\"\">2023</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite>.\nUASR is a fundamental challenge: success would enable downstream tasks such as speech synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib33\" title=\"\">2022</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib27\" title=\"\">2022</a>)</cite>, translation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib45\" title=\"\">2023a</a>)</cite>, and understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib41\" title=\"\">2023</a>)</cite>, paving the way for general-purpose voice assistants. It is also a central case of <em class=\"ltx_emph ltx_font_italic\">unpaired multimodal learning</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Artetxe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib4\" title=\"\">2018b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib3\" title=\"\">a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib5\" title=\"\">2019</a>; Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib25\" title=\"\">2018a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib26\" title=\"\">b</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib31\" title=\"\">2019</a>; Hoshen &amp; Wolf, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib21\" title=\"\">2018</a>)</cite>, where modalities need to be aligned without parallel data. In the absence of sentence-level alignment, the model must infer higher-level linguistic units&#8211;phones, syllables, and words&#8211;from raw speech waveforms in conjunction with global text statistics. Thus, UASR provides broader insights into representation learning and multimodal alignment without supervision.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "model",
                    "baevski",
                    "tseng",
                    "liu",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The best existing UASR systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib28\" title=\"\">2023</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite> operate at the <em class=\"ltx_emph ltx_font_italic\">phoneme-level</em>. To this end, they need to convert raw text units, or <em class=\"ltx_emph ltx_font_italic\">graphemes</em>, to phonemes, or minimal sound units that encode meaning, using a grapheme-to-phoneme convertor (G2P). However, training G2Ps requires resources such as pronunciation dictionaries, which can be time-consuming and labor-intensive to create. Without a G2P, such systems suffer from significant performance degradation due to misalignment between speech and raw text in many languages&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib28\" title=\"\">2023</a>); Ni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib33\" title=\"\">2022</a>)</cite>. Even when pronunciation dictionaries are available, the system may fail to detect clear phone-level boundaries due to strong co-articulation effects for languages such as Mandarin.</p>\n\n",
                "matched_terms": [
                    "liu",
                    "baevski",
                    "uasr",
                    "tseng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative approach to phone-based models is to build a word-level UASR system, which can be achieved without a G2P. Yet a major concern is the coverage of the system on <em class=\"ltx_emph ltx_font_italic\">rare words</em>, which can effectively be infinite in vocabulary size, making it significantly harder to acquire than phones. Furthermore, detecting word boundaries requires capturing long-range contextual dependencies in speech, which risk destabilizing segmentation mechanisms effective for phone-level UASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "word",
                    "wang",
                    "approach",
                    "uasr",
                    "2023c"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a third alternative &#8211; to build UASR at the <em class=\"ltx_emph ltx_font_italic\">syllable-level</em>, which can be justified on three grounds. First, unlike words, the number of distinct syllables for a language is finite, which reduces the long-tail token distribution issue and allows for better generalization to unseen words; second, many languages exhibit the best alignment between speech and text at the syllable level instead of the phone or word level.\nFor instance, Mandarin uses characters, which have strong correspondences to spoken syllables; therefore, we expect that a syllable-level UASR system could be more appropriate for UASR than a phoneme-based system, which we also found to be the case empirically.</p>\n\n",
                "matched_terms": [
                    "phone",
                    "word",
                    "token",
                    "syllable",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces <span class=\"ltx_text ltx_font_bold\">SylCipher</span>, to our knowledge, the first syllable-based UASR system. SylCipher jointly predicts syllable boundaries and embedding tokens from raw speech using a unified self-supervised objective. The learning mechanism avoids adversarial training, making it more stable and less sensitive to hyperparameters.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "uasr",
                    "syllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct extensive experiments across domains and languages. On LibriSpeech, SylCipher achieves up to 40% relative character error rate (CER) reduction over prior G2P-free UASR methods. On SpokenCOCO, improvements are even larger, demonstrating robustness across domains. On Mandarin, SylCipher achieves 12.2% phone error rate (PER), outperforming GAN-based UASR methods that fail to even converge.</p>\n\n",
                "matched_terms": [
                    "phone",
                    "g2pfree",
                    "librispeech",
                    "sylcipher",
                    "cer",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paper organizations.</span> Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S2\" title=\"2 Related work &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reviews related work. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S3\" title=\"3 Syllable-level unsupervised speech recognition &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> formalizes the syllable-level UASR problem. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4\" title=\"4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents SylCipher and its theoretical guarantees. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5\" title=\"5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports experiments and ablations. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S6\" title=\"6 Conclusion &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> concludes with limitations and future work.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on UASR assumed the existence of a reliable G2P and formulate the problem as an adversarial game, where a conditional generator predicts a phoneme sequence given a speech waveform, and a discriminator tries to tell real phonemized text apart from the generator&#8217;s output&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite>. Within this framework, several works have explored different segmentation mechanisms such as fixed unsupervised phoneme segmentation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib30\" title=\"\">2018</a>)</cite>, iterative forced alignment&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib10\" title=\"\">2019</a>)</cite>, de-duplication&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite> and reinforcement learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite> proposed an alternative formulation of UASR as an explicit distribution matching problem, by matching the lower-order <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math>-gram distributions of the generated and real texts. Later work further extended this reformulation to use G2P-free tokens such as words&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>; Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, more expressive architecture such as transformers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite> and more general text distribution such as masked prediction probabilities&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>. We adapt this word-level UASR approach to the syllable-level, and supplement it with a simplified training flow, a more stable differentiable boundary detector and additional learning objectives.</p>\n\n",
                "matched_terms": [
                    "g2pfree",
                    "wang",
                    "approach",
                    "forced",
                    "baevski",
                    "tseng",
                    "liu",
                    "uasr",
                    "2023c"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on syllable-level modeling of speech used signal processing techniques&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib49\" title=\"\">2009</a>)</cite>. To discover higher-level structure and more efficient self-supervised learning (SSL) representations from raw speech, <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite> proposed to induce syllabic structure from existing SSL models such as HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib22\" title=\"\">2021a</a>)</cite>. To this end,&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite> probed the self-attention layers of VG-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng &amp; Harwath, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib37\" title=\"\">2022</a>)</cite>, a visually grounded SSL model to detect syllable-like feature clusters and further refine such clusters using a min-cut algorithm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi &amp; Malik (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib40\" title=\"\">1997</a>)</cite>. To sidestep the need for visual data, <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite> employed a speech-only SSL model trained with utterance-level self-distillation from a HuBERT teacher. Due to the indirect manner of such approaches by which syllabic structures are derived, they are often noisy and unreliable. To cope with this issue, recent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> proposed a more direct and targeted approach by performing self-distillation at the syllable-level, which significantly improved syllable boundary detection and unit discovery performance by encouraging sharper contrast between within and between-syllable feature frames.</p>\n\n",
                "matched_terms": [
                    "model",
                    "syllable",
                    "approach"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we formulate syllable-level UASR as follows.\nLet <math alttext=\"X=[X_{1},\\cdots,X_{T}]\\in{\\mathcal{X}}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>X</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">X=[X_{1},\\cdots,X_{T}]\\in{\\mathcal{X}}^{T}</annotation></semantics></math> be a padded sequence of speech feature vectors and let <math alttext=\"Y=[Y_{1},\\cdots,Y_{L}]\\in{\\mathcal{Y}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>Y</mi><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">Y=[Y_{1},\\cdots,Y_{L}]\\in{\\mathcal{Y}}^{L}</annotation></semantics></math> a padded sequence of text tokens in the same language. Since a tokenized speech utterance typically uses more tokens than the text transcription of the same utterance, we assume the <math alttext=\"T\\geq L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>&#8805;</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">T\\geq L</annotation></semantics></math>. Assume <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> come from two <em class=\"ltx_emph ltx_font_italic\">unpaired</em> datasets and are therefore <em class=\"ltx_emph ltx_font_italic\">statistically independent</em>. Further, suppose they are <em class=\"ltx_emph ltx_font_italic\">matched</em>, i.e., there exists an ASR function <math alttext=\"y^{*}:{\\mathcal{X}}\\mapsto{\\mathcal{Y}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mrow><msup><mi>y</mi><mo>&#8727;</mo></msup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo stretchy=\"false\">&#8614;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">y^{*}:{\\mathcal{X}}\\mapsto{\\mathcal{Y}}</annotation></semantics></math> such that the distributions of <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m8\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math>, <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m9\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"p_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m10\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">p_{Y}</annotation></semantics></math> satisfy</p>\n\n",
                "matched_terms": [
                    "uasr",
                    "matched"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of UASR is to recover <math alttext=\"y^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m11\" intent=\":literal\"><semantics><msup><mi>y</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">y^{*}</annotation></semantics></math> given only unpaired <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m12\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m13\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math>. The syllable-level case is a special setting where the tokens of <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m14\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> are syllables. The learning problem resembles <em class=\"ltx_emph ltx_font_italic\">decipherment</em>, where one decodes a message in an unknown script without a lexicon or grammar. In practice, <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m15\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> is formed from frame-level SSL features&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib22\" title=\"\">2021a</a>)</cite>, and <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m16\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"p_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m17\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">p_{Y}</annotation></semantics></math> are only approximately matched due to finite-sample noise and domain mismatch. As shown in <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite>, UASR is ill-posed in general, but the mapping <math alttext=\"y^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m18\" intent=\":literal\"><semantics><msup><mi>y</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">y^{*}</annotation></semantics></math> in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S3.E1\" title=\"In 3 Syllable-level unsupervised speech recognition &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> becomes identifiable if syllable boundaries are known and the language satisfies mild conditions.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "uasr",
                    "matched",
                    "syllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we describe <span class=\"ltx_text ltx_font_bold\">SylCipher</span>, our proposed model for syllable-level UASR. We first present its architecture and training objective, then justify the design theoretically, and finally introduce several practical modifications for training and inference.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "uasr",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.F1\" title=\"Figure 1 &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, SylCipher is an encoder-only language model with a <em class=\"ltx_emph ltx_font_italic\">shared encoder</em> for speech and text modalities. To project both modalities into a joint embedding space, we use two uni-modal <em class=\"ltx_emph ltx_font_italic\">pre-nets</em>: <math alttext=\"e_{\\tilde{X}}:{\\mathcal{X}}^{T}\\mapsto{\\mathbb{R}}^{L\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>T</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">e_{\\tilde{X}}:{\\mathcal{X}}^{T}\\mapsto{\\mathbb{R}}^{L\\times d}</annotation></semantics></math> for speech and <math alttext=\"e_{Y}:{\\mathcal{Y}}^{L}\\mapsto{\\mathbb{R}}^{L\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mi>Y</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><mi>L</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">e_{Y}:{\\mathcal{Y}}^{L}\\mapsto{\\mathbb{R}}^{L\\times d}</annotation></semantics></math> for text, each implemented as a linear embedding layer. Before the speech pre-net, a <em class=\"ltx_emph ltx_font_italic\">speech syllabifier</em> converts the frame-level feature vectors into a syllable-level sequence. It consists of: (i) a <em class=\"ltx_emph ltx_font_italic\">differentiable soft-pooler</em> <math alttext=\"m:{\\mathcal{X}}^{T}\\mapsto{\\mathcal{X}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>T</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>L</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">m:{\\mathcal{X}}^{T}\\mapsto{\\mathcal{X}}^{L}</annotation></semantics></math> that aligns speech with text on the syllable level (ii) a tokenizer <math alttext=\"c:{\\mathcal{X}}^{L}\\mapsto\\tilde{{\\mathcal{X}}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>L</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>~</mo></mover><mi>L</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">c:{\\mathcal{X}}^{L}\\mapsto\\tilde{{\\mathcal{X}}}^{L}</annotation></semantics></math> to discretizes speech into syllable-like units. Thus the speech pre-net is</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "model",
                    "syllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\\circ g(x):=f(g(x))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>f</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8728;</mo><mi>g</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f\\circ g(x):=f(g(x))</annotation></semantics></math> for any functions <math alttext=\"f,g\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo>,</mo><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">f,g</annotation></semantics></math>, and <math alttext=\"\\tilde{X}:=c\\circ m(X)\\in\\tilde{{\\mathcal{X}}}^{L}.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mrow><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>:=</mo><mrow><mrow><mi>c</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8728;</mo><mi>m</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>~</mo></mover><mi>L</mi></msup></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\tilde{X}:=c\\circ m(X)\\in\\tilde{{\\mathcal{X}}}^{L}.</annotation></semantics></math>\nThe soft-pooler first estimates the boundary probabilities <math alttext=\"b(X)\\in[0,1]^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">b(X)\\in[0,1]^{T}</annotation></semantics></math> by learning from an unsupervised syllable detector Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>, then constructs a pooling mask <math alttext=\"a(X)\\in[0,1]^{L\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">a(X)\\in[0,1]^{L\\times T}</annotation></semantics></math>, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.F2\" title=\"Figure 2 &#8227; 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>:</p>\n\n",
                "matched_terms": [
                    "syllable",
                    "sylber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We prove that under regularity conditions, equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.E7\" title=\"In Distribution matching. &#8227; 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> matches true and generated text distributions in the same way as GANs, but without unstable straight-through gradients. Thus zero-error UASR is achievable under conditions in <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After pretraining with unsupervised syllable boundary labels <math alttext=\"[\\hat{b}_{1}(X),\\cdots,\\hat{b}_{T}(X)]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mover accent=\"true\"><mi>b</mi><mo>^</mo></mover><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><mrow><msub><mover accent=\"true\"><mi>b</mi><mo>^</mo></mover><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\hat{b}_{1}(X),\\cdots,\\hat{b}_{T}(X)]</annotation></semantics></math>, we perform <em class=\"ltx_emph ltx_font_italic\">joint end-to-end</em> (JE2E) training. In this stage, the soft-pooler is trained jointly with the rest of the model, guided by an additional soft constraint on the predicted <em class=\"ltx_emph ltx_font_italic\">syllable counts</em>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "syllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MLM objective encourages <em class=\"ltx_emph ltx_font_italic\">implicit</em> distribution matching, we found that <em class=\"ltx_emph ltx_font_italic\">explicit</em> distribution matching further improves a saturated MLM system. To this end, we adopt positional unigram and skipgram matching (PUSM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib48\" title=\"\">2024</a>; Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that using the <em class=\"ltx_emph ltx_font_italic\">first</em> transformer layer of the shared encoder, instead of the last, improves performance, consistent with findings at the word-level&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>. This may be due to over-contextualization in later layers. Finally, replacing each <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> token with the second most likely prediction reduces CER by about <math alttext=\"1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1\\%</annotation></semantics></math> compared to simply discarding <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span>s.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS1\" title=\"5.1 Datasets &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> introduces the datasets used for our experiments, followed by the syllabification steps for speech and text preprocessing in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS2\" title=\"5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS3\" title=\"5.3 Results: UASR &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> presents the main UASR results, and Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS4\" title=\"5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a> discusses SylCipher&#8217;s boundary refinement ability. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS5\" title=\"5.5 Ablation studies &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a> provides ablation studies on key design choices. Additional implementation details of our method and the baselines are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A4\" title=\"Appendix D Implementation details of SylCipher &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate SylCipher on three datasets. First, we train on the 460-hour clean subset of LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib35\" title=\"\">2015a</a>)</cite>, a standard UASR benchmark of audiobook recordings.\nSecond, to test domain generalization, we train another model on SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib23\" title=\"\">2021b</a>)</cite>, which contains 742 hours of spoken image captions. Third, to study languages with syllabic structures significantly different from English, we apply SylCipher to Mandarin using AISHELL-3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib42\" title=\"\">2021</a>)</cite>, which has 85 hours of read speech. We follow the same LibriSpeech split as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, and use the standard splits for SpokenCOCO and AISHELL-3. For LibriSpeech and SpokenCOCO, we consider both the <em class=\"ltx_emph ltx_font_italic\">matched</em> setting, where empirical speech and text probability distributions can be matched exactly, and the more realistic <em class=\"ltx_emph ltx_font_italic\">unmatched</em> setting where they cannot.\nIn the matched case, we use paired speech-text datasets with pairings removed; in the unmatched case, LibriSpeech uses LibriLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib36\" title=\"\">2015b</a>)</cite> with overlapping text removed, while SpokenCOCO is randomly split in half, with one half treated as speech-only and the other half treated as text-only. For AISHELL-3, we consider only the matched setting and compare models trained with/without tone labels. For all datasets, we apply a voice activity detector<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/wiseman/py-webrtcvad.git</span></span></span> to improve alignment.</p>\n\n",
                "matched_terms": [
                    "clean",
                    "librispeech",
                    "model",
                    "matched",
                    "unmatched",
                    "sylcipher",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English text, we use Pyphen<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/Kozea/Pyphen</span></span></span>, a rule-based hyphenation tool re-purposed for syllabification without a G2P. If Pyphen produces only a single chunk for a long word, we apply a simple rule-based fallback (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A5\" title=\"Appendix E Pseudo-code for the Pyphen+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>). We denote this combined approach as Pyphen+. We also experiment with other G2P-free approaches such as byte-pair encoding (BPE)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>); Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>, and find SylCipher robust to syllabification errors. To avoid long-tail distributions, we keep only the top-2048 most frequent English syllables and replace the rest with a special <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> token (replacing <math alttext=\"7\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">7\\%</annotation></semantics></math> of tokens in LibriSpeech and <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math> in SpokenCOCO. For Mandarin, we use the Pinyin of each Chinese character as a syllable, with or without tone labels. We keep the top-1024 most frequent syllables, covering 99.5% of occurrences. For speech syllabification, we use K-means clustering on syllable-level speech features created by mean pooling within the Sylber boundaries, with codebook size equal to the number of non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> text tokens.</p>\n\n",
                "matched_terms": [
                    "g2pfree",
                    "word",
                    "token",
                    "approach",
                    "librispeech",
                    "sylber",
                    "liu",
                    "syllable",
                    "sylcipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare SylCipher with UASR systems that differ in token type, training objective, and architecture.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "uasr",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Word-level:</span> <em class=\"ltx_emph ltx_font_italic\">JSTTI</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, the state-of-art word-level UASR system, which is GAN-free and architecturally similar to ours, using 2048 non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> words.</p>\n\n",
                "matched_terms": [
                    "uasr",
                    "ours",
                    "jstti"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character-level:</span> <em class=\"ltx_emph ltx_font_italic\">wav2vec-U</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite>, a strong GAN-based system; <em class=\"ltx_emph ltx_font_italic\">REBORN</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite>, a state-of-the-art phoneme-based GAN system; and a <em class=\"ltx_emph ltx_font_italic\">phone-level JSTTI</em>, trained with phoneme boundaries and 128 speech clusters (comparable to the number of character types).</p>\n\n",
                "matched_terms": [
                    "wav2vecu",
                    "jstti",
                    "baevski",
                    "tseng",
                    "reborn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level:</span> <em class=\"ltx_emph ltx_font_italic\">PUSM</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>, adapted from word-level UASR to syllables using the same syllable boundary detector and syllabifier as SylCipher.</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "wang",
                    "syllable",
                    "sylcipher",
                    "uasr",
                    "2023c"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also report results with <em class=\"ltx_emph ltx_font_italic\">self-training</em>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib10\" title=\"\">2019</a>); Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite>, where a wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib7\" title=\"\">2020</a>)</cite> student is further finetuned by distilling character (grapheme)-level pseudo-labels from a UASR system.\nPerformance is measured by character error rate (CER) for English and phone error rate (PER) for Mandarin, as both are tokenization-independent and easily comparable.</p>\n\n",
                "matched_terms": [
                    "phone",
                    "student",
                    "results",
                    "baevski",
                    "wav2vec",
                    "cer",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative training helps.</span> Stage-wise training shows progressive improvements: JE2E reduces CER modestly, while PUSM yields the largest gains (44% and 23% relative reductions over JE2E in matched/unmatched settings). Combining MLM-based stages with PUSM outperforms PUSM-only training by 33-34% relative, as MLM provides necessary initialization for PUSM to converge. Indeed, PUSM alone fails when SylCipher is randomly initialized, likely due to transformer training instability. Lastly, using unsupervised Sylber boundaries performs nearly as well as forced alignment, suggesting robustness to segmentation noise.\nSelf-training consistently improves performance, especially for stronger models. For SylCipher, CER is further reduced by 20% relative.</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "forced",
                    "sylber",
                    "sylcipher",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllables are robust to domain shifts.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T1.st2\" title=\"In Table 1 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a> reports results on SpokenCOCO. SylCipher again outperforms all baselines, surpassing PUSM by 32% and wav2vec-U by 49% relative CER after all stages. The margin is larger than LibriSpeech, especially in the unmatched setting. Notably, even after the first training stages, SylCipher already outperforms wav2vec-U by 22%. While wav2vec-U suffers from domain mismatch at the character level, both syllable-level methods perform better, suggesting syllable units are more robust to domain shifts. Self-training further improves SylCipher by 39% relative CER, likely due to higher syllable coverage reducing insertion/deletion errors. Moreover, performance degrades only slightly in the unmatched setting, indicating the sharper degradation on LibriSpeech stems from domain mismatch between its speech and text corpora.</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "wav2vecu",
                    "librispeech",
                    "results",
                    "syllable",
                    "unmatched",
                    "sylcipher",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level UASR works for Mandarin.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents results on Mandarin. Here, syllable-level models converge even without boundary refinement, while phone- and word-level approaches struggle, confirming that syllables are the most natural alignment unit for Mandarin. Compared to the best baseline (PUSM), SylCipher achieves over 5.5% relative PER reduction before self-training and 17% after. Interestingly, including tone labels does not harm performance&#8211;in fact, PER improves after the PUSM stage&#8211;suggesting that once phoneme labels are predicted correctly, tone prediction is also reliable.</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "phone",
                    "results",
                    "sylcipher",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the JE2E stage, we compare the syllable boundary detection performance against the teacher model Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> (which provides SylCipher&#8217;s initial boundaries) and other unsupervised approaches including Feat-Sim&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite>, SDHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite>, SylBoost&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>)</cite>. On LibriSpeech, Sylber is the strongest baseline on most metrics, except for the 20ms F1 score where SylBoost performs best. On SpokenCOCO, SylBoost outperforms Sylber on three of five metrics. However, our preliminary analysis show that SylBoost often over-segments, producing too many syllables and causing cross-modal misalignment and training instability in SylCipher. In contrast, Sylber predicts syllable counts closer to ground truth, making it more suitable as an initialization for UASR. When refined through JE2E, SylCipher improves upon its Sylber teacher, achieving +14% relative F1 (20ms tolerance) and +3% relative F1 (50ms) on LibriSpeech. On SpokenCOCO, it surpasses Sylber by +15% F1 and +11% R-value, and outperforms the best baseline Feat-Sim by +3% relative F1 score and +4.6% R-value. These results suggest that unpaired text provides additional guidance on syllable boundary detection. Visualizations of the speech-text alignment predicted by SylCipher can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A7\" title=\"Appendix G Spectrogram examples on LibriSpeech &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "model",
                    "sylber",
                    "results",
                    "syllable",
                    "sylcipher",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SylCipher is robust to syllabifiers.</span> We first test alternative syllabifiers beyond Pyphen, including the phoneme-based Syllabify<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/kylebgorman/syllabify</span></span></span> and a character-based approach using byte-pair encoding (BPE) tokenization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>. The latter is attractive because it is language-agnostic and integrated easily with spoken language models. For the BPE-based syllabifier, we train the first stage of SuperBPE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>)</cite> on LibriSpeech with a 17k vocabulary, roughly matching the number of syllable types. We then split BPE tokens containing multiple non-consecutive vowels and merge consecutive tokens to ensure each unit contains at least one vowel (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A6\" title=\"Appendix F Pseudo-code of BPE+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>). We refer to this method as BPE+. All syllabifier variants are trained only under the <em class=\"ltx_emph ltx_font_italic\">fixed-boundary stage</em>, since later stages show correlated trends. Instead of CER, we report syllable error rate (SER) to more directly reflects syllable-level performance. Among the methods, Pyphen+ achieves the lowest SER, outperforming even the phoneme-based Syllabify. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, BPE+ yields weaker but still competitive results, demonstrating that SylCipher can generalize to linguistically simpler, resource-free syllabifiers.</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "approach",
                    "results",
                    "liu",
                    "syllable",
                    "sylcipher",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced SylCipher, a UASR system that avoids phoneme-level resources such as G2P by recognizing speech at the <em class=\"ltx_emph ltx_font_italic\">syllable level</em>. Under the G2P-free setting, SylCipher outperforms the best existing systems by 17-40% relative CER on LibriSpeech and shows generalizability across other domains on SpokenCOCO, narrowing the gap with G2P-based systems. It also demonstrates cross-lingual robustness: on Mandarin, a tonal language where phoneme-based methods fail to converge, SylCipher achieves a PER of 13%. These results suggest that syllable-level modeling is a viable alternative to phoneme-level UASR and can push the horizon of more accessible and inclusive spoken language technology.</p>\n\n",
                "matched_terms": [
                    "g2pfree",
                    "librispeech",
                    "results",
                    "g2pbased",
                    "syllable",
                    "sylcipher",
                    "cer",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span> SylCipher is not yet language-universal, since different languages use different writing systems and require linguistic knowledge to properly syllabify. For example, languages such as Hebrew and Arabic omits vowels in their writing system, which could pose challenges to existing syllabifiers.\nWhile our experiments show that the method can be adapted to more resource-efficient tokenizers such as BPE with minimal modifications, coming up with a language-universal tokenization method remains an open problem.\nFurther, the iterative training procedure can be further simplified into an end-to-end approach. Lastly, improving the robustness of SylCipher under domain mismatch between speech and text remains an open challenge.\n\n</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "approach"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SylCipher is not yet language-universal, since different languages use different writing systems and require linguistic knowledge to properly syllabify. For example, languages such as Hebrew and Arabic omits vowels in their writing system, which could pose challenges to existing syllabifiers.\nWhile our experiments show that the method can be adapted to more resource-efficient tokenizers such as BPE with minimal modifications, coming up with a language-universal tokenization method remains an open problem.\nFurther, the iterative training procedure can be further simplified into an end-to-end approach. Lastly, improving the robustness of SylCipher under domain mismatch between speech and text remains an open challenge.\n</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "approach"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The proof consists of two main parts: (i) First, we prove that at least one minimizer <math alttext=\"(f_{\\tilde{X},1},g_{\\tilde{X},1},f_{Y,1},g_{Y,1})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mrow><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>g</mi><mrow><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>f</mi><mrow><mi>Y</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>g</mi><mrow><mi>Y</mi><mo>,</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(f_{\\tilde{X},1},g_{\\tilde{X},1},f_{Y,1},g_{Y,1})</annotation></semantics></math> can achieve a minimum of 0 for equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.E7\" title=\"In Distribution matching. &#8227; 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>; (ii) then we establish that for any minimizers <math alttext=\"(f_{\\tilde{X}}^{*},g_{\\tilde{X}}^{*},f_{Y}^{*},g_{Y}^{*})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>f</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>&#8727;</mo></msubsup><mo>,</mo><msubsup><mi>g</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>&#8727;</mo></msubsup><mo>,</mo><msubsup><mi>f</mi><mi>Y</mi><mo>&#8727;</mo></msubsup><mo>,</mo><msubsup><mi>g</mi><mi>Y</mi><mo>&#8727;</mo></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(f_{\\tilde{X}}^{*},g_{\\tilde{X}}^{*},f_{Y}^{*},g_{Y}^{*})</annotation></semantics></math>, the marginal distribution of the text posterior <math alttext=\"q_{Y|X}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m3\" intent=\":literal\"><semantics><msubsup><mi>q</mi><mrow><mi>Y</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">q_{Y|X}^{*}</annotation></semantics></math> matches the true text distribution <math alttext=\"p_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m4\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">p_{Y}</annotation></semantics></math>. As a result, by Assumption 4, Theorem 1 of <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite> guarantees that <math alttext=\"\\operatorname*{arg\\,max}_{y\\in{\\mathcal{Y}}^{L}}q_{Y|X}^{*}(y|X)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m5\" intent=\":literal\"><semantics><mrow><mrow><msub><mrow><mi>arg</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>max</mi></mrow><mrow><mi>y</mi><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><mi>L</mi></msup></mrow></msub><mo lspace=\"0.167em\">&#8289;</mo><msubsup><mi>q</mi><mrow><mi>Y</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo>&#8727;</mo></msubsup></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{arg\\,max}_{y\\in{\\mathcal{Y}}^{L}}q_{Y|X}^{*}(y|X)</annotation></semantics></math> achieves zero-error UASR.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English experiments, SylCipher uses a HuBERT-large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt</span></span></span> model pretrained on LibriLight&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib24\" title=\"\">2020</a>)</cite> as the SSL encoder in the speech syllabifier, and initialize the soft-pooler with unsupervised boundary labels from Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>. For Mandarin, we instead use XEUS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://huggingface.co/espnet/xeus/blob/main/model/xeus_checkpoint_new.pth</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib12\" title=\"\">2024</a>)</cite>, which is pretrained on Mandarin (among other languages) and significantly outperforms HuBERT. We also finetune Sylber on AISHELL-3 to ensure stable convergence. For the pre-nets, shared encoder, post-nets, MLM parameters, and most of the optimizer hyperparameters, we follow JSTTI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, as modifying them did not yield consistent improvements.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "jstti",
                    "model",
                    "sylber"
                ]
            }
        ]
    },
    "S5.T1.st2": {
        "source_file": "Towards Unsupervised Speech Recognition at the Syllable-Level",
        "caption": "(b) UASR results on SpokenCOCO",
        "body": "Model\nStudent\nToken\nMatched\nUnmatched\n\n\nCER (↓\\downarrow)\nCER (↓\\downarrow)\n\n\n\n\nwav2vec-U (Baevski et al., 2021)\n\nNone\nChar.\n45.0\n45.2\n\n\nwav2vec 2.0\nChar.\n46.3\n35.3\n\n\nJSTTI\nNone\nChar.\n78.3\n100\n\n\nJSTTI (Ni et al., 2025)\n\nNone\nWord\n64.5\n64.5\n\n\nPUSM (Sylber) (Wang et al., 2023c)\n\nNone\nSyllable\n41.5\n41.3\n\n\nwav2vec 2.0\nSyllable\n34.7\n34.3\n\n\nSylCipher (Ours, Sylber)\nNone\nSyllable\n34.9\n36.1\n\n\nSylCipher (Ours, Sylber+JE2E)\nNone\nSyllable\n31.2\n32.4\n\n\nSylCipher (Ours, Sylber+JE2E+PUSM)\nNone\nSyllable\n23.4\n26.8\n\n\nwav2vec 2.0\nSyllable\n13.9\n17.6",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Student</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Token</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Matched</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Unmatched</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">CER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.st2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">CER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.st2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\">wav2vec-U&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Char.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">45.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">45.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">wav2vec 2.0</th>\n<td class=\"ltx_td ltx_align_center\">Char.</td>\n<td class=\"ltx_td ltx_align_center\">46.3</td>\n<td class=\"ltx_td ltx_align_center\">35.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">JSTTI</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Char.</td>\n<td class=\"ltx_td ltx_align_center\">78.3</td>\n<td class=\"ltx_td ltx_align_center\">100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">JSTTI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Word</td>\n<td class=\"ltx_td ltx_align_center\">64.5</td>\n<td class=\"ltx_td ltx_align_center\">64.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" rowspan=\"2\">PUSM (Sylber)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\">41.5</td>\n<td class=\"ltx_td ltx_align_center\">41.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">wav2vec 2.0</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\">34.7</td>\n<td class=\"ltx_td ltx_align_center\">34.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">SylCipher (Ours, Sylber)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Syllable</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SylCipher (Ours, Sylber+JE2E)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\">31.2</td>\n<td class=\"ltx_td ltx_align_center\">32.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" rowspan=\"2\">SylCipher (Ours, Sylber+JE2E+PUSM)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">23.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">26.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">wav2vec 2.0</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Syllable</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">13.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">17.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "unmatched",
            "pusm",
            "token",
            "char",
            "jstti",
            "results",
            "baevski",
            "none",
            "sylberje2epusm",
            "sylcipher",
            "uasr",
            "wav2vecu",
            "wang",
            "student",
            "↓downarrow",
            "spokencoco",
            "2023c",
            "word",
            "ours",
            "sylber",
            "sylberje2e",
            "matched",
            "syllable",
            "wav2vec",
            "cer",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllables are robust to domain shifts.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T1.st2\" title=\"In Table 1 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a> reports results on SpokenCOCO. SylCipher again outperforms all baselines, surpassing PUSM by 32% and wav2vec-U by 49% relative CER after all stages. The margin is larger than LibriSpeech, especially in the unmatched setting. Notably, even after the first training stages, SylCipher already outperforms wav2vec-U by 22%. While wav2vec-U suffers from domain mismatch at the character level, both syllable-level methods perform better, suggesting syllable units are more robust to domain shifts. Self-training further improves SylCipher by 39% relative CER, likely due to higher syllable coverage reducing insertion/deletion errors. Moreover, performance degrades only slightly in the unmatched setting, indicating the sharper degradation on LibriSpeech stems from domain mismatch between its speech and text corpora.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Training speech recognizers with unpaired speech and text &#8211; known as unsupervised speech recognition (UASR) &#8211; is a crucial step toward extending ASR to low-resource languages in the long-tail distribution and enabling multimodal learning from non-parallel data. However, existing approaches based on phones often rely on costly resources such as grapheme-to-phoneme converters (G2Ps) and struggle to generalize to languages with ambiguous phoneme boundaries due to training instability. In this paper, we address both challenges by introducing a syllable-level UASR framework based on masked language modeling, which avoids the need for G2P and the instability of GAN-based methods. Our approach achieves up to a 40% relative reduction in character error rate (CER) on LibriSpeech and generalizes effectively to Mandarin, a language that has remained particularly difficult for prior methods. Code will be released upon acceptance.\n\n</p>\n\n",
                "matched_terms": [
                    "cer",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A promising step toward language-universal assistants is to build speech recognizers from <em class=\"ltx_emph ltx_font_italic\">unpaired</em> speech and text, or <em class=\"ltx_emph ltx_font_italic\">unsupervised speech recognition</em> (UASR)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib20\" title=\"\">2012</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib28\" title=\"\">2023</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite>.\nUASR is a fundamental challenge: success would enable downstream tasks such as speech synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib33\" title=\"\">2022</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib27\" title=\"\">2022</a>)</cite>, translation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib45\" title=\"\">2023a</a>)</cite>, and understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib41\" title=\"\">2023</a>)</cite>, paving the way for general-purpose voice assistants. It is also a central case of <em class=\"ltx_emph ltx_font_italic\">unpaired multimodal learning</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Artetxe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib4\" title=\"\">2018b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib3\" title=\"\">a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib5\" title=\"\">2019</a>; Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib25\" title=\"\">2018a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib26\" title=\"\">b</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib31\" title=\"\">2019</a>; Hoshen &amp; Wolf, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib21\" title=\"\">2018</a>)</cite>, where modalities need to be aligned without parallel data. In the absence of sentence-level alignment, the model must infer higher-level linguistic units&#8211;phones, syllables, and words&#8211;from raw speech waveforms in conjunction with global text statistics. Thus, UASR provides broader insights into representation learning and multimodal alignment without supervision.</p>\n\n",
                "matched_terms": [
                    "baevski",
                    "wang",
                    "uasr",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The best existing UASR systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib28\" title=\"\">2023</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite> operate at the <em class=\"ltx_emph ltx_font_italic\">phoneme-level</em>. To this end, they need to convert raw text units, or <em class=\"ltx_emph ltx_font_italic\">graphemes</em>, to phonemes, or minimal sound units that encode meaning, using a grapheme-to-phoneme convertor (G2P). However, training G2Ps requires resources such as pronunciation dictionaries, which can be time-consuming and labor-intensive to create. Without a G2P, such systems suffer from significant performance degradation due to misalignment between speech and raw text in many languages&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib28\" title=\"\">2023</a>); Ni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib33\" title=\"\">2022</a>)</cite>. Even when pronunciation dictionaries are available, the system may fail to detect clear phone-level boundaries due to strong co-articulation effects for languages such as Mandarin.</p>\n\n",
                "matched_terms": [
                    "baevski",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative approach to phone-based models is to build a word-level UASR system, which can be achieved without a G2P. Yet a major concern is the coverage of the system on <em class=\"ltx_emph ltx_font_italic\">rare words</em>, which can effectively be infinite in vocabulary size, making it significantly harder to acquire than phones. Furthermore, detecting word boundaries requires capturing long-range contextual dependencies in speech, which risk destabilizing segmentation mechanisms effective for phone-level UASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "word",
                    "wang",
                    "uasr",
                    "2023c"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a third alternative &#8211; to build UASR at the <em class=\"ltx_emph ltx_font_italic\">syllable-level</em>, which can be justified on three grounds. First, unlike words, the number of distinct syllables for a language is finite, which reduces the long-tail token distribution issue and allows for better generalization to unseen words; second, many languages exhibit the best alignment between speech and text at the syllable level instead of the phone or word level.\nFor instance, Mandarin uses characters, which have strong correspondences to spoken syllables; therefore, we expect that a syllable-level UASR system could be more appropriate for UASR than a phoneme-based system, which we also found to be the case empirically.</p>\n\n",
                "matched_terms": [
                    "word",
                    "uasr",
                    "syllable",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces <span class=\"ltx_text ltx_font_bold\">SylCipher</span>, to our knowledge, the first syllable-based UASR system. SylCipher jointly predicts syllable boundaries and embedding tokens from raw speech using a unified self-supervised objective. The learning mechanism avoids adversarial training, making it more stable and less sensitive to hyperparameters.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "uasr",
                    "syllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct extensive experiments across domains and languages. On LibriSpeech, SylCipher achieves up to 40% relative character error rate (CER) reduction over prior G2P-free UASR methods. On SpokenCOCO, improvements are even larger, demonstrating robustness across domains. On Mandarin, SylCipher achieves 12.2% phone error rate (PER), outperforming GAN-based UASR methods that fail to even converge.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "cer",
                    "uasr",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paper organizations.</span> Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S2\" title=\"2 Related work &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reviews related work. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S3\" title=\"3 Syllable-level unsupervised speech recognition &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> formalizes the syllable-level UASR problem. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4\" title=\"4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents SylCipher and its theoretical guarantees. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5\" title=\"5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports experiments and ablations. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S6\" title=\"6 Conclusion &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> concludes with limitations and future work.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on UASR assumed the existence of a reliable G2P and formulate the problem as an adversarial game, where a conditional generator predicts a phoneme sequence given a speech waveform, and a discriminator tries to tell real phonemized text apart from the generator&#8217;s output&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite>. Within this framework, several works have explored different segmentation mechanisms such as fixed unsupervised phoneme segmentation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib30\" title=\"\">2018</a>)</cite>, iterative forced alignment&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib10\" title=\"\">2019</a>)</cite>, de-duplication&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite> and reinforcement learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite> proposed an alternative formulation of UASR as an explicit distribution matching problem, by matching the lower-order <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math>-gram distributions of the generated and real texts. Later work further extended this reformulation to use G2P-free tokens such as words&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>; Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, more expressive architecture such as transformers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite> and more general text distribution such as masked prediction probabilities&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>. We adapt this word-level UASR approach to the syllable-level, and supplement it with a simplified training flow, a more stable differentiable boundary detector and additional learning objectives.</p>\n\n",
                "matched_terms": [
                    "baevski",
                    "wang",
                    "uasr",
                    "2023c"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on syllable-level modeling of speech used signal processing techniques&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib49\" title=\"\">2009</a>)</cite>. To discover higher-level structure and more efficient self-supervised learning (SSL) representations from raw speech, <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite> proposed to induce syllabic structure from existing SSL models such as HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib22\" title=\"\">2021a</a>)</cite>. To this end,&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite> probed the self-attention layers of VG-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng &amp; Harwath, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib37\" title=\"\">2022</a>)</cite>, a visually grounded SSL model to detect syllable-like feature clusters and further refine such clusters using a min-cut algorithm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi &amp; Malik (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib40\" title=\"\">1997</a>)</cite>. To sidestep the need for visual data, <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite> employed a speech-only SSL model trained with utterance-level self-distillation from a HuBERT teacher. Due to the indirect manner of such approaches by which syllabic structures are derived, they are often noisy and unreliable. To cope with this issue, recent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> proposed a more direct and targeted approach by performing self-distillation at the syllable-level, which significantly improved syllable boundary detection and unit discovery performance by encouraging sharper contrast between within and between-syllable feature frames.</p>\n\n",
                "matched_terms": [
                    "model",
                    "syllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we formulate syllable-level UASR as follows.\nLet <math alttext=\"X=[X_{1},\\cdots,X_{T}]\\in{\\mathcal{X}}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>X</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">X=[X_{1},\\cdots,X_{T}]\\in{\\mathcal{X}}^{T}</annotation></semantics></math> be a padded sequence of speech feature vectors and let <math alttext=\"Y=[Y_{1},\\cdots,Y_{L}]\\in{\\mathcal{Y}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>Y</mi><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">Y=[Y_{1},\\cdots,Y_{L}]\\in{\\mathcal{Y}}^{L}</annotation></semantics></math> a padded sequence of text tokens in the same language. Since a tokenized speech utterance typically uses more tokens than the text transcription of the same utterance, we assume the <math alttext=\"T\\geq L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>&#8805;</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">T\\geq L</annotation></semantics></math>. Assume <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> come from two <em class=\"ltx_emph ltx_font_italic\">unpaired</em> datasets and are therefore <em class=\"ltx_emph ltx_font_italic\">statistically independent</em>. Further, suppose they are <em class=\"ltx_emph ltx_font_italic\">matched</em>, i.e., there exists an ASR function <math alttext=\"y^{*}:{\\mathcal{X}}\\mapsto{\\mathcal{Y}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mrow><msup><mi>y</mi><mo>&#8727;</mo></msup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo stretchy=\"false\">&#8614;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">y^{*}:{\\mathcal{X}}\\mapsto{\\mathcal{Y}}</annotation></semantics></math> such that the distributions of <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m8\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math>, <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m9\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"p_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m10\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">p_{Y}</annotation></semantics></math> satisfy</p>\n\n",
                "matched_terms": [
                    "uasr",
                    "matched"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of UASR is to recover <math alttext=\"y^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m11\" intent=\":literal\"><semantics><msup><mi>y</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">y^{*}</annotation></semantics></math> given only unpaired <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m12\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m13\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math>. The syllable-level case is a special setting where the tokens of <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m14\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> are syllables. The learning problem resembles <em class=\"ltx_emph ltx_font_italic\">decipherment</em>, where one decodes a message in an unknown script without a lexicon or grammar. In practice, <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m15\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> is formed from frame-level SSL features&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib22\" title=\"\">2021a</a>)</cite>, and <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m16\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"p_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m17\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">p_{Y}</annotation></semantics></math> are only approximately matched due to finite-sample noise and domain mismatch. As shown in <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite>, UASR is ill-posed in general, but the mapping <math alttext=\"y^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m18\" intent=\":literal\"><semantics><msup><mi>y</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">y^{*}</annotation></semantics></math> in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S3.E1\" title=\"In 3 Syllable-level unsupervised speech recognition &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> becomes identifiable if syllable boundaries are known and the language satisfies mild conditions.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "uasr",
                    "matched",
                    "syllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we describe <span class=\"ltx_text ltx_font_bold\">SylCipher</span>, our proposed model for syllable-level UASR. We first present its architecture and training objective, then justify the design theoretically, and finally introduce several practical modifications for training and inference.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "uasr",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.F1\" title=\"Figure 1 &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, SylCipher is an encoder-only language model with a <em class=\"ltx_emph ltx_font_italic\">shared encoder</em> for speech and text modalities. To project both modalities into a joint embedding space, we use two uni-modal <em class=\"ltx_emph ltx_font_italic\">pre-nets</em>: <math alttext=\"e_{\\tilde{X}}:{\\mathcal{X}}^{T}\\mapsto{\\mathbb{R}}^{L\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>T</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">e_{\\tilde{X}}:{\\mathcal{X}}^{T}\\mapsto{\\mathbb{R}}^{L\\times d}</annotation></semantics></math> for speech and <math alttext=\"e_{Y}:{\\mathcal{Y}}^{L}\\mapsto{\\mathbb{R}}^{L\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mi>Y</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><mi>L</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">e_{Y}:{\\mathcal{Y}}^{L}\\mapsto{\\mathbb{R}}^{L\\times d}</annotation></semantics></math> for text, each implemented as a linear embedding layer. Before the speech pre-net, a <em class=\"ltx_emph ltx_font_italic\">speech syllabifier</em> converts the frame-level feature vectors into a syllable-level sequence. It consists of: (i) a <em class=\"ltx_emph ltx_font_italic\">differentiable soft-pooler</em> <math alttext=\"m:{\\mathcal{X}}^{T}\\mapsto{\\mathcal{X}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>T</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>L</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">m:{\\mathcal{X}}^{T}\\mapsto{\\mathcal{X}}^{L}</annotation></semantics></math> that aligns speech with text on the syllable level (ii) a tokenizer <math alttext=\"c:{\\mathcal{X}}^{L}\\mapsto\\tilde{{\\mathcal{X}}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>L</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>~</mo></mover><mi>L</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">c:{\\mathcal{X}}^{L}\\mapsto\\tilde{{\\mathcal{X}}}^{L}</annotation></semantics></math> to discretizes speech into syllable-like units. Thus the speech pre-net is</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "model",
                    "syllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\\circ g(x):=f(g(x))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>f</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8728;</mo><mi>g</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f\\circ g(x):=f(g(x))</annotation></semantics></math> for any functions <math alttext=\"f,g\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo>,</mo><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">f,g</annotation></semantics></math>, and <math alttext=\"\\tilde{X}:=c\\circ m(X)\\in\\tilde{{\\mathcal{X}}}^{L}.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mrow><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>:=</mo><mrow><mrow><mi>c</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8728;</mo><mi>m</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>~</mo></mover><mi>L</mi></msup></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\tilde{X}:=c\\circ m(X)\\in\\tilde{{\\mathcal{X}}}^{L}.</annotation></semantics></math>\nThe soft-pooler first estimates the boundary probabilities <math alttext=\"b(X)\\in[0,1]^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">b(X)\\in[0,1]^{T}</annotation></semantics></math> by learning from an unsupervised syllable detector Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>, then constructs a pooling mask <math alttext=\"a(X)\\in[0,1]^{L\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">a(X)\\in[0,1]^{L\\times T}</annotation></semantics></math>, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.F2\" title=\"Figure 2 &#8227; 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>:</p>\n\n",
                "matched_terms": [
                    "syllable",
                    "sylber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We prove that under regularity conditions, equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.E7\" title=\"In Distribution matching. &#8227; 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> matches true and generated text distributions in the same way as GANs, but without unstable straight-through gradients. Thus zero-error UASR is achievable under conditions in <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After pretraining with unsupervised syllable boundary labels <math alttext=\"[\\hat{b}_{1}(X),\\cdots,\\hat{b}_{T}(X)]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mover accent=\"true\"><mi>b</mi><mo>^</mo></mover><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><mrow><msub><mover accent=\"true\"><mi>b</mi><mo>^</mo></mover><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\hat{b}_{1}(X),\\cdots,\\hat{b}_{T}(X)]</annotation></semantics></math>, we perform <em class=\"ltx_emph ltx_font_italic\">joint end-to-end</em> (JE2E) training. In this stage, the soft-pooler is trained jointly with the rest of the model, guided by an additional soft constraint on the predicted <em class=\"ltx_emph ltx_font_italic\">syllable counts</em>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "syllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MLM objective encourages <em class=\"ltx_emph ltx_font_italic\">implicit</em> distribution matching, we found that <em class=\"ltx_emph ltx_font_italic\">explicit</em> distribution matching further improves a saturated MLM system. To this end, we adopt positional unigram and skipgram matching (PUSM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib48\" title=\"\">2024</a>; Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that using the <em class=\"ltx_emph ltx_font_italic\">first</em> transformer layer of the shared encoder, instead of the last, improves performance, consistent with findings at the word-level&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>. This may be due to over-contextualization in later layers. Finally, replacing each <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> token with the second most likely prediction reduces CER by about <math alttext=\"1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1\\%</annotation></semantics></math> compared to simply discarding <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span>s.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS1\" title=\"5.1 Datasets &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> introduces the datasets used for our experiments, followed by the syllabification steps for speech and text preprocessing in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS2\" title=\"5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS3\" title=\"5.3 Results: UASR &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> presents the main UASR results, and Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS4\" title=\"5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a> discusses SylCipher&#8217;s boundary refinement ability. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS5\" title=\"5.5 Ablation studies &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a> provides ablation studies on key design choices. Additional implementation details of our method and the baselines are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A4\" title=\"Appendix D Implementation details of SylCipher &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate SylCipher on three datasets. First, we train on the 460-hour clean subset of LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib35\" title=\"\">2015a</a>)</cite>, a standard UASR benchmark of audiobook recordings.\nSecond, to test domain generalization, we train another model on SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib23\" title=\"\">2021b</a>)</cite>, which contains 742 hours of spoken image captions. Third, to study languages with syllabic structures significantly different from English, we apply SylCipher to Mandarin using AISHELL-3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib42\" title=\"\">2021</a>)</cite>, which has 85 hours of read speech. We follow the same LibriSpeech split as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, and use the standard splits for SpokenCOCO and AISHELL-3. For LibriSpeech and SpokenCOCO, we consider both the <em class=\"ltx_emph ltx_font_italic\">matched</em> setting, where empirical speech and text probability distributions can be matched exactly, and the more realistic <em class=\"ltx_emph ltx_font_italic\">unmatched</em> setting where they cannot.\nIn the matched case, we use paired speech-text datasets with pairings removed; in the unmatched case, LibriSpeech uses LibriLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib36\" title=\"\">2015b</a>)</cite> with overlapping text removed, while SpokenCOCO is randomly split in half, with one half treated as speech-only and the other half treated as text-only. For AISHELL-3, we consider only the matched setting and compare models trained with/without tone labels. For all datasets, we apply a voice activity detector<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/wiseman/py-webrtcvad.git</span></span></span> to improve alignment.</p>\n\n",
                "matched_terms": [
                    "model",
                    "matched",
                    "unmatched",
                    "sylcipher",
                    "uasr",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English text, we use Pyphen<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/Kozea/Pyphen</span></span></span>, a rule-based hyphenation tool re-purposed for syllabification without a G2P. If Pyphen produces only a single chunk for a long word, we apply a simple rule-based fallback (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A5\" title=\"Appendix E Pseudo-code for the Pyphen+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>). We denote this combined approach as Pyphen+. We also experiment with other G2P-free approaches such as byte-pair encoding (BPE)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>); Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>, and find SylCipher robust to syllabification errors. To avoid long-tail distributions, we keep only the top-2048 most frequent English syllables and replace the rest with a special <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> token (replacing <math alttext=\"7\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">7\\%</annotation></semantics></math> of tokens in LibriSpeech and <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math> in SpokenCOCO. For Mandarin, we use the Pinyin of each Chinese character as a syllable, with or without tone labels. We keep the top-1024 most frequent syllables, covering 99.5% of occurrences. For speech syllabification, we use K-means clustering on syllable-level speech features created by mean pooling within the Sylber boundaries, with codebook size equal to the number of non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> text tokens.</p>\n\n",
                "matched_terms": [
                    "word",
                    "token",
                    "sylber",
                    "syllable",
                    "sylcipher",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare SylCipher with UASR systems that differ in token type, training objective, and architecture.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "uasr",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Word-level:</span> <em class=\"ltx_emph ltx_font_italic\">JSTTI</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, the state-of-art word-level UASR system, which is GAN-free and architecturally similar to ours, using 2048 non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> words.</p>\n\n",
                "matched_terms": [
                    "uasr",
                    "ours",
                    "jstti"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character-level:</span> <em class=\"ltx_emph ltx_font_italic\">wav2vec-U</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite>, a strong GAN-based system; <em class=\"ltx_emph ltx_font_italic\">REBORN</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite>, a state-of-the-art phoneme-based GAN system; and a <em class=\"ltx_emph ltx_font_italic\">phone-level JSTTI</em>, trained with phoneme boundaries and 128 speech clusters (comparable to the number of character types).</p>\n\n",
                "matched_terms": [
                    "wav2vecu",
                    "baevski",
                    "jstti"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level:</span> <em class=\"ltx_emph ltx_font_italic\">PUSM</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>, adapted from word-level UASR to syllables using the same syllable boundary detector and syllabifier as SylCipher.</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "wang",
                    "syllable",
                    "sylcipher",
                    "uasr",
                    "2023c"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also report results with <em class=\"ltx_emph ltx_font_italic\">self-training</em>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib10\" title=\"\">2019</a>); Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite>, where a wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib7\" title=\"\">2020</a>)</cite> student is further finetuned by distilling character (grapheme)-level pseudo-labels from a UASR system.\nPerformance is measured by character error rate (CER) for English and phone error rate (PER) for Mandarin, as both are tokenization-independent and easily comparable.</p>\n\n",
                "matched_terms": [
                    "student",
                    "results",
                    "baevski",
                    "wav2vec",
                    "cer",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level modeling performs best under G2P-free setting.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T1.st1\" title=\"In Table 1 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a> summarizes results on LibriSpeech. Among baselines, PUSM performs best in the matched setting, while wav2vec-U is strongest in the unmatched setting, though its performance is limited by the lack of G2P. Unlike phonemized training, REBORN trained directly on raw characters performs worse than wav2vec-U, suggesting sensitivity to misalignment between speech and text. By contrast, SylCipher with all three stages (Sylber+JE2E+PUSM) achieves 21.8% CER (matched) and 35.9% (unmatched), outperforming all baselines. Compared to the best-in-average system wav2vec-U (35.6%/43.3%), SylCipher reduces by CER by 40% (matched) and 17% (unmatched) relative.\nBoth syllable-level models (PUSM and SylCipher) outperform word- or character-level models, confirming that speech-text alignment is the best at the syllable level. Word-level JSTTI performs worst due to poor rare-word coverage (<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span>s <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p6.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 17%), and adapting JSTTI to phone-level degrades further, likely because phone clusters are noisier than syllable or word clusters.</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "wav2vecu",
                    "word",
                    "jstti",
                    "results",
                    "matched",
                    "syllable",
                    "sylberje2epusm",
                    "unmatched",
                    "sylcipher",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative training helps.</span> Stage-wise training shows progressive improvements: JE2E reduces CER modestly, while PUSM yields the largest gains (44% and 23% relative reductions over JE2E in matched/unmatched settings). Combining MLM-based stages with PUSM outperforms PUSM-only training by 33-34% relative, as MLM provides necessary initialization for PUSM to converge. Indeed, PUSM alone fails when SylCipher is randomly initialized, likely due to transformer training instability. Lastly, using unsupervised Sylber boundaries performs nearly as well as forced alignment, suggesting robustness to segmentation noise.\nSelf-training consistently improves performance, especially for stronger models. For SylCipher, CER is further reduced by 20% relative.</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "cer",
                    "sylber",
                    "sylcipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level UASR works for Mandarin.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents results on Mandarin. Here, syllable-level models converge even without boundary refinement, while phone- and word-level approaches struggle, confirming that syllables are the most natural alignment unit for Mandarin. Compared to the best baseline (PUSM), SylCipher achieves over 5.5% relative PER reduction before self-training and 17% after. Interestingly, including tone labels does not harm performance&#8211;in fact, PER improves after the PUSM stage&#8211;suggesting that once phoneme labels are predicted correctly, tone prediction is also reliable.</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "results",
                    "uasr",
                    "sylcipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the JE2E stage, we compare the syllable boundary detection performance against the teacher model Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> (which provides SylCipher&#8217;s initial boundaries) and other unsupervised approaches including Feat-Sim&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite>, SDHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite>, SylBoost&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>)</cite>. On LibriSpeech, Sylber is the strongest baseline on most metrics, except for the 20ms F1 score where SylBoost performs best. On SpokenCOCO, SylBoost outperforms Sylber on three of five metrics. However, our preliminary analysis show that SylBoost often over-segments, producing too many syllables and causing cross-modal misalignment and training instability in SylCipher. In contrast, Sylber predicts syllable counts closer to ground truth, making it more suitable as an initialization for UASR. When refined through JE2E, SylCipher improves upon its Sylber teacher, achieving +14% relative F1 (20ms tolerance) and +3% relative F1 (50ms) on LibriSpeech. On SpokenCOCO, it surpasses Sylber by +15% F1 and +11% R-value, and outperforms the best baseline Feat-Sim by +3% relative F1 score and +4.6% R-value. These results suggest that unpaired text provides additional guidance on syllable boundary detection. Visualizations of the speech-text alignment predicted by SylCipher can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A7\" title=\"Appendix G Spectrogram examples on LibriSpeech &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "sylber",
                    "results",
                    "syllable",
                    "sylcipher",
                    "uasr",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SylCipher is robust to syllabifiers.</span> We first test alternative syllabifiers beyond Pyphen, including the phoneme-based Syllabify<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/kylebgorman/syllabify</span></span></span> and a character-based approach using byte-pair encoding (BPE) tokenization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>. The latter is attractive because it is language-agnostic and integrated easily with spoken language models. For the BPE-based syllabifier, we train the first stage of SuperBPE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>)</cite> on LibriSpeech with a 17k vocabulary, roughly matching the number of syllable types. We then split BPE tokens containing multiple non-consecutive vowels and merge consecutive tokens to ensure each unit contains at least one vowel (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A6\" title=\"Appendix F Pseudo-code of BPE+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>). We refer to this method as BPE+. All syllabifier variants are trained only under the <em class=\"ltx_emph ltx_font_italic\">fixed-boundary stage</em>, since later stages show correlated trends. Instead of CER, we report syllable error rate (SER) to more directly reflects syllable-level performance. Among the methods, Pyphen+ achieves the lowest SER, outperforming even the phoneme-based Syllabify. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, BPE+ yields weaker but still competitive results, demonstrating that SylCipher can generalize to linguistically simpler, resource-free syllabifiers.</p>\n\n",
                "matched_terms": [
                    "results",
                    "cer",
                    "syllable",
                    "sylcipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Clamp-based soft-pooler trains more stably.</span> In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, we test our modified soft-pooler in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.E3\" title=\"In 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, which uses a clamp-based <math alttext=\"\\sigma_{\\epsilon}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#963;</mi><mi>&#1013;</mi></msub><annotation encoding=\"application/x-tex\">\\sigma_{\\epsilon}</annotation></semantics></math> function (&#8220;Clamp&#8221;), against alternatives based on <math alttext=\"\\tanh\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p3.m2\" intent=\":literal\"><semantics><mi>tanh</mi><annotation encoding=\"application/x-tex\">\\tanh</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib9\" title=\"\">2022</a>)</cite> (&#8220;Tanh&#8221;) and softmax&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib48\" title=\"\">2024</a>)</cite>) (&#8220;Softmax&#8221;). Following prior work, we set the tapering parameter <math alttext=\"\\epsilon=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.5</annotation></semantics></math> for &#8220;Clamp&#8221; and <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p3.m4\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math> for the other two approaches. On both SpokenCOCO and AISHELL-3, Clamp consistently outperforms the sigmoid-based poolers by 6-36%. This suggests that our design provides more stable and efficient training dynamics.\n</p>\n\n",
                "matched_terms": [
                    "wang",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced SylCipher, a UASR system that avoids phoneme-level resources such as G2P by recognizing speech at the <em class=\"ltx_emph ltx_font_italic\">syllable level</em>. Under the G2P-free setting, SylCipher outperforms the best existing systems by 17-40% relative CER on LibriSpeech and shows generalizability across other domains on SpokenCOCO, narrowing the gap with G2P-based systems. It also demonstrates cross-lingual robustness: on Mandarin, a tonal language where phoneme-based methods fail to converge, SylCipher achieves a PER of 13%. These results suggest that syllable-level modeling is a viable alternative to phoneme-level UASR and can push the horizon of more accessible and inclusive spoken language technology.</p>\n\n",
                "matched_terms": [
                    "results",
                    "syllable",
                    "sylcipher",
                    "cer",
                    "uasr",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The proof consists of two main parts: (i) First, we prove that at least one minimizer <math alttext=\"(f_{\\tilde{X},1},g_{\\tilde{X},1},f_{Y,1},g_{Y,1})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mrow><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>g</mi><mrow><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>f</mi><mrow><mi>Y</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>g</mi><mrow><mi>Y</mi><mo>,</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(f_{\\tilde{X},1},g_{\\tilde{X},1},f_{Y,1},g_{Y,1})</annotation></semantics></math> can achieve a minimum of 0 for equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.E7\" title=\"In Distribution matching. &#8227; 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>; (ii) then we establish that for any minimizers <math alttext=\"(f_{\\tilde{X}}^{*},g_{\\tilde{X}}^{*},f_{Y}^{*},g_{Y}^{*})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>f</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>&#8727;</mo></msubsup><mo>,</mo><msubsup><mi>g</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>&#8727;</mo></msubsup><mo>,</mo><msubsup><mi>f</mi><mi>Y</mi><mo>&#8727;</mo></msubsup><mo>,</mo><msubsup><mi>g</mi><mi>Y</mi><mo>&#8727;</mo></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(f_{\\tilde{X}}^{*},g_{\\tilde{X}}^{*},f_{Y}^{*},g_{Y}^{*})</annotation></semantics></math>, the marginal distribution of the text posterior <math alttext=\"q_{Y|X}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m3\" intent=\":literal\"><semantics><msubsup><mi>q</mi><mrow><mi>Y</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">q_{Y|X}^{*}</annotation></semantics></math> matches the true text distribution <math alttext=\"p_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m4\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">p_{Y}</annotation></semantics></math>. As a result, by Assumption 4, Theorem 1 of <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite> guarantees that <math alttext=\"\\operatorname*{arg\\,max}_{y\\in{\\mathcal{Y}}^{L}}q_{Y|X}^{*}(y|X)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m5\" intent=\":literal\"><semantics><mrow><mrow><msub><mrow><mi>arg</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>max</mi></mrow><mrow><mi>y</mi><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><mi>L</mi></msup></mrow></msub><mo lspace=\"0.167em\">&#8289;</mo><msubsup><mi>q</mi><mrow><mi>Y</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo>&#8727;</mo></msubsup></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{arg\\,max}_{y\\in{\\mathcal{Y}}^{L}}q_{Y|X}^{*}(y|X)</annotation></semantics></math> achieves zero-error UASR.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English experiments, SylCipher uses a HuBERT-large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt</span></span></span> model pretrained on LibriLight&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib24\" title=\"\">2020</a>)</cite> as the SSL encoder in the speech syllabifier, and initialize the soft-pooler with unsupervised boundary labels from Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>. For Mandarin, we instead use XEUS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://huggingface.co/espnet/xeus/blob/main/model/xeus_checkpoint_new.pth</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib12\" title=\"\">2024</a>)</cite>, which is pretrained on Mandarin (among other languages) and significantly outperforms HuBERT. We also finetune Sylber on AISHELL-3 to ensure stable convergence. For the pre-nets, shared encoder, post-nets, MLM parameters, and most of the optimizer hyperparameters, we follow JSTTI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, as modifying them did not yield consistent improvements.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "jstti",
                    "model",
                    "sylber"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "Towards Unsupervised Speech Recognition at the Syllable-Level",
        "caption": "Table 2: UASR results on AISHELL-3 test set. Inside the bracket lists the unsupervised boundary used for each model. “Student” stands for the student model used during the self-training stage using pseudo-labels from each model. Init./Final refers to initials and finals in the Chinese phonetic alphabet. For text data tokens, Syllable refers to the pinyin representation of each Chinese character, while Phone denotes the individual letters within the pinyin tokens. PER stands for phone error rate.",
        "body": "Model\nStudent\nToken\nw/o Tone\nw. Tone\n\n\nPER (↓\\downarrow)\nPER (↓\\downarrow)\n\n\n\n\nwav2vec-U (Baevski et al., 2021)\n\nNone\nPhone\n74.9\n76.2\n\n\nJSTTI\nNone\nInit./Final\n96.4\n100\n\n\nJSTTI (Ni et al., 2025)\n\nNone\nWord\n83.2\n169\n\n\nPUSM (Sylber) (Wang et al., 2023c)\n\nNone\nSyllable\n28.4\n26.5\n\n\nwav2vec 2.0\nSyllable\n18.5\n14.9\n\n\nSylCipher (Ours, forced align)\nNone\nSyllable\n38.1\n38.9\n\n\nSylCipher (Ours, Sylber)\nNone\nSyllable\n44.6\n48.3\n\n\nSylCipher (Ours, Sylber+JE2E)\nNone\nSyllable\n41.7\n45.1\n\n\nSylCipher (Ours, Sylber+JE2E+PUSM)\nNone\nSyllable\n26.9\n24.9\n\n\nwav2vec 2.0\nSyllable\n15.3\n12.2",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Student</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Token</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">w/o Tone</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">w. Tone</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">PER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">PER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">wav2vec-U&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Phone</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">74.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">76.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">JSTTI</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Init./Final</td>\n<td class=\"ltx_td ltx_align_center\">96.4</td>\n<td class=\"ltx_td ltx_align_center\">100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">JSTTI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Word</td>\n<td class=\"ltx_td ltx_align_center\">83.2</td>\n<td class=\"ltx_td ltx_align_center\">169</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" rowspan=\"2\">PUSM (Sylber)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\">28.4</td>\n<td class=\"ltx_td ltx_align_center\">26.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">wav2vec 2.0</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\">18.5</td>\n<td class=\"ltx_td ltx_align_center\">14.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">SylCipher (Ours, forced align)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Syllable</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SylCipher (Ours, Sylber)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\">44.6</td>\n<td class=\"ltx_td ltx_align_center\">48.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SylCipher (Ours, Sylber+JE2E)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\">41.7</td>\n<td class=\"ltx_td ltx_align_center\">45.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" rowspan=\"2\">SylCipher (Ours, Sylber+JE2E+PUSM)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">None</th>\n<td class=\"ltx_td ltx_align_center\">Syllable</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">26.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">24.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">wav2vec 2.0</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Syllable</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">15.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">12.2</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "inside",
            "forced",
            "pinyin",
            "data",
            "character",
            "individual",
            "letters",
            "initfinal",
            "finals",
            "chinese",
            "used",
            "tone",
            "text",
            "stands",
            "alphabet",
            "pusm",
            "bracket",
            "denotes",
            "error",
            "token",
            "stage",
            "rate",
            "jstti",
            "aishell3",
            "initials",
            "results",
            "baevski",
            "none",
            "sylberje2epusm",
            "selftraining",
            "each",
            "sylcipher",
            "uasr",
            "while",
            "wav2vecu",
            "wang",
            "student",
            "within",
            "“student”",
            "phonetic",
            "↓downarrow",
            "during",
            "lists",
            "refers",
            "from",
            "tokens",
            "test",
            "2023c",
            "phone",
            "word",
            "ours",
            "align",
            "set",
            "pseudolabels",
            "sylber",
            "sylberje2e",
            "representation",
            "syllable",
            "wav2vec",
            "unsupervised",
            "model",
            "boundary"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level UASR works for Mandarin.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents results on Mandarin. Here, syllable-level models converge even without boundary refinement, while phone- and word-level approaches struggle, confirming that syllables are the most natural alignment unit for Mandarin. Compared to the best baseline (PUSM), SylCipher achieves over 5.5% relative PER reduction before self-training and 17% after. Interestingly, including tone labels does not harm performance&#8211;in fact, PER improves after the PUSM stage&#8211;suggesting that once phoneme labels are predicted correctly, tone prediction is also reliable.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Training speech recognizers with unpaired speech and text &#8211; known as unsupervised speech recognition (UASR) &#8211; is a crucial step toward extending ASR to low-resource languages in the long-tail distribution and enabling multimodal learning from non-parallel data. However, existing approaches based on phones often rely on costly resources such as grapheme-to-phoneme converters (G2Ps) and struggle to generalize to languages with ambiguous phoneme boundaries due to training instability. In this paper, we address both challenges by introducing a syllable-level UASR framework based on masked language modeling, which avoids the need for G2P and the instability of GAN-based methods. Our approach achieves up to a 40% relative reduction in character error rate (CER) on LibriSpeech and generalizes effectively to Mandarin, a language that has remained particularly difficult for prior methods. Code will be released upon acceptance.\n\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "error",
                    "rate",
                    "data",
                    "character",
                    "unsupervised",
                    "from",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in self-supervised learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib7\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib22\" title=\"\">2021a</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib11\" title=\"\">2022</a>; Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib16\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib12\" title=\"\">2024</a>; Mohamed et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib32\" title=\"\">2022</a>)</cite> and spoken language modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arora et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib2\" title=\"\">2025</a>; Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib15\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib18\" title=\"\">2024</a>)</cite> have enabled voice assistants with increasingly human-like listening and speaking abilities. However, they are far from <em class=\"ltx_emph ltx_font_italic\">language-universal</em>: most systems support only a handful of languages in the world&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib12\" title=\"\">2024</a>; Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib17\" title=\"\">2021</a>)</cite>, due to a lack of large-scale paired speech and text training corpora.</p>\n\n",
                "matched_terms": [
                    "text",
                    "baevski",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A promising step toward language-universal assistants is to build speech recognizers from <em class=\"ltx_emph ltx_font_italic\">unpaired</em> speech and text, or <em class=\"ltx_emph ltx_font_italic\">unsupervised speech recognition</em> (UASR)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib20\" title=\"\">2012</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib28\" title=\"\">2023</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite>.\nUASR is a fundamental challenge: success would enable downstream tasks such as speech synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib33\" title=\"\">2022</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib27\" title=\"\">2022</a>)</cite>, translation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib45\" title=\"\">2023a</a>)</cite>, and understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib41\" title=\"\">2023</a>)</cite>, paving the way for general-purpose voice assistants. It is also a central case of <em class=\"ltx_emph ltx_font_italic\">unpaired multimodal learning</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Artetxe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib4\" title=\"\">2018b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib3\" title=\"\">a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib5\" title=\"\">2019</a>; Lample et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib25\" title=\"\">2018a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib26\" title=\"\">b</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib31\" title=\"\">2019</a>; Hoshen &amp; Wolf, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib21\" title=\"\">2018</a>)</cite>, where modalities need to be aligned without parallel data. In the absence of sentence-level alignment, the model must infer higher-level linguistic units&#8211;phones, syllables, and words&#8211;from raw speech waveforms in conjunction with global text statistics. Thus, UASR provides broader insights into representation learning and multimodal alignment without supervision.</p>\n\n",
                "matched_terms": [
                    "text",
                    "wang",
                    "model",
                    "data",
                    "baevski",
                    "representation",
                    "unsupervised",
                    "from",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The best existing UASR systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib28\" title=\"\">2023</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite> operate at the <em class=\"ltx_emph ltx_font_italic\">phoneme-level</em>. To this end, they need to convert raw text units, or <em class=\"ltx_emph ltx_font_italic\">graphemes</em>, to phonemes, or minimal sound units that encode meaning, using a grapheme-to-phoneme convertor (G2P). However, training G2Ps requires resources such as pronunciation dictionaries, which can be time-consuming and labor-intensive to create. Without a G2P, such systems suffer from significant performance degradation due to misalignment between speech and raw text in many languages&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib28\" title=\"\">2023</a>); Ni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib33\" title=\"\">2022</a>)</cite>. Even when pronunciation dictionaries are available, the system may fail to detect clear phone-level boundaries due to strong co-articulation effects for languages such as Mandarin.</p>\n\n",
                "matched_terms": [
                    "text",
                    "baevski",
                    "from",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An alternative approach to phone-based models is to build a word-level UASR system, which can be achieved without a G2P. Yet a major concern is the coverage of the system on <em class=\"ltx_emph ltx_font_italic\">rare words</em>, which can effectively be infinite in vocabulary size, making it significantly harder to acquire than phones. Furthermore, detecting word boundaries requires capturing long-range contextual dependencies in speech, which risk destabilizing segmentation mechanisms effective for phone-level UASR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "word",
                    "wang",
                    "uasr",
                    "2023c"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a third alternative &#8211; to build UASR at the <em class=\"ltx_emph ltx_font_italic\">syllable-level</em>, which can be justified on three grounds. First, unlike words, the number of distinct syllables for a language is finite, which reduces the long-tail token distribution issue and allows for better generalization to unseen words; second, many languages exhibit the best alignment between speech and text at the syllable level instead of the phone or word level.\nFor instance, Mandarin uses characters, which have strong correspondences to spoken syllables; therefore, we expect that a syllable-level UASR system could be more appropriate for UASR than a phoneme-based system, which we also found to be the case empirically.</p>\n\n",
                "matched_terms": [
                    "text",
                    "word",
                    "phone",
                    "token",
                    "syllable",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Last but not least, recent advancement in syllable boundary detection and unit discovery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> has made it possible to segment raw speech into syllable-like units without any textual supervision, and is often more reliable than unsupervised segmentation methods at the word-level, while capturing most of the word-level semantics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Peng &amp; Harwath (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib37\" title=\"\">2022</a>); Fuchs &amp; Hoshen (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib19\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "unsupervised",
                    "boundary",
                    "syllable",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces <span class=\"ltx_text ltx_font_bold\">SylCipher</span>, to our knowledge, the first syllable-based UASR system. SylCipher jointly predicts syllable boundaries and embedding tokens from raw speech using a unified self-supervised objective. The learning mechanism avoids adversarial training, making it more stable and less sensitive to hyperparameters.</p>\n\n",
                "matched_terms": [
                    "syllable",
                    "sylcipher",
                    "from",
                    "uasr",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct extensive experiments across domains and languages. On LibriSpeech, SylCipher achieves up to 40% relative character error rate (CER) reduction over prior G2P-free UASR methods. On SpokenCOCO, improvements are even larger, demonstrating robustness across domains. On Mandarin, SylCipher achieves 12.2% phone error rate (PER), outperforming GAN-based UASR methods that fail to even converge.</p>\n\n",
                "matched_terms": [
                    "phone",
                    "error",
                    "rate",
                    "character",
                    "sylcipher",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform careful ablation and error analysis, examining the effects of token vocabulary size, syllabifier choice, and segmentation mechanisms.</p>\n\n",
                "matched_terms": [
                    "token",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paper organizations.</span> Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S2\" title=\"2 Related work &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reviews related work. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S3\" title=\"3 Syllable-level unsupervised speech recognition &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> formalizes the syllable-level UASR problem. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4\" title=\"4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents SylCipher and its theoretical guarantees. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5\" title=\"5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports experiments and ablations. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S6\" title=\"6 Conclusion &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> concludes with limitations and future work.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on UASR assumed the existence of a reliable G2P and formulate the problem as an adversarial game, where a conditional generator predicts a phoneme sequence given a speech waveform, and a discriminator tries to tell real phonemized text apart from the generator&#8217;s output&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite>. Within this framework, several works have explored different segmentation mechanisms such as fixed unsupervised phoneme segmentation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib30\" title=\"\">2018</a>)</cite>, iterative forced alignment&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib10\" title=\"\">2019</a>)</cite>, de-duplication&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite> and reinforcement learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite> proposed an alternative formulation of UASR as an explicit distribution matching problem, by matching the lower-order <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math>-gram distributions of the generated and real texts. Later work further extended this reformulation to use G2P-free tokens such as words&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>; Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, more expressive architecture such as transformers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite> and more general text distribution such as masked prediction probabilities&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>. We adapt this word-level UASR approach to the syllable-level, and supplement it with a simplified training flow, a more stable differentiable boundary detector and additional learning objectives.</p>\n\n",
                "matched_terms": [
                    "text",
                    "wang",
                    "within",
                    "forced",
                    "baevski",
                    "unsupervised",
                    "from",
                    "uasr",
                    "boundary",
                    "tokens",
                    "2023c"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on syllable-level modeling of speech used signal processing techniques&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib49\" title=\"\">2009</a>)</cite>. To discover higher-level structure and more efficient self-supervised learning (SSL) representations from raw speech, <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite> proposed to induce syllabic structure from existing SSL models such as HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib22\" title=\"\">2021a</a>)</cite>. To this end,&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite> probed the self-attention layers of VG-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng &amp; Harwath, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib37\" title=\"\">2022</a>)</cite>, a visually grounded SSL model to detect syllable-like feature clusters and further refine such clusters using a min-cut algorithm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi &amp; Malik (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib40\" title=\"\">1997</a>)</cite>. To sidestep the need for visual data, <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite> employed a speech-only SSL model trained with utterance-level self-distillation from a HuBERT teacher. Due to the indirect manner of such approaches by which syllabic structures are derived, they are often noisy and unreliable. To cope with this issue, recent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> proposed a more direct and targeted approach by performing self-distillation at the syllable-level, which significantly improved syllable boundary detection and unit discovery performance by encouraging sharper contrast between within and between-syllable feature frames.</p>\n\n",
                "matched_terms": [
                    "within",
                    "from",
                    "data",
                    "syllable",
                    "used",
                    "model",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we formulate syllable-level UASR as follows.\nLet <math alttext=\"X=[X_{1},\\cdots,X_{T}]\\in{\\mathcal{X}}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>X</mi><mi>T</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">X=[X_{1},\\cdots,X_{T}]\\in{\\mathcal{X}}^{T}</annotation></semantics></math> be a padded sequence of speech feature vectors and let <math alttext=\"Y=[Y_{1},\\cdots,Y_{L}]\\in{\\mathcal{Y}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><msub><mi>Y</mi><mi>L</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><mi>L</mi></msup></mrow><annotation encoding=\"application/x-tex\">Y=[Y_{1},\\cdots,Y_{L}]\\in{\\mathcal{Y}}^{L}</annotation></semantics></math> a padded sequence of text tokens in the same language. Since a tokenized speech utterance typically uses more tokens than the text transcription of the same utterance, we assume the <math alttext=\"T\\geq L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>T</mi><mo>&#8805;</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">T\\geq L</annotation></semantics></math>. Assume <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> come from two <em class=\"ltx_emph ltx_font_italic\">unpaired</em> datasets and are therefore <em class=\"ltx_emph ltx_font_italic\">statistically independent</em>. Further, suppose they are <em class=\"ltx_emph ltx_font_italic\">matched</em>, i.e., there exists an ASR function <math alttext=\"y^{*}:{\\mathcal{X}}\\mapsto{\\mathcal{Y}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><mrow><msup><mi>y</mi><mo>&#8727;</mo></msup><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo stretchy=\"false\">&#8614;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">y^{*}:{\\mathcal{X}}\\mapsto{\\mathcal{Y}}</annotation></semantics></math> such that the distributions of <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m8\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math>, <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m9\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"p_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m10\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">p_{Y}</annotation></semantics></math> satisfy</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "uasr",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of UASR is to recover <math alttext=\"y^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m11\" intent=\":literal\"><semantics><msup><mi>y</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">y^{*}</annotation></semantics></math> given only unpaired <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m12\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m13\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math>. The syllable-level case is a special setting where the tokens of <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m14\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> are syllables. The learning problem resembles <em class=\"ltx_emph ltx_font_italic\">decipherment</em>, where one decodes a message in an unknown script without a lexicon or grammar. In practice, <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m15\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> is formed from frame-level SSL features&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib22\" title=\"\">2021a</a>)</cite>, and <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m16\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"p_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m17\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">p_{Y}</annotation></semantics></math> are only approximately matched due to finite-sample noise and domain mismatch. As shown in <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite>, UASR is ill-posed in general, but the mapping <math alttext=\"y^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m18\" intent=\":literal\"><semantics><msup><mi>y</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">y^{*}</annotation></semantics></math> in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S3.E1\" title=\"In 3 Syllable-level unsupervised speech recognition &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> becomes identifiable if syllable boundaries are known and the language satisfies mild conditions.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "syllable",
                    "from",
                    "uasr",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we describe <span class=\"ltx_text ltx_font_bold\">SylCipher</span>, our proposed model for syllable-level UASR. We first present its architecture and training objective, then justify the design theoretically, and finally introduce several practical modifications for training and inference.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "uasr",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.F1\" title=\"Figure 1 &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, SylCipher is an encoder-only language model with a <em class=\"ltx_emph ltx_font_italic\">shared encoder</em> for speech and text modalities. To project both modalities into a joint embedding space, we use two uni-modal <em class=\"ltx_emph ltx_font_italic\">pre-nets</em>: <math alttext=\"e_{\\tilde{X}}:{\\mathcal{X}}^{T}\\mapsto{\\mathbb{R}}^{L\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>T</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">e_{\\tilde{X}}:{\\mathcal{X}}^{T}\\mapsto{\\mathbb{R}}^{L\\times d}</annotation></semantics></math> for speech and <math alttext=\"e_{Y}:{\\mathcal{Y}}^{L}\\mapsto{\\mathbb{R}}^{L\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mi>Y</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><mi>L</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">e_{Y}:{\\mathcal{Y}}^{L}\\mapsto{\\mathbb{R}}^{L\\times d}</annotation></semantics></math> for text, each implemented as a linear embedding layer. Before the speech pre-net, a <em class=\"ltx_emph ltx_font_italic\">speech syllabifier</em> converts the frame-level feature vectors into a syllable-level sequence. It consists of: (i) a <em class=\"ltx_emph ltx_font_italic\">differentiable soft-pooler</em> <math alttext=\"m:{\\mathcal{X}}^{T}\\mapsto{\\mathcal{X}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>T</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>L</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">m:{\\mathcal{X}}^{T}\\mapsto{\\mathcal{X}}^{L}</annotation></semantics></math> that aligns speech with text on the syllable level (ii) a tokenizer <math alttext=\"c:{\\mathcal{X}}^{L}\\mapsto\\tilde{{\\mathcal{X}}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>L</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>~</mo></mover><mi>L</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">c:{\\mathcal{X}}^{L}\\mapsto\\tilde{{\\mathcal{X}}}^{L}</annotation></semantics></math> to discretizes speech into syllable-like units. Thus the speech pre-net is</p>\n\n",
                "matched_terms": [
                    "text",
                    "syllable",
                    "sylcipher",
                    "model",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\\circ g(x):=f(g(x))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>f</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8728;</mo><mi>g</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f\\circ g(x):=f(g(x))</annotation></semantics></math> for any functions <math alttext=\"f,g\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo>,</mo><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">f,g</annotation></semantics></math>, and <math alttext=\"\\tilde{X}:=c\\circ m(X)\\in\\tilde{{\\mathcal{X}}}^{L}.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mrow><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>:=</mo><mrow><mrow><mi>c</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8728;</mo><mi>m</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>~</mo></mover><mi>L</mi></msup></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\tilde{X}:=c\\circ m(X)\\in\\tilde{{\\mathcal{X}}}^{L}.</annotation></semantics></math>\nThe soft-pooler first estimates the boundary probabilities <math alttext=\"b(X)\\in[0,1]^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">b(X)\\in[0,1]^{T}</annotation></semantics></math> by learning from an unsupervised syllable detector Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>, then constructs a pooling mask <math alttext=\"a(X)\\in[0,1]^{L\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">a(X)\\in[0,1]^{L\\times T}</annotation></semantics></math>, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.F2\" title=\"Figure 2 &#8227; 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>:</p>\n\n",
                "matched_terms": [
                    "sylber",
                    "syllable",
                    "unsupervised",
                    "from",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\sigma_{\\epsilon}(x):=\\epsilon-|\\mathrm{clamp}(x,-\\epsilon,\\epsilon)|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#963;</mi><mi>&#1013;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mi>&#1013;</mi><mo>&#8722;</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mi>clamp</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mrow><mo>&#8722;</mo><mi>&#1013;</mi></mrow><mo>,</mo><mi>&#1013;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\sigma_{\\epsilon}(x):=\\epsilon-|\\mathrm{clamp}(x,-\\epsilon,\\epsilon)|</annotation></semantics></math> and <math alttext=\"\\mathrm{clamp}(x,a,b)=x\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>clamp</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">\\mathrm{clamp}(x,a,b)=x</annotation></semantics></math> for <math alttext=\"x\\in[a,b]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x\\in[a,b]</annotation></semantics></math>, <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> if <math alttext=\"x&lt;a\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>&lt;</mo><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">x&lt;a</annotation></semantics></math> and <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> otherwise. The hyperparameter <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> controls the <em class=\"ltx_emph ltx_font_italic\">tapering speed</em> of pooling weights outside each syllabic segment.\nThis creates sparse pooling weights that avoid overflow/underflow issues common in earlier soft-pooling methods with <math alttext=\"\\sigma_{\\epsilon}(x)=\\tanh(x/\\epsilon)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#963;</mi><mi>&#1013;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>tanh</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>/</mo><mi>&#1013;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\sigma_{\\epsilon}(x)=\\tanh(x/\\epsilon)</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib9\" title=\"\">2022</a>)</cite> and <math alttext=\"\\sigma_{\\epsilon}(x)=\\exp(x/\\epsilon)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#963;</mi><mi>&#1013;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>/</mo><mi>&#1013;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\sigma_{\\epsilon}(x)=\\exp(x/\\epsilon)</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib48\" title=\"\">2024</a>)</cite>.\nTo contextualize embeddings, both modalities are processed by a shared encoder <math alttext=\"f:{\\mathbb{R}}^{T\\times d}\\mapsto{\\mathbb{R}}^{T\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">f:{\\mathbb{R}}^{T\\times d}\\mapsto{\\mathbb{R}}^{T\\times d}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "each",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without paired speech-text data, we approximate each unimodal distribution <math alttext=\"p_{\\tilde{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover></msub><annotation encoding=\"application/x-tex\">p_{\\tilde{X}}</annotation></semantics></math> and <math alttext=\"p_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">p_{Y}</annotation></semantics></math>. Let <math alttext=\"g_{\\tilde{X}},\\,g_{Y}:{\\mathbb{R}}^{d\\times L}\\mapsto[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>g</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover></msub><mo>,</mo><msub><mi>g</mi><mi>Y</mi></msub></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mrow><mi>d</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup><mo stretchy=\"false\">&#8614;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">g_{\\tilde{X}},\\,g_{Y}:{\\mathbb{R}}^{d\\times L}\\mapsto[0,1]</annotation></semantics></math> be speech and text <em class=\"ltx_emph ltx_font_italic\">post-nets</em> such that</p>\n\n",
                "matched_terms": [
                    "text",
                    "data",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To prevent speech and text from occupying disjoint embedding regions, we further constrain the shared encoder <em class=\"ltx_emph ltx_font_italic\">entropy</em>. Let <math alttext=\"H(X)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>H</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">H(X)</annotation></semantics></math> denotes the entropy of discrete random variable <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>, consider</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "denotes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We prove that under regularity conditions, equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.E7\" title=\"In Distribution matching. &#8227; 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> matches true and generated text distributions in the same way as GANs, but without unstable straight-through gradients. Thus zero-error UASR is achievable under conditions in <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "wang",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\theta_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{X}</annotation></semantics></math> and <math alttext=\"\\theta_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{Y}</annotation></semantics></math> are speech-related and text-related parameters. Notice that <math alttext=\"g_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">g_{Y}</annotation></semantics></math> serves a dual role: it acts both as the text post-net for unimodal MLM and as a <em class=\"ltx_emph ltx_font_italic\">cross-modal decoder</em>, defined as (similar for <math alttext=\"q_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">q_{X}</annotation></semantics></math>) <math alttext=\"q_{Y}(Y^{M}|Z^{M}):=\\prod_{i\\in M}g_{Y,i}(Y_{i}|f_{Z}(Z_{i})).\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi>q</mi><mi>Y</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msup><mi>Y</mi><mi>M</mi></msup><mo fence=\"false\">|</mo><msup><mi>Z</mi><mi>M</mi></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.111em\">:=</mo><mrow><msub><mo>&#8719;</mo><mrow><mi>i</mi><mo>&#8712;</mo><mi>M</mi></mrow></msub><mrow><msub><mi>g</mi><mrow><mi>Y</mi><mo>,</mo><mi>i</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo fence=\"false\">|</mo><mrow><msub><mi>f</mi><mi>Z</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">q_{Y}(Y^{M}|Z^{M}):=\\prod_{i\\in M}g_{Y,i}(Y_{i}|f_{Z}(Z_{i})).</annotation></semantics></math> The additional term weighted by <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m7\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> balances these two functions, enabling <math alttext=\"g_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m8\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">g_{Y}</annotation></semantics></math> (or <math alttext=\"g_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m9\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">g_{X}</annotation></semantics></math>) to reconstruct text (or speech) tokens from either masked text or speech inputs. To further constrain the entropy of the encoding outputs <math alttext=\"f_{Z}(Z)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m10\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>Z</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{Z}(Z)</annotation></semantics></math>, we limit the transformer depth to two layers and apply <em class=\"ltx_emph ltx_font_italic\">random mix-up</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib1\" title=\"\">2022</a>)</cite>. During training, a random subset of encoder representations is quantized to a finite set of code vectors before passed to the post-nets. This restricts the variability of <math alttext=\"f_{Z}(Z)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>Z</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>Z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f_{Z}(Z)</annotation></semantics></math> without significantly reducing its predictive capacity for masked tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "set",
                    "during",
                    "from",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After pretraining with unsupervised syllable boundary labels <math alttext=\"[\\hat{b}_{1}(X),\\cdots,\\hat{b}_{T}(X)]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><msub><mover accent=\"true\"><mi>b</mi><mo>^</mo></mover><mn>1</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mi mathvariant=\"normal\">&#8943;</mi><mo>,</mo><mrow><msub><mover accent=\"true\"><mi>b</mi><mo>^</mo></mover><mi>T</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\hat{b}_{1}(X),\\cdots,\\hat{b}_{T}(X)]</annotation></semantics></math>, we perform <em class=\"ltx_emph ltx_font_italic\">joint end-to-end</em> (JE2E) training. In this stage, the soft-pooler is trained jointly with the rest of the model, guided by an additional soft constraint on the predicted <em class=\"ltx_emph ltx_font_italic\">syllable counts</em>:</p>\n\n",
                "matched_terms": [
                    "stage",
                    "syllable",
                    "unsupervised",
                    "model",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the MLM objective encourages <em class=\"ltx_emph ltx_font_italic\">implicit</em> distribution matching, we found that <em class=\"ltx_emph ltx_font_italic\">explicit</em> distribution matching further improves a saturated MLM system. To this end, we adopt positional unigram and skipgram matching (PUSM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib48\" title=\"\">2024</a>; Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "wang",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"q_{Y}(x):=q_{Y}(\\cdot|c\\circ m(x))\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>q</mi><mi>Y</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo>:=</mo><msub><mi>q</mi><mi>Y</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mi>c</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8728;</mo><mi>m</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">q_{Y}(x):=q_{Y}(\\cdot|c\\circ m(x))</annotation></semantics></math> and <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p3.m2\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> is the maximal skip length. The first term matches unigram distributions at each position, while the second aligns skipgram distributions. To stabilize training, we disable random mix-up and approximate the empirical probability distributions using the entire dataset as a single batch, reducing memory cost via gradient accumulation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>. The overall SylCipher objective combines all stages:</p>\n\n",
                "matched_terms": [
                    "each",
                    "while",
                    "sylcipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because the PUSM stage requires different batch sizes, we adopt an <em class=\"ltx_emph ltx_font_italic\">iterative training schedule</em> rather than fully end-to-end optimization. Specifically:</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fixed boundary stage</span>: freeze the boundary detector <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> and set <math alttext=\"\\lambda_{\\mathrm{JE2E}}=\\lambda_{\\mathrm{PUSM}}=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mi>JE2E</mi></msub><mo>=</mo><msub><mi>&#955;</mi><mi>PUSM</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\mathrm{JE2E}}=\\lambda_{\\mathrm{PUSM}}=0</annotation></semantics></math>;</p>\n\n",
                "matched_terms": [
                    "stage",
                    "set",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PUSM stage</span>: disable <math alttext=\"\\lambda_{\\mathrm{wMLM}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mi>wMLM</mi></msub><annotation encoding=\"application/x-tex\">\\lambda_{\\mathrm{wMLM}}</annotation></semantics></math>, enable <math alttext=\"\\lambda_{\\mathrm{PUSM}}&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mi>PUSM</mi></msub><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\mathrm{PUSM}}&gt;0</annotation></semantics></math> for explicit distribution matching.</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During inference, the ASR system cascades the speech syllabifier, speech pre-net and the text post-net as done in the PUSM stage:</p>\n\n",
                "matched_terms": [
                    "text",
                    "stage",
                    "pusm",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that using the <em class=\"ltx_emph ltx_font_italic\">first</em> transformer layer of the shared encoder, instead of the last, improves performance, consistent with findings at the word-level&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>. This may be due to over-contextualization in later layers. Finally, replacing each <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> token with the second most likely prediction reduces CER by about <math alttext=\"1\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">1\\%</annotation></semantics></math> compared to simply discarding <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span>s.</p>\n\n",
                "matched_terms": [
                    "each",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS1\" title=\"5.1 Datasets &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> introduces the datasets used for our experiments, followed by the syllabification steps for speech and text preprocessing in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS2\" title=\"5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS3\" title=\"5.3 Results: UASR &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> presents the main UASR results, and Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS4\" title=\"5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a> discusses SylCipher&#8217;s boundary refinement ability. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS5\" title=\"5.5 Ablation studies &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a> provides ablation studies on key design choices. Additional implementation details of our method and the baselines are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A4\" title=\"Appendix D Implementation details of SylCipher &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "results",
                    "used",
                    "uasr",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate SylCipher on three datasets. First, we train on the 460-hour clean subset of LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib35\" title=\"\">2015a</a>)</cite>, a standard UASR benchmark of audiobook recordings.\nSecond, to test domain generalization, we train another model on SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib23\" title=\"\">2021b</a>)</cite>, which contains 742 hours of spoken image captions. Third, to study languages with syllabic structures significantly different from English, we apply SylCipher to Mandarin using AISHELL-3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib42\" title=\"\">2021</a>)</cite>, which has 85 hours of read speech. We follow the same LibriSpeech split as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, and use the standard splits for SpokenCOCO and AISHELL-3. For LibriSpeech and SpokenCOCO, we consider both the <em class=\"ltx_emph ltx_font_italic\">matched</em> setting, where empirical speech and text probability distributions can be matched exactly, and the more realistic <em class=\"ltx_emph ltx_font_italic\">unmatched</em> setting where they cannot.\nIn the matched case, we use paired speech-text datasets with pairings removed; in the unmatched case, LibriSpeech uses LibriLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib36\" title=\"\">2015b</a>)</cite> with overlapping text removed, while SpokenCOCO is randomly split in half, with one half treated as speech-only and the other half treated as text-only. For AISHELL-3, we consider only the matched setting and compare models trained with/without tone labels. For all datasets, we apply a voice activity detector<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/wiseman/py-webrtcvad.git</span></span></span> to improve alignment.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "aishell3",
                    "sylcipher",
                    "test",
                    "from",
                    "uasr",
                    "tone",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English text, we use Pyphen<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/Kozea/Pyphen</span></span></span>, a rule-based hyphenation tool re-purposed for syllabification without a G2P. If Pyphen produces only a single chunk for a long word, we apply a simple rule-based fallback (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A5\" title=\"Appendix E Pseudo-code for the Pyphen+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>). We denote this combined approach as Pyphen+. We also experiment with other G2P-free approaches such as byte-pair encoding (BPE)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>); Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>, and find SylCipher robust to syllabification errors. To avoid long-tail distributions, we keep only the top-2048 most frequent English syllables and replace the rest with a special <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> token (replacing <math alttext=\"7\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">7\\%</annotation></semantics></math> of tokens in LibriSpeech and <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math> in SpokenCOCO. For Mandarin, we use the Pinyin of each Chinese character as a syllable, with or without tone labels. We keep the top-1024 most frequent syllables, covering 99.5% of occurrences. For speech syllabification, we use K-means clustering on syllable-level speech features created by mean pooling within the Sylber boundaries, with codebook size equal to the number of non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> text tokens.</p>\n\n",
                "matched_terms": [
                    "text",
                    "word",
                    "within",
                    "token",
                    "pinyin",
                    "sylcipher",
                    "sylber",
                    "character",
                    "syllable",
                    "chinese",
                    "each",
                    "tone",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare SylCipher with UASR systems that differ in token type, training objective, and architecture.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "uasr",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Word-level:</span> <em class=\"ltx_emph ltx_font_italic\">JSTTI</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, the state-of-art word-level UASR system, which is GAN-free and architecturally similar to ours, using 2048 non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> words.</p>\n\n",
                "matched_terms": [
                    "uasr",
                    "ours",
                    "jstti"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character-level:</span> <em class=\"ltx_emph ltx_font_italic\">wav2vec-U</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite>, a strong GAN-based system; <em class=\"ltx_emph ltx_font_italic\">REBORN</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib43\" title=\"\">2024</a>)</cite>, a state-of-the-art phoneme-based GAN system; and a <em class=\"ltx_emph ltx_font_italic\">phone-level JSTTI</em>, trained with phoneme boundaries and 128 speech clusters (comparable to the number of character types).</p>\n\n",
                "matched_terms": [
                    "wav2vecu",
                    "baevski",
                    "character",
                    "jstti"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level:</span> <em class=\"ltx_emph ltx_font_italic\">PUSM</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>, adapted from word-level UASR to syllables using the same syllable boundary detector and syllabifier as SylCipher.</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "wang",
                    "syllable",
                    "sylcipher",
                    "from",
                    "uasr",
                    "boundary",
                    "2023c"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also report results with <em class=\"ltx_emph ltx_font_italic\">self-training</em>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib10\" title=\"\">2019</a>); Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib8\" title=\"\">2021</a>)</cite>, where a wav2vec 2.0&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib7\" title=\"\">2020</a>)</cite> student is further finetuned by distilling character (grapheme)-level pseudo-labels from a UASR system.\nPerformance is measured by character error rate (CER) for English and phone error rate (PER) for Mandarin, as both are tokenization-independent and easily comparable.</p>\n\n",
                "matched_terms": [
                    "phone",
                    "wav2vec",
                    "student",
                    "error",
                    "rate",
                    "pseudolabels",
                    "results",
                    "baevski",
                    "character",
                    "selftraining",
                    "from",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level modeling performs best under G2P-free setting.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T1.st1\" title=\"In Table 1 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a> summarizes results on LibriSpeech. Among baselines, PUSM performs best in the matched setting, while wav2vec-U is strongest in the unmatched setting, though its performance is limited by the lack of G2P. Unlike phonemized training, REBORN trained directly on raw characters performs worse than wav2vec-U, suggesting sensitivity to misalignment between speech and text. By contrast, SylCipher with all three stages (Sylber+JE2E+PUSM) achieves 21.8% CER (matched) and 35.9% (unmatched), outperforming all baselines. Compared to the best-in-average system wav2vec-U (35.6%/43.3%), SylCipher reduces by CER by 40% (matched) and 17% (unmatched) relative.\nBoth syllable-level models (PUSM and SylCipher) outperform word- or character-level models, confirming that speech-text alignment is the best at the syllable level. Word-level JSTTI performs worst due to poor rare-word coverage (<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span>s <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p6.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 17%), and adapting JSTTI to phone-level degrades further, likely because phone clusters are noisier than syllable or word clusters.</p>\n\n",
                "matched_terms": [
                    "text",
                    "pusm",
                    "wav2vecu",
                    "phone",
                    "word",
                    "jstti",
                    "results",
                    "syllable",
                    "sylberje2epusm",
                    "sylcipher",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative training helps.</span> Stage-wise training shows progressive improvements: JE2E reduces CER modestly, while PUSM yields the largest gains (44% and 23% relative reductions over JE2E in matched/unmatched settings). Combining MLM-based stages with PUSM outperforms PUSM-only training by 33-34% relative, as MLM provides necessary initialization for PUSM to converge. Indeed, PUSM alone fails when SylCipher is randomly initialized, likely due to transformer training instability. Lastly, using unsupervised Sylber boundaries performs nearly as well as forced alignment, suggesting robustness to segmentation noise.\nSelf-training consistently improves performance, especially for stronger models. For SylCipher, CER is further reduced by 20% relative.</p>\n\n",
                "matched_terms": [
                    "pusm",
                    "forced",
                    "sylber",
                    "unsupervised",
                    "selftraining",
                    "sylcipher",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllables are robust to domain shifts.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T1.st2\" title=\"In Table 1 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a> reports results on SpokenCOCO. SylCipher again outperforms all baselines, surpassing PUSM by 32% and wav2vec-U by 49% relative CER after all stages. The margin is larger than LibriSpeech, especially in the unmatched setting. Notably, even after the first training stages, SylCipher already outperforms wav2vec-U by 22%. While wav2vec-U suffers from domain mismatch at the character level, both syllable-level methods perform better, suggesting syllable units are more robust to domain shifts. Self-training further improves SylCipher by 39% relative CER, likely due to higher syllable coverage reducing insertion/deletion errors. Moreover, performance degrades only slightly in the unmatched setting, indicating the sharper degradation on LibriSpeech stems from domain mismatch between its speech and text corpora.</p>\n\n",
                "matched_terms": [
                    "text",
                    "pusm",
                    "wav2vecu",
                    "results",
                    "character",
                    "syllable",
                    "selftraining",
                    "sylcipher",
                    "from",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the JE2E stage, we compare the syllable boundary detection performance against the teacher model Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> (which provides SylCipher&#8217;s initial boundaries) and other unsupervised approaches including Feat-Sim&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite>, SDHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite>, SylBoost&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>)</cite>. On LibriSpeech, Sylber is the strongest baseline on most metrics, except for the 20ms F1 score where SylBoost performs best. On SpokenCOCO, SylBoost outperforms Sylber on three of five metrics. However, our preliminary analysis show that SylBoost often over-segments, producing too many syllables and causing cross-modal misalignment and training instability in SylCipher. In contrast, Sylber predicts syllable counts closer to ground truth, making it more suitable as an initialization for UASR. When refined through JE2E, SylCipher improves upon its Sylber teacher, achieving +14% relative F1 (20ms tolerance) and +3% relative F1 (50ms) on LibriSpeech. On SpokenCOCO, it surpasses Sylber by +15% F1 and +11% R-value, and outperforms the best baseline Feat-Sim by +3% relative F1 score and +4.6% R-value. These results suggest that unpaired text provides additional guidance on syllable boundary detection. Visualizations of the speech-text alignment predicted by SylCipher can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A7\" title=\"Appendix G Spectrogram examples on LibriSpeech &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "stage",
                    "model",
                    "sylber",
                    "results",
                    "unsupervised",
                    "syllable",
                    "sylcipher",
                    "uasr",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SylCipher is robust to syllabifiers.</span> We first test alternative syllabifiers beyond Pyphen, including the phoneme-based Syllabify<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/kylebgorman/syllabify</span></span></span> and a character-based approach using byte-pair encoding (BPE) tokenization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>. The latter is attractive because it is language-agnostic and integrated easily with spoken language models. For the BPE-based syllabifier, we train the first stage of SuperBPE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>)</cite> on LibriSpeech with a 17k vocabulary, roughly matching the number of syllable types. We then split BPE tokens containing multiple non-consecutive vowels and merge consecutive tokens to ensure each unit contains at least one vowel (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A6\" title=\"Appendix F Pseudo-code of BPE+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>). We refer to this method as BPE+. All syllabifier variants are trained only under the <em class=\"ltx_emph ltx_font_italic\">fixed-boundary stage</em>, since later stages show correlated trends. Instead of CER, we report syllable error rate (SER) to more directly reflects syllable-level performance. Among the methods, Pyphen+ achieves the lowest SER, outperforming even the phoneme-based Syllabify. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, BPE+ yields weaker but still competitive results, demonstrating that SylCipher can generalize to linguistically simpler, resource-free syllabifiers.</p>\n\n",
                "matched_terms": [
                    "error",
                    "stage",
                    "rate",
                    "sylcipher",
                    "results",
                    "syllable",
                    "each",
                    "tokens",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Clamp-based soft-pooler trains more stably.</span> In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, we test our modified soft-pooler in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.E3\" title=\"In 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, which uses a clamp-based <math alttext=\"\\sigma_{\\epsilon}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#963;</mi><mi>&#1013;</mi></msub><annotation encoding=\"application/x-tex\">\\sigma_{\\epsilon}</annotation></semantics></math> function (&#8220;Clamp&#8221;), against alternatives based on <math alttext=\"\\tanh\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p3.m2\" intent=\":literal\"><semantics><mi>tanh</mi><annotation encoding=\"application/x-tex\">\\tanh</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib9\" title=\"\">2022</a>)</cite> (&#8220;Tanh&#8221;) and softmax&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib48\" title=\"\">2024</a>)</cite>) (&#8220;Softmax&#8221;). Following prior work, we set the tapering parameter <math alttext=\"\\epsilon=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p3.m3\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.5</annotation></semantics></math> for &#8220;Clamp&#8221; and <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p3.m4\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math> for the other two approaches. On both SpokenCOCO and AISHELL-3, Clamp consistently outperforms the sigmoid-based poolers by 6-36%. This suggests that our design provides more stable and efficient training dynamics.\n</p>\n\n",
                "matched_terms": [
                    "wang",
                    "aishell3",
                    "test",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SylCipher works with different vocabulary sizes.</span> We also experiment with varying the non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> vocabulary size in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.F3.sf3\" title=\"In Figure 3 &#8227; 5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>. SylCipher displays consistent SERs across a wide range of vocabulary sizes, reaching the lowest overall SER with 2048 tokens and the lowest non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> SER with 1024 tokens.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced SylCipher, a UASR system that avoids phoneme-level resources such as G2P by recognizing speech at the <em class=\"ltx_emph ltx_font_italic\">syllable level</em>. Under the G2P-free setting, SylCipher outperforms the best existing systems by 17-40% relative CER on LibriSpeech and shows generalizability across other domains on SpokenCOCO, narrowing the gap with G2P-based systems. It also demonstrates cross-lingual robustness: on Mandarin, a tonal language where phoneme-based methods fail to converge, SylCipher achieves a PER of 13%. These results suggest that syllable-level modeling is a viable alternative to phoneme-level UASR and can push the horizon of more accessible and inclusive spoken language technology.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uasr",
                    "syllable",
                    "sylcipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span> SylCipher is not yet language-universal, since different languages use different writing systems and require linguistic knowledge to properly syllabify. For example, languages such as Hebrew and Arabic omits vowels in their writing system, which could pose challenges to existing syllabifiers.\nWhile our experiments show that the method can be adapted to more resource-efficient tokenizers such as BPE with minimal modifications, coming up with a language-universal tokenization method remains an open problem.\nFurther, the iterative training procedure can be further simplified into an end-to-end approach. Lastly, improving the robustness of SylCipher under domain mismatch between speech and text remains an open challenge.\n\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "while",
                    "sylcipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SylCipher is not yet language-universal, since different languages use different writing systems and require linguistic knowledge to properly syllabify. For example, languages such as Hebrew and Arabic omits vowels in their writing system, which could pose challenges to existing syllabifiers.\nWhile our experiments show that the method can be adapted to more resource-efficient tokenizers such as BPE with minimal modifications, coming up with a language-universal tokenization method remains an open problem.\nFurther, the iterative training procedure can be further simplified into an end-to-end approach. Lastly, improving the robustness of SylCipher under domain mismatch between speech and text remains an open challenge.\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "while",
                    "sylcipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">The true syllable boundaries are used, i.e., </span>\n  <math alttext=\"b_{t}(X)=\\mathbbm{1}[y^{*}(X_{t})\\neq y^{*}(X_{t+1})]\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi>b</mi>\n            <mi>t</mi>\n          </msub>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo stretchy=\"false\">(</mo>\n            <mi>X</mi>\n            <mo stretchy=\"false\">)</mo>\n          </mrow>\n        </mrow>\n        <mo>=</mo>\n        <mrow>\n          <mn>&#120793;</mn>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo stretchy=\"false\">[</mo>\n            <mrow>\n              <mrow>\n                <msup>\n                  <mi>y</mi>\n                  <mo>&#8727;</mo>\n                </msup>\n                <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                <mrow>\n                  <mo stretchy=\"false\">(</mo>\n                  <msub>\n                    <mi>X</mi>\n                    <mi>t</mi>\n                  </msub>\n                  <mo stretchy=\"false\">)</mo>\n                </mrow>\n              </mrow>\n              <mo>&#8800;</mo>\n              <mrow>\n                <msup>\n                  <mi>y</mi>\n                  <mo>&#8727;</mo>\n                </msup>\n                <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                <mrow>\n                  <mo stretchy=\"false\">(</mo>\n                  <msub>\n                    <mi>X</mi>\n                    <mrow>\n                      <mi>t</mi>\n                      <mo>+</mo>\n                      <mn>1</mn>\n                    </mrow>\n                  </msub>\n                  <mo stretchy=\"false\">)</mo>\n                </mrow>\n              </mrow>\n            </mrow>\n            <mo stretchy=\"false\">]</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">b_{t}(X)=\\mathbbm{1}[y^{*}(X_{t})\\neq y^{*}(X_{t+1})]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text ltx_font_italic\">;</span>\n</p>\n\n",
                "matched_terms": [
                    "used",
                    "syllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The proof consists of two main parts: (i) First, we prove that at least one minimizer <math alttext=\"(f_{\\tilde{X},1},g_{\\tilde{X},1},f_{Y,1},g_{Y,1})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mrow><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>g</mi><mrow><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>f</mi><mrow><mi>Y</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>g</mi><mrow><mi>Y</mi><mo>,</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(f_{\\tilde{X},1},g_{\\tilde{X},1},f_{Y,1},g_{Y,1})</annotation></semantics></math> can achieve a minimum of 0 for equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.E7\" title=\"In Distribution matching. &#8227; 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>; (ii) then we establish that for any minimizers <math alttext=\"(f_{\\tilde{X}}^{*},g_{\\tilde{X}}^{*},f_{Y}^{*},g_{Y}^{*})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>f</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>&#8727;</mo></msubsup><mo>,</mo><msubsup><mi>g</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>&#8727;</mo></msubsup><mo>,</mo><msubsup><mi>f</mi><mi>Y</mi><mo>&#8727;</mo></msubsup><mo>,</mo><msubsup><mi>g</mi><mi>Y</mi><mo>&#8727;</mo></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(f_{\\tilde{X}}^{*},g_{\\tilde{X}}^{*},f_{Y}^{*},g_{Y}^{*})</annotation></semantics></math>, the marginal distribution of the text posterior <math alttext=\"q_{Y|X}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m3\" intent=\":literal\"><semantics><msubsup><mi>q</mi><mrow><mi>Y</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">q_{Y|X}^{*}</annotation></semantics></math> matches the true text distribution <math alttext=\"p_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m4\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">p_{Y}</annotation></semantics></math>. As a result, by Assumption 4, Theorem 1 of <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib46\" title=\"\">2023b</a>)</cite> guarantees that <math alttext=\"\\operatorname*{arg\\,max}_{y\\in{\\mathcal{Y}}^{L}}q_{Y|X}^{*}(y|X)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p4.m5\" intent=\":literal\"><semantics><mrow><mrow><msub><mrow><mi>arg</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>max</mi></mrow><mrow><mi>y</mi><mo>&#8712;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><mi>L</mi></msup></mrow></msub><mo lspace=\"0.167em\">&#8289;</mo><msubsup><mi>q</mi><mrow><mi>Y</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo>&#8727;</mo></msubsup></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo fence=\"false\">|</mo><mi>X</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\operatorname*{arg\\,max}_{y\\in{\\mathcal{Y}}^{L}}q_{Y|X}^{*}(y|X)</annotation></semantics></math> achieves zero-error UASR.</p>\n\n",
                "matched_terms": [
                    "text",
                    "wang",
                    "uasr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To prove (i), by the definition of <math alttext=\"y^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p5.m1\" intent=\":literal\"><semantics><msup><mi>y</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">y^{*}</annotation></semantics></math> and Assumption 1, <math alttext=\"y^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p5.m2\" intent=\":literal\"><semantics><msup><mi>y</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">y^{*}</annotation></semantics></math> is a invertible mapping between <math alttext=\"{\\mathcal{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p5.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{X}}</annotation></semantics></math> and <math alttext=\"{\\mathcal{Y}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p5.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><annotation encoding=\"application/x-tex\">{\\mathcal{Y}}</annotation></semantics></math>. Then let <math alttext=\"t_{i}=\\min\\{\\tau:\\sum_{t=1}^{\\tau}b_{t}(X)\\geq i-1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p5.m5\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>min</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mi>&#964;</mi><mo lspace=\"0.278em\" rspace=\"0.111em\">:</mo><mrow><mrow><msubsup><mo>&#8721;</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>&#964;</mi></msubsup><mrow><msub><mi>b</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo>&#8805;</mo><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">t_{i}=\\min\\{\\tau:\\sum_{t=1}^{\\tau}b_{t}(X)\\geq i-1\\}</annotation></semantics></math> be the <em class=\"ltx_emph ltx_font_italic\">starting time</em> of each syllable and apply Assumption 2, we have</p>\n\n",
                "matched_terms": [
                    "each",
                    "syllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English experiments, SylCipher uses a HuBERT-large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt</span></span></span> model pretrained on LibriLight&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib24\" title=\"\">2020</a>)</cite> as the SSL encoder in the speech syllabifier, and initialize the soft-pooler with unsupervised boundary labels from Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>. For Mandarin, we instead use XEUS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://huggingface.co/espnet/xeus/blob/main/model/xeus_checkpoint_new.pth</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib12\" title=\"\">2024</a>)</cite>, which is pretrained on Mandarin (among other languages) and significantly outperforms HuBERT. We also finetune Sylber on AISHELL-3 to ensure stable convergence. For the pre-nets, shared encoder, post-nets, MLM parameters, and most of the optimizer hyperparameters, we follow JSTTI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, as modifying them did not yield consistent improvements.</p>\n\n",
                "matched_terms": [
                    "jstti",
                    "sylber",
                    "aishell3",
                    "unsupervised",
                    "sylcipher",
                    "from",
                    "model",
                    "boundary"
                ]
            }
        ]
    },
    "S5.T3.st1": {
        "source_file": "Towards Unsupervised Speech Recognition at the Syllable-Level",
        "caption": "(a) Boundary detection results on LibriSpeech",
        "body": "F150 (↑\\uparrow)\nF120 (↑\\uparrow)\nP. (↑\\uparrow)\nRe. (↑\\uparrow)\nR (↑\\uparrow)\n\n\n\n\nFeat-Sim (Peng et al., 2023)\n\n47.3\n24.7\n46.6\n48.0\n54.4\n\n\nSDHuBERT (Cho et al., 2024)\n\n66.1\n32.2\n64.9\n67.4\n70.7\n\n\nSylBoost (Baade et al., 2025)\n\n73.2\n44.6\n72.1\n74.4\n76.9\n\n\nSylber (Cho et al., 2025)\n\n83.4\n44.1\n84.8\n84.1\n86.4\n\n\nSylCipher (Ours, Sylber+JE2E)\n86.1\n50.8\n86.6\n86.1\n88.1",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">F1<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">50</span></sup> (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.st1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">F1<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">20</span></sup> (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.st1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">P. (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.st1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Re. (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.st1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">R (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.st1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\">Feat-Sim&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">47.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">24.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">46.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">48.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">54.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SDHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">66.1</td>\n<td class=\"ltx_td ltx_align_center\">32.2</td>\n<td class=\"ltx_td ltx_align_center\">64.9</td>\n<td class=\"ltx_td ltx_align_center\">67.4</td>\n<td class=\"ltx_td ltx_align_center\">70.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SylBoost&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">73.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">44.6</span></td>\n<td class=\"ltx_td ltx_align_center\">72.1</td>\n<td class=\"ltx_td ltx_align_center\">74.4</td>\n<td class=\"ltx_td ltx_align_center\">76.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">83.4</span></td>\n<td class=\"ltx_td ltx_align_center\">44.1</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">84.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">84.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">86.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">SylCipher (Ours, Sylber+JE2E)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">86.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">50.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">86.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">86.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">88.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ours",
            "librispeech",
            "detection",
            "cho",
            "sdhubert",
            "sylboost",
            "sylber",
            "sylcipher",
            "f150",
            "results",
            "peng",
            "baade",
            "sylberje2e",
            "↑uparrow",
            "f120",
            "featsim",
            "boundary"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Last but not least, recent advancement in syllable boundary detection and unit discovery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> has made it possible to segment raw speech into syllable-like units without any textual supervision, and is often more reliable than unsupervised segmentation methods at the word-level, while capturing most of the word-level semantics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Peng &amp; Harwath (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib37\" title=\"\">2022</a>); Fuchs &amp; Hoshen (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib19\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "cho",
                    "baade",
                    "peng",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct extensive experiments across domains and languages. On LibriSpeech, SylCipher achieves up to 40% relative character error rate (CER) reduction over prior G2P-free UASR methods. On SpokenCOCO, improvements are even larger, demonstrating robustness across domains. On Mandarin, SylCipher achieves 12.2% phone error rate (PER), outperforming GAN-based UASR methods that fail to even converge.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on syllable-level modeling of speech used signal processing techniques&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib49\" title=\"\">2009</a>)</cite>. To discover higher-level structure and more efficient self-supervised learning (SSL) representations from raw speech, <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite> proposed to induce syllabic structure from existing SSL models such as HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib22\" title=\"\">2021a</a>)</cite>. To this end,&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite> probed the self-attention layers of VG-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng &amp; Harwath, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib37\" title=\"\">2022</a>)</cite>, a visually grounded SSL model to detect syllable-like feature clusters and further refine such clusters using a min-cut algorithm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi &amp; Malik (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib40\" title=\"\">1997</a>)</cite>. To sidestep the need for visual data, <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite> employed a speech-only SSL model trained with utterance-level self-distillation from a HuBERT teacher. Due to the indirect manner of such approaches by which syllabic structures are derived, they are often noisy and unreliable. To cope with this issue, recent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> proposed a more direct and targeted approach by performing self-distillation at the syllable-level, which significantly improved syllable boundary detection and unit discovery performance by encouraging sharper contrast between within and between-syllable feature frames.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "cho",
                    "baade",
                    "peng",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\\circ g(x):=f(g(x))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>f</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8728;</mo><mi>g</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f\\circ g(x):=f(g(x))</annotation></semantics></math> for any functions <math alttext=\"f,g\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo>,</mo><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">f,g</annotation></semantics></math>, and <math alttext=\"\\tilde{X}:=c\\circ m(X)\\in\\tilde{{\\mathcal{X}}}^{L}.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mrow><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>:=</mo><mrow><mrow><mi>c</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8728;</mo><mi>m</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>~</mo></mover><mi>L</mi></msup></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\tilde{X}:=c\\circ m(X)\\in\\tilde{{\\mathcal{X}}}^{L}.</annotation></semantics></math>\nThe soft-pooler first estimates the boundary probabilities <math alttext=\"b(X)\\in[0,1]^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">b(X)\\in[0,1]^{T}</annotation></semantics></math> by learning from an unsupervised syllable detector Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>, then constructs a pooling mask <math alttext=\"a(X)\\in[0,1]^{L\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">a(X)\\in[0,1]^{L\\times T}</annotation></semantics></math>, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.F2\" title=\"Figure 2 &#8227; 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>:</p>\n\n",
                "matched_terms": [
                    "cho",
                    "boundary",
                    "sylber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS1\" title=\"5.1 Datasets &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> introduces the datasets used for our experiments, followed by the syllabification steps for speech and text preprocessing in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS2\" title=\"5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS3\" title=\"5.3 Results: UASR &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> presents the main UASR results, and Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS4\" title=\"5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a> discusses SylCipher&#8217;s boundary refinement ability. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS5\" title=\"5.5 Ablation studies &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a> provides ablation studies on key design choices. Additional implementation details of our method and the baselines are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A4\" title=\"Appendix D Implementation details of SylCipher &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate SylCipher on three datasets. First, we train on the 460-hour clean subset of LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib35\" title=\"\">2015a</a>)</cite>, a standard UASR benchmark of audiobook recordings.\nSecond, to test domain generalization, we train another model on SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib23\" title=\"\">2021b</a>)</cite>, which contains 742 hours of spoken image captions. Third, to study languages with syllabic structures significantly different from English, we apply SylCipher to Mandarin using AISHELL-3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib42\" title=\"\">2021</a>)</cite>, which has 85 hours of read speech. We follow the same LibriSpeech split as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, and use the standard splits for SpokenCOCO and AISHELL-3. For LibriSpeech and SpokenCOCO, we consider both the <em class=\"ltx_emph ltx_font_italic\">matched</em> setting, where empirical speech and text probability distributions can be matched exactly, and the more realistic <em class=\"ltx_emph ltx_font_italic\">unmatched</em> setting where they cannot.\nIn the matched case, we use paired speech-text datasets with pairings removed; in the unmatched case, LibriSpeech uses LibriLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib36\" title=\"\">2015b</a>)</cite> with overlapping text removed, while SpokenCOCO is randomly split in half, with one half treated as speech-only and the other half treated as text-only. For AISHELL-3, we consider only the matched setting and compare models trained with/without tone labels. For all datasets, we apply a voice activity detector<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/wiseman/py-webrtcvad.git</span></span></span> to improve alignment.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English text, we use Pyphen<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/Kozea/Pyphen</span></span></span>, a rule-based hyphenation tool re-purposed for syllabification without a G2P. If Pyphen produces only a single chunk for a long word, we apply a simple rule-based fallback (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A5\" title=\"Appendix E Pseudo-code for the Pyphen+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>). We denote this combined approach as Pyphen+. We also experiment with other G2P-free approaches such as byte-pair encoding (BPE)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>); Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>, and find SylCipher robust to syllabification errors. To avoid long-tail distributions, we keep only the top-2048 most frequent English syllables and replace the rest with a special <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> token (replacing <math alttext=\"7\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">7\\%</annotation></semantics></math> of tokens in LibriSpeech and <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math> in SpokenCOCO. For Mandarin, we use the Pinyin of each Chinese character as a syllable, with or without tone labels. We keep the top-1024 most frequent syllables, covering 99.5% of occurrences. For speech syllabification, we use K-means clustering on syllable-level speech features created by mean pooling within the Sylber boundaries, with codebook size equal to the number of non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> text tokens.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "sylber",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level:</span> <em class=\"ltx_emph ltx_font_italic\">PUSM</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>, adapted from word-level UASR to syllables using the same syllable boundary detector and syllabifier as SylCipher.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level modeling performs best under G2P-free setting.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T1.st1\" title=\"In Table 1 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a> summarizes results on LibriSpeech. Among baselines, PUSM performs best in the matched setting, while wav2vec-U is strongest in the unmatched setting, though its performance is limited by the lack of G2P. Unlike phonemized training, REBORN trained directly on raw characters performs worse than wav2vec-U, suggesting sensitivity to misalignment between speech and text. By contrast, SylCipher with all three stages (Sylber+JE2E+PUSM) achieves 21.8% CER (matched) and 35.9% (unmatched), outperforming all baselines. Compared to the best-in-average system wav2vec-U (35.6%/43.3%), SylCipher reduces by CER by 40% (matched) and 17% (unmatched) relative.\nBoth syllable-level models (PUSM and SylCipher) outperform word- or character-level models, confirming that speech-text alignment is the best at the syllable level. Word-level JSTTI performs worst due to poor rare-word coverage (<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span>s <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p6.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 17%), and adapting JSTTI to phone-level degrades further, likely because phone clusters are noisier than syllable or word clusters.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "results",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative training helps.</span> Stage-wise training shows progressive improvements: JE2E reduces CER modestly, while PUSM yields the largest gains (44% and 23% relative reductions over JE2E in matched/unmatched settings). Combining MLM-based stages with PUSM outperforms PUSM-only training by 33-34% relative, as MLM provides necessary initialization for PUSM to converge. Indeed, PUSM alone fails when SylCipher is randomly initialized, likely due to transformer training instability. Lastly, using unsupervised Sylber boundaries performs nearly as well as forced alignment, suggesting robustness to segmentation noise.\nSelf-training consistently improves performance, especially for stronger models. For SylCipher, CER is further reduced by 20% relative.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "sylber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllables are robust to domain shifts.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T1.st2\" title=\"In Table 1 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a> reports results on SpokenCOCO. SylCipher again outperforms all baselines, surpassing PUSM by 32% and wav2vec-U by 49% relative CER after all stages. The margin is larger than LibriSpeech, especially in the unmatched setting. Notably, even after the first training stages, SylCipher already outperforms wav2vec-U by 22%. While wav2vec-U suffers from domain mismatch at the character level, both syllable-level methods perform better, suggesting syllable units are more robust to domain shifts. Self-training further improves SylCipher by 39% relative CER, likely due to higher syllable coverage reducing insertion/deletion errors. Moreover, performance degrades only slightly in the unmatched setting, indicating the sharper degradation on LibriSpeech stems from domain mismatch between its speech and text corpora.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "results",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level UASR works for Mandarin.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents results on Mandarin. Here, syllable-level models converge even without boundary refinement, while phone- and word-level approaches struggle, confirming that syllables are the most natural alignment unit for Mandarin. Compared to the best baseline (PUSM), SylCipher achieves over 5.5% relative PER reduction before self-training and 17% after. Interestingly, including tone labels does not harm performance&#8211;in fact, PER improves after the PUSM stage&#8211;suggesting that once phoneme labels are predicted correctly, tone prediction is also reliable.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "results",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the JE2E stage, we compare the syllable boundary detection performance against the teacher model Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> (which provides SylCipher&#8217;s initial boundaries) and other unsupervised approaches including Feat-Sim&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite>, SDHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite>, SylBoost&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>)</cite>. On LibriSpeech, Sylber is the strongest baseline on most metrics, except for the 20ms F1 score where SylBoost performs best. On SpokenCOCO, SylBoost outperforms Sylber on three of five metrics. However, our preliminary analysis show that SylBoost often over-segments, producing too many syllables and causing cross-modal misalignment and training instability in SylCipher. In contrast, Sylber predicts syllable counts closer to ground truth, making it more suitable as an initialization for UASR. When refined through JE2E, SylCipher improves upon its Sylber teacher, achieving +14% relative F1 (20ms tolerance) and +3% relative F1 (50ms) on LibriSpeech. On SpokenCOCO, it surpasses Sylber by +15% F1 and +11% R-value, and outperforms the best baseline Feat-Sim by +3% relative F1 score and +4.6% R-value. These results suggest that unpaired text provides additional guidance on syllable boundary detection. Visualizations of the speech-text alignment predicted by SylCipher can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A7\" title=\"Appendix G Spectrogram examples on LibriSpeech &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "cho",
                    "librispeech",
                    "sylboost",
                    "sdhubert",
                    "sylber",
                    "peng",
                    "results",
                    "baade",
                    "featsim",
                    "sylcipher",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SylCipher is robust to syllabifiers.</span> We first test alternative syllabifiers beyond Pyphen, including the phoneme-based Syllabify<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/kylebgorman/syllabify</span></span></span> and a character-based approach using byte-pair encoding (BPE) tokenization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>. The latter is attractive because it is language-agnostic and integrated easily with spoken language models. For the BPE-based syllabifier, we train the first stage of SuperBPE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>)</cite> on LibriSpeech with a 17k vocabulary, roughly matching the number of syllable types. We then split BPE tokens containing multiple non-consecutive vowels and merge consecutive tokens to ensure each unit contains at least one vowel (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A6\" title=\"Appendix F Pseudo-code of BPE+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>). We refer to this method as BPE+. All syllabifier variants are trained only under the <em class=\"ltx_emph ltx_font_italic\">fixed-boundary stage</em>, since later stages show correlated trends. Instead of CER, we report syllable error rate (SER) to more directly reflects syllable-level performance. Among the methods, Pyphen+ achieves the lowest SER, outperforming even the phoneme-based Syllabify. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, BPE+ yields weaker but still competitive results, demonstrating that SylCipher can generalize to linguistically simpler, resource-free syllabifiers.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "results",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced SylCipher, a UASR system that avoids phoneme-level resources such as G2P by recognizing speech at the <em class=\"ltx_emph ltx_font_italic\">syllable level</em>. Under the G2P-free setting, SylCipher outperforms the best existing systems by 17-40% relative CER on LibriSpeech and shows generalizability across other domains on SpokenCOCO, narrowing the gap with G2P-based systems. It also demonstrates cross-lingual robustness: on Mandarin, a tonal language where phoneme-based methods fail to converge, SylCipher achieves a PER of 13%. These results suggest that syllable-level modeling is a viable alternative to phoneme-level UASR and can push the horizon of more accessible and inclusive spoken language technology.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "results",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English experiments, SylCipher uses a HuBERT-large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt</span></span></span> model pretrained on LibriLight&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib24\" title=\"\">2020</a>)</cite> as the SSL encoder in the speech syllabifier, and initialize the soft-pooler with unsupervised boundary labels from Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>. For Mandarin, we instead use XEUS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://huggingface.co/espnet/xeus/blob/main/model/xeus_checkpoint_new.pth</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib12\" title=\"\">2024</a>)</cite>, which is pretrained on Mandarin (among other languages) and significantly outperforms HuBERT. We also finetune Sylber on AISHELL-3 to ensure stable convergence. For the pre-nets, shared encoder, post-nets, MLM parameters, and most of the optimizer hyperparameters, we follow JSTTI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, as modifying them did not yield consistent improvements.</p>\n\n",
                "matched_terms": [
                    "cho",
                    "boundary",
                    "sylber",
                    "sylcipher"
                ]
            }
        ]
    },
    "S5.T3.st2": {
        "source_file": "Towards Unsupervised Speech Recognition at the Syllable-Level",
        "caption": "(b) Boundary detection results on SpokenCOCO",
        "body": "F150 (↑\\uparrow)\nF120 (↑\\uparrow)\n\nP. (↑\\uparrow)\n\nRe. (↑\\uparrow)\nR (↑\\uparrow)\n\n\n\n\nFeat-Sim (Peng et al., 2023)\n\n60.3\n-\n57.4\n63.6\n64.3\n\n\nSylBoost (Baade et al., 2025)\n\n55.6\n30.2\n48\n65.2\n50.8\n\n\nSylber (Cho et al., 2025)\n\n53.5\n28.6\n52.3\n54.9\n59.6\n\n\nSylCipher (Ours, Sylber+JE2E)\n62.3\n31.0\n61.1\n63.6\n67.4",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">F1<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">50</span></sup> (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.st2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">F1<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">20</span></sup> (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.st2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">P.</span> (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.st2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">Re.</span> (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.st2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">R (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.st2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\">Feat-Sim&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">60.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">57.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">63.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">64.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SylBoost&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">55.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">30.2</span></td>\n<td class=\"ltx_td ltx_align_center\">48</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">65.2</span></td>\n<td class=\"ltx_td ltx_align_center\">50.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">53.5</td>\n<td class=\"ltx_td ltx_align_center\">28.6</td>\n<td class=\"ltx_td ltx_align_center\">52.3</td>\n<td class=\"ltx_td ltx_align_center\">54.9</td>\n<td class=\"ltx_td ltx_align_center\">59.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">SylCipher (Ours, Sylber+JE2E)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">62.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">31.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">61.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">63.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">67.4</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ours",
            "detection",
            "cho",
            "sylboost",
            "sylber",
            "sylcipher",
            "f150",
            "results",
            "peng",
            "baade",
            "sylberje2e",
            "↑uparrow",
            "f120",
            "featsim",
            "boundary",
            "spokencoco"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Last but not least, recent advancement in syllable boundary detection and unit discovery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> has made it possible to segment raw speech into syllable-like units without any textual supervision, and is often more reliable than unsupervised segmentation methods at the word-level, while capturing most of the word-level semantics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Peng &amp; Harwath (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib37\" title=\"\">2022</a>); Fuchs &amp; Hoshen (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib19\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "cho",
                    "baade",
                    "peng",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct extensive experiments across domains and languages. On LibriSpeech, SylCipher achieves up to 40% relative character error rate (CER) reduction over prior G2P-free UASR methods. On SpokenCOCO, improvements are even larger, demonstrating robustness across domains. On Mandarin, SylCipher achieves 12.2% phone error rate (PER), outperforming GAN-based UASR methods that fail to even converge.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on syllable-level modeling of speech used signal processing techniques&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Glass, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib49\" title=\"\">2009</a>)</cite>. To discover higher-level structure and more efficient self-supervised learning (SSL) representations from raw speech, <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite> proposed to induce syllabic structure from existing SSL models such as HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib22\" title=\"\">2021a</a>)</cite>. To this end,&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite> probed the self-attention layers of VG-HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng &amp; Harwath, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib37\" title=\"\">2022</a>)</cite>, a visually grounded SSL model to detect syllable-like feature clusters and further refine such clusters using a min-cut algorithm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Shi &amp; Malik (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib40\" title=\"\">1997</a>)</cite>. To sidestep the need for visual data, <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite> employed a speech-only SSL model trained with utterance-level self-distillation from a HuBERT teacher. Due to the indirect manner of such approaches by which syllabic structures are derived, they are often noisy and unreliable. To cope with this issue, recent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>; Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> proposed a more direct and targeted approach by performing self-distillation at the syllable-level, which significantly improved syllable boundary detection and unit discovery performance by encouraging sharper contrast between within and between-syllable feature frames.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "cho",
                    "baade",
                    "peng",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\\circ g(x):=f(g(x))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>f</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8728;</mo><mi>g</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>:=</mo><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f\\circ g(x):=f(g(x))</annotation></semantics></math> for any functions <math alttext=\"f,g\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo>,</mo><mi>g</mi></mrow><annotation encoding=\"application/x-tex\">f,g</annotation></semantics></math>, and <math alttext=\"\\tilde{X}:=c\\circ m(X)\\in\\tilde{{\\mathcal{X}}}^{L}.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mrow><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mo>:=</mo><mrow><mrow><mi>c</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8728;</mo><mi>m</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>~</mo></mover><mi>L</mi></msup></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\tilde{X}:=c\\circ m(X)\\in\\tilde{{\\mathcal{X}}}^{L}.</annotation></semantics></math>\nThe soft-pooler first estimates the boundary probabilities <math alttext=\"b(X)\\in[0,1]^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">b(X)\\in[0,1]^{T}</annotation></semantics></math> by learning from an unsupervised syllable detector Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>, then constructs a pooling mask <math alttext=\"a(X)\\in[0,1]^{L\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">a(X)\\in[0,1]^{L\\times T}</annotation></semantics></math>, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.F2\" title=\"Figure 2 &#8227; 4.1 Training: UASR via information compression &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>:</p>\n\n",
                "matched_terms": [
                    "cho",
                    "boundary",
                    "sylber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS1\" title=\"5.1 Datasets &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> introduces the datasets used for our experiments, followed by the syllabification steps for speech and text preprocessing in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS2\" title=\"5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS3\" title=\"5.3 Results: UASR &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> presents the main UASR results, and Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS4\" title=\"5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a> discusses SylCipher&#8217;s boundary refinement ability. Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.SS5\" title=\"5.5 Ablation studies &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a> provides ablation studies on key design choices. Additional implementation details of our method and the baselines are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A4\" title=\"Appendix D Implementation details of SylCipher &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate SylCipher on three datasets. First, we train on the 460-hour clean subset of LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib35\" title=\"\">2015a</a>)</cite>, a standard UASR benchmark of audiobook recordings.\nSecond, to test domain generalization, we train another model on SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib23\" title=\"\">2021b</a>)</cite>, which contains 742 hours of spoken image captions. Third, to study languages with syllabic structures significantly different from English, we apply SylCipher to Mandarin using AISHELL-3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib42\" title=\"\">2021</a>)</cite>, which has 85 hours of read speech. We follow the same LibriSpeech split as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, and use the standard splits for SpokenCOCO and AISHELL-3. For LibriSpeech and SpokenCOCO, we consider both the <em class=\"ltx_emph ltx_font_italic\">matched</em> setting, where empirical speech and text probability distributions can be matched exactly, and the more realistic <em class=\"ltx_emph ltx_font_italic\">unmatched</em> setting where they cannot.\nIn the matched case, we use paired speech-text datasets with pairings removed; in the unmatched case, LibriSpeech uses LibriLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib36\" title=\"\">2015b</a>)</cite> with overlapping text removed, while SpokenCOCO is randomly split in half, with one half treated as speech-only and the other half treated as text-only. For AISHELL-3, we consider only the matched setting and compare models trained with/without tone labels. For all datasets, we apply a voice activity detector<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/wiseman/py-webrtcvad.git</span></span></span> to improve alignment.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English text, we use Pyphen<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/Kozea/Pyphen</span></span></span>, a rule-based hyphenation tool re-purposed for syllabification without a G2P. If Pyphen produces only a single chunk for a long word, we apply a simple rule-based fallback (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A5\" title=\"Appendix E Pseudo-code for the Pyphen+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>). We denote this combined approach as Pyphen+. We also experiment with other G2P-free approaches such as byte-pair encoding (BPE)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>); Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>, and find SylCipher robust to syllabification errors. To avoid long-tail distributions, we keep only the top-2048 most frequent English syllables and replace the rest with a special <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> token (replacing <math alttext=\"7\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">7\\%</annotation></semantics></math> of tokens in LibriSpeech and <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math> in SpokenCOCO. For Mandarin, we use the Pinyin of each Chinese character as a syllable, with or without tone labels. We keep the top-1024 most frequent syllables, covering 99.5% of occurrences. For speech syllabification, we use K-means clustering on syllable-level speech features created by mean pooling within the Sylber boundaries, with codebook size equal to the number of non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> text tokens.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "sylber",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level:</span> <em class=\"ltx_emph ltx_font_italic\">PUSM</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>, adapted from word-level UASR to syllables using the same syllable boundary detector and syllabifier as SylCipher.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level modeling performs best under G2P-free setting.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T1.st1\" title=\"In Table 1 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a> summarizes results on LibriSpeech. Among baselines, PUSM performs best in the matched setting, while wav2vec-U is strongest in the unmatched setting, though its performance is limited by the lack of G2P. Unlike phonemized training, REBORN trained directly on raw characters performs worse than wav2vec-U, suggesting sensitivity to misalignment between speech and text. By contrast, SylCipher with all three stages (Sylber+JE2E+PUSM) achieves 21.8% CER (matched) and 35.9% (unmatched), outperforming all baselines. Compared to the best-in-average system wav2vec-U (35.6%/43.3%), SylCipher reduces by CER by 40% (matched) and 17% (unmatched) relative.\nBoth syllable-level models (PUSM and SylCipher) outperform word- or character-level models, confirming that speech-text alignment is the best at the syllable level. Word-level JSTTI performs worst due to poor rare-word coverage (<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span>s <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p6.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 17%), and adapting JSTTI to phone-level degrades further, likely because phone clusters are noisier than syllable or word clusters.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative training helps.</span> Stage-wise training shows progressive improvements: JE2E reduces CER modestly, while PUSM yields the largest gains (44% and 23% relative reductions over JE2E in matched/unmatched settings). Combining MLM-based stages with PUSM outperforms PUSM-only training by 33-34% relative, as MLM provides necessary initialization for PUSM to converge. Indeed, PUSM alone fails when SylCipher is randomly initialized, likely due to transformer training instability. Lastly, using unsupervised Sylber boundaries performs nearly as well as forced alignment, suggesting robustness to segmentation noise.\nSelf-training consistently improves performance, especially for stronger models. For SylCipher, CER is further reduced by 20% relative.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "sylber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllables are robust to domain shifts.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T1.st2\" title=\"In Table 1 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a> reports results on SpokenCOCO. SylCipher again outperforms all baselines, surpassing PUSM by 32% and wav2vec-U by 49% relative CER after all stages. The margin is larger than LibriSpeech, especially in the unmatched setting. Notably, even after the first training stages, SylCipher already outperforms wav2vec-U by 22%. While wav2vec-U suffers from domain mismatch at the character level, both syllable-level methods perform better, suggesting syllable units are more robust to domain shifts. Self-training further improves SylCipher by 39% relative CER, likely due to higher syllable coverage reducing insertion/deletion errors. Moreover, performance degrades only slightly in the unmatched setting, indicating the sharper degradation on LibriSpeech stems from domain mismatch between its speech and text corpora.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "results",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level UASR works for Mandarin.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech and text syllabification &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents results on Mandarin. Here, syllable-level models converge even without boundary refinement, while phone- and word-level approaches struggle, confirming that syllables are the most natural alignment unit for Mandarin. Compared to the best baseline (PUSM), SylCipher achieves over 5.5% relative PER reduction before self-training and 17% after. Interestingly, including tone labels does not harm performance&#8211;in fact, PER improves after the PUSM stage&#8211;suggesting that once phoneme labels are predicted correctly, tone prediction is also reliable.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "results",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the JE2E stage, we compare the syllable boundary detection performance against the teacher model Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> (which provides SylCipher&#8217;s initial boundaries) and other unsupervised approaches including Feat-Sim&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite>, SDHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite>, SylBoost&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>)</cite>. On LibriSpeech, Sylber is the strongest baseline on most metrics, except for the 20ms F1 score where SylBoost performs best. On SpokenCOCO, SylBoost outperforms Sylber on three of five metrics. However, our preliminary analysis show that SylBoost often over-segments, producing too many syllables and causing cross-modal misalignment and training instability in SylCipher. In contrast, Sylber predicts syllable counts closer to ground truth, making it more suitable as an initialization for UASR. When refined through JE2E, SylCipher improves upon its Sylber teacher, achieving +14% relative F1 (20ms tolerance) and +3% relative F1 (50ms) on LibriSpeech. On SpokenCOCO, it surpasses Sylber by +15% F1 and +11% R-value, and outperforms the best baseline Feat-Sim by +3% relative F1 score and +4.6% R-value. These results suggest that unpaired text provides additional guidance on syllable boundary detection. Visualizations of the speech-text alignment predicted by SylCipher can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A7\" title=\"Appendix G Spectrogram examples on LibriSpeech &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "cho",
                    "sylboost",
                    "sylber",
                    "peng",
                    "results",
                    "baade",
                    "featsim",
                    "sylcipher",
                    "boundary",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SylCipher is robust to syllabifiers.</span> We first test alternative syllabifiers beyond Pyphen, including the phoneme-based Syllabify<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/kylebgorman/syllabify</span></span></span> and a character-based approach using byte-pair encoding (BPE) tokenization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>. The latter is attractive because it is language-agnostic and integrated easily with spoken language models. For the BPE-based syllabifier, we train the first stage of SuperBPE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>)</cite> on LibriSpeech with a 17k vocabulary, roughly matching the number of syllable types. We then split BPE tokens containing multiple non-consecutive vowels and merge consecutive tokens to ensure each unit contains at least one vowel (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A6\" title=\"Appendix F Pseudo-code of BPE+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>). We refer to this method as BPE+. All syllabifier variants are trained only under the <em class=\"ltx_emph ltx_font_italic\">fixed-boundary stage</em>, since later stages show correlated trends. Instead of CER, we report syllable error rate (SER) to more directly reflects syllable-level performance. Among the methods, Pyphen+ achieves the lowest SER, outperforming even the phoneme-based Syllabify. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, BPE+ yields weaker but still competitive results, demonstrating that SylCipher can generalize to linguistically simpler, resource-free syllabifiers.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduced SylCipher, a UASR system that avoids phoneme-level resources such as G2P by recognizing speech at the <em class=\"ltx_emph ltx_font_italic\">syllable level</em>. Under the G2P-free setting, SylCipher outperforms the best existing systems by 17-40% relative CER on LibriSpeech and shows generalizability across other domains on SpokenCOCO, narrowing the gap with G2P-based systems. It also demonstrates cross-lingual robustness: on Mandarin, a tonal language where phoneme-based methods fail to converge, SylCipher achieves a PER of 13%. These results suggest that syllable-level modeling is a viable alternative to phoneme-level UASR and can push the horizon of more accessible and inclusive spoken language technology.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "results",
                    "spokencoco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English experiments, SylCipher uses a HuBERT-large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt</span></span></span> model pretrained on LibriLight&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib24\" title=\"\">2020</a>)</cite> as the SSL encoder in the speech syllabifier, and initialize the soft-pooler with unsupervised boundary labels from Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>. For Mandarin, we instead use XEUS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://huggingface.co/espnet/xeus/blob/main/model/xeus_checkpoint_new.pth</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib12\" title=\"\">2024</a>)</cite>, which is pretrained on Mandarin (among other languages) and significantly outperforms HuBERT. We also finetune Sylber on AISHELL-3 to ensure stable convergence. For the pre-nets, shared encoder, post-nets, MLM parameters, and most of the optimizer hyperparameters, we follow JSTTI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, as modifying them did not yield consistent improvements.</p>\n\n",
                "matched_terms": [
                    "cho",
                    "boundary",
                    "sylber",
                    "sylcipher"
                ]
            }
        ]
    },
    "S5.F3.sf1": {
        "source_file": "Towards Unsupervised Speech Recognition at the Syllable-Level",
        "caption": "(a) Effect of syllabifier type",
        "body": "Model\nTokenizer\nResource\nSER (↓\\downarrow)\n\n\n\n\nSylCipher (Sylber)\nPyphen+\nMedium\n48.9\n\n\nSylCipher (Sylber+JE2E)\nPyphen+\nMedium\n44.9\n\n\nSylCipher (Sylber)\nSyllabify\nHigh\n51.3\n\n\nSylCipher (Sylber)\nBPE+\nLow\n52.5\n\n\nSylCipher (Sylber+JE2E)\nBPE+\nLow\n49.9",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Tokenizer</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Resource</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F3.sf1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">SylCipher (Sylber)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Pyphen+</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Medium</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">48.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SylCipher (Sylber+JE2E)</th>\n<td class=\"ltx_td ltx_align_center\">Pyphen+</td>\n<td class=\"ltx_td ltx_align_center\">Medium</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">44.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SylCipher (Sylber)</th>\n<td class=\"ltx_td ltx_align_center\">Syllabify</td>\n<td class=\"ltx_td ltx_align_center\">High</td>\n<td class=\"ltx_td ltx_align_center\">51.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SylCipher (Sylber)</th>\n<td class=\"ltx_td ltx_align_center\">BPE+</td>\n<td class=\"ltx_td ltx_align_center\">Low</td>\n<td class=\"ltx_td ltx_align_center\">52.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">SylCipher (Sylber+JE2E)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">BPE+</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Low</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">49.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "↓downarrow",
            "ser",
            "resource",
            "model",
            "syllabifier",
            "sylber",
            "medium",
            "sylberje2e",
            "effect",
            "high",
            "bpe",
            "low",
            "sylcipher",
            "tokenizer",
            "syllabify",
            "type",
            "pyphen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SylCipher is robust to syllabifiers.</span> We first test alternative syllabifiers beyond Pyphen, including the phoneme-based Syllabify<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/kylebgorman/syllabify</span></span></span> and a character-based approach using byte-pair encoding (BPE) tokenization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>. The latter is attractive because it is language-agnostic and integrated easily with spoken language models. For the BPE-based syllabifier, we train the first stage of SuperBPE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>)</cite> on LibriSpeech with a 17k vocabulary, roughly matching the number of syllable types. We then split BPE tokens containing multiple non-consecutive vowels and merge consecutive tokens to ensure each unit contains at least one vowel (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A6\" title=\"Appendix F Pseudo-code of BPE+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>). We refer to this method as BPE+. All syllabifier variants are trained only under the <em class=\"ltx_emph ltx_font_italic\">fixed-boundary stage</em>, since later stages show correlated trends. Instead of CER, we report syllable error rate (SER) to more directly reflects syllable-level performance. Among the methods, Pyphen+ achieves the lowest SER, outperforming even the phoneme-based Syllabify. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, BPE+ yields weaker but still competitive results, demonstrating that SylCipher can generalize to linguistically simpler, resource-free syllabifiers.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In this section, we describe <span class=\"ltx_text ltx_font_bold\">SylCipher</span>, our proposed model for syllable-level UASR. We first present its architecture and training objective, then justify the design theoretically, and finally introduce several practical modifications for training and inference.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S4.F1\" title=\"Figure 1 &#8227; 4 SylCipher: Syllable-level UASR via information-constrained masked language modeling &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, SylCipher is an encoder-only language model with a <em class=\"ltx_emph ltx_font_italic\">shared encoder</em> for speech and text modalities. To project both modalities into a joint embedding space, we use two uni-modal <em class=\"ltx_emph ltx_font_italic\">pre-nets</em>: <math alttext=\"e_{\\tilde{X}}:{\\mathcal{X}}^{T}\\mapsto{\\mathbb{R}}^{L\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>T</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">e_{\\tilde{X}}:{\\mathcal{X}}^{T}\\mapsto{\\mathbb{R}}^{L\\times d}</annotation></semantics></math> for speech and <math alttext=\"e_{Y}:{\\mathcal{Y}}^{L}\\mapsto{\\mathbb{R}}^{L\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>e</mi><mi>Y</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><mi>L</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">e_{Y}:{\\mathcal{Y}}^{L}\\mapsto{\\mathbb{R}}^{L\\times d}</annotation></semantics></math> for text, each implemented as a linear embedding layer. Before the speech pre-net, a <em class=\"ltx_emph ltx_font_italic\">speech syllabifier</em> converts the frame-level feature vectors into a syllable-level sequence. It consists of: (i) a <em class=\"ltx_emph ltx_font_italic\">differentiable soft-pooler</em> <math alttext=\"m:{\\mathcal{X}}^{T}\\mapsto{\\mathcal{X}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>T</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>L</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">m:{\\mathcal{X}}^{T}\\mapsto{\\mathcal{X}}^{L}</annotation></semantics></math> that aligns speech with text on the syllable level (ii) a tokenizer <math alttext=\"c:{\\mathcal{X}}^{L}\\mapsto\\tilde{{\\mathcal{X}}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mi>L</mi></msup><mo stretchy=\"false\">&#8614;</mo><msup><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>~</mo></mover><mi>L</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">c:{\\mathcal{X}}^{L}\\mapsto\\tilde{{\\mathcal{X}}}^{L}</annotation></semantics></math> to discretizes speech into syllable-like units. Thus the speech pre-net is</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "tokenizer",
                    "model",
                    "syllabifier"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate SylCipher on three datasets. First, we train on the 460-hour clean subset of LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib35\" title=\"\">2015a</a>)</cite>, a standard UASR benchmark of audiobook recordings.\nSecond, to test domain generalization, we train another model on SpokenCOCO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib23\" title=\"\">2021b</a>)</cite>, which contains 742 hours of spoken image captions. Third, to study languages with syllabic structures significantly different from English, we apply SylCipher to Mandarin using AISHELL-3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib42\" title=\"\">2021</a>)</cite>, which has 85 hours of read speech. We follow the same LibriSpeech split as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, and use the standard splits for SpokenCOCO and AISHELL-3. For LibriSpeech and SpokenCOCO, we consider both the <em class=\"ltx_emph ltx_font_italic\">matched</em> setting, where empirical speech and text probability distributions can be matched exactly, and the more realistic <em class=\"ltx_emph ltx_font_italic\">unmatched</em> setting where they cannot.\nIn the matched case, we use paired speech-text datasets with pairings removed; in the unmatched case, LibriSpeech uses LibriLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib36\" title=\"\">2015b</a>)</cite> with overlapping text removed, while SpokenCOCO is randomly split in half, with one half treated as speech-only and the other half treated as text-only. For AISHELL-3, we consider only the matched setting and compare models trained with/without tone labels. For all datasets, we apply a voice activity detector<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/wiseman/py-webrtcvad.git</span></span></span> to improve alignment.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English text, we use Pyphen<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/Kozea/Pyphen</span></span></span>, a rule-based hyphenation tool re-purposed for syllabification without a G2P. If Pyphen produces only a single chunk for a long word, we apply a simple rule-based fallback (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A5\" title=\"Appendix E Pseudo-code for the Pyphen+ syllabifier &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>). We denote this combined approach as Pyphen+. We also experiment with other G2P-free approaches such as byte-pair encoding (BPE)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib29\" title=\"\">2025</a>); Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib39\" title=\"\">2016</a>)</cite>, and find SylCipher robust to syllabification errors. To avoid long-tail distributions, we keep only the top-2048 most frequent English syllables and replace the rest with a special <span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> token (replacing <math alttext=\"7\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">7\\%</annotation></semantics></math> of tokens in LibriSpeech and <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math> in SpokenCOCO. For Mandarin, we use the Pinyin of each Chinese character as a syllable, with or without tone labels. We keep the top-1024 most frequent syllables, covering 99.5% of occurrences. For speech syllabification, we use K-means clustering on syllable-level speech features created by mean pooling within the Sylber boundaries, with codebook size equal to the number of non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> text tokens.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "bpe",
                    "sylber",
                    "pyphen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare SylCipher with UASR systems that differ in token type, training objective, and architecture.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Syllable-level:</span> <em class=\"ltx_emph ltx_font_italic\">PUSM</em>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib47\" title=\"\">2023c</a>)</cite>, adapted from word-level UASR to syllables using the same syllable boundary detector and syllabifier as SylCipher.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "syllabifier"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative training helps.</span> Stage-wise training shows progressive improvements: JE2E reduces CER modestly, while PUSM yields the largest gains (44% and 23% relative reductions over JE2E in matched/unmatched settings). Combining MLM-based stages with PUSM outperforms PUSM-only training by 33-34% relative, as MLM provides necessary initialization for PUSM to converge. Indeed, PUSM alone fails when SylCipher is randomly initialized, likely due to transformer training instability. Lastly, using unsupervised Sylber boundaries performs nearly as well as forced alignment, suggesting robustness to segmentation noise.\nSelf-training consistently improves performance, especially for stronger models. For SylCipher, CER is further reduced by 20% relative.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "sylber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the JE2E stage, we compare the syllable boundary detection performance against the teacher model Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite> (which provides SylCipher&#8217;s initial boundaries) and other unsupervised approaches including Feat-Sim&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib38\" title=\"\">2023</a>)</cite>, SDHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib13\" title=\"\">2024</a>)</cite>, SylBoost&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib6\" title=\"\">2025</a>)</cite>. On LibriSpeech, Sylber is the strongest baseline on most metrics, except for the 20ms F1 score where SylBoost performs best. On SpokenCOCO, SylBoost outperforms Sylber on three of five metrics. However, our preliminary analysis show that SylBoost often over-segments, producing too many syllables and causing cross-modal misalignment and training instability in SylCipher. In contrast, Sylber predicts syllable counts closer to ground truth, making it more suitable as an initialization for UASR. When refined through JE2E, SylCipher improves upon its Sylber teacher, achieving +14% relative F1 (20ms tolerance) and +3% relative F1 (50ms) on LibriSpeech. On SpokenCOCO, it surpasses Sylber by +15% F1 and +11% R-value, and outperforms the best baseline Feat-Sim by +3% relative F1 score and +4.6% R-value. These results suggest that unpaired text provides additional guidance on syllable boundary detection. Visualizations of the speech-text alignment predicted by SylCipher can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#A7\" title=\"Appendix G Spectrogram examples on LibriSpeech &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "model",
                    "sylber"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SylCipher works with different vocabulary sizes.</span> We also experiment with varying the non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> vocabulary size in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#S5.F3.sf3\" title=\"In Figure 3 &#8227; 5.4 Results: Unsupervised syllable boundary detection &#8227; 5 Experiments &#8227; Towards Unsupervised Speech Recognition at the Syllable-Level\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>. SylCipher displays consistent SERs across a wide range of vocabulary sizes, reaching the lowest overall SER with 2048 tokens and the lowest non-<span class=\"ltx_text ltx_font_typewriter\">&lt;OOV&gt;</span> SER with 1024 tokens.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "ser"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span> SylCipher is not yet language-universal, since different languages use different writing systems and require linguistic knowledge to properly syllabify. For example, languages such as Hebrew and Arabic omits vowels in their writing system, which could pose challenges to existing syllabifiers.\nWhile our experiments show that the method can be adapted to more resource-efficient tokenizers such as BPE with minimal modifications, coming up with a language-universal tokenization method remains an open problem.\nFurther, the iterative training procedure can be further simplified into an end-to-end approach. Lastly, improving the robustness of SylCipher under domain mismatch between speech and text remains an open challenge.\n\n</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "bpe",
                    "syllabify"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SylCipher is not yet language-universal, since different languages use different writing systems and require linguistic knowledge to properly syllabify. For example, languages such as Hebrew and Arabic omits vowels in their writing system, which could pose challenges to existing syllabifiers.\nWhile our experiments show that the method can be adapted to more resource-efficient tokenizers such as BPE with minimal modifications, coming up with a language-universal tokenization method remains an open problem.\nFurther, the iterative training procedure can be further simplified into an end-to-end approach. Lastly, improving the robustness of SylCipher under domain mismatch between speech and text remains an open challenge.\n</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "bpe",
                    "syllabify"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English experiments, SylCipher uses a HuBERT-large<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://dl.fbaipublicfiles.com/hubert/hubert_large_ll60k.pt</span></span></span> model pretrained on LibriLight&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib24\" title=\"\">2020</a>)</cite> as the SSL encoder in the speech syllabifier, and initialize the soft-pooler with unsupervised boundary labels from Sylber&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib14\" title=\"\">2025</a>)</cite>. For Mandarin, we instead use XEUS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://huggingface.co/espnet/xeus/blob/main/model/xeus_checkpoint_new.pth</span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib12\" title=\"\">2024</a>)</cite>, which is pretrained on Mandarin (among other languages) and significantly outperforms HuBERT. We also finetune Sylber on AISHELL-3 to ensure stable convergence. For the pre-nets, shared encoder, post-nets, MLM parameters, and most of the optimizer hyperparameters, we follow JSTTI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.03639v1#bib.bib34\" title=\"\">2025</a>)</cite>, as modifying them did not yield consistent improvements.</p>\n\n",
                "matched_terms": [
                    "sylcipher",
                    "model",
                    "syllabifier",
                    "sylber"
                ]
            }
        ]
    }
}