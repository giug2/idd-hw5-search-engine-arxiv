{
    "S4.T1.fig1": {
        "caption": "Table 1: Accuracies (%) on MMAU (v05.15.25) benchmark. † represents results reproduced in this work.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_border_t\" rowspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_t\" colspan=\"4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMAU Test-mini</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Sound</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Music</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Baselines:</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gemini 2.0 Flash</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">75.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">GPT-4o Audio</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">LTU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">GAMA </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SALMONN </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.4</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Audio Flamingo 2 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">71.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Audio-Reasoner </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-Audio-7B-Instruct </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib11\" title=\"\">11</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.3</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni-7B </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">78.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.6</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">71.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Ours:</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-Audio-7B-Instruct</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8224;</span></sup>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.4</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ MATA</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.8</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni-7B</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8224;</span></sup>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ MATA</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">79.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">68.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">72.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_b\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "mmau",
            "qwen25omni7b†",
            "baselines",
            "flamingo",
            "qwen2audio7binstruct†",
            "v051525",
            "audio",
            "qwen2audio7binstruct",
            "ltu",
            "reproduced",
            "audioreasoner",
            "sound",
            "represents",
            "avg",
            "testmini",
            "accuracies",
            "results",
            "speech",
            "gama",
            "benchmark",
            "ours",
            "gpt4o",
            "gemini",
            "qwen25omni7b",
            "work",
            "music",
            "flash",
            "mata",
            "salmonn"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#S4.T1.fig1\" title=\"Table 1 &#8227; 4.2 Experimental results on the MMAU dataset &#8227; 4 EXPERIMENTS &#8227; Pay more attention to audio: mitigating imbalance of cross-modal attention in large audio language models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents accuracy comparisons between our MATA method and mainstream baselines on MMAU (v05.15.25). The dataset categorizes audio inputs into three distinct tasks: sound, music, and speech. Given that the ground-truth answers for the full MMAU 9000 test set are not publicly available, all models were evaluated on the 1000 subset. As shown in Table 1, Qwen2.5-Omni achieves the best overall performance, with an average accuracy of 71.5%. In contrast, Qwen2-Audio, once the top-performing open-source model in its original publication, exhibits a noticeable performance gap relative to the latest LALM, reflecting the rapid advancement of LALM architectures.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Audio-Language Models (LALMs) often suffer from audio-textual attention imbalance, prioritizing text over acoustic information, particularly in the multi-modal fusion layers of the Transformer architecture. This bias hinders their ability to fully utilize acoustic cues, causing suboptimal performance on audio reasoning tasks.\nTo mitigate this, we propose <span class=\"ltx_text ltx_font_bold\">MATA</span>, a novel training-free method that dynamically pushes LALMs to pay <span class=\"ltx_text ltx_font_bold\">M</span>ore <span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">T</span>o <span class=\"ltx_text ltx_font_bold\">A</span>udio tokens within the self-attention mechanism.\nSpecifically, MATA intervenes post raw attention scoring, targeting only the last token in intermediate layers without introducing additional parameters or computational overhead. Experiments on the MMAU and MMAR benchmarks confirm MATA&#8217;s effectiveness, with consistent performance gains. Notably, on MMAR, MATA enables an open-source model to surpass the proprietary Gemini 2.0 Flash for the first time. Our work provides an efficient solution to mitigate attention bias and opens a new research direction for enhancing the audio-processing capabilities of multi-modal models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "mmau",
                    "gemini",
                    "audio",
                    "work",
                    "flash",
                    "mata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the realm of artificial intelligence, deriving logical conclusions or uncovering implicit intentions from perceived information, which is known as reasoning, is a key ability that supports advanced cognitive tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib4\" title=\"\">4</a>]</cite>.\nIn the field of audio processing, audio reasoning tasks require models to deeply understand aural information, such as speaker intent, emotional tone, and subtle environmental cues <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib7\" title=\"\">7</a>]</cite>. For instance, in a complex soundscape, a model must be able to reason that a sudden, high-pitched scream indicates distress or danger, while the consistent hum of an engine suggests an ongoing process. Effectively performing these tasks demands a comprehensive grasp of all available audio information.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rise of large audio-language models (LALMs), which integrate large language models (LLMs) with audio encoders <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib11\" title=\"\">11</a>]</cite>, has shown great promise in handling complex audio-textual tasks. These models leverage the robust language understanding capabilities of large language models while incorporating audio information, enabling them to perform a variety of tasks, from audio captioning to complex question answering. However, despite their impressive performance, a critical challenge remains: the attention bias between audio and text modalities. Previous research has already demonstrated similar issues in large vision-language models (LVLMs) and LLMs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib15\" title=\"\">15</a>]</cite>, where models show inconsistent attention to different input types, often prioritizing text over visual or other modalities. Yet, analogous investigations into the attention dynamics between audio and text in LALMs remain scarce. This oversight is non-trivial because if LALMs disproportionately prioritize text over audio, the task-critical acoustic information will be underutilized, hindering the model&#8217;s capacity to capture underlying intent and causing suboptimal or erroneous inferences.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we are the first to systematically investigate the attention allocation of LALMs when processing multi-modal audio and text inputs. Our analysis, performed on the Qwen2.5-Omni-7B model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib16\" title=\"\">16</a>]</cite>, reveals a significant attention disparity: the model allocates considerably more attention to text tokens than to audio tokens throughout the answer generation process. This bias is particularly pronounced in the intermediate layers of the decoder, which are responsible for the critical fusion of multi-modal information <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib18\" title=\"\">18</a>]</cite>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Pay more attention to audio: mitigating imbalance of cross-modal attention in large audio language models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we first save the attention distributions at each auto-regressive decoding step, then average these distributions to obtain the mean attention weight for each decoder layer. The results show that in the intermediate layers, attention to audio tokens consistently remains low, while attention to system and instruction text tokens stays high. We argue that this imbalance prevents the model from effectively utilizing the rich information embedded in the audio signals during the decoding process, which can lead to poor reasoning performance and hallucinations.</p>\n\n",
                "matched_terms": [
                    "work",
                    "audio",
                    "qwen25omni7b",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, we propose a novel, training-free technique to enhance the model&#8217;s focus on the audio modality during inference. By directly increasing the attention weights allocated to audio tokens within the intermediate layers of the LALM&#8217;s decoder, our method encourages a more audio-centric reasoning process. We evaluate our approach on two prominent audio reasoning benchmarks, MMAU <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib19\" title=\"\">19</a>]</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib20\" title=\"\">20</a>]</cite>. Experimental results validate the effectiveness of our method in improving performance when integrated with open-source LALMs. Notably, when our technique is applied to the Qwen2.5-Omni-7B model fine-tuned by reinforcement learning (RL), it achieves a new state-of-the-art performance on the MMAR benchmark, surpassing even the closed-source Gemini 2.0 Flash for the first time, with an average accuracy of 66.8%. This work highlights a fundamental limitation in current LALMs and provides a straightforward yet highly effective solution to further unleash their potential for audio reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "mmau",
                    "gemini",
                    "audio",
                    "qwen25omni7b",
                    "work",
                    "flash",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid evolution of LLMs has driven the development of LALMs, which extend the strengths of LLMs into the multi-modal audio-textual domain. Early works like LTU <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib8\" title=\"\">8</a>]</cite> and GAMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib9\" title=\"\">9</a>]</cite> aligned pre-trained audio encoders with LLMs via fine-tuning, laying the groundwork for basic audio understanding. Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib11\" title=\"\">11</a>]</cite> marked a significant milestone by integrating Reinforcement Learning with Human Feedback (RLHF), enhancing the model&#8217;s ability to understand and respond to audio inputs in a more human-aligned manner. Building upon Qwen2-Audio, subsequent works like Audio-CoT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib7\" title=\"\">7</a>]</cite> and Audio-Reasoner <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib5\" title=\"\">5</a>]</cite> introduced Chain-of-Thought (CoT) techniques to the audio-language domain. By breaking down complex audio reasoning tasks into sequential, interpretable steps, these methods partially improved the models&#8217; reasoning capabilities. More recently, omni-modal models like Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib16\" title=\"\">16</a>]</cite> and Baichuan-Omni-1.5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib21\" title=\"\">21</a>]</cite> have demonstrated exceptional performance in audio-textual scenarios. Despite these notable advancements, a critical challenge remains: the inherent attention imbalance between the audio and text modalities within these models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "ltu",
                    "audioreasoner",
                    "gama"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal attention bias occurs when multi-modal models disproportionately focus on one modality, like text, while neglecting others, such as audio or vision. While this bias has been extensively studied in LVLMs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib23\" title=\"\">23</a>]</cite>, it remains largely unexplored in LALMs. In LVLMs, this imbalance often causes hallucinations, as the models, pre-trained on vast text corpora, favor text over visual information <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib15\" title=\"\">15</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the audio-textual attention imbalance in LALMs while circumventing the high computational cost of full model retraining, we propose a approach referred to as Pay <span class=\"ltx_text ltx_font_bold\">M</span>ore <span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">T</span>o <span class=\"ltx_text ltx_font_bold\">A</span>udio (MATA). MATA is a training-free technique that dynamically increases the model&#8217;s focus on audio tokens within its self-attention mechanism. The core motivation for MATA stems from our key finding: in the intermediate layers of the Transformer architecture, the primary locus of multi-modal information integration, LALMs exhibit a significant disparity in attention allocation between text and audio tokens. By intervening at this critical stage, we can guide the model towards a more balanced multi-modal representation before the information propagates to deeper layers for output decoding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our MATA method intervenes in the self-attention computation process after the raw attention scores are calculated but before the softmax function is applied. This timing is crucial as it ensures the enhancement of audio token attention without disrupting the overall attention normalization process. Furthermore, MATA is applied only to the attention scores of the last token in the sequence. This token is a key determinant of the next output, and thus benefits most from a balanced audio-textual context. We define a small hyperparameter <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> to control the enhancement strength. The modified attention scores, denoted as <math alttext=\"\\mathbf{\\hat{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119808;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathbf{\\hat{A}}</annotation></semantics></math>, are calculated as follows:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m3\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, and <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m5\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> denote the attention head, query position, and key position, respectively. <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the total sequence length, and <math alttext=\"[a_{s},a_{e}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>a</mi><mi>s</mi></msub><mo>,</mo><msub><mi>a</mi><mi>e</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[a_{s},a_{e}]</annotation></semantics></math> represents the start and end indices of the audio tokens. The modified scores are then normalized via the softmax function and used to compute the final attention output with the value <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m8\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> tokens. It is important to note that MATA is implemented by modifying the forward pass of the LALM&#8217;s self-attention module, requiring no additional trainable parameters. This ensures that the computational overhead is negligible, making the method highly efficient and easy to deploy across pre-trained LALMs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "represents",
                    "mata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically evaluate the effectiveness of our proposed MATA method, we conducted experiments on two mainstream audio reasoning benchmarks: MMAU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/Sakshi113/MMAU\" title=\"\">https://github.com/Sakshi113/MMAU</a></span></span></span> and MMAR<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/ddlBoJack/MMAR\" title=\"\">https://github.com/ddlBoJack/MMAR</a></span></span></span>. For the MMAU dataset, since the initial version is no longer available, we used the Test-mini set with 1k audios of the updated version (v05.15.25).\nWe selected two state-of-the-art LALMs, Qwen2-Audio and Qwen2.5-Omni, as our primary baseline models. These models were chosen for their superior performance across various audio-textual multi-modal tasks, which provides a rigorous benchmark for evaluating the improvements offered by MATA. For hyperparameter settings, the enhancement strength <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> was defaulted to 0.1, and the MATA intervention was applied to the LLM decoder layers from 10 to 20.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "mmau",
                    "testmini",
                    "v051525",
                    "audio",
                    "mata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When integrated with our MATA method, both baseline models demonstrate significant performance improvements across all categories, which validates the effectiveness of our approach. For Qwen2-Audio (MATA), its accuracy increases from 59.4% (reproduced) to 64.8% on average. For the stronger baseline Qwen2.5-Omni, MATA still delivers consistent enhancements: its average accuracy rises from 71.1% (reproduced) to 73.6%, representing a 2.5% improvement. Notably, the magnitude of improvement on Qwen2.5-Omni is slightly less pronounced compared to Qwen2-Audio, a phenomenon we attribute to its already robust baseline performance. Nonetheless, Qwen2.5-Omni (MATA) achieves state-of-the-art results in most categories, confirming MATA&#8217;s value even for cutting-edge models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "mata",
                    "results",
                    "reproduced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Experimental results on the MMAU dataset &#8227; 4 EXPERIMENTS &#8227; Pay more attention to audio: mitigating imbalance of cross-modal attention in large audio language models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents accuracy comparisons on the MMAR benchmark, which comprises 1,000 test audio clips covering both single modality and mixed modality audio reasoning tasks. Among baseline models, proprietary closed-source systems, particularly Gemini 2.0 Flash and GPT-4o Audio, significantly outperform open-source alternatives. Specifically, Gemini 2.0 Flash achieves the highest average accuracy of 65.6%, while the top open-source model, Qwen2.5-Omni, lags behind by nearly 9%. This notable gap underscores the performance disparity between existing closed and open LALMs.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "gpt4o",
                    "gemini",
                    "audio",
                    "flash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our MATA method consistently enhances model performance. For Qwen2.5-Omni, MATA increases average accuracy from 56.6% (reproduced) to 61.2%, with improvements across most categories. A slight decrease is noted in the sound-music mixed modality task; however, this is statistically negligible as this category contains only 11 audio clips, corresponding to a single misclassified sample.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mata",
                    "reproduced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, Ke-Omni-R-7B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/shuaijiang/Ke-Omni-R\" title=\"\">https://github.com/shuaijiang/Ke-Omni-R</a></span></span></span>, a RL-fine-tuned variant of Qwen2.5-Omni, exhibits strong baseline performance with a 64.1% average accuracy. Applying MATA to this RL-enhanced model yields further significant gains, pushing its average accuracy to 66.8%, surpassing Gemini 2.0 Flash (65.6%) and marking the first time that an open-source model has outperformed closed-source counterparts on MMAR benchmark. This outcome highlights MATA&#8217;s versatility and compatibility with existing fine-tuning strategies, as it can further enhance the performance of models already optimized through techniques like RL.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "gemini",
                    "flash",
                    "mata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted ablation studies using the original Qwen2.5-Omni as the baseline to investigate the impact of enhancement strength <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and intervention layer range. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#S4.T3.fig1\" title=\"Table 3 &#8227; 4.3 Experimental results on the MMAR dataset &#8227; 4 EXPERIMENTS &#8227; Pay more attention to audio: mitigating imbalance of cross-modal attention in large audio language models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the results on the MMAU benchmark. Our analysis shows that a moderate <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> value of 0.10 yields the highest average accuracy (73.6%), with consistent improvements across all categories. In contrast, smaller or larger <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> values lead to either limited gains or slight performance degradation, suggesting that excessive audio attention amplification may disrupt the delicate balance of multi-modal information integration.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "audio",
                    "mmau",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the layer analysis, intervening on intermediate layers yields the best results, aligning with our earlier observation that these layers are the primary site of audio-text fusion where audio attention is inherently underweighted. In contrast, intervention in early layers (0-10) or late layers (20-28) is relatively poor. Early layer intervention leads to a catastrophic performance drop, likely due to distorted initial contextual encoding. Late layer intervention fails to surpass the baseline, as critical multi-modal fusion has already been completed. This validates the necessity of targeting specific intermediate layers to achieve optimal results.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate the critical problem of audio-textual attention imbalance in LALMs, identifying that they disproportionately allocate attention to text over audio, particularly in the crucial intermediate layers for multi-modal fusion. To mitigate this issue, we propose MATA, a training-free technique that dynamically amplifies attention to audio tokens within the self-attention mechanism. Experiments on the MMAU benchmark demonstrate that MATA improves the average accuracy of Qwen2-Audio and Qwen2.5-Omni by 5.4% and 2.5%, respectively. On the MMAR benchmark, MATA boosts the performance of the RL-fine-tuned Ke-Omni-R-7B model from 64.1% to 66.8%, marking the first time an open-source model surpasses the proprietary Gemini 2.0 Flash. For future work, we plan to extend the MATA method to a broader range of LALM architectures and explore its application in other multi-modal models to further improve their audio processing capabilities.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "mmau",
                    "gemini",
                    "audio",
                    "work",
                    "flash",
                    "mata"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Accuracies (%) on MMAR benchmark. † represents results reproduced in this work. Ke-Omni-R-7B denotes a reinforcement learning fine-tuned version of Qwen-2.5-Omni-7B.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Single Modality</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Mixed Modalities</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Avg (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound-Music</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound-Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Music-Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sound-Music-Speech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Baselines:</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gemini 2.0 Flash</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">61.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">51.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">72.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">81.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">72.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">65.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">GPT-4o Audio</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">51.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">72.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">75.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">LTU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">GAMA </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SALMONN </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Audio Flamingo 2 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Audio-CoT </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Audio-Reasoner </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-Audio-7B-Instruct </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib11\" title=\"\">11</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Qwen-2.5-Omni </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">67.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Ours:</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Qwen-2.5-Omni-7B</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8224;</span></sup><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8202;&#8194;&#8196;&#8202;+ MATA</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Ke-Omni-R-7B</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8224;</span></sup><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">54.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">62.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8202;&#8194;&#8196;&#8202;+ MATA</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">66.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">70.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">72.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">69.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">62.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">66.8</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "finetuned",
            "qwen25omni7b†",
            "baselines",
            "denotes",
            "soundmusic",
            "reinforcement",
            "flamingo",
            "single",
            "keomnir7b†",
            "mmar",
            "modality",
            "audio",
            "qwen2audio7binstruct",
            "ltu",
            "reproduced",
            "learning",
            "audioreasoner",
            "sound",
            "represents",
            "avg",
            "audiocot",
            "soundspeech",
            "musicspeech",
            "soundmusicspeech",
            "accuracies",
            "results",
            "keomnir7b",
            "speech",
            "mixed",
            "gama",
            "qwen25omni",
            "benchmark",
            "ours",
            "version",
            "gpt4o",
            "gemini",
            "model",
            "qwen25omni7b",
            "work",
            "music",
            "flash",
            "modalities",
            "mata",
            "salmonn"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#S4.T2\" title=\"Table 2 &#8227; 4.2 Experimental results on the MMAU dataset &#8227; 4 EXPERIMENTS &#8227; Pay more attention to audio: mitigating imbalance of cross-modal attention in large audio language models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents accuracy comparisons on the MMAR benchmark, which comprises 1,000 test audio clips covering both single modality and mixed modality audio reasoning tasks. Among baseline models, proprietary closed-source systems, particularly Gemini 2.0 Flash and GPT-4o Audio, significantly outperform open-source alternatives. Specifically, Gemini 2.0 Flash achieves the highest average accuracy of 65.6%, while the top open-source model, Qwen2.5-Omni, lags behind by nearly 9%. This notable gap underscores the performance disparity between existing closed and open LALMs.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Audio-Language Models (LALMs) often suffer from audio-textual attention imbalance, prioritizing text over acoustic information, particularly in the multi-modal fusion layers of the Transformer architecture. This bias hinders their ability to fully utilize acoustic cues, causing suboptimal performance on audio reasoning tasks.\nTo mitigate this, we propose <span class=\"ltx_text ltx_font_bold\">MATA</span>, a novel training-free method that dynamically pushes LALMs to pay <span class=\"ltx_text ltx_font_bold\">M</span>ore <span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">T</span>o <span class=\"ltx_text ltx_font_bold\">A</span>udio tokens within the self-attention mechanism.\nSpecifically, MATA intervenes post raw attention scoring, targeting only the last token in intermediate layers without introducing additional parameters or computational overhead. Experiments on the MMAU and MMAR benchmarks confirm MATA&#8217;s effectiveness, with consistent performance gains. Notably, on MMAR, MATA enables an open-source model to surpass the proprietary Gemini 2.0 Flash for the first time. Our work provides an efficient solution to mitigate attention bias and opens a new research direction for enhancing the audio-processing capabilities of multi-modal models.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "mmar",
                    "audio",
                    "model",
                    "work",
                    "flash",
                    "mata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nReasoning, Understanding, Attenion Bias, Large Audio Language Model (LALM)</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the realm of artificial intelligence, deriving logical conclusions or uncovering implicit intentions from perceived information, which is known as reasoning, is a key ability that supports advanced cognitive tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib4\" title=\"\">4</a>]</cite>.\nIn the field of audio processing, audio reasoning tasks require models to deeply understand aural information, such as speaker intent, emotional tone, and subtle environmental cues <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib7\" title=\"\">7</a>]</cite>. For instance, in a complex soundscape, a model must be able to reason that a sudden, high-pitched scream indicates distress or danger, while the consistent hum of an engine suggests an ongoing process. Effectively performing these tasks demands a comprehensive grasp of all available audio information.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rise of large audio-language models (LALMs), which integrate large language models (LLMs) with audio encoders <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib11\" title=\"\">11</a>]</cite>, has shown great promise in handling complex audio-textual tasks. These models leverage the robust language understanding capabilities of large language models while incorporating audio information, enabling them to perform a variety of tasks, from audio captioning to complex question answering. However, despite their impressive performance, a critical challenge remains: the attention bias between audio and text modalities. Previous research has already demonstrated similar issues in large vision-language models (LVLMs) and LLMs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib15\" title=\"\">15</a>]</cite>, where models show inconsistent attention to different input types, often prioritizing text over visual or other modalities. Yet, analogous investigations into the attention dynamics between audio and text in LALMs remain scarce. This oversight is non-trivial because if LALMs disproportionately prioritize text over audio, the task-critical acoustic information will be underutilized, hindering the model&#8217;s capacity to capture underlying intent and causing suboptimal or erroneous inferences.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "modalities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we are the first to systematically investigate the attention allocation of LALMs when processing multi-modal audio and text inputs. Our analysis, performed on the Qwen2.5-Omni-7B model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib16\" title=\"\">16</a>]</cite>, reveals a significant attention disparity: the model allocates considerably more attention to text tokens than to audio tokens throughout the answer generation process. This bias is particularly pronounced in the intermediate layers of the decoder, which are responsible for the critical fusion of multi-modal information <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib18\" title=\"\">18</a>]</cite>. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Pay more attention to audio: mitigating imbalance of cross-modal attention in large audio language models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we first save the attention distributions at each auto-regressive decoding step, then average these distributions to obtain the mean attention weight for each decoder layer. The results show that in the intermediate layers, attention to audio tokens consistently remains low, while attention to system and instruction text tokens stays high. We argue that this imbalance prevents the model from effectively utilizing the rich information embedded in the audio signals during the decoding process, which can lead to poor reasoning performance and hallucinations.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "qwen25omni7b",
                    "work",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, we propose a novel, training-free technique to enhance the model&#8217;s focus on the audio modality during inference. By directly increasing the attention weights allocated to audio tokens within the intermediate layers of the LALM&#8217;s decoder, our method encourages a more audio-centric reasoning process. We evaluate our approach on two prominent audio reasoning benchmarks, MMAU <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib19\" title=\"\">19</a>]</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib20\" title=\"\">20</a>]</cite>. Experimental results validate the effectiveness of our method in improving performance when integrated with open-source LALMs. Notably, when our technique is applied to the Qwen2.5-Omni-7B model fine-tuned by reinforcement learning (RL), it achieves a new state-of-the-art performance on the MMAR benchmark, surpassing even the closed-source Gemini 2.0 Flash for the first time, with an average accuracy of 66.8%. This work highlights a fundamental limitation in current LALMs and provides a straightforward yet highly effective solution to further unleash their potential for audio reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "gemini",
                    "finetuned",
                    "mmar",
                    "modality",
                    "audio",
                    "model",
                    "qwen25omni7b",
                    "learning",
                    "work",
                    "flash",
                    "results",
                    "reinforcement"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid evolution of LLMs has driven the development of LALMs, which extend the strengths of LLMs into the multi-modal audio-textual domain. Early works like LTU <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib8\" title=\"\">8</a>]</cite> and GAMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib9\" title=\"\">9</a>]</cite> aligned pre-trained audio encoders with LLMs via fine-tuning, laying the groundwork for basic audio understanding. Qwen2-Audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib11\" title=\"\">11</a>]</cite> marked a significant milestone by integrating Reinforcement Learning with Human Feedback (RLHF), enhancing the model&#8217;s ability to understand and respond to audio inputs in a more human-aligned manner. Building upon Qwen2-Audio, subsequent works like Audio-CoT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib7\" title=\"\">7</a>]</cite> and Audio-Reasoner <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib5\" title=\"\">5</a>]</cite> introduced Chain-of-Thought (CoT) techniques to the audio-language domain. By breaking down complex audio reasoning tasks into sequential, interpretable steps, these methods partially improved the models&#8217; reasoning capabilities. More recently, omni-modal models like Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib16\" title=\"\">16</a>]</cite> and Baichuan-Omni-1.5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib21\" title=\"\">21</a>]</cite> have demonstrated exceptional performance in audio-textual scenarios. Despite these notable advancements, a critical challenge remains: the inherent attention imbalance between the audio and text modalities within these models.</p>\n\n",
                "matched_terms": [
                    "audiocot",
                    "audio",
                    "ltu",
                    "learning",
                    "audioreasoner",
                    "modalities",
                    "reinforcement",
                    "gama",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-modal attention bias occurs when multi-modal models disproportionately focus on one modality, like text, while neglecting others, such as audio or vision. While this bias has been extensively studied in LVLMs <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib23\" title=\"\">23</a>]</cite>, it remains largely unexplored in LALMs. In LVLMs, this imbalance often causes hallucinations, as the models, pre-trained on vast text corpora, favor text over visual information <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib15\" title=\"\">15</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the LALM domain lacks systematic research on this audio-textual attention bias. A recent study by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib24\" title=\"\">24</a>]</cite> indirectly highlights this gap, showing that LALMs can perform competitively on audio reasoning tasks even with the audio input completely removed, relying solely on text instructions. This suggests that current LALMs have a tendency to neglect the audio modality and rely excessively on textual information, underscoring the need for deeper understanding and effective solutions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the audio-textual attention imbalance in LALMs while circumventing the high computational cost of full model retraining, we propose a approach referred to as Pay <span class=\"ltx_text ltx_font_bold\">M</span>ore <span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">T</span>o <span class=\"ltx_text ltx_font_bold\">A</span>udio (MATA). MATA is a training-free technique that dynamically increases the model&#8217;s focus on audio tokens within its self-attention mechanism. The core motivation for MATA stems from our key finding: in the intermediate layers of the Transformer architecture, the primary locus of multi-modal information integration, LALMs exhibit a significant disparity in attention allocation between text and audio tokens. By intervening at this critical stage, we can guide the model towards a more balanced multi-modal representation before the information propagates to deeper layers for output decoding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "mata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our MATA method intervenes in the self-attention computation process after the raw attention scores are calculated but before the softmax function is applied. This timing is crucial as it ensures the enhancement of audio token attention without disrupting the overall attention normalization process. Furthermore, MATA is applied only to the attention scores of the last token in the sequence. This token is a key determinant of the next output, and thus benefits most from a balanced audio-textual context. We define a small hyperparameter <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> to control the enhancement strength. The modified attention scores, denoted as <math alttext=\"\\mathbf{\\hat{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119808;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathbf{\\hat{A}}</annotation></semantics></math>, are calculated as follows:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m3\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m4\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, and <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m5\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> denote the attention head, query position, and key position, respectively. <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the total sequence length, and <math alttext=\"[a_{s},a_{e}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>a</mi><mi>s</mi></msub><mo>,</mo><msub><mi>a</mi><mi>e</mi></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[a_{s},a_{e}]</annotation></semantics></math> represents the start and end indices of the audio tokens. The modified scores are then normalized via the softmax function and used to compute the final attention output with the value <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m8\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> tokens. It is important to note that MATA is implemented by modifying the forward pass of the LALM&#8217;s self-attention module, requiring no additional trainable parameters. This ensures that the computational overhead is negligible, making the method highly efficient and easy to deploy across pre-trained LALMs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "represents",
                    "mata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically evaluate the effectiveness of our proposed MATA method, we conducted experiments on two mainstream audio reasoning benchmarks: MMAU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/Sakshi113/MMAU\" title=\"\">https://github.com/Sakshi113/MMAU</a></span></span></span> and MMAR<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/ddlBoJack/MMAR\" title=\"\">https://github.com/ddlBoJack/MMAR</a></span></span></span>. For the MMAU dataset, since the initial version is no longer available, we used the Test-mini set with 1k audios of the updated version (v05.15.25).\nWe selected two state-of-the-art LALMs, Qwen2-Audio and Qwen2.5-Omni, as our primary baseline models. These models were chosen for their superior performance across various audio-textual multi-modal tasks, which provides a rigorous benchmark for evaluating the improvements offered by MATA. For hyperparameter settings, the enhancement strength <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> was defaulted to 0.1, and the MATA intervention was applied to the LLM decoder layers from 10 to 20.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "version",
                    "audio",
                    "mata",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#S4.T1.fig1\" title=\"Table 1 &#8227; 4.2 Experimental results on the MMAU dataset &#8227; 4 EXPERIMENTS &#8227; Pay more attention to audio: mitigating imbalance of cross-modal attention in large audio language models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents accuracy comparisons between our MATA method and mainstream baselines on MMAU (v05.15.25). The dataset categorizes audio inputs into three distinct tasks: sound, music, and speech. Given that the ground-truth answers for the full MMAU 9000 test set are not publicly available, all models were evaluated on the 1000 subset. As shown in Table 1, Qwen2.5-Omni achieves the best overall performance, with an average accuracy of 71.5%. In contrast, Qwen2-Audio, once the top-performing open-source model in its original publication, exhibits a noticeable performance gap relative to the latest LALM, reflecting the rapid advancement of LALM architectures.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "baselines",
                    "music",
                    "sound",
                    "mata",
                    "speech",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When integrated with our MATA method, both baseline models demonstrate significant performance improvements across all categories, which validates the effectiveness of our approach. For Qwen2-Audio (MATA), its accuracy increases from 59.4% (reproduced) to 64.8% on average. For the stronger baseline Qwen2.5-Omni, MATA still delivers consistent enhancements: its average accuracy rises from 71.1% (reproduced) to 73.6%, representing a 2.5% improvement. Notably, the magnitude of improvement on Qwen2.5-Omni is slightly less pronounced compared to Qwen2-Audio, a phenomenon we attribute to its already robust baseline performance. Nonetheless, Qwen2.5-Omni (MATA) achieves state-of-the-art results in most categories, confirming MATA&#8217;s value even for cutting-edge models.</p>\n\n",
                "matched_terms": [
                    "results",
                    "mata",
                    "reproduced",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our MATA method consistently enhances model performance. For Qwen2.5-Omni, MATA increases average accuracy from 56.6% (reproduced) to 61.2%, with improvements across most categories. A slight decrease is noted in the sound-music mixed modality task; however, this is statistically negligible as this category contains only 11 audio clips, corresponding to a single misclassified sample.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "modality",
                    "model",
                    "reproduced",
                    "soundmusic",
                    "mata",
                    "mixed",
                    "single",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, Ke-Omni-R-7B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/shuaijiang/Ke-Omni-R\" title=\"\">https://github.com/shuaijiang/Ke-Omni-R</a></span></span></span>, a RL-fine-tuned variant of Qwen2.5-Omni, exhibits strong baseline performance with a 64.1% average accuracy. Applying MATA to this RL-enhanced model yields further significant gains, pushing its average accuracy to 66.8%, surpassing Gemini 2.0 Flash (65.6%) and marking the first time that an open-source model has outperformed closed-source counterparts on MMAR benchmark. This outcome highlights MATA&#8217;s versatility and compatibility with existing fine-tuning strategies, as it can further enhance the performance of models already optimized through techniques like RL.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "gemini",
                    "mmar",
                    "model",
                    "flash",
                    "mata",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted ablation studies using the original Qwen2.5-Omni as the baseline to investigate the impact of enhancement strength <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and intervention layer range. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#S4.T3.fig1\" title=\"Table 3 &#8227; 4.3 Experimental results on the MMAR dataset &#8227; 4 EXPERIMENTS &#8227; Pay more attention to audio: mitigating imbalance of cross-modal attention in large audio language models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the results on the MMAU benchmark. Our analysis shows that a moderate <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> value of 0.10 yields the highest average accuracy (73.6%), with consistent improvements across all categories. In contrast, smaller or larger <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> values lead to either limited gains or slight performance degradation, suggesting that excessive audio attention amplification may disrupt the delicate balance of multi-modal information integration.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "audio",
                    "results",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the layer analysis, intervening on intermediate layers yields the best results, aligning with our earlier observation that these layers are the primary site of audio-text fusion where audio attention is inherently underweighted. In contrast, intervention in early layers (0-10) or late layers (20-28) is relatively poor. Early layer intervention leads to a catastrophic performance drop, likely due to distorted initial contextual encoding. Late layer intervention fails to surpass the baseline, as critical multi-modal fusion has already been completed. This validates the necessity of targeting specific intermediate layers to achieve optimal results.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate the critical problem of audio-textual attention imbalance in LALMs, identifying that they disproportionately allocate attention to text over audio, particularly in the crucial intermediate layers for multi-modal fusion. To mitigate this issue, we propose MATA, a training-free technique that dynamically amplifies attention to audio tokens within the self-attention mechanism. Experiments on the MMAU benchmark demonstrate that MATA improves the average accuracy of Qwen2-Audio and Qwen2.5-Omni by 5.4% and 2.5%, respectively. On the MMAR benchmark, MATA boosts the performance of the RL-fine-tuned Ke-Omni-R-7B model from 64.1% to 66.8%, marking the first time an open-source model surpasses the proprietary Gemini 2.0 Flash. For future work, we plan to extend the MATA method to a broader range of LALM architectures and explore its application in other multi-modal models to further improve their audio processing capabilities.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "gemini",
                    "mmar",
                    "audio",
                    "model",
                    "work",
                    "flash",
                    "mata",
                    "keomnir7b",
                    "qwen25omni"
                ]
            }
        ]
    },
    "S4.T3.fig1": {
        "caption": "Table 3: Ablation studies with different layers and α\\alpha. Accuracies (%) on MMAU (v05.15.25).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\"><math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Layers</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMAU Test-mini</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Sound</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Music</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.8</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.7</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.9</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.1</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10-20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">78.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.8</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10-20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">79.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">68.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">72.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10-20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">79.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.4</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0-10</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">20-28</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">80.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0-28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.4</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "αalpha",
            "mmau",
            "avg",
            "studies",
            "testmini",
            "v051525",
            "ablation",
            "accuracies",
            "music",
            "speech",
            "sound",
            "different",
            "layers"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conducted ablation studies using the original Qwen2.5-Omni as the baseline to investigate the impact of enhancement strength <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and intervention layer range. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#S4.T3.fig1\" title=\"Table 3 &#8227; 4.3 Experimental results on the MMAR dataset &#8227; 4 EXPERIMENTS &#8227; Pay more attention to audio: mitigating imbalance of cross-modal attention in large audio language models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the results on the MMAU benchmark. Our analysis shows that a moderate <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> value of 0.10 yields the highest average accuracy (73.6%), with consistent improvements across all categories. In contrast, smaller or larger <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> values lead to either limited gains or slight performance degradation, suggesting that excessive audio attention amplification may disrupt the delicate balance of multi-modal information integration.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Audio-Language Models (LALMs) often suffer from audio-textual attention imbalance, prioritizing text over acoustic information, particularly in the multi-modal fusion layers of the Transformer architecture. This bias hinders their ability to fully utilize acoustic cues, causing suboptimal performance on audio reasoning tasks.\nTo mitigate this, we propose <span class=\"ltx_text ltx_font_bold\">MATA</span>, a novel training-free method that dynamically pushes LALMs to pay <span class=\"ltx_text ltx_font_bold\">M</span>ore <span class=\"ltx_text ltx_font_bold\">A</span>ttention <span class=\"ltx_text ltx_font_bold\">T</span>o <span class=\"ltx_text ltx_font_bold\">A</span>udio tokens within the self-attention mechanism.\nSpecifically, MATA intervenes post raw attention scoring, targeting only the last token in intermediate layers without introducing additional parameters or computational overhead. Experiments on the MMAU and MMAR benchmarks confirm MATA&#8217;s effectiveness, with consistent performance gains. Notably, on MMAR, MATA enables an open-source model to surpass the proprietary Gemini 2.0 Flash for the first time. Our work provides an efficient solution to mitigate attention bias and opens a new research direction for enhancing the audio-processing capabilities of multi-modal models.</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, we propose a novel, training-free technique to enhance the model&#8217;s focus on the audio modality during inference. By directly increasing the attention weights allocated to audio tokens within the intermediate layers of the LALM&#8217;s decoder, our method encourages a more audio-centric reasoning process. We evaluate our approach on two prominent audio reasoning benchmarks, MMAU <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib19\" title=\"\">19</a>]</cite> and MMAR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#bib.bib20\" title=\"\">20</a>]</cite>. Experimental results validate the effectiveness of our method in improving performance when integrated with open-source LALMs. Notably, when our technique is applied to the Qwen2.5-Omni-7B model fine-tuned by reinforcement learning (RL), it achieves a new state-of-the-art performance on the MMAR benchmark, surpassing even the closed-source Gemini 2.0 Flash for the first time, with an average accuracy of 66.8%. This work highlights a fundamental limitation in current LALMs and provides a straightforward yet highly effective solution to further unleash their potential for audio reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To systematically evaluate the effectiveness of our proposed MATA method, we conducted experiments on two mainstream audio reasoning benchmarks: MMAU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/Sakshi113/MMAU\" title=\"\">https://github.com/Sakshi113/MMAU</a></span></span></span> and MMAR<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/ddlBoJack/MMAR\" title=\"\">https://github.com/ddlBoJack/MMAR</a></span></span></span>. For the MMAU dataset, since the initial version is no longer available, we used the Test-mini set with 1k audios of the updated version (v05.15.25).\nWe selected two state-of-the-art LALMs, Qwen2-Audio and Qwen2.5-Omni, as our primary baseline models. These models were chosen for their superior performance across various audio-textual multi-modal tasks, which provides a rigorous benchmark for evaluating the improvements offered by MATA. For hyperparameter settings, the enhancement strength <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> was defaulted to 0.1, and the MATA intervention was applied to the LLM decoder layers from 10 to 20.</p>\n\n",
                "matched_terms": [
                    "αalpha",
                    "mmau",
                    "testmini",
                    "v051525",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18816v1#S4.T1.fig1\" title=\"Table 1 &#8227; 4.2 Experimental results on the MMAU dataset &#8227; 4 EXPERIMENTS &#8227; Pay more attention to audio: mitigating imbalance of cross-modal attention in large audio language models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents accuracy comparisons between our MATA method and mainstream baselines on MMAU (v05.15.25). The dataset categorizes audio inputs into three distinct tasks: sound, music, and speech. Given that the ground-truth answers for the full MMAU 9000 test set are not publicly available, all models were evaluated on the 1000 subset. As shown in Table 1, Qwen2.5-Omni achieves the best overall performance, with an average accuracy of 71.5%. In contrast, Qwen2-Audio, once the top-performing open-source model in its original publication, exhibits a noticeable performance gap relative to the latest LALM, reflecting the rapid advancement of LALM architectures.</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "v051525",
                    "music",
                    "sound",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we investigate the critical problem of audio-textual attention imbalance in LALMs, identifying that they disproportionately allocate attention to text over audio, particularly in the crucial intermediate layers for multi-modal fusion. To mitigate this issue, we propose MATA, a training-free technique that dynamically amplifies attention to audio tokens within the self-attention mechanism. Experiments on the MMAU benchmark demonstrate that MATA improves the average accuracy of Qwen2-Audio and Qwen2.5-Omni by 5.4% and 2.5%, respectively. On the MMAR benchmark, MATA boosts the performance of the RL-fine-tuned Ke-Omni-R-7B model from 64.1% to 66.8%, marking the first time an open-source model surpasses the proprietary Gemini 2.0 Flash. For future work, we plan to extend the MATA method to a broader range of LALM architectures and explore its application in other multi-modal models to further improve their audio processing capabilities.</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "layers"
                ]
            }
        ]
    }
}