{
    "S5.T1": {
        "caption": "Table 1: Offline WERs for models trained on the 960 hours LibriSpeech data (WER ↓\\downarrow).",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">size(M)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">dev-clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">dev-other</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">test-clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">test-other</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">ave.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">wav2vec2 base&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">95</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">wav2vec2 large&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">317</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.5</td>\n<td class=\"ltx_td ltx_align_center\">3.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">w2v-Conformer XL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib42\" title=\"\">2020c</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">600</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.5</td>\n<td class=\"ltx_td ltx_align_center\">2.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">BEST-RQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">600</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.9</td>\n<td class=\"ltx_td ltx_align_center\">2.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Chunk SSL Base</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">79</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Chunk SSL Large</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">337</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">1.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">4.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">1.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">4.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "testclean",
            "wer",
            "wers",
            "librispeech",
            "ave",
            "chiu",
            "baevski",
            "↓downarrow",
            "2020b",
            "bestrq",
            "hours",
            "devclean",
            "base",
            "2020c",
            "trained",
            "chunk",
            "sizem",
            "w2vconformer",
            "zhang",
            "devother",
            "testother",
            "wav2vec2",
            "offline",
            "model",
            "large",
            "data",
            "ssl"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The pre-trained Chunk SSL models are fine-tuned with dynamic chunk training and they could be used for both streaming and offline ASR.\nWe list the offline and streaming recognition results from the Chunk SSL models in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>.\nThe reported literature results are presented in the first part of the tables.\nIt is clear that the chunk SSL pre-trained models could achieve very competitive results compared to other strong baselines.\nThe base and large models outperform the corresponding wav2vec2 base and large models.</p>\n\n",
            "<p class=\"ltx_p\">In the streaming evaluation, the Chunk SSL model with the base configuration excels all models reported in the literature shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>. The large configuration model pushes the WER even lower and it achieves another 0.5 average WER reduction compared to the base configure model. Comparing results in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>,\nthe proposed method significantly reduces the performance gap between the streaming and offline applications. The WER gap between the large configure\nmodels is 0.8 in average for Chunk SSL while the average WER difference is 2.5 for BEST-RQ.\nNote, literature models listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a> are initialized with dedicated pre-trained models either offline or streaming, while a Chunk SSL model could be initialized with the same pre-trained model and operated in both modes.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Low latency speech human-machine communication is becoming increasingly necessary as speech technology advances quickly in the last decade.\nOne of the primary factors behind the advancement of speech technology is self-supervised learning.\nMost self-supervised learning algorithms are designed with full utterance assumption and compromises have to made if partial utterances are presented, which are common in the streaming applications.\nIn this work, we propose a chunk based self-supervised learning (Chunk SSL) algorithm as an unified solution for both streaming and offline speech pre-training.\nChunk SSL is optimized with the masked prediction loss and\nan acoustic encoder is encouraged to restore indices of those masked speech frames with help from unmasked frames in the same chunk and preceding chunks.\nA copy and append data augmentation approach is proposed to conduct efficient chunk based pre-training.\nChunk SSL utilizes a finite scalar quantization (FSQ) module to discretize input speech features and our study shows a high resolution FSQ codebook, i.e., a codebook with vocabulary size up to a few millions, is beneficial to transfer knowledge from the pre-training task to the downstream tasks. A group masked prediction loss is employed during pre-training to alleviate the high memory and computation cost introduced by the large codebook.\nThe proposed approach is examined in two speech to text tasks, i.e., speech recognition and speech translation.\nExperimental results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> datasets show that the proposed method could achieve\nvery competitive results for speech to text tasks at both streaming and offline modes.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "librispeech",
                    "large",
                    "data",
                    "ssl",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Transcribe speech into text in real time is critical for applications requiring immediate feedback, such as real-time transcription of broadcast contents, voice assistants, and simultaneous speech translation etc.\nIt requires processing audio incrementally as it is received, rather than waiting for the entire utterance to be available.\nFor neural network based end-to-end systems,\nthe requirement includes two-fold. First, a speech encoder, such as causal encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib41\" title=\"\">2020b</a>)</cite> and chunk encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tsunoo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib36\" title=\"\">2019</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib33\" title=\"\">2021</a>)</cite>, is able to process input audio cumulatively without dependency on the future input.\nSecond, a decoder is required to decode transcription incrementally based on partial encoder outputs.\nFrame-Synchronous decoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib10\" title=\"\">2020</a>)</cite>, such as CTC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib14\" title=\"\">2013</a>)</cite> Transducer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Graves, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib13\" title=\"\">2012</a>)</cite> CIF&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong &amp; Xu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib9\" title=\"\">2020</a>)</cite> and TAED&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib35\" title=\"\">2023</a>)</cite>, are capable of generating transcription based on partial encoder outputs and meet the streaming requirement naturally.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "zhang",
                    "2020b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Self-supervised pre-training leverages vast amounts of unlabeled data and learn universal feature representations for downstream tasks, especially for tasks with limited supervised training data.\nSpeech pre-training methods have emerged as the backbone of many speech processing tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib8\" title=\"\">2018</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib5\" title=\"\">2021</a>; Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib34\" title=\"\">2022</a>; Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib2\" title=\"\">2025</a>)</cite>.\nThose methods are designed for the\nspeech applications with full utterance available, and compromises have to be made if the downstream tasks are streaming applications.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> propose to conduct pre-training with a causal encoder, which sacrifices right context information. <cite class=\"ltx_cite ltx_citemacro_citet\">Fu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib11\" title=\"\">2024</a>)</cite> modify the encoder structure\nand conduct continual pre-training to adapt the model for streaming applications.</p>\n\n",
                "matched_terms": [
                    "model",
                    "data",
                    "chiu",
                    "baevski",
                    "2020b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all those methods aforementioned, a dedicated pre-trained model has to be built for the streaming scenario instead of sharing the same model with the offline application.\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we focus on building an encoder suitable for both streaming and offline modes,\nwith a chunkwise self-supervised learning (Chunk SSL) framework as depicted in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>.\nChunk SSL aims to\nrestore discrete indices of the masked frames based on unmasked frames in the last chunk and previous chunks.\nThe discrete indices are estimated with finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> (FSQ) with vocabulary size up to millions&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).The pre-training FSQ token has more fine-grained resolution compared with the downstream modeling units, such as phonemes or sentencepiece tokens,\nwhich usually are with vocabulary size ranging from tens to tens of thousands.\nWe hypothesize that speech frames associated with a high resolution FSQ token are mainly mapped to an unique modeling unit in the downstream task and it makes the knowledge transfer easier from the pre-training stage to the fine-tuning stage.\nHowever, those high resolution FSQ tokens pose a great challenge for modeling and we propose to decompose a large codebook\ninto small channel based sub-codebooks to alleviate the memory and computation cost during pre-training. Details could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "large",
                    "chunk",
                    "ssl",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, speech features are first extracted from the input audio, and grouped into equal sized chunks.\nInstead of calculating those chunks from left to right sequentially, a copy and append data augmentation (CADA) is introduced to parallelize the computation for all chunks in the same utterance (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S2\" title=\"2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).\nCADA augments the input sequence by copying input chunks and appending them to the end of utterance as extended chunks.\nMasking is applied to frames in extended chunks only.\nAugmented utterances are then processed by a CADA compatible Conformer encoder, which could handle those augmented chunks properly even though frames in extended chunks are with altered location information. Finally,\nthe model is optimized by restoring FSQ indices of those masked frames from Conformer encoder outputs.\nExperiments on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> datasets indicate that\nthe same model initialized with Chunk SSL could achieve very competitive results\non both streaming and offline speech to text tasks.\nIt shows that Chunk SSL eliminates the need to build a dedicated streaming and dedicated offline model.\nTo summarize, our contributions includes:</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "librispeech",
                    "data",
                    "ssl",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A copy and append data augmentation improves the pre-training efficiency by reusing the chunk level computation results and parallelizing the chunkwise based computation.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results show the proposed method can build one model for both scenarios and achieve competitive results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> datasets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The naive Chunk SSL algorithm, which recovers masked frame indices in the right most chunk based on the unmasked frames in the same chunk and preceding chunks, is executed chunk by chunk in a sequential order instead of computing all chunks from one utterance in a parallel fashion.\nInspired by the implementation of the chunk encoder with a look-ahead chunk for the streaming speech translation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib24\" title=\"\">2021</a>)</cite>,\nwe propose a copy and append data augmentation for the Chunk SSL,\nwhich reorganizes\ninput data and changes the augmented data computation, but it is still strictly equivalent to the computation in the naive Chunk SSL algorithm.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training procedure is described in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. Input features are first downsampled with a stacked subsampling module. The outputs are then divided into chunks with fixed duration, They are considered as base chunks.\nThe consecutive chunk of each base chunk is copied and appended to the end of the utterance in a sequential order. Those newly created chunks are called extended chunks.\nFor example, there are 12 speech input frames in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a> after subsampling. Assuming the chunk size is 4 frames, the speech features are segmented into three base chunks (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#00FFFF;\">cyan</span>): &#8220;B1&#8221;, &#8220;B2&#8221; and &#8220;B3&#8221;.\nChunk &#8220;B2&#8221; and &#8220;B3&#8221; are copied and appended to the end of the utterance as &#8220;E1&#8221; and &#8220;E2&#8221; (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#BFFF00;\">lime</span>).\nThey correspond to the extended chunks of &#8220;B1&#8221; and &#8220;B2&#8221; respectively.\nThere is no extended chunk for the last chunk (&#8220;B3&#8221;).\nExtended chunks act as right most chunks with different preceding chunks in the naive Chunk SSL algorithm and\nmasking is only applied on frames of extended chunks.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "ssl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of input frames in an utterance and is a multiple of chunk size <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>.\nThe augmented input sequence length is <math alttext=\"N^{\\prime}=2N-C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>N</mi><mo>&#8242;</mo></msup><mo>=</mo><mrow><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow><mo>&#8722;</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">N^{\\prime}=2N-C</annotation></semantics></math>, the number of base chunks is <math alttext=\"M=N/C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/C</annotation></semantics></math> and the number of extended chunks is <math alttext=\"M-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M-1</annotation></semantics></math>.\nWe define <math alttext=\"m(i)=\\lfloor{i/C}\\rfloor\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">&#8970;</mo><mrow><mi>i</mi><mo>/</mo><mi>C</mi></mrow><mo stretchy=\"false\">&#8971;</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m(i)=\\lfloor{i/C}\\rfloor</annotation></semantics></math> as the chunk index of frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. If the left context is infinite, i.e., the model could access all history information, a CADA masking matrix <math alttext=\"\\bm{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">\\bm{\\alpha}</annotation></semantics></math> is defined as</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the second row represents a frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> from base chunks can also access limited future information from its extended chunk</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the fourth row means a frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i4.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> from extended chunks can access all information in its base chunk and preceding chunks of the base chunk</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S2.F2.sf1\" title=\"Figure 2(a) &#8227; Figure 2 &#8227; 2.1 CADA self-attention module &#8227; 2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> depicts a CADA masking example for an utterance with 6 frames before CADA. The first column is the frame index. The number in the bracket (<span class=\"ltx_text\" style=\"--ltx-fg-color:#BF8040;\">brown</span>) stands for the extended chunk frame&#8217;s original position, i.e., the position of the base chunk frame that the frame is copied from.\n<math alttext=\"{\\bm{\\alpha}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">{\\bm{\\alpha}}</annotation></semantics></math> for the extended chunks are presented in italic fonts. A self-attention module equipped with a CADA masking matrix can process a CADA sequence with position information preserved.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A convolutional layer has a set of filters which slide across input features within the same utterance and extract localized representations. A convolutional layer assumes the input frames organized as consecutively, which no longer holds true for extended chunk frames in a CADA utterance.\nThis problem can be resolved via a chunk based convolutional computation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib23\" title=\"\">2023</a>)</cite>.\nWithout loss of generality, we assume the convolution module is a depth-wise convolution layer with <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> left and <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> right context frames. The modified computation includes three steps.\nFirst, we separate the CADA utterance into chunks and pair a base chunk and its extended chunk together, for example &#8220;B1&#8221; and &#8220;E1&#8221; in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>; then the last <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> frames from the chunk preceding the base chunk\nare appended to the left as left context. If the base chunk is the first chunk, <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math>&#160;zero padding frames are attached. Similarly, <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> zero padding frames are appended to the right and it leads to a new concatenated subsequence with length <math alttext=\"L_{lc}+2C+L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi></mrow><mo>+</mo><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">L_{lc}+2C+L_{rc}</annotation></semantics></math>.\nSecond, the concatenated subsequence is fed to the convolution module and obtains output subsequence with length <math alttext=\"2C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">2C</annotation></semantics></math>. <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> and <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> context frames are absorbed during convolution\ncomputation. The first <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> output frames correspond to the base chunk and the second <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m11\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> frames are for the extended chunk.\nFinally, those output chunks are placed back to their original positions in the augmented utterance as the outputs from the CADA convolutional module.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S2.F2.sf2\" title=\"Figure 2(b) &#8227; Figure 2 &#8227; 2.1 CADA self-attention module &#8227; 2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> demonstrates three steps to conduct convolution computation on an augmented utterance with base sequence length 12 and chunk size 4.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Dynamic chunk training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib40\" title=\"\">2020a</a>; Weninger et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib39\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib23\" title=\"\">2023</a>)</cite> is a useful fine-tuning approach for the chunk based ASR encoder.\nDuring training, the chunk duration varied from hundreds of million seconds to a few seconds is randomly chosen for every model update. A model built with dynamic chunk training is suitable for both streaming and offline modes.\nWe extend dynamic chunk training for the Chunk SSL pre-training. Chunk duration is chosen from 6 durations (<math alttext=\"[640,1280,1920,2560,3200,3840]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>640</mn><mo>,</mo><mn>1280</mn><mo>,</mo><mn>1920</mn><mo>,</mo><mn>2560</mn><mo>,</mo><mn>3200</mn><mo>,</mo><mn>3840</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[640,1280,1920,2560,3200,3840]</annotation></semantics></math> million seconds) randomly for every model update.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "zhang",
                    "ssl",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, we leverage finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> to quantize\ninput frames and obtain their discrete indices.\nA FSQ module is built prior to the Chunk SSL pre-training.\nIn the quantization module, an utterance based channel mean and variance normalization is first applied to input frames <math alttext=\"{\\bm{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119935;</mi><annotation encoding=\"application/x-tex\">{\\bm{X}}</annotation></semantics></math> to obtain <math alttext=\"\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#119935;</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119961;</mi><mo>^</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}</annotation></semantics></math>, then it is processed by a FSQ encoder <math alttext=\"\\text{Enc}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mtext>Enc</mtext><mi>f</mi></msub><annotation encoding=\"application/x-tex\">\\text{Enc}_{f}</annotation></semantics></math> and projected into a low-dimension space with size <math alttext=\"d^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>d</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">d^{\\prime}</annotation></semantics></math>, where <math alttext=\"d^{\\prime}\\ll d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>d</mi><mo>&#8242;</mo></msup><mo>&#8810;</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d^{\\prime}\\ll d</annotation></semantics></math>. The output of each channel <math alttext=\"r\\in[1,d^{\\prime}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><msup><mi>d</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in[1,d^{\\prime}]</annotation></semantics></math> for input frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is rounded to an integer <math alttext=\"h_{i,r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,r}</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder is optimized with masked prediction loss. We encourage the Chunk SSL model to reconstruct the masked frames based on the context information provided.\nMore specifically, we mask half consecutive frames in every extended chunk. The start position of the masking frame is randomly chosen from <math alttext=\"[0,\\frac{C}{4}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mfrac><mi>C</mi><mn>4</mn></mfrac><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,\\frac{C}{4}]</annotation></semantics></math> at every extended chunk, so we have enough unmasked frames on both sides of the masked frames as context to infer those masked frames.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming the Chunk SSL model outputs for the augmented sequence is <math alttext=\"{\\bm{O}}=\\{{\\bm{o}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119926;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119952;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{O}}=\\{{\\bm{o}}_{i}\\}</annotation></semantics></math>. The FSQ index of <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is denoted as <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math> and\n<math alttext=\"{\\mathbf{e}}_{\\mu_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><msub><mi>&#956;</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">{\\mathbf{e}}_{\\mu_{i}}</annotation></semantics></math> is the output embedding of <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math>. The masked prediction loss is defined as</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize a high resolution FSQ codebook would be beneficial for downstreaming tasks due to two reasons. First, FSQ does not suffered from codebook collapse and can achieve high codebook usage.\nSecond, a high resolution codebook divides the feature space into more fine-grained bins and frames sharing the same token index might be more closer to each other compared with frames associated with a token index from a low resolution codebook.\nWhen the FSQ codebook is large enough, each FSQ token could be mainly associated with one modeling unit, such as phoneme, in the downstreaming task, hence it will make the knowledge transfer easier from the pre-training stage to the fine-tuning stage. This hypothesis is examined in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S5.SS2\" title=\"5.2 Impact of FSQ codebook sizes &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.\nHowever, a high resolution FSQ codebook poses a great challenge for optimization.\nFor example, the codebook with vocabulary size 1,265,625 and embedding size 512 would take about 2.4G memory if they are stored in float data type.\nIn order to alleviate this issue, we propose to decompose the codebook into a group of channel based sub-codebooks and compute prediction loss for\neach group one by one. Given frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, the sub-codebook index at <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>th channel is <math alttext=\"h_{i,r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,r}</annotation></semantics></math>.\nWe define the group masked prediction loss as</p>\n\n",
                "matched_terms": [
                    "data",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pre-training</span>:\nThere are two pre-training tasks in this work, i.e., FSQ SSL training and Chunk SSL training. We first build FSQ module to discretize the input\nspeech feature and then pre-train speech encoder with Chunk SSL to generate contextual representation.\n<span class=\"ltx_text ltx_font_smallcaps\">Libri-light</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib18\" title=\"\">2019</a>)</cite>, which includes 60k hours of unlabelled English speech data, is used in both pre-training tasks.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk",
                    "data",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Tuning</span>: For speech recognition task, the pre-trained models are finetuned and evaluated on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib29\" title=\"\">2015</a>)</cite> dataset. We use two <span class=\"ltx_text ltx_font_typewriter\">dev</span> sets for development and report all results from <span class=\"ltx_text ltx_font_typewriter\">dev</span> and <span class=\"ltx_text ltx_font_typewriter\">test</span> sets.\nFor speech translation task, experiments are conducted on two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib12\" title=\"\">2019</a>)</cite>\nlanguage pairs: English to German (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>DE) and English to Spanish (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ES). Sequence level knowledge\ndistillation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Rush, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib20\" title=\"\">2016</a>)</cite> is applied to boost the speech translation quality.\nThe models are developed on\nthe <span class=\"ltx_text ltx_font_typewriter\">dev</span> set, and the final results are reported on the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> set.</p>\n\n",
                "matched_terms": [
                    "models",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speech encoder is a Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib15\" title=\"\">2020</a>)</cite> based chunk encoder.\nThe encoder is equipped with a relative positional embedding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib32\" title=\"\">2018</a>)</cite> and starts with a stacking layer to down-sample the input features by four times.\nTwo model configurations: base and large, have been explored. The base encoder has 12 Conformer layers, input embedding size of 512, 8 attention heads, feedforward layer dimension 2048 and convolutional module kernel size 31. The large encoder has 24 Conformer layers, input embedding size of 768, 16 attention heads, feedforward layer dimension 3072 and convolutional module kernel size 5.\nTransducer is adopted in the fine-tuning experiments.\nThe predictor module is with one Transformer layer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> for speech recognition tasks and two layers for speech translation tasks.\nThe input embedding and feedforward layer dimension are the same as ones in the encoder setting if not mentioned specifically. The joint module is a feedforward layer as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib41\" title=\"\">2020b</a>)</cite> with embedding dimension 1024.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "base",
                    "model",
                    "zhang",
                    "large",
                    "2020b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Input speech is represented as 80-dimensional log mel-filterbank coefficients computed every 10ms with a 25ms window. Global channel mean and variance normalization is applied so the trained model could be used for both streaming and offline scenarios.\nWe set the maximum utterance duration to 75 seconds and minimum duration to 5 seconds in pre-training.\nDuring fine-tuning,\na look-ahead chunk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib33\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib24\" title=\"\">2021</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib35\" title=\"\">2023</a>)</cite> is utilized and\ndynamic chunk training is employed by default. We alternate between offline training with infinite chunk size, and streaming training with chunk duration sampled from [160, 320, 640, 960, 1280, 1600] ms randomly within each epoch.\nMore details about optimization, such as batch sizes and training schedulers, for different experiments are presented in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A1\" title=\"Appendix A Optimization configures &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nThe target labels are encoded with SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib21\" title=\"\">2018</a>)</cite>. For both speech recognition and speech translation tasks, the vocabulary is an unigram model with size 1024 and full character coverage on the corresponding training text data.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "data",
                    "trained",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results are evaluated using an averaged model from checkpoints of the best 10 epochs.\nSpeech recognition experiments are evaluated with WER while speech translation results are measure with scacre BLEU score<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>signature: nrefs:1<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>case:mixed<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>eff:no<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>tok:13a<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>smooth:exp<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>version:2.4.2</span></span></span>.\nWe report both streaming and offline results.\nFor the streaming decoding, we set the chunk size to 320ms by default if not mentioned specifically. No language model is used in all experiments.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "offline",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the speech translation experiments, we evaluate on both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; directions and the results are listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. The baselines are initialized with a speech recognition acoustic encoder (&#8220;ASR&#8221; in the column &#8220;Init.&#8221;), which is trained with the English data in the <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> English-Spanish direction. Both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; results show that the encoder initialized with Chunk SSL outperforms the one initialized with a speech recognition encoder trained with <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> data only. We could draw similar conclusion as the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> experiment that the Chunk SSL could effectively improve speech translation quality for both streaming and offline cases.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "librispeech",
                    "data",
                    "ssl",
                    "trained",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to verify our a high resolution FSQ codebook hypothesis discussed in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>,\nwe study FSQ tokenizers with different codebook sizes. Two aspects are examined: first the agreement among FSQ tokens and phonemes, and second WERs for ASR models initialized with different FSQ tokenizers.\nFor the agreement among FSQ tokens and phonemes, we compare the overlap of features associated with different tokens and phonemes. It can be measured by phone purity and phone-normalized mutual information (PNMI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib16\" title=\"\">2021a</a>)</cite>. Phone purity denotes the conditional probability of a phoneme given a FSQ index while PNMI measures the percentage of uncertainty about a phoneme label eliminated after observing a specific FSQ token.\nWe generate phoneme based alignment with Montreal Aligner&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib27\" title=\"\">2017</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/MontrealCorpusTools/mfa-models/releases/download/acoustic-english_us_arpa-v3.0.0/english_us_arpa.zip</span></span></span>\non two <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> <span class=\"ltx_text ltx_font_typewriter\">dev</span> sets.</p>\n\n",
                "matched_terms": [
                    "wers",
                    "models",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> lists models trained with different codebook sizes with the base configure. The first column gives FSQ vocabulary sizes, the second column provides the Chunk SSL training time (in seconds) for 1000 updates.\n&#8220;levels&#8221; is the FSQ levels for quantization. For example, &#8220;4(<math alttext=\"8\\times 1-5\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow></mrow><annotation encoding=\"application/x-tex\">8\\times 1-5\\times 3</annotation></semantics></math>)&#8221; in the first row means there are 4 output channels, the first channel is with level 8 and the remaining 3 channels are with level 5. They span a codebook with size 1000.\n&#8220;phn pur.&#8221; stands for phone purity.</p>\n\n",
                "matched_terms": [
                    "models",
                    "chunk",
                    "base",
                    "ssl",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first two rows listed models trained with FSQ vocabulary size 1000. &#8220;1000*&#8221; means the model is optimized with a full codebook as&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E4\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, while &#8220;1000&#8221; indicates the model is pretrained with the group masked prediction loss following&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E5\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>. The results shows two models\nachieve comparable accuracies\nwith similar pre-training time.\nAs shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, when the codebook size increases, &#8220;phn pur.&#8221; and &#8220;PNMI&#8221; increase steadily.\nBoth metrics show tokens in the codebook with a high resolution have better consistency with phonemes. This confirms our assumption that frames associated with a specific FSQ token are more likely being labelled with the same phoneme if the codebook resolution is high enough.\nOn the other hand, we observe WER is reduced when FSQ codebook size becomes bigger.\nIt is peaked for the codebook size around 6 millions.\nWhen the codebook size gets extremely big, i.e. 791 millions in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, the &#8220;dev-other&#8221; WER increases from 5.0 to 5.4.\nOur hypothesis is that an extremely large codebook makes the optimization harder, and more training time might be required to get good representation.\nBased on above observations, we conclude that a codebook with high resolution helps Chunk SSL to transfer knowledge to the downstream task. However, an extremely large FSQ codebook might make the pre-training hard and hurt the downstream task.</p>\n\n",
                "matched_terms": [
                    "models",
                    "chunk",
                    "wer",
                    "model",
                    "large",
                    "ssl",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The next thing we examined is computation cost.\nWe don&#8217;t observe any statistical difference for the FSQ training time when the codebook size changes, since majority of computation time is spent on data input/output, encoder and decoder Transformer/Conformer layer computation. On the other hand, a Chunk SSL training with a large codebook can increase the training time slightly that we find\nabout 10% training time increase when the codebook size grows from 1000 to 6 millions as shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk",
                    "data",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the base Transducer model to analyze the impact of different chunk sizes on latency and performance. We use Length-Adaptive Average Lagging(LAAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib30\" title=\"\">2022</a>)</cite> as latency scorer, which is evaluated with the Simuleval toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib26\" title=\"\">2020</a>)</cite>. We employ greedy decoding instead of beam search decoding for simplicity in this study.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a> depicts the model performance (<span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">blue</span>) and latency (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">red</span>) under different chunk sizes for speech recognition&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>) and speech translation&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf3\" title=\"Figure 3(c) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>). The speech recognition is evaluated on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> <span class=\"ltx_text ltx_font_typewriter\">dev-other</span> set and speech translation tasks are evaluated on the corresponding <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> <span class=\"ltx_text ltx_font_typewriter\">dev</span> set.\nIt can be seen that the model is with a small latency (LAAL 0.47 second) but high WER (7.6) when the chunk size is small (160ms). The performance improves steadily when the chunk size increases but at the cost of increasing latency. The speech translation experiments have similar observations. The results indicate the model can switch from streaming mode to offline mode smoothly as the chunk sizes change.\nOther latency evaluation results, such as Average Lagging (AL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib25\" title=\"\">2019</a>)</cite> and Differentiable Average Lagging (DAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arivazhagan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib1\" title=\"\">2019</a>)</cite>, could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A2\" title=\"Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a></p>\n\n",
                "matched_terms": [
                    "chunk",
                    "wer",
                    "librispeech",
                    "model",
                    "devother",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech self-supervised learning algorithms form contextual representations through learning to predict\nmissing information due to time constraint or masking. <cite class=\"ltx_cite ltx_citemacro_citet\">van&#160;den Oord et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib37\" title=\"\">2018</a>)</cite> propose an auto-regressive based SSL approach to predict the future samples based on the previous samples. Speech SSL methods such as wav2vec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>, HuBert&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>)</cite> and BEST-RQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> choose to randomly mask audio span and the model learns to restore those masked regions. Chunk SSL combines both approaches that it masks audio span in the extended chunk\nand recovers masking frame indices in an chunk based auto-regressive manner.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "2020b",
                    "chiu",
                    "baevski",
                    "ssl",
                    "wav2vec2",
                    "bestrq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Vector quantization (VQ) is one of the important components in the speech SSL. <cite class=\"ltx_cite ltx_citemacro_citet\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib3\" title=\"\">2020a</a>)</cite> leverages Gumbel-Softmax with multiple variable groups to discretize\naudio features. Methods such as diversity loss have to take to alleviate codeword collapse issue.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>)</cite> resorts k-means to cluster the encoder outputs to build the codebook. In order to achieve a good representation, an iterative training is required to keep re-building the encoder with a new codebook.\nBEST-RQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> chooses an alternative way, which initializes the codebook randomly and no further update is applied during SSL.\nCompared with these VQ methods aforementioned, FSQ does not suffer from codebook collapse and achieves high codebook usage&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "chiu",
                    "baevski",
                    "ssl",
                    "bestrq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, a chunkwise speech self-supervised learning approach is proposed. The model is trained to\nlearn to reconstruct the masked information in the right most chunk based on unmasked frames in the same chunk and preceding chunks. An efficient copy and append data augmentation method is proposed to\nparallel chunk-wise computation.\nFSQ is employed to generate high resolution discrete tokens and a group based decomposition method is proposed to alleviate the computation challenge.\nThe model is trained with varied chunk durations to fit\nthe different scenarios, hence a pre-trained model could be used for both streaming and offline\napplications. Our experiments show that a model initialized with Chunk SSL could achieve very\ncompetitive offline results and excellent streaming results on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> translation directions.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "librispeech",
                    "data",
                    "ssl",
                    "trained",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FSQ model is built with learning rate is 2.0e-4 and ExponentialLR scheduler from PyTorch. The effective batch size is 3200 seconds with up to 180,000 updates. The FSQ module is optimized with MSE loss to restore mel-filterbank features from their quantized audio representations. It takes 4 A10 GPUs for 42 hours to build a FSQ model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training is scheduled as Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> with warmup step 25,000 and maximum learning rate 3e-4. The batch size is 16,000 seconds for the base configure model. We choose a smaller effective batch size for the large configure model with 6,000 seconds due to computation resource limitation. The model is trained up to 400,000 updates. It takes about 10 days for 8 A100 GPUs to build a Chunk SSL encoder with the base configure.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "base",
                    "model",
                    "large",
                    "ssl",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">960 hours fine-tuning</span>\nFor the base configure model, we choose the three stages scheduler scheme as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>, i.e., warmup, hold and annealing. The warmup stage takes about 1000 updates with encoder parameters frozen, the hold stage takes about 40,000 updates and the remaining 60,000 updates are in an annealing stage. The peak learning rate in fine-tuning is 2.0e-3.\nThe SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib31\" title=\"\">2019</a>)</cite> data augmentation is applied with 2 frequency maskings of 27, and 10 time maskings at a maximum ratio of 0.05 in fine-tuning experiments.\nThe effective batch size for fine-tuning is 10,000 seconds. For the large configure model, we choose Transformer scheduler&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> since it has less hype-parameters to explore. The warmup step is 5000 with encoder parameters frozen. Similar as the base configure model, the total update step is up to 100,000. The peak learning rate is 5e-4. The effective batch size for the large model is 7200 seconds.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "base",
                    "model",
                    "large",
                    "data",
                    "baevski",
                    "2020b"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Streaming WERs for models trained on the 960 hours LibriSpeech data (WER ↓\\downarrow).",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">size(M)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">dev-clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">dev-other</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">test-clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">test-other</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">ave.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">100</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">600</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">9.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">9.4</td>\n<td class=\"ltx_td ltx_align_center\">6.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">wav2vec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">600</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">8.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">7.9</td>\n<td class=\"ltx_td ltx_align_center\">5.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">w2v-BERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">600</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">8.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">8.1</td>\n<td class=\"ltx_td ltx_align_center\">5.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">BEST-RQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">600</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">6.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">6.6</td>\n<td class=\"ltx_td ltx_align_center\">4.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Chunk SSL Base</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">79</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">6.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">6.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Chunk SSL Large</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">337</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">2.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">5.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">2.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">5.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "testclean",
            "wer",
            "wers",
            "librispeech",
            "ave",
            "chiu",
            "↓downarrow",
            "bestrq",
            "hours",
            "devclean",
            "base",
            "streaming",
            "trained",
            "chunk",
            "sizem",
            "devother",
            "testother",
            "conformer",
            "wav2vec2",
            "w2vbert",
            "model",
            "large",
            "data",
            "ssl"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The pre-trained Chunk SSL models are fine-tuned with dynamic chunk training and they could be used for both streaming and offline ASR.\nWe list the offline and streaming recognition results from the Chunk SSL models in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>.\nThe reported literature results are presented in the first part of the tables.\nIt is clear that the chunk SSL pre-trained models could achieve very competitive results compared to other strong baselines.\nThe base and large models outperform the corresponding wav2vec2 base and large models.</p>\n\n",
            "<p class=\"ltx_p\">In the streaming evaluation, the Chunk SSL model with the base configuration excels all models reported in the literature shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>. The large configuration model pushes the WER even lower and it achieves another 0.5 average WER reduction compared to the base configure model. Comparing results in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>,\nthe proposed method significantly reduces the performance gap between the streaming and offline applications. The WER gap between the large configure\nmodels is 0.8 in average for Chunk SSL while the average WER difference is 2.5 for BEST-RQ.\nNote, literature models listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a> are initialized with dedicated pre-trained models either offline or streaming, while a Chunk SSL model could be initialized with the same pre-trained model and operated in both modes.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Low latency speech human-machine communication is becoming increasingly necessary as speech technology advances quickly in the last decade.\nOne of the primary factors behind the advancement of speech technology is self-supervised learning.\nMost self-supervised learning algorithms are designed with full utterance assumption and compromises have to made if partial utterances are presented, which are common in the streaming applications.\nIn this work, we propose a chunk based self-supervised learning (Chunk SSL) algorithm as an unified solution for both streaming and offline speech pre-training.\nChunk SSL is optimized with the masked prediction loss and\nan acoustic encoder is encouraged to restore indices of those masked speech frames with help from unmasked frames in the same chunk and preceding chunks.\nA copy and append data augmentation approach is proposed to conduct efficient chunk based pre-training.\nChunk SSL utilizes a finite scalar quantization (FSQ) module to discretize input speech features and our study shows a high resolution FSQ codebook, i.e., a codebook with vocabulary size up to a few millions, is beneficial to transfer knowledge from the pre-training task to the downstream tasks. A group masked prediction loss is employed during pre-training to alleviate the high memory and computation cost introduced by the large codebook.\nThe proposed approach is examined in two speech to text tasks, i.e., speech recognition and speech translation.\nExperimental results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> datasets show that the proposed method could achieve\nvery competitive results for speech to text tasks at both streaming and offline modes.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "librispeech",
                    "large",
                    "streaming",
                    "data",
                    "ssl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Transcribe speech into text in real time is critical for applications requiring immediate feedback, such as real-time transcription of broadcast contents, voice assistants, and simultaneous speech translation etc.\nIt requires processing audio incrementally as it is received, rather than waiting for the entire utterance to be available.\nFor neural network based end-to-end systems,\nthe requirement includes two-fold. First, a speech encoder, such as causal encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib41\" title=\"\">2020b</a>)</cite> and chunk encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tsunoo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib36\" title=\"\">2019</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib33\" title=\"\">2021</a>)</cite>, is able to process input audio cumulatively without dependency on the future input.\nSecond, a decoder is required to decode transcription incrementally based on partial encoder outputs.\nFrame-Synchronous decoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib10\" title=\"\">2020</a>)</cite>, such as CTC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib14\" title=\"\">2013</a>)</cite> Transducer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Graves, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib13\" title=\"\">2012</a>)</cite> CIF&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong &amp; Xu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib9\" title=\"\">2020</a>)</cite> and TAED&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib35\" title=\"\">2023</a>)</cite>, are capable of generating transcription based on partial encoder outputs and meet the streaming requirement naturally.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Self-supervised pre-training leverages vast amounts of unlabeled data and learn universal feature representations for downstream tasks, especially for tasks with limited supervised training data.\nSpeech pre-training methods have emerged as the backbone of many speech processing tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib8\" title=\"\">2018</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib5\" title=\"\">2021</a>; Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib34\" title=\"\">2022</a>; Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib2\" title=\"\">2025</a>)</cite>.\nThose methods are designed for the\nspeech applications with full utterance available, and compromises have to be made if the downstream tasks are streaming applications.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> propose to conduct pre-training with a causal encoder, which sacrifices right context information. <cite class=\"ltx_cite ltx_citemacro_citet\">Fu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib11\" title=\"\">2024</a>)</cite> modify the encoder structure\nand conduct continual pre-training to adapt the model for streaming applications.</p>\n\n",
                "matched_terms": [
                    "chiu",
                    "streaming",
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all those methods aforementioned, a dedicated pre-trained model has to be built for the streaming scenario instead of sharing the same model with the offline application.\n</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we focus on building an encoder suitable for both streaming and offline modes,\nwith a chunkwise self-supervised learning (Chunk SSL) framework as depicted in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>.\nChunk SSL aims to\nrestore discrete indices of the masked frames based on unmasked frames in the last chunk and previous chunks.\nThe discrete indices are estimated with finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> (FSQ) with vocabulary size up to millions&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).The pre-training FSQ token has more fine-grained resolution compared with the downstream modeling units, such as phonemes or sentencepiece tokens,\nwhich usually are with vocabulary size ranging from tens to tens of thousands.\nWe hypothesize that speech frames associated with a high resolution FSQ token are mainly mapped to an unique modeling unit in the downstream task and it makes the knowledge transfer easier from the pre-training stage to the fine-tuning stage.\nHowever, those high resolution FSQ tokens pose a great challenge for modeling and we propose to decompose a large codebook\ninto small channel based sub-codebooks to alleviate the memory and computation cost during pre-training. Details could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "streaming",
                    "chunk",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, speech features are first extracted from the input audio, and grouped into equal sized chunks.\nInstead of calculating those chunks from left to right sequentially, a copy and append data augmentation (CADA) is introduced to parallelize the computation for all chunks in the same utterance (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S2\" title=\"2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).\nCADA augments the input sequence by copying input chunks and appending them to the end of utterance as extended chunks.\nMasking is applied to frames in extended chunks only.\nAugmented utterances are then processed by a CADA compatible Conformer encoder, which could handle those augmented chunks properly even though frames in extended chunks are with altered location information. Finally,\nthe model is optimized by restoring FSQ indices of those masked frames from Conformer encoder outputs.\nExperiments on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> datasets indicate that\nthe same model initialized with Chunk SSL could achieve very competitive results\non both streaming and offline speech to text tasks.\nIt shows that Chunk SSL eliminates the need to build a dedicated streaming and dedicated offline model.\nTo summarize, our contributions includes:</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "librispeech",
                    "streaming",
                    "data",
                    "ssl",
                    "conformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A copy and append data augmentation improves the pre-training efficiency by reusing the chunk level computation results and parallelizing the chunkwise based computation.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results show the proposed method can build one model for both scenarios and achieve competitive results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> datasets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The naive Chunk SSL algorithm, which recovers masked frame indices in the right most chunk based on the unmasked frames in the same chunk and preceding chunks, is executed chunk by chunk in a sequential order instead of computing all chunks from one utterance in a parallel fashion.\nInspired by the implementation of the chunk encoder with a look-ahead chunk for the streaming speech translation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib24\" title=\"\">2021</a>)</cite>,\nwe propose a copy and append data augmentation for the Chunk SSL,\nwhich reorganizes\ninput data and changes the augmented data computation, but it is still strictly equivalent to the computation in the naive Chunk SSL algorithm.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "streaming",
                    "chunk",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training procedure is described in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. Input features are first downsampled with a stacked subsampling module. The outputs are then divided into chunks with fixed duration, They are considered as base chunks.\nThe consecutive chunk of each base chunk is copied and appended to the end of the utterance in a sequential order. Those newly created chunks are called extended chunks.\nFor example, there are 12 speech input frames in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a> after subsampling. Assuming the chunk size is 4 frames, the speech features are segmented into three base chunks (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#00FFFF;\">cyan</span>): &#8220;B1&#8221;, &#8220;B2&#8221; and &#8220;B3&#8221;.\nChunk &#8220;B2&#8221; and &#8220;B3&#8221; are copied and appended to the end of the utterance as &#8220;E1&#8221; and &#8220;E2&#8221; (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#BFFF00;\">lime</span>).\nThey correspond to the extended chunks of &#8220;B1&#8221; and &#8220;B2&#8221; respectively.\nThere is no extended chunk for the last chunk (&#8220;B3&#8221;).\nExtended chunks act as right most chunks with different preceding chunks in the naive Chunk SSL algorithm and\nmasking is only applied on frames of extended chunks.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "ssl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A speech encoder, such as Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> and Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib15\" title=\"\">2020</a>)</cite>, contains two types of modules. One is the intra-frame module where the computation is based on one frame and no location information or other frames are required, for example, LayerNorm and feedforward layers.\nThe other type is the inter-frame module where the computation is related to the input location and requires information from neighbouring frames.\nThe inter-frame modules include self-attention and convolutional layers.\nAn augmented utterance aforementioned, can be processed as a normal utterance in the intra-frame modules and no modification is required.\nOn the other hand, the position information is invalid among extended chunk frames, since they are copied and appended to the end of utterance.\nModifications are introduced for two inter-frame modules: self-attention and convolutional layers, to handle inconsistent position information in augmented utterances as described in\nthe following subsections.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "conformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of input frames in an utterance and is a multiple of chunk size <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>.\nThe augmented input sequence length is <math alttext=\"N^{\\prime}=2N-C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>N</mi><mo>&#8242;</mo></msup><mo>=</mo><mrow><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow><mo>&#8722;</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">N^{\\prime}=2N-C</annotation></semantics></math>, the number of base chunks is <math alttext=\"M=N/C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/C</annotation></semantics></math> and the number of extended chunks is <math alttext=\"M-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M-1</annotation></semantics></math>.\nWe define <math alttext=\"m(i)=\\lfloor{i/C}\\rfloor\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">&#8970;</mo><mrow><mi>i</mi><mo>/</mo><mi>C</mi></mrow><mo stretchy=\"false\">&#8971;</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m(i)=\\lfloor{i/C}\\rfloor</annotation></semantics></math> as the chunk index of frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. If the left context is infinite, i.e., the model could access all history information, a CADA masking matrix <math alttext=\"\\bm{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">\\bm{\\alpha}</annotation></semantics></math> is defined as</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the second row represents a frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> from base chunks can also access limited future information from its extended chunk</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the fourth row means a frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i4.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> from extended chunks can access all information in its base chunk and preceding chunks of the base chunk</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S2.F2.sf1\" title=\"Figure 2(a) &#8227; Figure 2 &#8227; 2.1 CADA self-attention module &#8227; 2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> depicts a CADA masking example for an utterance with 6 frames before CADA. The first column is the frame index. The number in the bracket (<span class=\"ltx_text\" style=\"--ltx-fg-color:#BF8040;\">brown</span>) stands for the extended chunk frame&#8217;s original position, i.e., the position of the base chunk frame that the frame is copied from.\n<math alttext=\"{\\bm{\\alpha}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">{\\bm{\\alpha}}</annotation></semantics></math> for the extended chunks are presented in italic fonts. A self-attention module equipped with a CADA masking matrix can process a CADA sequence with position information preserved.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A convolutional layer has a set of filters which slide across input features within the same utterance and extract localized representations. A convolutional layer assumes the input frames organized as consecutively, which no longer holds true for extended chunk frames in a CADA utterance.\nThis problem can be resolved via a chunk based convolutional computation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib23\" title=\"\">2023</a>)</cite>.\nWithout loss of generality, we assume the convolution module is a depth-wise convolution layer with <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> left and <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> right context frames. The modified computation includes three steps.\nFirst, we separate the CADA utterance into chunks and pair a base chunk and its extended chunk together, for example &#8220;B1&#8221; and &#8220;E1&#8221; in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>; then the last <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> frames from the chunk preceding the base chunk\nare appended to the left as left context. If the base chunk is the first chunk, <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math>&#160;zero padding frames are attached. Similarly, <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> zero padding frames are appended to the right and it leads to a new concatenated subsequence with length <math alttext=\"L_{lc}+2C+L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi></mrow><mo>+</mo><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">L_{lc}+2C+L_{rc}</annotation></semantics></math>.\nSecond, the concatenated subsequence is fed to the convolution module and obtains output subsequence with length <math alttext=\"2C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">2C</annotation></semantics></math>. <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> and <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> context frames are absorbed during convolution\ncomputation. The first <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> output frames correspond to the base chunk and the second <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m11\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> frames are for the extended chunk.\nFinally, those output chunks are placed back to their original positions in the augmented utterance as the outputs from the CADA convolutional module.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S2.F2.sf2\" title=\"Figure 2(b) &#8227; Figure 2 &#8227; 2.1 CADA self-attention module &#8227; 2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> demonstrates three steps to conduct convolution computation on an augmented utterance with base sequence length 12 and chunk size 4.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Dynamic chunk training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib40\" title=\"\">2020a</a>; Weninger et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib39\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib23\" title=\"\">2023</a>)</cite> is a useful fine-tuning approach for the chunk based ASR encoder.\nDuring training, the chunk duration varied from hundreds of million seconds to a few seconds is randomly chosen for every model update. A model built with dynamic chunk training is suitable for both streaming and offline modes.\nWe extend dynamic chunk training for the Chunk SSL pre-training. Chunk duration is chosen from 6 durations (<math alttext=\"[640,1280,1920,2560,3200,3840]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>640</mn><mo>,</mo><mn>1280</mn><mo>,</mo><mn>1920</mn><mo>,</mo><mn>2560</mn><mo>,</mo><mn>3200</mn><mo>,</mo><mn>3840</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[640,1280,1920,2560,3200,3840]</annotation></semantics></math> million seconds) randomly for every model update.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "streaming",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, we leverage finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> to quantize\ninput frames and obtain their discrete indices.\nA FSQ module is built prior to the Chunk SSL pre-training.\nIn the quantization module, an utterance based channel mean and variance normalization is first applied to input frames <math alttext=\"{\\bm{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119935;</mi><annotation encoding=\"application/x-tex\">{\\bm{X}}</annotation></semantics></math> to obtain <math alttext=\"\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#119935;</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119961;</mi><mo>^</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}</annotation></semantics></math>, then it is processed by a FSQ encoder <math alttext=\"\\text{Enc}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mtext>Enc</mtext><mi>f</mi></msub><annotation encoding=\"application/x-tex\">\\text{Enc}_{f}</annotation></semantics></math> and projected into a low-dimension space with size <math alttext=\"d^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>d</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">d^{\\prime}</annotation></semantics></math>, where <math alttext=\"d^{\\prime}\\ll d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>d</mi><mo>&#8242;</mo></msup><mo>&#8810;</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d^{\\prime}\\ll d</annotation></semantics></math>. The output of each channel <math alttext=\"r\\in[1,d^{\\prime}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><msup><mi>d</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in[1,d^{\\prime}]</annotation></semantics></math> for input frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is rounded to an integer <math alttext=\"h_{i,r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,r}</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder is optimized with masked prediction loss. We encourage the Chunk SSL model to reconstruct the masked frames based on the context information provided.\nMore specifically, we mask half consecutive frames in every extended chunk. The start position of the masking frame is randomly chosen from <math alttext=\"[0,\\frac{C}{4}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mfrac><mi>C</mi><mn>4</mn></mfrac><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,\\frac{C}{4}]</annotation></semantics></math> at every extended chunk, so we have enough unmasked frames on both sides of the masked frames as context to infer those masked frames.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming the Chunk SSL model outputs for the augmented sequence is <math alttext=\"{\\bm{O}}=\\{{\\bm{o}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119926;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119952;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{O}}=\\{{\\bm{o}}_{i}\\}</annotation></semantics></math>. The FSQ index of <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is denoted as <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math> and\n<math alttext=\"{\\mathbf{e}}_{\\mu_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><msub><mi>&#956;</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">{\\mathbf{e}}_{\\mu_{i}}</annotation></semantics></math> is the output embedding of <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math>. The masked prediction loss is defined as</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize a high resolution FSQ codebook would be beneficial for downstreaming tasks due to two reasons. First, FSQ does not suffered from codebook collapse and can achieve high codebook usage.\nSecond, a high resolution codebook divides the feature space into more fine-grained bins and frames sharing the same token index might be more closer to each other compared with frames associated with a token index from a low resolution codebook.\nWhen the FSQ codebook is large enough, each FSQ token could be mainly associated with one modeling unit, such as phoneme, in the downstreaming task, hence it will make the knowledge transfer easier from the pre-training stage to the fine-tuning stage. This hypothesis is examined in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S5.SS2\" title=\"5.2 Impact of FSQ codebook sizes &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.\nHowever, a high resolution FSQ codebook poses a great challenge for optimization.\nFor example, the codebook with vocabulary size 1,265,625 and embedding size 512 would take about 2.4G memory if they are stored in float data type.\nIn order to alleviate this issue, we propose to decompose the codebook into a group of channel based sub-codebooks and compute prediction loss for\neach group one by one. Given frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, the sub-codebook index at <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>th channel is <math alttext=\"h_{i,r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,r}</annotation></semantics></math>.\nWe define the group masked prediction loss as</p>\n\n",
                "matched_terms": [
                    "data",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pre-training</span>:\nThere are two pre-training tasks in this work, i.e., FSQ SSL training and Chunk SSL training. We first build FSQ module to discretize the input\nspeech feature and then pre-train speech encoder with Chunk SSL to generate contextual representation.\n<span class=\"ltx_text ltx_font_smallcaps\">Libri-light</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib18\" title=\"\">2019</a>)</cite>, which includes 60k hours of unlabelled English speech data, is used in both pre-training tasks.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk",
                    "data",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Tuning</span>: For speech recognition task, the pre-trained models are finetuned and evaluated on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib29\" title=\"\">2015</a>)</cite> dataset. We use two <span class=\"ltx_text ltx_font_typewriter\">dev</span> sets for development and report all results from <span class=\"ltx_text ltx_font_typewriter\">dev</span> and <span class=\"ltx_text ltx_font_typewriter\">test</span> sets.\nFor speech translation task, experiments are conducted on two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib12\" title=\"\">2019</a>)</cite>\nlanguage pairs: English to German (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>DE) and English to Spanish (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ES). Sequence level knowledge\ndistillation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Rush, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib20\" title=\"\">2016</a>)</cite> is applied to boost the speech translation quality.\nThe models are developed on\nthe <span class=\"ltx_text ltx_font_typewriter\">dev</span> set, and the final results are reported on the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> set.</p>\n\n",
                "matched_terms": [
                    "models",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speech encoder is a Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib15\" title=\"\">2020</a>)</cite> based chunk encoder.\nThe encoder is equipped with a relative positional embedding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib32\" title=\"\">2018</a>)</cite> and starts with a stacking layer to down-sample the input features by four times.\nTwo model configurations: base and large, have been explored. The base encoder has 12 Conformer layers, input embedding size of 512, 8 attention heads, feedforward layer dimension 2048 and convolutional module kernel size 31. The large encoder has 24 Conformer layers, input embedding size of 768, 16 attention heads, feedforward layer dimension 3072 and convolutional module kernel size 5.\nTransducer is adopted in the fine-tuning experiments.\nThe predictor module is with one Transformer layer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> for speech recognition tasks and two layers for speech translation tasks.\nThe input embedding and feedforward layer dimension are the same as ones in the encoder setting if not mentioned specifically. The joint module is a feedforward layer as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib41\" title=\"\">2020b</a>)</cite> with embedding dimension 1024.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "base",
                    "model",
                    "large",
                    "conformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Input speech is represented as 80-dimensional log mel-filterbank coefficients computed every 10ms with a 25ms window. Global channel mean and variance normalization is applied so the trained model could be used for both streaming and offline scenarios.\nWe set the maximum utterance duration to 75 seconds and minimum duration to 5 seconds in pre-training.\nDuring fine-tuning,\na look-ahead chunk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib33\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib24\" title=\"\">2021</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib35\" title=\"\">2023</a>)</cite> is utilized and\ndynamic chunk training is employed by default. We alternate between offline training with infinite chunk size, and streaming training with chunk duration sampled from [160, 320, 640, 960, 1280, 1600] ms randomly within each epoch.\nMore details about optimization, such as batch sizes and training schedulers, for different experiments are presented in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A1\" title=\"Appendix A Optimization configures &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nThe target labels are encoded with SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib21\" title=\"\">2018</a>)</cite>. For both speech recognition and speech translation tasks, the vocabulary is an unigram model with size 1024 and full character coverage on the corresponding training text data.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "streaming",
                    "data",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results are evaluated using an averaged model from checkpoints of the best 10 epochs.\nSpeech recognition experiments are evaluated with WER while speech translation results are measure with scacre BLEU score<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>signature: nrefs:1<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>case:mixed<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>eff:no<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>tok:13a<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>smooth:exp<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>version:2.4.2</span></span></span>.\nWe report both streaming and offline results.\nFor the streaming decoding, we set the chunk size to 320ms by default if not mentioned specifically. No language model is used in all experiments.</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "chunk",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the speech translation experiments, we evaluate on both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; directions and the results are listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. The baselines are initialized with a speech recognition acoustic encoder (&#8220;ASR&#8221; in the column &#8220;Init.&#8221;), which is trained with the English data in the <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> English-Spanish direction. Both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; results show that the encoder initialized with Chunk SSL outperforms the one initialized with a speech recognition encoder trained with <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> data only. We could draw similar conclusion as the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> experiment that the Chunk SSL could effectively improve speech translation quality for both streaming and offline cases.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "librispeech",
                    "streaming",
                    "data",
                    "ssl",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to verify our a high resolution FSQ codebook hypothesis discussed in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>,\nwe study FSQ tokenizers with different codebook sizes. Two aspects are examined: first the agreement among FSQ tokens and phonemes, and second WERs for ASR models initialized with different FSQ tokenizers.\nFor the agreement among FSQ tokens and phonemes, we compare the overlap of features associated with different tokens and phonemes. It can be measured by phone purity and phone-normalized mutual information (PNMI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib16\" title=\"\">2021a</a>)</cite>. Phone purity denotes the conditional probability of a phoneme given a FSQ index while PNMI measures the percentage of uncertainty about a phoneme label eliminated after observing a specific FSQ token.\nWe generate phoneme based alignment with Montreal Aligner&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib27\" title=\"\">2017</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/MontrealCorpusTools/mfa-models/releases/download/acoustic-english_us_arpa-v3.0.0/english_us_arpa.zip</span></span></span>\non two <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> <span class=\"ltx_text ltx_font_typewriter\">dev</span> sets.</p>\n\n",
                "matched_terms": [
                    "wers",
                    "models",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> lists models trained with different codebook sizes with the base configure. The first column gives FSQ vocabulary sizes, the second column provides the Chunk SSL training time (in seconds) for 1000 updates.\n&#8220;levels&#8221; is the FSQ levels for quantization. For example, &#8220;4(<math alttext=\"8\\times 1-5\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow></mrow><annotation encoding=\"application/x-tex\">8\\times 1-5\\times 3</annotation></semantics></math>)&#8221; in the first row means there are 4 output channels, the first channel is with level 8 and the remaining 3 channels are with level 5. They span a codebook with size 1000.\n&#8220;phn pur.&#8221; stands for phone purity.</p>\n\n",
                "matched_terms": [
                    "models",
                    "chunk",
                    "base",
                    "ssl",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first two rows listed models trained with FSQ vocabulary size 1000. &#8220;1000*&#8221; means the model is optimized with a full codebook as&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E4\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, while &#8220;1000&#8221; indicates the model is pretrained with the group masked prediction loss following&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E5\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>. The results shows two models\nachieve comparable accuracies\nwith similar pre-training time.\nAs shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, when the codebook size increases, &#8220;phn pur.&#8221; and &#8220;PNMI&#8221; increase steadily.\nBoth metrics show tokens in the codebook with a high resolution have better consistency with phonemes. This confirms our assumption that frames associated with a specific FSQ token are more likely being labelled with the same phoneme if the codebook resolution is high enough.\nOn the other hand, we observe WER is reduced when FSQ codebook size becomes bigger.\nIt is peaked for the codebook size around 6 millions.\nWhen the codebook size gets extremely big, i.e. 791 millions in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, the &#8220;dev-other&#8221; WER increases from 5.0 to 5.4.\nOur hypothesis is that an extremely large codebook makes the optimization harder, and more training time might be required to get good representation.\nBased on above observations, we conclude that a codebook with high resolution helps Chunk SSL to transfer knowledge to the downstream task. However, an extremely large FSQ codebook might make the pre-training hard and hurt the downstream task.</p>\n\n",
                "matched_terms": [
                    "models",
                    "chunk",
                    "wer",
                    "model",
                    "large",
                    "ssl",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The next thing we examined is computation cost.\nWe don&#8217;t observe any statistical difference for the FSQ training time when the codebook size changes, since majority of computation time is spent on data input/output, encoder and decoder Transformer/Conformer layer computation. On the other hand, a Chunk SSL training with a large codebook can increase the training time slightly that we find\nabout 10% training time increase when the codebook size grows from 1000 to 6 millions as shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk",
                    "data",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the base Transducer model to analyze the impact of different chunk sizes on latency and performance. We use Length-Adaptive Average Lagging(LAAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib30\" title=\"\">2022</a>)</cite> as latency scorer, which is evaluated with the Simuleval toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib26\" title=\"\">2020</a>)</cite>. We employ greedy decoding instead of beam search decoding for simplicity in this study.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a> depicts the model performance (<span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">blue</span>) and latency (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">red</span>) under different chunk sizes for speech recognition&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>) and speech translation&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf3\" title=\"Figure 3(c) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>). The speech recognition is evaluated on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> <span class=\"ltx_text ltx_font_typewriter\">dev-other</span> set and speech translation tasks are evaluated on the corresponding <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> <span class=\"ltx_text ltx_font_typewriter\">dev</span> set.\nIt can be seen that the model is with a small latency (LAAL 0.47 second) but high WER (7.6) when the chunk size is small (160ms). The performance improves steadily when the chunk size increases but at the cost of increasing latency. The speech translation experiments have similar observations. The results indicate the model can switch from streaming mode to offline mode smoothly as the chunk sizes change.\nOther latency evaluation results, such as Average Lagging (AL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib25\" title=\"\">2019</a>)</cite> and Differentiable Average Lagging (DAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arivazhagan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib1\" title=\"\">2019</a>)</cite>, could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A2\" title=\"Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a></p>\n\n",
                "matched_terms": [
                    "chunk",
                    "wer",
                    "librispeech",
                    "model",
                    "streaming",
                    "devother"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech self-supervised learning algorithms form contextual representations through learning to predict\nmissing information due to time constraint or masking. <cite class=\"ltx_cite ltx_citemacro_citet\">van&#160;den Oord et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib37\" title=\"\">2018</a>)</cite> propose an auto-regressive based SSL approach to predict the future samples based on the previous samples. Speech SSL methods such as wav2vec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>, HuBert&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>)</cite> and BEST-RQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> choose to randomly mask audio span and the model learns to restore those masked regions. Chunk SSL combines both approaches that it masks audio span in the extended chunk\nand recovers masking frame indices in an chunk based auto-regressive manner.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "chiu",
                    "ssl",
                    "wav2vec2",
                    "bestrq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Vector quantization (VQ) is one of the important components in the speech SSL. <cite class=\"ltx_cite ltx_citemacro_citet\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib3\" title=\"\">2020a</a>)</cite> leverages Gumbel-Softmax with multiple variable groups to discretize\naudio features. Methods such as diversity loss have to take to alleviate codeword collapse issue.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>)</cite> resorts k-means to cluster the encoder outputs to build the codebook. In order to achieve a good representation, an iterative training is required to keep re-building the encoder with a new codebook.\nBEST-RQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> chooses an alternative way, which initializes the codebook randomly and no further update is applied during SSL.\nCompared with these VQ methods aforementioned, FSQ does not suffer from codebook collapse and achieves high codebook usage&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "chiu",
                    "ssl",
                    "bestrq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, a chunkwise speech self-supervised learning approach is proposed. The model is trained to\nlearn to reconstruct the masked information in the right most chunk based on unmasked frames in the same chunk and preceding chunks. An efficient copy and append data augmentation method is proposed to\nparallel chunk-wise computation.\nFSQ is employed to generate high resolution discrete tokens and a group based decomposition method is proposed to alleviate the computation challenge.\nThe model is trained with varied chunk durations to fit\nthe different scenarios, hence a pre-trained model could be used for both streaming and offline\napplications. Our experiments show that a model initialized with Chunk SSL could achieve very\ncompetitive offline results and excellent streaming results on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> translation directions.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "librispeech",
                    "streaming",
                    "data",
                    "ssl",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FSQ model is built with learning rate is 2.0e-4 and ExponentialLR scheduler from PyTorch. The effective batch size is 3200 seconds with up to 180,000 updates. The FSQ module is optimized with MSE loss to restore mel-filterbank features from their quantized audio representations. It takes 4 A10 GPUs for 42 hours to build a FSQ model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training is scheduled as Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> with warmup step 25,000 and maximum learning rate 3e-4. The batch size is 16,000 seconds for the base configure model. We choose a smaller effective batch size for the large configure model with 6,000 seconds due to computation resource limitation. The model is trained up to 400,000 updates. It takes about 10 days for 8 A100 GPUs to build a Chunk SSL encoder with the base configure.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "base",
                    "model",
                    "large",
                    "ssl",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">960 hours fine-tuning</span>\nFor the base configure model, we choose the three stages scheduler scheme as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>, i.e., warmup, hold and annealing. The warmup stage takes about 1000 updates with encoder parameters frozen, the hold stage takes about 40,000 updates and the remaining 60,000 updates are in an annealing stage. The peak learning rate in fine-tuning is 2.0e-3.\nThe SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib31\" title=\"\">2019</a>)</cite> data augmentation is applied with 2 frequency maskings of 27, and 10 time maskings at a maximum ratio of 0.05 in fine-tuning experiments.\nThe effective batch size for fine-tuning is 10,000 seconds. For the large configure model, we choose Transformer scheduler&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> since it has less hype-parameters to explore. The warmup step is 5000 with encoder parameters frozen. Similar as the base configure model, the total update step is up to 100,000. The peak learning rate is 5e-4. The effective batch size for the large model is 7200 seconds.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "base",
                    "model",
                    "large",
                    "data"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Speech Translation Result on the MuST-C dataset (BLEU ↑\\uparrow).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Lang</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Init.</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Offline</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Streaming</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">dev</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">tst-COMMON</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">dev</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">tst-COMMON</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASR</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Chunk SSL</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASR</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Chunk SSL</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.2</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "en→rightarrowes",
            "↑uparrow",
            "chunk",
            "translation",
            "result",
            "init",
            "dev",
            "streaming",
            "tstcommon",
            "en→rightarrowde",
            "lang",
            "offline",
            "asr",
            "speech",
            "dataset",
            "ssl",
            "mustc",
            "bleu"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In the speech translation experiments, we evaluate on both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; directions and the results are listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. The baselines are initialized with a speech recognition acoustic encoder (&#8220;ASR&#8221; in the column &#8220;Init.&#8221;), which is trained with the English data in the <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> English-Spanish direction. Both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; results show that the encoder initialized with Chunk SSL outperforms the one initialized with a speech recognition encoder trained with <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> data only. We could draw similar conclusion as the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> experiment that the Chunk SSL could effectively improve speech translation quality for both streaming and offline cases.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Low latency speech human-machine communication is becoming increasingly necessary as speech technology advances quickly in the last decade.\nOne of the primary factors behind the advancement of speech technology is self-supervised learning.\nMost self-supervised learning algorithms are designed with full utterance assumption and compromises have to made if partial utterances are presented, which are common in the streaming applications.\nIn this work, we propose a chunk based self-supervised learning (Chunk SSL) algorithm as an unified solution for both streaming and offline speech pre-training.\nChunk SSL is optimized with the masked prediction loss and\nan acoustic encoder is encouraged to restore indices of those masked speech frames with help from unmasked frames in the same chunk and preceding chunks.\nA copy and append data augmentation approach is proposed to conduct efficient chunk based pre-training.\nChunk SSL utilizes a finite scalar quantization (FSQ) module to discretize input speech features and our study shows a high resolution FSQ codebook, i.e., a codebook with vocabulary size up to a few millions, is beneficial to transfer knowledge from the pre-training task to the downstream tasks. A group masked prediction loss is employed during pre-training to alleviate the high memory and computation cost introduced by the large codebook.\nThe proposed approach is examined in two speech to text tasks, i.e., speech recognition and speech translation.\nExperimental results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> datasets show that the proposed method could achieve\nvery competitive results for speech to text tasks at both streaming and offline modes.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "mustc",
                    "translation",
                    "streaming",
                    "speech",
                    "ssl",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Transcribe speech into text in real time is critical for applications requiring immediate feedback, such as real-time transcription of broadcast contents, voice assistants, and simultaneous speech translation etc.\nIt requires processing audio incrementally as it is received, rather than waiting for the entire utterance to be available.\nFor neural network based end-to-end systems,\nthe requirement includes two-fold. First, a speech encoder, such as causal encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib41\" title=\"\">2020b</a>)</cite> and chunk encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tsunoo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib36\" title=\"\">2019</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib33\" title=\"\">2021</a>)</cite>, is able to process input audio cumulatively without dependency on the future input.\nSecond, a decoder is required to decode transcription incrementally based on partial encoder outputs.\nFrame-Synchronous decoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib10\" title=\"\">2020</a>)</cite>, such as CTC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib14\" title=\"\">2013</a>)</cite> Transducer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Graves, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib13\" title=\"\">2012</a>)</cite> CIF&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong &amp; Xu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib9\" title=\"\">2020</a>)</cite> and TAED&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib35\" title=\"\">2023</a>)</cite>, are capable of generating transcription based on partial encoder outputs and meet the streaming requirement naturally.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "streaming",
                    "chunk",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Self-supervised pre-training leverages vast amounts of unlabeled data and learn universal feature representations for downstream tasks, especially for tasks with limited supervised training data.\nSpeech pre-training methods have emerged as the backbone of many speech processing tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib8\" title=\"\">2018</a>; Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib5\" title=\"\">2021</a>; Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib34\" title=\"\">2022</a>; Baade et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib2\" title=\"\">2025</a>)</cite>.\nThose methods are designed for the\nspeech applications with full utterance available, and compromises have to be made if the downstream tasks are streaming applications.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> propose to conduct pre-training with a causal encoder, which sacrifices right context information. <cite class=\"ltx_cite ltx_citemacro_citet\">Fu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib11\" title=\"\">2024</a>)</cite> modify the encoder structure\nand conduct continual pre-training to adapt the model for streaming applications.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "streaming"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all those methods aforementioned, a dedicated pre-trained model has to be built for the streaming scenario instead of sharing the same model with the offline application.\n</p>\n\n",
                "matched_terms": [
                    "streaming",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we focus on building an encoder suitable for both streaming and offline modes,\nwith a chunkwise self-supervised learning (Chunk SSL) framework as depicted in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>.\nChunk SSL aims to\nrestore discrete indices of the masked frames based on unmasked frames in the last chunk and previous chunks.\nThe discrete indices are estimated with finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> (FSQ) with vocabulary size up to millions&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).The pre-training FSQ token has more fine-grained resolution compared with the downstream modeling units, such as phonemes or sentencepiece tokens,\nwhich usually are with vocabulary size ranging from tens to tens of thousands.\nWe hypothesize that speech frames associated with a high resolution FSQ token are mainly mapped to an unique modeling unit in the downstream task and it makes the knowledge transfer easier from the pre-training stage to the fine-tuning stage.\nHowever, those high resolution FSQ tokens pose a great challenge for modeling and we propose to decompose a large codebook\ninto small channel based sub-codebooks to alleviate the memory and computation cost during pre-training. Details could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "streaming",
                    "speech",
                    "ssl",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, speech features are first extracted from the input audio, and grouped into equal sized chunks.\nInstead of calculating those chunks from left to right sequentially, a copy and append data augmentation (CADA) is introduced to parallelize the computation for all chunks in the same utterance (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S2\" title=\"2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).\nCADA augments the input sequence by copying input chunks and appending them to the end of utterance as extended chunks.\nMasking is applied to frames in extended chunks only.\nAugmented utterances are then processed by a CADA compatible Conformer encoder, which could handle those augmented chunks properly even though frames in extended chunks are with altered location information. Finally,\nthe model is optimized by restoring FSQ indices of those masked frames from Conformer encoder outputs.\nExperiments on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> datasets indicate that\nthe same model initialized with Chunk SSL could achieve very competitive results\non both streaming and offline speech to text tasks.\nIt shows that Chunk SSL eliminates the need to build a dedicated streaming and dedicated offline model.\nTo summarize, our contributions includes:</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "mustc",
                    "streaming",
                    "speech",
                    "ssl",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a chunkwise speech self-supervised learning algorithm for both streaming and offline speech to text tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "streaming",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The naive Chunk SSL algorithm, which recovers masked frame indices in the right most chunk based on the unmasked frames in the same chunk and preceding chunks, is executed chunk by chunk in a sequential order instead of computing all chunks from one utterance in a parallel fashion.\nInspired by the implementation of the chunk encoder with a look-ahead chunk for the streaming speech translation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib24\" title=\"\">2021</a>)</cite>,\nwe propose a copy and append data augmentation for the Chunk SSL,\nwhich reorganizes\ninput data and changes the augmented data computation, but it is still strictly equivalent to the computation in the naive Chunk SSL algorithm.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "translation",
                    "streaming",
                    "speech",
                    "ssl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training procedure is described in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. Input features are first downsampled with a stacked subsampling module. The outputs are then divided into chunks with fixed duration, They are considered as base chunks.\nThe consecutive chunk of each base chunk is copied and appended to the end of the utterance in a sequential order. Those newly created chunks are called extended chunks.\nFor example, there are 12 speech input frames in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a> after subsampling. Assuming the chunk size is 4 frames, the speech features are segmented into three base chunks (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#00FFFF;\">cyan</span>): &#8220;B1&#8221;, &#8220;B2&#8221; and &#8220;B3&#8221;.\nChunk &#8220;B2&#8221; and &#8220;B3&#8221; are copied and appended to the end of the utterance as &#8220;E1&#8221; and &#8220;E2&#8221; (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#BFFF00;\">lime</span>).\nThey correspond to the extended chunks of &#8220;B1&#8221; and &#8220;B2&#8221; respectively.\nThere is no extended chunk for the last chunk (&#8220;B3&#8221;).\nExtended chunks act as right most chunks with different preceding chunks in the naive Chunk SSL algorithm and\nmasking is only applied on frames of extended chunks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "chunk",
                    "ssl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A speech encoder, such as Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> and Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib15\" title=\"\">2020</a>)</cite>, contains two types of modules. One is the intra-frame module where the computation is based on one frame and no location information or other frames are required, for example, LayerNorm and feedforward layers.\nThe other type is the inter-frame module where the computation is related to the input location and requires information from neighbouring frames.\nThe inter-frame modules include self-attention and convolutional layers.\nAn augmented utterance aforementioned, can be processed as a normal utterance in the intra-frame modules and no modification is required.\nOn the other hand, the position information is invalid among extended chunk frames, since they are copied and appended to the end of utterance.\nModifications are introduced for two inter-frame modules: self-attention and convolutional layers, to handle inconsistent position information in augmented utterances as described in\nthe following subsections.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Dynamic chunk training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib40\" title=\"\">2020a</a>; Weninger et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib39\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib23\" title=\"\">2023</a>)</cite> is a useful fine-tuning approach for the chunk based ASR encoder.\nDuring training, the chunk duration varied from hundreds of million seconds to a few seconds is randomly chosen for every model update. A model built with dynamic chunk training is suitable for both streaming and offline modes.\nWe extend dynamic chunk training for the Chunk SSL pre-training. Chunk duration is chosen from 6 durations (<math alttext=\"[640,1280,1920,2560,3200,3840]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>640</mn><mo>,</mo><mn>1280</mn><mo>,</mo><mn>1920</mn><mo>,</mo><mn>2560</mn><mo>,</mo><mn>3200</mn><mo>,</mo><mn>3840</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[640,1280,1920,2560,3200,3840]</annotation></semantics></math> million seconds) randomly for every model update.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "streaming",
                    "asr",
                    "ssl",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, we leverage finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> to quantize\ninput frames and obtain their discrete indices.\nA FSQ module is built prior to the Chunk SSL pre-training.\nIn the quantization module, an utterance based channel mean and variance normalization is first applied to input frames <math alttext=\"{\\bm{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119935;</mi><annotation encoding=\"application/x-tex\">{\\bm{X}}</annotation></semantics></math> to obtain <math alttext=\"\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#119935;</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119961;</mi><mo>^</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}</annotation></semantics></math>, then it is processed by a FSQ encoder <math alttext=\"\\text{Enc}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mtext>Enc</mtext><mi>f</mi></msub><annotation encoding=\"application/x-tex\">\\text{Enc}_{f}</annotation></semantics></math> and projected into a low-dimension space with size <math alttext=\"d^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>d</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">d^{\\prime}</annotation></semantics></math>, where <math alttext=\"d^{\\prime}\\ll d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>d</mi><mo>&#8242;</mo></msup><mo>&#8810;</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d^{\\prime}\\ll d</annotation></semantics></math>. The output of each channel <math alttext=\"r\\in[1,d^{\\prime}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><msup><mi>d</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in[1,d^{\\prime}]</annotation></semantics></math> for input frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is rounded to an integer <math alttext=\"h_{i,r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,r}</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder is optimized with masked prediction loss. We encourage the Chunk SSL model to reconstruct the masked frames based on the context information provided.\nMore specifically, we mask half consecutive frames in every extended chunk. The start position of the masking frame is randomly chosen from <math alttext=\"[0,\\frac{C}{4}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mfrac><mi>C</mi><mn>4</mn></mfrac><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,\\frac{C}{4}]</annotation></semantics></math> at every extended chunk, so we have enough unmasked frames on both sides of the masked frames as context to infer those masked frames.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming the Chunk SSL model outputs for the augmented sequence is <math alttext=\"{\\bm{O}}=\\{{\\bm{o}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119926;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119952;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{O}}=\\{{\\bm{o}}_{i}\\}</annotation></semantics></math>. The FSQ index of <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is denoted as <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math> and\n<math alttext=\"{\\mathbf{e}}_{\\mu_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><msub><mi>&#956;</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">{\\mathbf{e}}_{\\mu_{i}}</annotation></semantics></math> is the output embedding of <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math>. The masked prediction loss is defined as</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pre-training</span>:\nThere are two pre-training tasks in this work, i.e., FSQ SSL training and Chunk SSL training. We first build FSQ module to discretize the input\nspeech feature and then pre-train speech encoder with Chunk SSL to generate contextual representation.\n<span class=\"ltx_text ltx_font_smallcaps\">Libri-light</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib18\" title=\"\">2019</a>)</cite>, which includes 60k hours of unlabelled English speech data, is used in both pre-training tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "chunk",
                    "ssl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Tuning</span>: For speech recognition task, the pre-trained models are finetuned and evaluated on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib29\" title=\"\">2015</a>)</cite> dataset. We use two <span class=\"ltx_text ltx_font_typewriter\">dev</span> sets for development and report all results from <span class=\"ltx_text ltx_font_typewriter\">dev</span> and <span class=\"ltx_text ltx_font_typewriter\">test</span> sets.\nFor speech translation task, experiments are conducted on two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib12\" title=\"\">2019</a>)</cite>\nlanguage pairs: English to German (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>DE) and English to Spanish (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ES). Sequence level knowledge\ndistillation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Rush, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib20\" title=\"\">2016</a>)</cite> is applied to boost the speech translation quality.\nThe models are developed on\nthe <span class=\"ltx_text ltx_font_typewriter\">dev</span> set, and the final results are reported on the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> set.</p>\n\n",
                "matched_terms": [
                    "en→rightarrowes",
                    "translation",
                    "dev",
                    "tstcommon",
                    "en→rightarrowde",
                    "speech",
                    "dataset",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speech encoder is a Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib15\" title=\"\">2020</a>)</cite> based chunk encoder.\nThe encoder is equipped with a relative positional embedding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib32\" title=\"\">2018</a>)</cite> and starts with a stacking layer to down-sample the input features by four times.\nTwo model configurations: base and large, have been explored. The base encoder has 12 Conformer layers, input embedding size of 512, 8 attention heads, feedforward layer dimension 2048 and convolutional module kernel size 31. The large encoder has 24 Conformer layers, input embedding size of 768, 16 attention heads, feedforward layer dimension 3072 and convolutional module kernel size 5.\nTransducer is adopted in the fine-tuning experiments.\nThe predictor module is with one Transformer layer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> for speech recognition tasks and two layers for speech translation tasks.\nThe input embedding and feedforward layer dimension are the same as ones in the encoder setting if not mentioned specifically. The joint module is a feedforward layer as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib41\" title=\"\">2020b</a>)</cite> with embedding dimension 1024.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "chunk",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Input speech is represented as 80-dimensional log mel-filterbank coefficients computed every 10ms with a 25ms window. Global channel mean and variance normalization is applied so the trained model could be used for both streaming and offline scenarios.\nWe set the maximum utterance duration to 75 seconds and minimum duration to 5 seconds in pre-training.\nDuring fine-tuning,\na look-ahead chunk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib33\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib24\" title=\"\">2021</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib35\" title=\"\">2023</a>)</cite> is utilized and\ndynamic chunk training is employed by default. We alternate between offline training with infinite chunk size, and streaming training with chunk duration sampled from [160, 320, 640, 960, 1280, 1600] ms randomly within each epoch.\nMore details about optimization, such as batch sizes and training schedulers, for different experiments are presented in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A1\" title=\"Appendix A Optimization configures &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nThe target labels are encoded with SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib21\" title=\"\">2018</a>)</cite>. For both speech recognition and speech translation tasks, the vocabulary is an unigram model with size 1024 and full character coverage on the corresponding training text data.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "translation",
                    "streaming",
                    "speech",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results are evaluated using an averaged model from checkpoints of the best 10 epochs.\nSpeech recognition experiments are evaluated with WER while speech translation results are measure with scacre BLEU score<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>signature: nrefs:1<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>case:mixed<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>eff:no<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>tok:13a<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>smooth:exp<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>version:2.4.2</span></span></span>.\nWe report both streaming and offline results.\nFor the streaming decoding, we set the chunk size to 320ms by default if not mentioned specifically. No language model is used in all experiments.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "translation",
                    "streaming",
                    "speech",
                    "offline",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained Chunk SSL models are fine-tuned with dynamic chunk training and they could be used for both streaming and offline ASR.\nWe list the offline and streaming recognition results from the Chunk SSL models in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>.\nThe reported literature results are presented in the first part of the tables.\nIt is clear that the chunk SSL pre-trained models could achieve very competitive results compared to other strong baselines.\nThe base and large models outperform the corresponding wav2vec2 base and large models.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "streaming",
                    "asr",
                    "ssl",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the streaming evaluation, the Chunk SSL model with the base configuration excels all models reported in the literature shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>. The large configuration model pushes the WER even lower and it achieves another 0.5 average WER reduction compared to the base configure model. Comparing results in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>,\nthe proposed method significantly reduces the performance gap between the streaming and offline applications. The WER gap between the large configure\nmodels is 0.8 in average for Chunk SSL while the average WER difference is 2.5 for BEST-RQ.\nNote, literature models listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a> are initialized with dedicated pre-trained models either offline or streaming, while a Chunk SSL model could be initialized with the same pre-trained model and operated in both modes.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "streaming",
                    "chunk",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to verify our a high resolution FSQ codebook hypothesis discussed in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>,\nwe study FSQ tokenizers with different codebook sizes. Two aspects are examined: first the agreement among FSQ tokens and phonemes, and second WERs for ASR models initialized with different FSQ tokenizers.\nFor the agreement among FSQ tokens and phonemes, we compare the overlap of features associated with different tokens and phonemes. It can be measured by phone purity and phone-normalized mutual information (PNMI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib16\" title=\"\">2021a</a>)</cite>. Phone purity denotes the conditional probability of a phoneme given a FSQ index while PNMI measures the percentage of uncertainty about a phoneme label eliminated after observing a specific FSQ token.\nWe generate phoneme based alignment with Montreal Aligner&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib27\" title=\"\">2017</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/MontrealCorpusTools/mfa-models/releases/download/acoustic-english_us_arpa-v3.0.0/english_us_arpa.zip</span></span></span>\non two <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> <span class=\"ltx_text ltx_font_typewriter\">dev</span> sets.</p>\n\n",
                "matched_terms": [
                    "dev",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> lists models trained with different codebook sizes with the base configure. The first column gives FSQ vocabulary sizes, the second column provides the Chunk SSL training time (in seconds) for 1000 updates.\n&#8220;levels&#8221; is the FSQ levels for quantization. For example, &#8220;4(<math alttext=\"8\\times 1-5\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow></mrow><annotation encoding=\"application/x-tex\">8\\times 1-5\\times 3</annotation></semantics></math>)&#8221; in the first row means there are 4 output channels, the first channel is with level 8 and the remaining 3 channels are with level 5. They span a codebook with size 1000.\n&#8220;phn pur.&#8221; stands for phone purity.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first two rows listed models trained with FSQ vocabulary size 1000. &#8220;1000*&#8221; means the model is optimized with a full codebook as&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E4\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, while &#8220;1000&#8221; indicates the model is pretrained with the group masked prediction loss following&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E5\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>. The results shows two models\nachieve comparable accuracies\nwith similar pre-training time.\nAs shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, when the codebook size increases, &#8220;phn pur.&#8221; and &#8220;PNMI&#8221; increase steadily.\nBoth metrics show tokens in the codebook with a high resolution have better consistency with phonemes. This confirms our assumption that frames associated with a specific FSQ token are more likely being labelled with the same phoneme if the codebook resolution is high enough.\nOn the other hand, we observe WER is reduced when FSQ codebook size becomes bigger.\nIt is peaked for the codebook size around 6 millions.\nWhen the codebook size gets extremely big, i.e. 791 millions in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, the &#8220;dev-other&#8221; WER increases from 5.0 to 5.4.\nOur hypothesis is that an extremely large codebook makes the optimization harder, and more training time might be required to get good representation.\nBased on above observations, we conclude that a codebook with high resolution helps Chunk SSL to transfer knowledge to the downstream task. However, an extremely large FSQ codebook might make the pre-training hard and hurt the downstream task.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The next thing we examined is computation cost.\nWe don&#8217;t observe any statistical difference for the FSQ training time when the codebook size changes, since majority of computation time is spent on data input/output, encoder and decoder Transformer/Conformer layer computation. On the other hand, a Chunk SSL training with a large codebook can increase the training time slightly that we find\nabout 10% training time increase when the codebook size grows from 1000 to 6 millions as shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a> depicts the model performance (<span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">blue</span>) and latency (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">red</span>) under different chunk sizes for speech recognition&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>) and speech translation&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf3\" title=\"Figure 3(c) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>). The speech recognition is evaluated on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> <span class=\"ltx_text ltx_font_typewriter\">dev-other</span> set and speech translation tasks are evaluated on the corresponding <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> <span class=\"ltx_text ltx_font_typewriter\">dev</span> set.\nIt can be seen that the model is with a small latency (LAAL 0.47 second) but high WER (7.6) when the chunk size is small (160ms). The performance improves steadily when the chunk size increases but at the cost of increasing latency. The speech translation experiments have similar observations. The results indicate the model can switch from streaming mode to offline mode smoothly as the chunk sizes change.\nOther latency evaluation results, such as Average Lagging (AL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib25\" title=\"\">2019</a>)</cite> and Differentiable Average Lagging (DAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arivazhagan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib1\" title=\"\">2019</a>)</cite>, could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A2\" title=\"Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a></p>\n\n",
                "matched_terms": [
                    "chunk",
                    "translation",
                    "dev",
                    "streaming",
                    "speech",
                    "mustc",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech self-supervised learning algorithms form contextual representations through learning to predict\nmissing information due to time constraint or masking. <cite class=\"ltx_cite ltx_citemacro_citet\">van&#160;den Oord et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib37\" title=\"\">2018</a>)</cite> propose an auto-regressive based SSL approach to predict the future samples based on the previous samples. Speech SSL methods such as wav2vec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>, HuBert&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>)</cite> and BEST-RQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> choose to randomly mask audio span and the model learns to restore those masked regions. Chunk SSL combines both approaches that it masks audio span in the extended chunk\nand recovers masking frame indices in an chunk based auto-regressive manner.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "chunk",
                    "ssl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Vector quantization (VQ) is one of the important components in the speech SSL. <cite class=\"ltx_cite ltx_citemacro_citet\">Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib3\" title=\"\">2020a</a>)</cite> leverages Gumbel-Softmax with multiple variable groups to discretize\naudio features. Methods such as diversity loss have to take to alleviate codeword collapse issue.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>)</cite> resorts k-means to cluster the encoder outputs to build the codebook. In order to achieve a good representation, an iterative training is required to keep re-building the encoder with a new codebook.\nBEST-RQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> chooses an alternative way, which initializes the codebook randomly and no further update is applied during SSL.\nCompared with these VQ methods aforementioned, FSQ does not suffer from codebook collapse and achieves high codebook usage&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ssl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, a chunkwise speech self-supervised learning approach is proposed. The model is trained to\nlearn to reconstruct the masked information in the right most chunk based on unmasked frames in the same chunk and preceding chunks. An efficient copy and append data augmentation method is proposed to\nparallel chunk-wise computation.\nFSQ is employed to generate high resolution discrete tokens and a group based decomposition method is proposed to alleviate the computation challenge.\nThe model is trained with varied chunk durations to fit\nthe different scenarios, hence a pre-trained model could be used for both streaming and offline\napplications. Our experiments show that a model initialized with Chunk SSL could achieve very\ncompetitive offline results and excellent streaming results on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> translation directions.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "mustc",
                    "translation",
                    "streaming",
                    "speech",
                    "ssl",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training is scheduled as Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> with warmup step 25,000 and maximum learning rate 3e-4. The batch size is 16,000 seconds for the base configure model. We choose a smaller effective batch size for the large configure model with 6,000 seconds due to computation resource limitation. The model is trained up to 400,000 updates. It takes about 10 days for 8 A100 GPUs to build a Chunk SSL encoder with the base configure.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "chunk"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Comparison of finite scalar quantization with different vocabulary sizes. “phn pur.” and “PNMI” stands for phone purity and phone-normalized mutual information.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">vocab size</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">time(s)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">levels</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">phn pur.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">PNMI</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">dev-clean</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">dev-other</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1000*</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1990</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">4 (<math alttext=\"8\\times 1-5\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow></mrow><annotation encoding=\"application/x-tex\">8\\times 1-5\\times 3</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1000</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1988</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4 (<math alttext=\"8\\times 1-5\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow></mrow><annotation encoding=\"application/x-tex\">8\\times 1-5\\times 3</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.2</td>\n<td class=\"ltx_td ltx_align_center\">5.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">50,625</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1998</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">8 (<math alttext=\"5\\times 4-3\\times 4\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m3\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow><mo>&#8722;</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow></mrow><annotation encoding=\"application/x-tex\">5\\times 4-3\\times 4</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.1</td>\n<td class=\"ltx_td ltx_align_center\">5.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1,265,625</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2039</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">10 (<math alttext=\"5\\times 6-3\\times 4\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m4\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>6</mn></mrow><mo>&#8722;</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow></mrow><annotation encoding=\"application/x-tex\">5\\times 6-3\\times 4</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.0</td>\n<td class=\"ltx_td ltx_align_center\">5.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">6,834,375</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2240</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">12 (<math alttext=\"5\\times 5-3\\times 7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>5</mn></mrow><mo>&#8722;</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>7</mn></mrow></mrow><annotation encoding=\"application/x-tex\">5\\times 5-3\\times 7</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.0</td>\n<td class=\"ltx_td ltx_align_center\">5.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">791,015,625</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">2250</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">14 (<math alttext=\"5\\times 10-3\\times 4\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m6\" intent=\":literal\"><semantics><mrow><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>10</mn></mrow><mo>&#8722;</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow></mrow><annotation encoding=\"application/x-tex\">5\\times 10-3\\times 4</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">2.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">5.4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pur",
            "8×1−5×38times",
            "pnmi",
            "phn",
            "purity",
            "size",
            "mutual",
            "5×4−3×45times",
            "comparison",
            "information",
            "5×5−3×75times",
            "vocab",
            "devclean",
            "“phn",
            "5×10−3×45times",
            "53times",
            "103times",
            "stands",
            "sizes",
            "“pnmi”",
            "15times",
            "finite",
            "phone",
            "devother",
            "vocabulary",
            "pur”",
            "scalar",
            "phonenormalized",
            "43times",
            "63times",
            "quantization",
            "5×6−3×45times",
            "times",
            "different",
            "levels"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> lists models trained with different codebook sizes with the base configure. The first column gives FSQ vocabulary sizes, the second column provides the Chunk SSL training time (in seconds) for 1000 updates.\n&#8220;levels&#8221; is the FSQ levels for quantization. For example, &#8220;4(<math alttext=\"8\\times 1-5\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow></mrow><annotation encoding=\"application/x-tex\">8\\times 1-5\\times 3</annotation></semantics></math>)&#8221; in the first row means there are 4 output channels, the first channel is with level 8 and the remaining 3 channels are with level 5. They span a codebook with size 1000.\n&#8220;phn pur.&#8221; stands for phone purity.</p>\n\n",
            "<p class=\"ltx_p\">The first two rows listed models trained with FSQ vocabulary size 1000. &#8220;1000*&#8221; means the model is optimized with a full codebook as&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E4\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, while &#8220;1000&#8221; indicates the model is pretrained with the group masked prediction loss following&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E5\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>. The results shows two models\nachieve comparable accuracies\nwith similar pre-training time.\nAs shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, when the codebook size increases, &#8220;phn pur.&#8221; and &#8220;PNMI&#8221; increase steadily.\nBoth metrics show tokens in the codebook with a high resolution have better consistency with phonemes. This confirms our assumption that frames associated with a specific FSQ token are more likely being labelled with the same phoneme if the codebook resolution is high enough.\nOn the other hand, we observe WER is reduced when FSQ codebook size becomes bigger.\nIt is peaked for the codebook size around 6 millions.\nWhen the codebook size gets extremely big, i.e. 791 millions in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, the &#8220;dev-other&#8221; WER increases from 5.0 to 5.4.\nOur hypothesis is that an extremely large codebook makes the optimization harder, and more training time might be required to get good representation.\nBased on above observations, we conclude that a codebook with high resolution helps Chunk SSL to transfer knowledge to the downstream task. However, an extremely large FSQ codebook might make the pre-training hard and hurt the downstream task.</p>\n\n",
            "<p class=\"ltx_p\">The next thing we examined is computation cost.\nWe don&#8217;t observe any statistical difference for the FSQ training time when the codebook size changes, since majority of computation time is spent on data input/output, encoder and decoder Transformer/Conformer layer computation. On the other hand, a Chunk SSL training with a large codebook can increase the training time slightly that we find\nabout 10% training time increase when the codebook size grows from 1000 to 6 millions as shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Low latency speech human-machine communication is becoming increasingly necessary as speech technology advances quickly in the last decade.\nOne of the primary factors behind the advancement of speech technology is self-supervised learning.\nMost self-supervised learning algorithms are designed with full utterance assumption and compromises have to made if partial utterances are presented, which are common in the streaming applications.\nIn this work, we propose a chunk based self-supervised learning (Chunk SSL) algorithm as an unified solution for both streaming and offline speech pre-training.\nChunk SSL is optimized with the masked prediction loss and\nan acoustic encoder is encouraged to restore indices of those masked speech frames with help from unmasked frames in the same chunk and preceding chunks.\nA copy and append data augmentation approach is proposed to conduct efficient chunk based pre-training.\nChunk SSL utilizes a finite scalar quantization (FSQ) module to discretize input speech features and our study shows a high resolution FSQ codebook, i.e., a codebook with vocabulary size up to a few millions, is beneficial to transfer knowledge from the pre-training task to the downstream tasks. A group masked prediction loss is employed during pre-training to alleviate the high memory and computation cost introduced by the large codebook.\nThe proposed approach is examined in two speech to text tasks, i.e., speech recognition and speech translation.\nExperimental results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> datasets show that the proposed method could achieve\nvery competitive results for speech to text tasks at both streaming and offline modes.</p>\n\n",
                "matched_terms": [
                    "scalar",
                    "finite",
                    "quantization",
                    "size",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we focus on building an encoder suitable for both streaming and offline modes,\nwith a chunkwise self-supervised learning (Chunk SSL) framework as depicted in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>.\nChunk SSL aims to\nrestore discrete indices of the masked frames based on unmasked frames in the last chunk and previous chunks.\nThe discrete indices are estimated with finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> (FSQ) with vocabulary size up to millions&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).The pre-training FSQ token has more fine-grained resolution compared with the downstream modeling units, such as phonemes or sentencepiece tokens,\nwhich usually are with vocabulary size ranging from tens to tens of thousands.\nWe hypothesize that speech frames associated with a high resolution FSQ token are mainly mapped to an unique modeling unit in the downstream task and it makes the knowledge transfer easier from the pre-training stage to the fine-tuning stage.\nHowever, those high resolution FSQ tokens pose a great challenge for modeling and we propose to decompose a large codebook\ninto small channel based sub-codebooks to alleviate the memory and computation cost during pre-training. Details could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "scalar",
                    "finite",
                    "quantization",
                    "size",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training procedure is described in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. Input features are first downsampled with a stacked subsampling module. The outputs are then divided into chunks with fixed duration, They are considered as base chunks.\nThe consecutive chunk of each base chunk is copied and appended to the end of the utterance in a sequential order. Those newly created chunks are called extended chunks.\nFor example, there are 12 speech input frames in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a> after subsampling. Assuming the chunk size is 4 frames, the speech features are segmented into three base chunks (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#00FFFF;\">cyan</span>): &#8220;B1&#8221;, &#8220;B2&#8221; and &#8220;B3&#8221;.\nChunk &#8220;B2&#8221; and &#8220;B3&#8221; are copied and appended to the end of the utterance as &#8220;E1&#8221; and &#8220;E2&#8221; (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#BFFF00;\">lime</span>).\nThey correspond to the extended chunks of &#8220;B1&#8221; and &#8220;B2&#8221; respectively.\nThere is no extended chunk for the last chunk (&#8220;B3&#8221;).\nExtended chunks act as right most chunks with different preceding chunks in the naive Chunk SSL algorithm and\nmasking is only applied on frames of extended chunks.</p>\n\n",
                "matched_terms": [
                    "size",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of input frames in an utterance and is a multiple of chunk size <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>.\nThe augmented input sequence length is <math alttext=\"N^{\\prime}=2N-C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>N</mi><mo>&#8242;</mo></msup><mo>=</mo><mrow><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow><mo>&#8722;</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">N^{\\prime}=2N-C</annotation></semantics></math>, the number of base chunks is <math alttext=\"M=N/C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/C</annotation></semantics></math> and the number of extended chunks is <math alttext=\"M-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M-1</annotation></semantics></math>.\nWe define <math alttext=\"m(i)=\\lfloor{i/C}\\rfloor\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">&#8970;</mo><mrow><mi>i</mi><mo>/</mo><mi>C</mi></mrow><mo stretchy=\"false\">&#8971;</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m(i)=\\lfloor{i/C}\\rfloor</annotation></semantics></math> as the chunk index of frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. If the left context is infinite, i.e., the model could access all history information, a CADA masking matrix <math alttext=\"\\bm{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">\\bm{\\alpha}</annotation></semantics></math> is defined as</p>\n\n",
                "matched_terms": [
                    "size",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S2.F2.sf1\" title=\"Figure 2(a) &#8227; Figure 2 &#8227; 2.1 CADA self-attention module &#8227; 2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> depicts a CADA masking example for an utterance with 6 frames before CADA. The first column is the frame index. The number in the bracket (<span class=\"ltx_text\" style=\"--ltx-fg-color:#BF8040;\">brown</span>) stands for the extended chunk frame&#8217;s original position, i.e., the position of the base chunk frame that the frame is copied from.\n<math alttext=\"{\\bm{\\alpha}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">{\\bm{\\alpha}}</annotation></semantics></math> for the extended chunks are presented in italic fonts. A self-attention module equipped with a CADA masking matrix can process a CADA sequence with position information preserved.</p>\n\n",
                "matched_terms": [
                    "stands",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, we leverage finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> to quantize\ninput frames and obtain their discrete indices.\nA FSQ module is built prior to the Chunk SSL pre-training.\nIn the quantization module, an utterance based channel mean and variance normalization is first applied to input frames <math alttext=\"{\\bm{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119935;</mi><annotation encoding=\"application/x-tex\">{\\bm{X}}</annotation></semantics></math> to obtain <math alttext=\"\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#119935;</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119961;</mi><mo>^</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}</annotation></semantics></math>, then it is processed by a FSQ encoder <math alttext=\"\\text{Enc}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mtext>Enc</mtext><mi>f</mi></msub><annotation encoding=\"application/x-tex\">\\text{Enc}_{f}</annotation></semantics></math> and projected into a low-dimension space with size <math alttext=\"d^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>d</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">d^{\\prime}</annotation></semantics></math>, where <math alttext=\"d^{\\prime}\\ll d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>d</mi><mo>&#8242;</mo></msup><mo>&#8810;</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d^{\\prime}\\ll d</annotation></semantics></math>. The output of each channel <math alttext=\"r\\in[1,d^{\\prime}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><msup><mi>d</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in[1,d^{\\prime}]</annotation></semantics></math> for input frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is rounded to an integer <math alttext=\"h_{i,r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,r}</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "size",
                    "quantization",
                    "finite",
                    "scalar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\bm{e}}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#119942;</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{e}}_{j}</annotation></semantics></math> is the <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m7\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>-th output embedding, <math alttext=\"{\\bm{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m8\" intent=\":literal\"><semantics><mi>&#119924;</mi><annotation encoding=\"application/x-tex\">{\\bm{M}}</annotation></semantics></math> is the masked frames set, and <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m9\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> is vocabulary size from the quantization module.</p>\n\n",
                "matched_terms": [
                    "size",
                    "quantization",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize a high resolution FSQ codebook would be beneficial for downstreaming tasks due to two reasons. First, FSQ does not suffered from codebook collapse and can achieve high codebook usage.\nSecond, a high resolution codebook divides the feature space into more fine-grained bins and frames sharing the same token index might be more closer to each other compared with frames associated with a token index from a low resolution codebook.\nWhen the FSQ codebook is large enough, each FSQ token could be mainly associated with one modeling unit, such as phoneme, in the downstreaming task, hence it will make the knowledge transfer easier from the pre-training stage to the fine-tuning stage. This hypothesis is examined in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S5.SS2\" title=\"5.2 Impact of FSQ codebook sizes &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.\nHowever, a high resolution FSQ codebook poses a great challenge for optimization.\nFor example, the codebook with vocabulary size 1,265,625 and embedding size 512 would take about 2.4G memory if they are stored in float data type.\nIn order to alleviate this issue, we propose to decompose the codebook into a group of channel based sub-codebooks and compute prediction loss for\neach group one by one. Given frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, the sub-codebook index at <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>th channel is <math alttext=\"h_{i,r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,r}</annotation></semantics></math>.\nWe define the group masked prediction loss as</p>\n\n",
                "matched_terms": [
                    "size",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\bm{e}}_{j}^{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119942;</mi><mi>j</mi><mi>r</mi></msubsup><annotation encoding=\"application/x-tex\">{\\bm{e}}_{j}^{r}</annotation></semantics></math> is the <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math>th output embedding at the <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>th channel sub-codebook. Optimization on sub-codebook individually is equivalent to optimizing the full codebook, since a perfect system that accomplishes the\nsub-tasks in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E5\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a> could solve the&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E4\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a> too, but with much less memory requirement.\nFor the same codebook with number of levels per channel <math alttext=\"[5,5,5,5,5,5,3,3,3,3]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>3</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[5,5,5,5,5,5,3,3,3,3]</annotation></semantics></math> and size 1,265,625 aforementioned, they only take about 84k storage memory.</p>\n\n",
                "matched_terms": [
                    "size",
                    "levels"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speech encoder is a Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib15\" title=\"\">2020</a>)</cite> based chunk encoder.\nThe encoder is equipped with a relative positional embedding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib32\" title=\"\">2018</a>)</cite> and starts with a stacking layer to down-sample the input features by four times.\nTwo model configurations: base and large, have been explored. The base encoder has 12 Conformer layers, input embedding size of 512, 8 attention heads, feedforward layer dimension 2048 and convolutional module kernel size 31. The large encoder has 24 Conformer layers, input embedding size of 768, 16 attention heads, feedforward layer dimension 3072 and convolutional module kernel size 5.\nTransducer is adopted in the fine-tuning experiments.\nThe predictor module is with one Transformer layer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> for speech recognition tasks and two layers for speech translation tasks.\nThe input embedding and feedforward layer dimension are the same as ones in the encoder setting if not mentioned specifically. The joint module is a feedforward layer as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib41\" title=\"\">2020b</a>)</cite> with embedding dimension 1024.</p>\n\n",
                "matched_terms": [
                    "size",
                    "times"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Input speech is represented as 80-dimensional log mel-filterbank coefficients computed every 10ms with a 25ms window. Global channel mean and variance normalization is applied so the trained model could be used for both streaming and offline scenarios.\nWe set the maximum utterance duration to 75 seconds and minimum duration to 5 seconds in pre-training.\nDuring fine-tuning,\na look-ahead chunk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib33\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib24\" title=\"\">2021</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib35\" title=\"\">2023</a>)</cite> is utilized and\ndynamic chunk training is employed by default. We alternate between offline training with infinite chunk size, and streaming training with chunk duration sampled from [160, 320, 640, 960, 1280, 1600] ms randomly within each epoch.\nMore details about optimization, such as batch sizes and training schedulers, for different experiments are presented in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A1\" title=\"Appendix A Optimization configures &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nThe target labels are encoded with SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib21\" title=\"\">2018</a>)</cite>. For both speech recognition and speech translation tasks, the vocabulary is an unigram model with size 1024 and full character coverage on the corresponding training text data.</p>\n\n",
                "matched_terms": [
                    "size",
                    "vocabulary",
                    "different",
                    "sizes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FSQ module is built with the ResNet based encoder and decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Langman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib22\" title=\"\">2024</a>)</cite>. There are 12 layers for both encoder and encoder with embedding size 512.\nThe frontend processing is the same as the one described in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S4.SS2\" title=\"4.2 Model configures &#8227; 4 Experimental Settings &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>. The mel-filterbank feature is with a 25ms window and 10ms shift.\n4 mel-filterbank stacked features are used as input for the FSQ encoder and the reconstructed target for the FSQ decoder. The default encoder output dimension is 12 with number of levels per channel <math alttext=\"[5,5,5,5,5,3,3,3,3,3,3,3]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>3</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[5,5,5,5,5,3,3,3,3,3,3,3]</annotation></semantics></math>. It is equivalent to a codebook with vocabulary size 6,834,375.</p>\n\n",
                "matched_terms": [
                    "size",
                    "vocabulary",
                    "levels"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to verify our a high resolution FSQ codebook hypothesis discussed in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>,\nwe study FSQ tokenizers with different codebook sizes. Two aspects are examined: first the agreement among FSQ tokens and phonemes, and second WERs for ASR models initialized with different FSQ tokenizers.\nFor the agreement among FSQ tokens and phonemes, we compare the overlap of features associated with different tokens and phonemes. It can be measured by phone purity and phone-normalized mutual information (PNMI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib16\" title=\"\">2021a</a>)</cite>. Phone purity denotes the conditional probability of a phoneme given a FSQ index while PNMI measures the percentage of uncertainty about a phoneme label eliminated after observing a specific FSQ token.\nWe generate phoneme based alignment with Montreal Aligner&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib27\" title=\"\">2017</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/MontrealCorpusTools/mfa-models/releases/download/acoustic-english_us_arpa-v3.0.0/english_us_arpa.zip</span></span></span>\non two <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> <span class=\"ltx_text ltx_font_typewriter\">dev</span> sets.</p>\n\n",
                "matched_terms": [
                    "phonenormalized",
                    "pnmi",
                    "phone",
                    "purity",
                    "mutual",
                    "sizes",
                    "different",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the base Transducer model to analyze the impact of different chunk sizes on latency and performance. We use Length-Adaptive Average Lagging(LAAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib30\" title=\"\">2022</a>)</cite> as latency scorer, which is evaluated with the Simuleval toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib26\" title=\"\">2020</a>)</cite>. We employ greedy decoding instead of beam search decoding for simplicity in this study.</p>\n\n",
                "matched_terms": [
                    "different",
                    "sizes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a> depicts the model performance (<span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">blue</span>) and latency (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">red</span>) under different chunk sizes for speech recognition&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>) and speech translation&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf3\" title=\"Figure 3(c) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>). The speech recognition is evaluated on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> <span class=\"ltx_text ltx_font_typewriter\">dev-other</span> set and speech translation tasks are evaluated on the corresponding <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> <span class=\"ltx_text ltx_font_typewriter\">dev</span> set.\nIt can be seen that the model is with a small latency (LAAL 0.47 second) but high WER (7.6) when the chunk size is small (160ms). The performance improves steadily when the chunk size increases but at the cost of increasing latency. The speech translation experiments have similar observations. The results indicate the model can switch from streaming mode to offline mode smoothly as the chunk sizes change.\nOther latency evaluation results, such as Average Lagging (AL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib25\" title=\"\">2019</a>)</cite> and Differentiable Average Lagging (DAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arivazhagan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib1\" title=\"\">2019</a>)</cite>, could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A2\" title=\"Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a></p>\n\n",
                "matched_terms": [
                    "size",
                    "different",
                    "devother",
                    "sizes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, a chunkwise speech self-supervised learning approach is proposed. The model is trained to\nlearn to reconstruct the masked information in the right most chunk based on unmasked frames in the same chunk and preceding chunks. An efficient copy and append data augmentation method is proposed to\nparallel chunk-wise computation.\nFSQ is employed to generate high resolution discrete tokens and a group based decomposition method is proposed to alleviate the computation challenge.\nThe model is trained with varied chunk durations to fit\nthe different scenarios, hence a pre-trained model could be used for both streaming and offline\napplications. Our experiments show that a model initialized with Chunk SSL could achieve very\ncompetitive offline results and excellent streaming results on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> translation directions.</p>\n\n",
                "matched_terms": [
                    "different",
                    "information"
                ]
            }
        ]
    },
    "A2.T5": {
        "caption": "Table 5: Latency and WER vs chunk size on base model trained with Librispeech evaluated on dev-other.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Chunk Size (ms)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">LAAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">AL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">AP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">DAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">ATD</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">160</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">469.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-1753.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">900.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">760.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">320</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">682.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-1492.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1133.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">908.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">640</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1161.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-833.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1686.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">724.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">960</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1627.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-177.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2223.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">785.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1280</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2106.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">492.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2754.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">928.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1600</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2566.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1116.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3246.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1083.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1920</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2988.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1686.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3678.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1230.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2240</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3373.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2215.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">4058.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1365.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "chunk",
            "atd",
            "evaluated",
            "wer",
            "base",
            "dal",
            "model",
            "librispeech",
            "size",
            "latency",
            "devother",
            "laal",
            "trained"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The full latency evaluation results including Average Lagging (AL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib25\" title=\"\">2019</a>)</cite>, Length Adaptive Average Lagging (LAAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib30\" title=\"\">2022</a>)</cite>,\nAverage Proportion (AP)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho &amp; Esipova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib7\" title=\"\">2016</a>)</cite>, Differentiable Average Lagging (DAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arivazhagan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib1\" title=\"\">2019</a>)</cite>, and Average Token Delay (ATD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib19\" title=\"\">2023</a>)</cite> for the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> are presented in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#A2.T5\" title=\"Table 5 &#8227; Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#A2.T6\" title=\"Table 6 &#8227; Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Low latency speech human-machine communication is becoming increasingly necessary as speech technology advances quickly in the last decade.\nOne of the primary factors behind the advancement of speech technology is self-supervised learning.\nMost self-supervised learning algorithms are designed with full utterance assumption and compromises have to made if partial utterances are presented, which are common in the streaming applications.\nIn this work, we propose a chunk based self-supervised learning (Chunk SSL) algorithm as an unified solution for both streaming and offline speech pre-training.\nChunk SSL is optimized with the masked prediction loss and\nan acoustic encoder is encouraged to restore indices of those masked speech frames with help from unmasked frames in the same chunk and preceding chunks.\nA copy and append data augmentation approach is proposed to conduct efficient chunk based pre-training.\nChunk SSL utilizes a finite scalar quantization (FSQ) module to discretize input speech features and our study shows a high resolution FSQ codebook, i.e., a codebook with vocabulary size up to a few millions, is beneficial to transfer knowledge from the pre-training task to the downstream tasks. A group masked prediction loss is employed during pre-training to alleviate the high memory and computation cost introduced by the large codebook.\nThe proposed approach is examined in two speech to text tasks, i.e., speech recognition and speech translation.\nExperimental results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> datasets show that the proposed method could achieve\nvery competitive results for speech to text tasks at both streaming and offline modes.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk",
                    "librispeech",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we focus on building an encoder suitable for both streaming and offline modes,\nwith a chunkwise self-supervised learning (Chunk SSL) framework as depicted in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>.\nChunk SSL aims to\nrestore discrete indices of the masked frames based on unmasked frames in the last chunk and previous chunks.\nThe discrete indices are estimated with finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> (FSQ) with vocabulary size up to millions&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).The pre-training FSQ token has more fine-grained resolution compared with the downstream modeling units, such as phonemes or sentencepiece tokens,\nwhich usually are with vocabulary size ranging from tens to tens of thousands.\nWe hypothesize that speech frames associated with a high resolution FSQ token are mainly mapped to an unique modeling unit in the downstream task and it makes the knowledge transfer easier from the pre-training stage to the fine-tuning stage.\nHowever, those high resolution FSQ tokens pose a great challenge for modeling and we propose to decompose a large codebook\ninto small channel based sub-codebooks to alleviate the memory and computation cost during pre-training. Details could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, speech features are first extracted from the input audio, and grouped into equal sized chunks.\nInstead of calculating those chunks from left to right sequentially, a copy and append data augmentation (CADA) is introduced to parallelize the computation for all chunks in the same utterance (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S2\" title=\"2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).\nCADA augments the input sequence by copying input chunks and appending them to the end of utterance as extended chunks.\nMasking is applied to frames in extended chunks only.\nAugmented utterances are then processed by a CADA compatible Conformer encoder, which could handle those augmented chunks properly even though frames in extended chunks are with altered location information. Finally,\nthe model is optimized by restoring FSQ indices of those masked frames from Conformer encoder outputs.\nExperiments on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> datasets indicate that\nthe same model initialized with Chunk SSL could achieve very competitive results\non both streaming and offline speech to text tasks.\nIt shows that Chunk SSL eliminates the need to build a dedicated streaming and dedicated offline model.\nTo summarize, our contributions includes:</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results show the proposed method can build one model for both scenarios and achieve competitive results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> datasets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training procedure is described in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. Input features are first downsampled with a stacked subsampling module. The outputs are then divided into chunks with fixed duration, They are considered as base chunks.\nThe consecutive chunk of each base chunk is copied and appended to the end of the utterance in a sequential order. Those newly created chunks are called extended chunks.\nFor example, there are 12 speech input frames in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a> after subsampling. Assuming the chunk size is 4 frames, the speech features are segmented into three base chunks (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#00FFFF;\">cyan</span>): &#8220;B1&#8221;, &#8220;B2&#8221; and &#8220;B3&#8221;.\nChunk &#8220;B2&#8221; and &#8220;B3&#8221; are copied and appended to the end of the utterance as &#8220;E1&#8221; and &#8220;E2&#8221; (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#BFFF00;\">lime</span>).\nThey correspond to the extended chunks of &#8220;B1&#8221; and &#8220;B2&#8221; respectively.\nThere is no extended chunk for the last chunk (&#8220;B3&#8221;).\nExtended chunks act as right most chunks with different preceding chunks in the naive Chunk SSL algorithm and\nmasking is only applied on frames of extended chunks.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of input frames in an utterance and is a multiple of chunk size <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>.\nThe augmented input sequence length is <math alttext=\"N^{\\prime}=2N-C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>N</mi><mo>&#8242;</mo></msup><mo>=</mo><mrow><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow><mo>&#8722;</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">N^{\\prime}=2N-C</annotation></semantics></math>, the number of base chunks is <math alttext=\"M=N/C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/C</annotation></semantics></math> and the number of extended chunks is <math alttext=\"M-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M-1</annotation></semantics></math>.\nWe define <math alttext=\"m(i)=\\lfloor{i/C}\\rfloor\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">&#8970;</mo><mrow><mi>i</mi><mo>/</mo><mi>C</mi></mrow><mo stretchy=\"false\">&#8971;</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m(i)=\\lfloor{i/C}\\rfloor</annotation></semantics></math> as the chunk index of frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. If the left context is infinite, i.e., the model could access all history information, a CADA masking matrix <math alttext=\"\\bm{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">\\bm{\\alpha}</annotation></semantics></math> is defined as</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the second row represents a frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> from base chunks can also access limited future information from its extended chunk</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the fourth row means a frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i4.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> from extended chunks can access all information in its base chunk and preceding chunks of the base chunk</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S2.F2.sf1\" title=\"Figure 2(a) &#8227; Figure 2 &#8227; 2.1 CADA self-attention module &#8227; 2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> depicts a CADA masking example for an utterance with 6 frames before CADA. The first column is the frame index. The number in the bracket (<span class=\"ltx_text\" style=\"--ltx-fg-color:#BF8040;\">brown</span>) stands for the extended chunk frame&#8217;s original position, i.e., the position of the base chunk frame that the frame is copied from.\n<math alttext=\"{\\bm{\\alpha}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">{\\bm{\\alpha}}</annotation></semantics></math> for the extended chunks are presented in italic fonts. A self-attention module equipped with a CADA masking matrix can process a CADA sequence with position information preserved.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A convolutional layer has a set of filters which slide across input features within the same utterance and extract localized representations. A convolutional layer assumes the input frames organized as consecutively, which no longer holds true for extended chunk frames in a CADA utterance.\nThis problem can be resolved via a chunk based convolutional computation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib23\" title=\"\">2023</a>)</cite>.\nWithout loss of generality, we assume the convolution module is a depth-wise convolution layer with <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> left and <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> right context frames. The modified computation includes three steps.\nFirst, we separate the CADA utterance into chunks and pair a base chunk and its extended chunk together, for example &#8220;B1&#8221; and &#8220;E1&#8221; in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>; then the last <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> frames from the chunk preceding the base chunk\nare appended to the left as left context. If the base chunk is the first chunk, <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math>&#160;zero padding frames are attached. Similarly, <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> zero padding frames are appended to the right and it leads to a new concatenated subsequence with length <math alttext=\"L_{lc}+2C+L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi></mrow><mo>+</mo><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">L_{lc}+2C+L_{rc}</annotation></semantics></math>.\nSecond, the concatenated subsequence is fed to the convolution module and obtains output subsequence with length <math alttext=\"2C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">2C</annotation></semantics></math>. <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> and <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> context frames are absorbed during convolution\ncomputation. The first <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> output frames correspond to the base chunk and the second <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m11\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> frames are for the extended chunk.\nFinally, those output chunks are placed back to their original positions in the augmented utterance as the outputs from the CADA convolutional module.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S2.F2.sf2\" title=\"Figure 2(b) &#8227; Figure 2 &#8227; 2.1 CADA self-attention module &#8227; 2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> demonstrates three steps to conduct convolution computation on an augmented utterance with base sequence length 12 and chunk size 4.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Dynamic chunk training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib40\" title=\"\">2020a</a>; Weninger et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib39\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib23\" title=\"\">2023</a>)</cite> is a useful fine-tuning approach for the chunk based ASR encoder.\nDuring training, the chunk duration varied from hundreds of million seconds to a few seconds is randomly chosen for every model update. A model built with dynamic chunk training is suitable for both streaming and offline modes.\nWe extend dynamic chunk training for the Chunk SSL pre-training. Chunk duration is chosen from 6 durations (<math alttext=\"[640,1280,1920,2560,3200,3840]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>640</mn><mo>,</mo><mn>1280</mn><mo>,</mo><mn>1920</mn><mo>,</mo><mn>2560</mn><mo>,</mo><mn>3200</mn><mo>,</mo><mn>3840</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[640,1280,1920,2560,3200,3840]</annotation></semantics></math> million seconds) randomly for every model update.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, we leverage finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> to quantize\ninput frames and obtain their discrete indices.\nA FSQ module is built prior to the Chunk SSL pre-training.\nIn the quantization module, an utterance based channel mean and variance normalization is first applied to input frames <math alttext=\"{\\bm{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119935;</mi><annotation encoding=\"application/x-tex\">{\\bm{X}}</annotation></semantics></math> to obtain <math alttext=\"\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#119935;</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119961;</mi><mo>^</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}</annotation></semantics></math>, then it is processed by a FSQ encoder <math alttext=\"\\text{Enc}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mtext>Enc</mtext><mi>f</mi></msub><annotation encoding=\"application/x-tex\">\\text{Enc}_{f}</annotation></semantics></math> and projected into a low-dimension space with size <math alttext=\"d^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>d</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">d^{\\prime}</annotation></semantics></math>, where <math alttext=\"d^{\\prime}\\ll d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>d</mi><mo>&#8242;</mo></msup><mo>&#8810;</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d^{\\prime}\\ll d</annotation></semantics></math>. The output of each channel <math alttext=\"r\\in[1,d^{\\prime}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><msup><mi>d</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in[1,d^{\\prime}]</annotation></semantics></math> for input frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is rounded to an integer <math alttext=\"h_{i,r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,r}</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder is optimized with masked prediction loss. We encourage the Chunk SSL model to reconstruct the masked frames based on the context information provided.\nMore specifically, we mask half consecutive frames in every extended chunk. The start position of the masking frame is randomly chosen from <math alttext=\"[0,\\frac{C}{4}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mfrac><mi>C</mi><mn>4</mn></mfrac><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,\\frac{C}{4}]</annotation></semantics></math> at every extended chunk, so we have enough unmasked frames on both sides of the masked frames as context to infer those masked frames.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming the Chunk SSL model outputs for the augmented sequence is <math alttext=\"{\\bm{O}}=\\{{\\bm{o}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119926;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119952;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{O}}=\\{{\\bm{o}}_{i}\\}</annotation></semantics></math>. The FSQ index of <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is denoted as <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math> and\n<math alttext=\"{\\mathbf{e}}_{\\mu_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><msub><mi>&#956;</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">{\\mathbf{e}}_{\\mu_{i}}</annotation></semantics></math> is the output embedding of <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math>. The masked prediction loss is defined as</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Tuning</span>: For speech recognition task, the pre-trained models are finetuned and evaluated on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib29\" title=\"\">2015</a>)</cite> dataset. We use two <span class=\"ltx_text ltx_font_typewriter\">dev</span> sets for development and report all results from <span class=\"ltx_text ltx_font_typewriter\">dev</span> and <span class=\"ltx_text ltx_font_typewriter\">test</span> sets.\nFor speech translation task, experiments are conducted on two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib12\" title=\"\">2019</a>)</cite>\nlanguage pairs: English to German (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>DE) and English to Spanish (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ES). Sequence level knowledge\ndistillation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Rush, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib20\" title=\"\">2016</a>)</cite> is applied to boost the speech translation quality.\nThe models are developed on\nthe <span class=\"ltx_text ltx_font_typewriter\">dev</span> set, and the final results are reported on the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> set.</p>\n\n",
                "matched_terms": [
                    "evaluated",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speech encoder is a Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib15\" title=\"\">2020</a>)</cite> based chunk encoder.\nThe encoder is equipped with a relative positional embedding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib32\" title=\"\">2018</a>)</cite> and starts with a stacking layer to down-sample the input features by four times.\nTwo model configurations: base and large, have been explored. The base encoder has 12 Conformer layers, input embedding size of 512, 8 attention heads, feedforward layer dimension 2048 and convolutional module kernel size 31. The large encoder has 24 Conformer layers, input embedding size of 768, 16 attention heads, feedforward layer dimension 3072 and convolutional module kernel size 5.\nTransducer is adopted in the fine-tuning experiments.\nThe predictor module is with one Transformer layer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> for speech recognition tasks and two layers for speech translation tasks.\nThe input embedding and feedforward layer dimension are the same as ones in the encoder setting if not mentioned specifically. The joint module is a feedforward layer as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib41\" title=\"\">2020b</a>)</cite> with embedding dimension 1024.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Input speech is represented as 80-dimensional log mel-filterbank coefficients computed every 10ms with a 25ms window. Global channel mean and variance normalization is applied so the trained model could be used for both streaming and offline scenarios.\nWe set the maximum utterance duration to 75 seconds and minimum duration to 5 seconds in pre-training.\nDuring fine-tuning,\na look-ahead chunk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib33\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib24\" title=\"\">2021</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib35\" title=\"\">2023</a>)</cite> is utilized and\ndynamic chunk training is employed by default. We alternate between offline training with infinite chunk size, and streaming training with chunk duration sampled from [160, 320, 640, 960, 1280, 1600] ms randomly within each epoch.\nMore details about optimization, such as batch sizes and training schedulers, for different experiments are presented in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A1\" title=\"Appendix A Optimization configures &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nThe target labels are encoded with SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib21\" title=\"\">2018</a>)</cite>. For both speech recognition and speech translation tasks, the vocabulary is an unigram model with size 1024 and full character coverage on the corresponding training text data.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk",
                    "model",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results are evaluated using an averaged model from checkpoints of the best 10 epochs.\nSpeech recognition experiments are evaluated with WER while speech translation results are measure with scacre BLEU score<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>signature: nrefs:1<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>case:mixed<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>eff:no<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>tok:13a<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>smooth:exp<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>version:2.4.2</span></span></span>.\nWe report both streaming and offline results.\nFor the streaming decoding, we set the chunk size to 320ms by default if not mentioned specifically. No language model is used in all experiments.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "evaluated",
                    "wer",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained Chunk SSL models are fine-tuned with dynamic chunk training and they could be used for both streaming and offline ASR.\nWe list the offline and streaming recognition results from the Chunk SSL models in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>.\nThe reported literature results are presented in the first part of the tables.\nIt is clear that the chunk SSL pre-trained models could achieve very competitive results compared to other strong baselines.\nThe base and large models outperform the corresponding wav2vec2 base and large models.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the streaming evaluation, the Chunk SSL model with the base configuration excels all models reported in the literature shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>. The large configuration model pushes the WER even lower and it achieves another 0.5 average WER reduction compared to the base configure model. Comparing results in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>,\nthe proposed method significantly reduces the performance gap between the streaming and offline applications. The WER gap between the large configure\nmodels is 0.8 in average for Chunk SSL while the average WER difference is 2.5 for BEST-RQ.\nNote, literature models listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a> are initialized with dedicated pre-trained models either offline or streaming, while a Chunk SSL model could be initialized with the same pre-trained model and operated in both modes.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the speech translation experiments, we evaluate on both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; directions and the results are listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. The baselines are initialized with a speech recognition acoustic encoder (&#8220;ASR&#8221; in the column &#8220;Init.&#8221;), which is trained with the English data in the <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> English-Spanish direction. Both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; results show that the encoder initialized with Chunk SSL outperforms the one initialized with a speech recognition encoder trained with <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> data only. We could draw similar conclusion as the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> experiment that the Chunk SSL could effectively improve speech translation quality for both streaming and offline cases.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "librispeech",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> lists models trained with different codebook sizes with the base configure. The first column gives FSQ vocabulary sizes, the second column provides the Chunk SSL training time (in seconds) for 1000 updates.\n&#8220;levels&#8221; is the FSQ levels for quantization. For example, &#8220;4(<math alttext=\"8\\times 1-5\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow></mrow><annotation encoding=\"application/x-tex\">8\\times 1-5\\times 3</annotation></semantics></math>)&#8221; in the first row means there are 4 output channels, the first channel is with level 8 and the remaining 3 channels are with level 5. They span a codebook with size 1000.\n&#8220;phn pur.&#8221; stands for phone purity.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "trained",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first two rows listed models trained with FSQ vocabulary size 1000. &#8220;1000*&#8221; means the model is optimized with a full codebook as&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E4\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, while &#8220;1000&#8221; indicates the model is pretrained with the group masked prediction loss following&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E5\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>. The results shows two models\nachieve comparable accuracies\nwith similar pre-training time.\nAs shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, when the codebook size increases, &#8220;phn pur.&#8221; and &#8220;PNMI&#8221; increase steadily.\nBoth metrics show tokens in the codebook with a high resolution have better consistency with phonemes. This confirms our assumption that frames associated with a specific FSQ token are more likely being labelled with the same phoneme if the codebook resolution is high enough.\nOn the other hand, we observe WER is reduced when FSQ codebook size becomes bigger.\nIt is peaked for the codebook size around 6 millions.\nWhen the codebook size gets extremely big, i.e. 791 millions in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, the &#8220;dev-other&#8221; WER increases from 5.0 to 5.4.\nOur hypothesis is that an extremely large codebook makes the optimization harder, and more training time might be required to get good representation.\nBased on above observations, we conclude that a codebook with high resolution helps Chunk SSL to transfer knowledge to the downstream task. However, an extremely large FSQ codebook might make the pre-training hard and hurt the downstream task.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "wer",
                    "model",
                    "size",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The next thing we examined is computation cost.\nWe don&#8217;t observe any statistical difference for the FSQ training time when the codebook size changes, since majority of computation time is spent on data input/output, encoder and decoder Transformer/Conformer layer computation. On the other hand, a Chunk SSL training with a large codebook can increase the training time slightly that we find\nabout 10% training time increase when the codebook size grows from 1000 to 6 millions as shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the base Transducer model to analyze the impact of different chunk sizes on latency and performance. We use Length-Adaptive Average Lagging(LAAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib30\" title=\"\">2022</a>)</cite> as latency scorer, which is evaluated with the Simuleval toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib26\" title=\"\">2020</a>)</cite>. We employ greedy decoding instead of beam search decoding for simplicity in this study.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "evaluated",
                    "base",
                    "model",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a> depicts the model performance (<span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">blue</span>) and latency (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">red</span>) under different chunk sizes for speech recognition&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>) and speech translation&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf3\" title=\"Figure 3(c) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>). The speech recognition is evaluated on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> <span class=\"ltx_text ltx_font_typewriter\">dev-other</span> set and speech translation tasks are evaluated on the corresponding <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> <span class=\"ltx_text ltx_font_typewriter\">dev</span> set.\nIt can be seen that the model is with a small latency (LAAL 0.47 second) but high WER (7.6) when the chunk size is small (160ms). The performance improves steadily when the chunk size increases but at the cost of increasing latency. The speech translation experiments have similar observations. The results indicate the model can switch from streaming mode to offline mode smoothly as the chunk sizes change.\nOther latency evaluation results, such as Average Lagging (AL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib25\" title=\"\">2019</a>)</cite> and Differentiable Average Lagging (DAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arivazhagan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib1\" title=\"\">2019</a>)</cite>, could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A2\" title=\"Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a></p>\n\n",
                "matched_terms": [
                    "chunk",
                    "evaluated",
                    "wer",
                    "dal",
                    "model",
                    "librispeech",
                    "size",
                    "latency",
                    "devother",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech self-supervised learning algorithms form contextual representations through learning to predict\nmissing information due to time constraint or masking. <cite class=\"ltx_cite ltx_citemacro_citet\">van&#160;den Oord et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib37\" title=\"\">2018</a>)</cite> propose an auto-regressive based SSL approach to predict the future samples based on the previous samples. Speech SSL methods such as wav2vec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>, HuBert&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>)</cite> and BEST-RQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> choose to randomly mask audio span and the model learns to restore those masked regions. Chunk SSL combines both approaches that it masks audio span in the extended chunk\nand recovers masking frame indices in an chunk based auto-regressive manner.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, a chunkwise speech self-supervised learning approach is proposed. The model is trained to\nlearn to reconstruct the masked information in the right most chunk based on unmasked frames in the same chunk and preceding chunks. An efficient copy and append data augmentation method is proposed to\nparallel chunk-wise computation.\nFSQ is employed to generate high resolution discrete tokens and a group based decomposition method is proposed to alleviate the computation challenge.\nThe model is trained with varied chunk durations to fit\nthe different scenarios, hence a pre-trained model could be used for both streaming and offline\napplications. Our experiments show that a model initialized with Chunk SSL could achieve very\ncompetitive offline results and excellent streaming results on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> translation directions.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model",
                    "librispeech",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FSQ model is built with learning rate is 2.0e-4 and ExponentialLR scheduler from PyTorch. The effective batch size is 3200 seconds with up to 180,000 updates. The FSQ module is optimized with MSE loss to restore mel-filterbank features from their quantized audio representations. It takes 4 A10 GPUs for 42 hours to build a FSQ model.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training is scheduled as Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> with warmup step 25,000 and maximum learning rate 3e-4. The batch size is 16,000 seconds for the base configure model. We choose a smaller effective batch size for the large configure model with 6,000 seconds due to computation resource limitation. The model is trained up to 400,000 updates. It takes about 10 days for 8 A100 GPUs to build a Chunk SSL encoder with the base configure.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "base",
                    "model",
                    "size",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">960 hours fine-tuning</span>\nFor the base configure model, we choose the three stages scheduler scheme as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>, i.e., warmup, hold and annealing. The warmup stage takes about 1000 updates with encoder parameters frozen, the hold stage takes about 40,000 updates and the remaining 60,000 updates are in an annealing stage. The peak learning rate in fine-tuning is 2.0e-3.\nThe SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib31\" title=\"\">2019</a>)</cite> data augmentation is applied with 2 frequency maskings of 27, and 10 time maskings at a maximum ratio of 0.05 in fine-tuning experiments.\nThe effective batch size for fine-tuning is 10,000 seconds. For the large configure model, we choose Transformer scheduler&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> since it has less hype-parameters to explore. The warmup step is 5000 with encoder parameters frozen. Similar as the base configure model, the total update step is up to 100,000. The peak learning rate is 5e-4. The effective batch size for the large model is 7200 seconds.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The translation Transducer model is optimized with Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> scheduler with warmup step 10,000 and total update step up to 100,000. The peak learning rate is 5e-4. The effective batch size is 6400 seconds.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            }
        ]
    },
    "A2.T6": {
        "caption": "Table 6: Latency and BLEU vs chunk size on base model trained with MustC EN-DE dev dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Chunk Size (ms)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">BLEU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">LAAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">AL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">AP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">DAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">ATD</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">320</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">782.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-2475.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1329.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">447.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">640</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1208.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-1841.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1826.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">440.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">960</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1635.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-1188.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2308.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">538.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1280</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2056.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-534.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2779.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">665.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1600</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2455.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3190.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">781.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1920</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2803.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">625.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3558.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">904.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2240</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3137.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1141.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3882.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1010.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "chunk",
            "atd",
            "ende",
            "laal",
            "base",
            "dal",
            "model",
            "size",
            "dev",
            "latency",
            "mustc",
            "dataset",
            "trained",
            "bleu"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The full latency evaluation results including Average Lagging (AL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib25\" title=\"\">2019</a>)</cite>, Length Adaptive Average Lagging (LAAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib30\" title=\"\">2022</a>)</cite>,\nAverage Proportion (AP)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho &amp; Esipova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib7\" title=\"\">2016</a>)</cite>, Differentiable Average Lagging (DAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arivazhagan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib1\" title=\"\">2019</a>)</cite>, and Average Token Delay (ATD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib19\" title=\"\">2023</a>)</cite> for the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> are presented in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#A2.T5\" title=\"Table 5 &#8227; Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#A2.T6\" title=\"Table 6 &#8227; Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Low latency speech human-machine communication is becoming increasingly necessary as speech technology advances quickly in the last decade.\nOne of the primary factors behind the advancement of speech technology is self-supervised learning.\nMost self-supervised learning algorithms are designed with full utterance assumption and compromises have to made if partial utterances are presented, which are common in the streaming applications.\nIn this work, we propose a chunk based self-supervised learning (Chunk SSL) algorithm as an unified solution for both streaming and offline speech pre-training.\nChunk SSL is optimized with the masked prediction loss and\nan acoustic encoder is encouraged to restore indices of those masked speech frames with help from unmasked frames in the same chunk and preceding chunks.\nA copy and append data augmentation approach is proposed to conduct efficient chunk based pre-training.\nChunk SSL utilizes a finite scalar quantization (FSQ) module to discretize input speech features and our study shows a high resolution FSQ codebook, i.e., a codebook with vocabulary size up to a few millions, is beneficial to transfer knowledge from the pre-training task to the downstream tasks. A group masked prediction loss is employed during pre-training to alleviate the high memory and computation cost introduced by the large codebook.\nThe proposed approach is examined in two speech to text tasks, i.e., speech recognition and speech translation.\nExperimental results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> datasets show that the proposed method could achieve\nvery competitive results for speech to text tasks at both streaming and offline modes.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk",
                    "latency",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we focus on building an encoder suitable for both streaming and offline modes,\nwith a chunkwise self-supervised learning (Chunk SSL) framework as depicted in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>.\nChunk SSL aims to\nrestore discrete indices of the masked frames based on unmasked frames in the last chunk and previous chunks.\nThe discrete indices are estimated with finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> (FSQ) with vocabulary size up to millions&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).The pre-training FSQ token has more fine-grained resolution compared with the downstream modeling units, such as phonemes or sentencepiece tokens,\nwhich usually are with vocabulary size ranging from tens to tens of thousands.\nWe hypothesize that speech frames associated with a high resolution FSQ token are mainly mapped to an unique modeling unit in the downstream task and it makes the knowledge transfer easier from the pre-training stage to the fine-tuning stage.\nHowever, those high resolution FSQ tokens pose a great challenge for modeling and we propose to decompose a large codebook\ninto small channel based sub-codebooks to alleviate the memory and computation cost during pre-training. Details could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, speech features are first extracted from the input audio, and grouped into equal sized chunks.\nInstead of calculating those chunks from left to right sequentially, a copy and append data augmentation (CADA) is introduced to parallelize the computation for all chunks in the same utterance (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S2\" title=\"2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).\nCADA augments the input sequence by copying input chunks and appending them to the end of utterance as extended chunks.\nMasking is applied to frames in extended chunks only.\nAugmented utterances are then processed by a CADA compatible Conformer encoder, which could handle those augmented chunks properly even though frames in extended chunks are with altered location information. Finally,\nthe model is optimized by restoring FSQ indices of those masked frames from Conformer encoder outputs.\nExperiments on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> datasets indicate that\nthe same model initialized with Chunk SSL could achieve very competitive results\non both streaming and offline speech to text tasks.\nIt shows that Chunk SSL eliminates the need to build a dedicated streaming and dedicated offline model.\nTo summarize, our contributions includes:</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results show the proposed method can build one model for both scenarios and achieve competitive results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> datasets.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training procedure is described in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. Input features are first downsampled with a stacked subsampling module. The outputs are then divided into chunks with fixed duration, They are considered as base chunks.\nThe consecutive chunk of each base chunk is copied and appended to the end of the utterance in a sequential order. Those newly created chunks are called extended chunks.\nFor example, there are 12 speech input frames in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a> after subsampling. Assuming the chunk size is 4 frames, the speech features are segmented into three base chunks (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#00FFFF;\">cyan</span>): &#8220;B1&#8221;, &#8220;B2&#8221; and &#8220;B3&#8221;.\nChunk &#8220;B2&#8221; and &#8220;B3&#8221; are copied and appended to the end of the utterance as &#8220;E1&#8221; and &#8220;E2&#8221; (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#BFFF00;\">lime</span>).\nThey correspond to the extended chunks of &#8220;B1&#8221; and &#8220;B2&#8221; respectively.\nThere is no extended chunk for the last chunk (&#8220;B3&#8221;).\nExtended chunks act as right most chunks with different preceding chunks in the naive Chunk SSL algorithm and\nmasking is only applied on frames of extended chunks.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of input frames in an utterance and is a multiple of chunk size <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>.\nThe augmented input sequence length is <math alttext=\"N^{\\prime}=2N-C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>N</mi><mo>&#8242;</mo></msup><mo>=</mo><mrow><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow><mo>&#8722;</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">N^{\\prime}=2N-C</annotation></semantics></math>, the number of base chunks is <math alttext=\"M=N/C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/C</annotation></semantics></math> and the number of extended chunks is <math alttext=\"M-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M-1</annotation></semantics></math>.\nWe define <math alttext=\"m(i)=\\lfloor{i/C}\\rfloor\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">&#8970;</mo><mrow><mi>i</mi><mo>/</mo><mi>C</mi></mrow><mo stretchy=\"false\">&#8971;</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m(i)=\\lfloor{i/C}\\rfloor</annotation></semantics></math> as the chunk index of frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. If the left context is infinite, i.e., the model could access all history information, a CADA masking matrix <math alttext=\"\\bm{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">\\bm{\\alpha}</annotation></semantics></math> is defined as</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the second row represents a frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> from base chunks can also access limited future information from its extended chunk</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the fourth row means a frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i4.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> from extended chunks can access all information in its base chunk and preceding chunks of the base chunk</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S2.F2.sf1\" title=\"Figure 2(a) &#8227; Figure 2 &#8227; 2.1 CADA self-attention module &#8227; 2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> depicts a CADA masking example for an utterance with 6 frames before CADA. The first column is the frame index. The number in the bracket (<span class=\"ltx_text\" style=\"--ltx-fg-color:#BF8040;\">brown</span>) stands for the extended chunk frame&#8217;s original position, i.e., the position of the base chunk frame that the frame is copied from.\n<math alttext=\"{\\bm{\\alpha}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">{\\bm{\\alpha}}</annotation></semantics></math> for the extended chunks are presented in italic fonts. A self-attention module equipped with a CADA masking matrix can process a CADA sequence with position information preserved.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A convolutional layer has a set of filters which slide across input features within the same utterance and extract localized representations. A convolutional layer assumes the input frames organized as consecutively, which no longer holds true for extended chunk frames in a CADA utterance.\nThis problem can be resolved via a chunk based convolutional computation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib23\" title=\"\">2023</a>)</cite>.\nWithout loss of generality, we assume the convolution module is a depth-wise convolution layer with <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> left and <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> right context frames. The modified computation includes three steps.\nFirst, we separate the CADA utterance into chunks and pair a base chunk and its extended chunk together, for example &#8220;B1&#8221; and &#8220;E1&#8221; in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>; then the last <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> frames from the chunk preceding the base chunk\nare appended to the left as left context. If the base chunk is the first chunk, <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math>&#160;zero padding frames are attached. Similarly, <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> zero padding frames are appended to the right and it leads to a new concatenated subsequence with length <math alttext=\"L_{lc}+2C+L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi></mrow><mo>+</mo><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">L_{lc}+2C+L_{rc}</annotation></semantics></math>.\nSecond, the concatenated subsequence is fed to the convolution module and obtains output subsequence with length <math alttext=\"2C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">2C</annotation></semantics></math>. <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> and <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> context frames are absorbed during convolution\ncomputation. The first <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> output frames correspond to the base chunk and the second <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m11\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> frames are for the extended chunk.\nFinally, those output chunks are placed back to their original positions in the augmented utterance as the outputs from the CADA convolutional module.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S2.F2.sf2\" title=\"Figure 2(b) &#8227; Figure 2 &#8227; 2.1 CADA self-attention module &#8227; 2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> demonstrates three steps to conduct convolution computation on an augmented utterance with base sequence length 12 and chunk size 4.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Dynamic chunk training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib40\" title=\"\">2020a</a>; Weninger et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib39\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib23\" title=\"\">2023</a>)</cite> is a useful fine-tuning approach for the chunk based ASR encoder.\nDuring training, the chunk duration varied from hundreds of million seconds to a few seconds is randomly chosen for every model update. A model built with dynamic chunk training is suitable for both streaming and offline modes.\nWe extend dynamic chunk training for the Chunk SSL pre-training. Chunk duration is chosen from 6 durations (<math alttext=\"[640,1280,1920,2560,3200,3840]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>640</mn><mo>,</mo><mn>1280</mn><mo>,</mo><mn>1920</mn><mo>,</mo><mn>2560</mn><mo>,</mo><mn>3200</mn><mo>,</mo><mn>3840</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[640,1280,1920,2560,3200,3840]</annotation></semantics></math> million seconds) randomly for every model update.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, we leverage finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> to quantize\ninput frames and obtain their discrete indices.\nA FSQ module is built prior to the Chunk SSL pre-training.\nIn the quantization module, an utterance based channel mean and variance normalization is first applied to input frames <math alttext=\"{\\bm{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119935;</mi><annotation encoding=\"application/x-tex\">{\\bm{X}}</annotation></semantics></math> to obtain <math alttext=\"\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#119935;</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119961;</mi><mo>^</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}</annotation></semantics></math>, then it is processed by a FSQ encoder <math alttext=\"\\text{Enc}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mtext>Enc</mtext><mi>f</mi></msub><annotation encoding=\"application/x-tex\">\\text{Enc}_{f}</annotation></semantics></math> and projected into a low-dimension space with size <math alttext=\"d^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>d</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">d^{\\prime}</annotation></semantics></math>, where <math alttext=\"d^{\\prime}\\ll d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>d</mi><mo>&#8242;</mo></msup><mo>&#8810;</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d^{\\prime}\\ll d</annotation></semantics></math>. The output of each channel <math alttext=\"r\\in[1,d^{\\prime}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><msup><mi>d</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in[1,d^{\\prime}]</annotation></semantics></math> for input frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is rounded to an integer <math alttext=\"h_{i,r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,r}</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder is optimized with masked prediction loss. We encourage the Chunk SSL model to reconstruct the masked frames based on the context information provided.\nMore specifically, we mask half consecutive frames in every extended chunk. The start position of the masking frame is randomly chosen from <math alttext=\"[0,\\frac{C}{4}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mfrac><mi>C</mi><mn>4</mn></mfrac><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,\\frac{C}{4}]</annotation></semantics></math> at every extended chunk, so we have enough unmasked frames on both sides of the masked frames as context to infer those masked frames.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming the Chunk SSL model outputs for the augmented sequence is <math alttext=\"{\\bm{O}}=\\{{\\bm{o}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119926;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119952;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{O}}=\\{{\\bm{o}}_{i}\\}</annotation></semantics></math>. The FSQ index of <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is denoted as <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math> and\n<math alttext=\"{\\mathbf{e}}_{\\mu_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><msub><mi>&#956;</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">{\\mathbf{e}}_{\\mu_{i}}</annotation></semantics></math> is the output embedding of <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math>. The masked prediction loss is defined as</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Tuning</span>: For speech recognition task, the pre-trained models are finetuned and evaluated on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib29\" title=\"\">2015</a>)</cite> dataset. We use two <span class=\"ltx_text ltx_font_typewriter\">dev</span> sets for development and report all results from <span class=\"ltx_text ltx_font_typewriter\">dev</span> and <span class=\"ltx_text ltx_font_typewriter\">test</span> sets.\nFor speech translation task, experiments are conducted on two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib12\" title=\"\">2019</a>)</cite>\nlanguage pairs: English to German (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>DE) and English to Spanish (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ES). Sequence level knowledge\ndistillation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Rush, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib20\" title=\"\">2016</a>)</cite> is applied to boost the speech translation quality.\nThe models are developed on\nthe <span class=\"ltx_text ltx_font_typewriter\">dev</span> set, and the final results are reported on the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> set.</p>\n\n",
                "matched_terms": [
                    "dev",
                    "dataset",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speech encoder is a Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib15\" title=\"\">2020</a>)</cite> based chunk encoder.\nThe encoder is equipped with a relative positional embedding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib32\" title=\"\">2018</a>)</cite> and starts with a stacking layer to down-sample the input features by four times.\nTwo model configurations: base and large, have been explored. The base encoder has 12 Conformer layers, input embedding size of 512, 8 attention heads, feedforward layer dimension 2048 and convolutional module kernel size 31. The large encoder has 24 Conformer layers, input embedding size of 768, 16 attention heads, feedforward layer dimension 3072 and convolutional module kernel size 5.\nTransducer is adopted in the fine-tuning experiments.\nThe predictor module is with one Transformer layer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> for speech recognition tasks and two layers for speech translation tasks.\nThe input embedding and feedforward layer dimension are the same as ones in the encoder setting if not mentioned specifically. The joint module is a feedforward layer as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib41\" title=\"\">2020b</a>)</cite> with embedding dimension 1024.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Input speech is represented as 80-dimensional log mel-filterbank coefficients computed every 10ms with a 25ms window. Global channel mean and variance normalization is applied so the trained model could be used for both streaming and offline scenarios.\nWe set the maximum utterance duration to 75 seconds and minimum duration to 5 seconds in pre-training.\nDuring fine-tuning,\na look-ahead chunk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib33\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib24\" title=\"\">2021</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib35\" title=\"\">2023</a>)</cite> is utilized and\ndynamic chunk training is employed by default. We alternate between offline training with infinite chunk size, and streaming training with chunk duration sampled from [160, 320, 640, 960, 1280, 1600] ms randomly within each epoch.\nMore details about optimization, such as batch sizes and training schedulers, for different experiments are presented in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A1\" title=\"Appendix A Optimization configures &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nThe target labels are encoded with SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib21\" title=\"\">2018</a>)</cite>. For both speech recognition and speech translation tasks, the vocabulary is an unigram model with size 1024 and full character coverage on the corresponding training text data.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk",
                    "model",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results are evaluated using an averaged model from checkpoints of the best 10 epochs.\nSpeech recognition experiments are evaluated with WER while speech translation results are measure with scacre BLEU score<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>signature: nrefs:1<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>case:mixed<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>eff:no<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>tok:13a<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>smooth:exp<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>version:2.4.2</span></span></span>.\nWe report both streaming and offline results.\nFor the streaming decoding, we set the chunk size to 320ms by default if not mentioned specifically. No language model is used in all experiments.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk",
                    "model",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained Chunk SSL models are fine-tuned with dynamic chunk training and they could be used for both streaming and offline ASR.\nWe list the offline and streaming recognition results from the Chunk SSL models in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>.\nThe reported literature results are presented in the first part of the tables.\nIt is clear that the chunk SSL pre-trained models could achieve very competitive results compared to other strong baselines.\nThe base and large models outperform the corresponding wav2vec2 base and large models.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the streaming evaluation, the Chunk SSL model with the base configuration excels all models reported in the literature shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>. The large configuration model pushes the WER even lower and it achieves another 0.5 average WER reduction compared to the base configure model. Comparing results in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>,\nthe proposed method significantly reduces the performance gap between the streaming and offline applications. The WER gap between the large configure\nmodels is 0.8 in average for Chunk SSL while the average WER difference is 2.5 for BEST-RQ.\nNote, literature models listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a> are initialized with dedicated pre-trained models either offline or streaming, while a Chunk SSL model could be initialized with the same pre-trained model and operated in both modes.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the speech translation experiments, we evaluate on both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; directions and the results are listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. The baselines are initialized with a speech recognition acoustic encoder (&#8220;ASR&#8221; in the column &#8220;Init.&#8221;), which is trained with the English data in the <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> English-Spanish direction. Both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; results show that the encoder initialized with Chunk SSL outperforms the one initialized with a speech recognition encoder trained with <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> data only. We could draw similar conclusion as the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> experiment that the Chunk SSL could effectively improve speech translation quality for both streaming and offline cases.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "chunk",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> lists models trained with different codebook sizes with the base configure. The first column gives FSQ vocabulary sizes, the second column provides the Chunk SSL training time (in seconds) for 1000 updates.\n&#8220;levels&#8221; is the FSQ levels for quantization. For example, &#8220;4(<math alttext=\"8\\times 1-5\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow></mrow><annotation encoding=\"application/x-tex\">8\\times 1-5\\times 3</annotation></semantics></math>)&#8221; in the first row means there are 4 output channels, the first channel is with level 8 and the remaining 3 channels are with level 5. They span a codebook with size 1000.\n&#8220;phn pur.&#8221; stands for phone purity.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "trained",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first two rows listed models trained with FSQ vocabulary size 1000. &#8220;1000*&#8221; means the model is optimized with a full codebook as&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E4\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, while &#8220;1000&#8221; indicates the model is pretrained with the group masked prediction loss following&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E5\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>. The results shows two models\nachieve comparable accuracies\nwith similar pre-training time.\nAs shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, when the codebook size increases, &#8220;phn pur.&#8221; and &#8220;PNMI&#8221; increase steadily.\nBoth metrics show tokens in the codebook with a high resolution have better consistency with phonemes. This confirms our assumption that frames associated with a specific FSQ token are more likely being labelled with the same phoneme if the codebook resolution is high enough.\nOn the other hand, we observe WER is reduced when FSQ codebook size becomes bigger.\nIt is peaked for the codebook size around 6 millions.\nWhen the codebook size gets extremely big, i.e. 791 millions in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, the &#8220;dev-other&#8221; WER increases from 5.0 to 5.4.\nOur hypothesis is that an extremely large codebook makes the optimization harder, and more training time might be required to get good representation.\nBased on above observations, we conclude that a codebook with high resolution helps Chunk SSL to transfer knowledge to the downstream task. However, an extremely large FSQ codebook might make the pre-training hard and hurt the downstream task.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk",
                    "model",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The next thing we examined is computation cost.\nWe don&#8217;t observe any statistical difference for the FSQ training time when the codebook size changes, since majority of computation time is spent on data input/output, encoder and decoder Transformer/Conformer layer computation. On the other hand, a Chunk SSL training with a large codebook can increase the training time slightly that we find\nabout 10% training time increase when the codebook size grows from 1000 to 6 millions as shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the base Transducer model to analyze the impact of different chunk sizes on latency and performance. We use Length-Adaptive Average Lagging(LAAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib30\" title=\"\">2022</a>)</cite> as latency scorer, which is evaluated with the Simuleval toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib26\" title=\"\">2020</a>)</cite>. We employ greedy decoding instead of beam search decoding for simplicity in this study.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a> depicts the model performance (<span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">blue</span>) and latency (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">red</span>) under different chunk sizes for speech recognition&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>) and speech translation&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf3\" title=\"Figure 3(c) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>). The speech recognition is evaluated on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> <span class=\"ltx_text ltx_font_typewriter\">dev-other</span> set and speech translation tasks are evaluated on the corresponding <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> <span class=\"ltx_text ltx_font_typewriter\">dev</span> set.\nIt can be seen that the model is with a small latency (LAAL 0.47 second) but high WER (7.6) when the chunk size is small (160ms). The performance improves steadily when the chunk size increases but at the cost of increasing latency. The speech translation experiments have similar observations. The results indicate the model can switch from streaming mode to offline mode smoothly as the chunk sizes change.\nOther latency evaluation results, such as Average Lagging (AL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib25\" title=\"\">2019</a>)</cite> and Differentiable Average Lagging (DAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arivazhagan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib1\" title=\"\">2019</a>)</cite>, could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A2\" title=\"Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a></p>\n\n",
                "matched_terms": [
                    "chunk",
                    "dal",
                    "model",
                    "size",
                    "dev",
                    "latency",
                    "laal",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech self-supervised learning algorithms form contextual representations through learning to predict\nmissing information due to time constraint or masking. <cite class=\"ltx_cite ltx_citemacro_citet\">van&#160;den Oord et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib37\" title=\"\">2018</a>)</cite> propose an auto-regressive based SSL approach to predict the future samples based on the previous samples. Speech SSL methods such as wav2vec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>, HuBert&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>)</cite> and BEST-RQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> choose to randomly mask audio span and the model learns to restore those masked regions. Chunk SSL combines both approaches that it masks audio span in the extended chunk\nand recovers masking frame indices in an chunk based auto-regressive manner.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, a chunkwise speech self-supervised learning approach is proposed. The model is trained to\nlearn to reconstruct the masked information in the right most chunk based on unmasked frames in the same chunk and preceding chunks. An efficient copy and append data augmentation method is proposed to\nparallel chunk-wise computation.\nFSQ is employed to generate high resolution discrete tokens and a group based decomposition method is proposed to alleviate the computation challenge.\nThe model is trained with varied chunk durations to fit\nthe different scenarios, hence a pre-trained model could be used for both streaming and offline\napplications. Our experiments show that a model initialized with Chunk SSL could achieve very\ncompetitive offline results and excellent streaming results on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> translation directions.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "chunk",
                    "model",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FSQ model is built with learning rate is 2.0e-4 and ExponentialLR scheduler from PyTorch. The effective batch size is 3200 seconds with up to 180,000 updates. The FSQ module is optimized with MSE loss to restore mel-filterbank features from their quantized audio representations. It takes 4 A10 GPUs for 42 hours to build a FSQ model.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training is scheduled as Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> with warmup step 25,000 and maximum learning rate 3e-4. The batch size is 16,000 seconds for the base configure model. We choose a smaller effective batch size for the large configure model with 6,000 seconds due to computation resource limitation. The model is trained up to 400,000 updates. It takes about 10 days for 8 A100 GPUs to build a Chunk SSL encoder with the base configure.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "base",
                    "model",
                    "size",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">960 hours fine-tuning</span>\nFor the base configure model, we choose the three stages scheduler scheme as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>, i.e., warmup, hold and annealing. The warmup stage takes about 1000 updates with encoder parameters frozen, the hold stage takes about 40,000 updates and the remaining 60,000 updates are in an annealing stage. The peak learning rate in fine-tuning is 2.0e-3.\nThe SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib31\" title=\"\">2019</a>)</cite> data augmentation is applied with 2 frequency maskings of 27, and 10 time maskings at a maximum ratio of 0.05 in fine-tuning experiments.\nThe effective batch size for fine-tuning is 10,000 seconds. For the large configure model, we choose Transformer scheduler&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> since it has less hype-parameters to explore. The warmup step is 5000 with encoder parameters frozen. Similar as the base configure model, the total update step is up to 100,000. The peak learning rate is 5e-4. The effective batch size for the large model is 7200 seconds.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The translation Transducer model is optimized with Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> scheduler with warmup step 10,000 and total update step up to 100,000. The peak learning rate is 5e-4. The effective batch size is 6400 seconds.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            }
        ]
    },
    "A2.T7": {
        "caption": "Table 7: Latency and BLEU vs chunk size on base model trained with MustC EN-ES dev dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Chunk Size (ms)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">BLEU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">LAAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">AL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">AP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">DAL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">ATD</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">320</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">680.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-2601.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1277.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">495.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">640</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1115.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-1905.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1791.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">461.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">960</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1561.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-1248.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2297.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">559.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1280</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1979.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">-607.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2782.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">680.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1600</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2402.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3228.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">819.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1920</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2781.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">584.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3630.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">940.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2240</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3131.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1106.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3990.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1047.7</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "chunk",
            "atd",
            "laal",
            "base",
            "dal",
            "model",
            "dataset",
            "size",
            "dev",
            "latency",
            "mustc",
            "enes",
            "trained",
            "bleu"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The full latency evaluation results including Average Lagging (AL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib25\" title=\"\">2019</a>)</cite>, Length Adaptive Average Lagging (LAAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib30\" title=\"\">2022</a>)</cite>,\nAverage Proportion (AP)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cho &amp; Esipova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib7\" title=\"\">2016</a>)</cite>, Differentiable Average Lagging (DAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arivazhagan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib1\" title=\"\">2019</a>)</cite>, and Average Token Delay (ATD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib19\" title=\"\">2023</a>)</cite> for the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> are presented in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#A2.T5\" title=\"Table 5 &#8227; Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;5</span></a>, <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#A2.T6\" title=\"Table 6 &#8227; Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;6</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;7</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Low latency speech human-machine communication is becoming increasingly necessary as speech technology advances quickly in the last decade.\nOne of the primary factors behind the advancement of speech technology is self-supervised learning.\nMost self-supervised learning algorithms are designed with full utterance assumption and compromises have to made if partial utterances are presented, which are common in the streaming applications.\nIn this work, we propose a chunk based self-supervised learning (Chunk SSL) algorithm as an unified solution for both streaming and offline speech pre-training.\nChunk SSL is optimized with the masked prediction loss and\nan acoustic encoder is encouraged to restore indices of those masked speech frames with help from unmasked frames in the same chunk and preceding chunks.\nA copy and append data augmentation approach is proposed to conduct efficient chunk based pre-training.\nChunk SSL utilizes a finite scalar quantization (FSQ) module to discretize input speech features and our study shows a high resolution FSQ codebook, i.e., a codebook with vocabulary size up to a few millions, is beneficial to transfer knowledge from the pre-training task to the downstream tasks. A group masked prediction loss is employed during pre-training to alleviate the high memory and computation cost introduced by the large codebook.\nThe proposed approach is examined in two speech to text tasks, i.e., speech recognition and speech translation.\nExperimental results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> datasets show that the proposed method could achieve\nvery competitive results for speech to text tasks at both streaming and offline modes.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk",
                    "latency",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we focus on building an encoder suitable for both streaming and offline modes,\nwith a chunkwise self-supervised learning (Chunk SSL) framework as depicted in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>.\nChunk SSL aims to\nrestore discrete indices of the masked frames based on unmasked frames in the last chunk and previous chunks.\nThe discrete indices are estimated with finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> (FSQ) with vocabulary size up to millions&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>).The pre-training FSQ token has more fine-grained resolution compared with the downstream modeling units, such as phonemes or sentencepiece tokens,\nwhich usually are with vocabulary size ranging from tens to tens of thousands.\nWe hypothesize that speech frames associated with a high resolution FSQ token are mainly mapped to an unique modeling unit in the downstream task and it makes the knowledge transfer easier from the pre-training stage to the fine-tuning stage.\nHowever, those high resolution FSQ tokens pose a great challenge for modeling and we propose to decompose a large codebook\ninto small channel based sub-codebooks to alleviate the memory and computation cost during pre-training. Details could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3\" title=\"3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, speech features are first extracted from the input audio, and grouped into equal sized chunks.\nInstead of calculating those chunks from left to right sequentially, a copy and append data augmentation (CADA) is introduced to parallelize the computation for all chunks in the same utterance (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S2\" title=\"2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>).\nCADA augments the input sequence by copying input chunks and appending them to the end of utterance as extended chunks.\nMasking is applied to frames in extended chunks only.\nAugmented utterances are then processed by a CADA compatible Conformer encoder, which could handle those augmented chunks properly even though frames in extended chunks are with altered location information. Finally,\nthe model is optimized by restoring FSQ indices of those masked frames from Conformer encoder outputs.\nExperiments on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> datasets indicate that\nthe same model initialized with Chunk SSL could achieve very competitive results\non both streaming and offline speech to text tasks.\nIt shows that Chunk SSL eliminates the need to build a dedicated streaming and dedicated offline model.\nTo summarize, our contributions includes:</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results show the proposed method can build one model for both scenarios and achieve competitive results on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> datasets.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training procedure is described in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>. Input features are first downsampled with a stacked subsampling module. The outputs are then divided into chunks with fixed duration, They are considered as base chunks.\nThe consecutive chunk of each base chunk is copied and appended to the end of the utterance in a sequential order. Those newly created chunks are called extended chunks.\nFor example, there are 12 speech input frames in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a> after subsampling. Assuming the chunk size is 4 frames, the speech features are segmented into three base chunks (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#00FFFF;\">cyan</span>): &#8220;B1&#8221;, &#8220;B2&#8221; and &#8220;B3&#8221;.\nChunk &#8220;B2&#8221; and &#8220;B3&#8221; are copied and appended to the end of the utterance as &#8220;E1&#8221; and &#8220;E2&#8221; (in <span class=\"ltx_text\" style=\"--ltx-fg-color:#BFFF00;\">lime</span>).\nThey correspond to the extended chunks of &#8220;B1&#8221; and &#8220;B2&#8221; respectively.\nThere is no extended chunk for the last chunk (&#8220;B3&#8221;).\nExtended chunks act as right most chunks with different preceding chunks in the naive Chunk SSL algorithm and\nmasking is only applied on frames of extended chunks.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of input frames in an utterance and is a multiple of chunk size <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>.\nThe augmented input sequence length is <math alttext=\"N^{\\prime}=2N-C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mi>N</mi><mo>&#8242;</mo></msup><mo>=</mo><mrow><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow><mo>&#8722;</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">N^{\\prime}=2N-C</annotation></semantics></math>, the number of base chunks is <math alttext=\"M=N/C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>=</mo><mrow><mi>N</mi><mo>/</mo><mi>C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">M=N/C</annotation></semantics></math> and the number of extended chunks is <math alttext=\"M-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M-1</annotation></semantics></math>.\nWe define <math alttext=\"m(i)=\\lfloor{i/C}\\rfloor\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">&#8970;</mo><mrow><mi>i</mi><mo>/</mo><mi>C</mi></mrow><mo stretchy=\"false\">&#8971;</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m(i)=\\lfloor{i/C}\\rfloor</annotation></semantics></math> as the chunk index of frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. If the left context is infinite, i.e., the model could access all history information, a CADA masking matrix <math alttext=\"\\bm{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">\\bm{\\alpha}</annotation></semantics></math> is defined as</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the second row represents a frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> from base chunks can also access limited future information from its extended chunk</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">the fourth row means a frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i4.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> from extended chunks can access all information in its base chunk and preceding chunks of the base chunk</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S2.F2.sf1\" title=\"Figure 2(a) &#8227; Figure 2 &#8227; 2.1 CADA self-attention module &#8227; 2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> depicts a CADA masking example for an utterance with 6 frames before CADA. The first column is the frame index. The number in the bracket (<span class=\"ltx_text\" style=\"--ltx-fg-color:#BF8040;\">brown</span>) stands for the extended chunk frame&#8217;s original position, i.e., the position of the base chunk frame that the frame is copied from.\n<math alttext=\"{\\bm{\\alpha}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">{\\bm{\\alpha}}</annotation></semantics></math> for the extended chunks are presented in italic fonts. A self-attention module equipped with a CADA masking matrix can process a CADA sequence with position information preserved.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A convolutional layer has a set of filters which slide across input features within the same utterance and extract localized representations. A convolutional layer assumes the input frames organized as consecutively, which no longer holds true for extended chunk frames in a CADA utterance.\nThis problem can be resolved via a chunk based convolutional computation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib23\" title=\"\">2023</a>)</cite>.\nWithout loss of generality, we assume the convolution module is a depth-wise convolution layer with <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> left and <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> right context frames. The modified computation includes three steps.\nFirst, we separate the CADA utterance into chunks and pair a base chunk and its extended chunk together, for example &#8220;B1&#8221; and &#8220;E1&#8221; in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>; then the last <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> frames from the chunk preceding the base chunk\nare appended to the left as left context. If the base chunk is the first chunk, <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math>&#160;zero padding frames are attached. Similarly, <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> zero padding frames are appended to the right and it leads to a new concatenated subsequence with length <math alttext=\"L_{lc}+2C+L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi></mrow><mo>+</mo><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">L_{lc}+2C+L_{rc}</annotation></semantics></math>.\nSecond, the concatenated subsequence is fed to the convolution module and obtains output subsequence with length <math alttext=\"2C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">2C</annotation></semantics></math>. <math alttext=\"L_{lc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{lc}</annotation></semantics></math> and <math alttext=\"L_{rc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msub><mi>L</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">L_{rc}</annotation></semantics></math> context frames are absorbed during convolution\ncomputation. The first <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> output frames correspond to the base chunk and the second <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m11\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> frames are for the extended chunk.\nFinally, those output chunks are placed back to their original positions in the augmented utterance as the outputs from the CADA convolutional module.\n<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S2.F2.sf2\" title=\"Figure 2(b) &#8227; Figure 2 &#8227; 2.1 CADA self-attention module &#8227; 2 Copy and Append Data Augmentation &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> demonstrates three steps to conduct convolution computation on an augmented utterance with base sequence length 12 and chunk size 4.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Dynamic chunk training&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib40\" title=\"\">2020a</a>; Weninger et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib39\" title=\"\">2022</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib23\" title=\"\">2023</a>)</cite> is a useful fine-tuning approach for the chunk based ASR encoder.\nDuring training, the chunk duration varied from hundreds of million seconds to a few seconds is randomly chosen for every model update. A model built with dynamic chunk training is suitable for both streaming and offline modes.\nWe extend dynamic chunk training for the Chunk SSL pre-training. Chunk duration is chosen from 6 durations (<math alttext=\"[640,1280,1920,2560,3200,3840]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>640</mn><mo>,</mo><mn>1280</mn><mo>,</mo><mn>1920</mn><mo>,</mo><mn>2560</mn><mo>,</mo><mn>3200</mn><mo>,</mo><mn>3840</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[640,1280,1920,2560,3200,3840]</annotation></semantics></math> million seconds) randomly for every model update.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, we leverage finite scalar quantization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mentzer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib28\" title=\"\">2024</a>)</cite> to quantize\ninput frames and obtain their discrete indices.\nA FSQ module is built prior to the Chunk SSL pre-training.\nIn the quantization module, an utterance based channel mean and variance normalization is first applied to input frames <math alttext=\"{\\bm{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>&#119935;</mi><annotation encoding=\"application/x-tex\">{\\bm{X}}</annotation></semantics></math> to obtain <math alttext=\"\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#119935;</mi><mo>^</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119961;</mi><mo>^</mo></mover><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{{\\bm{X}}}=\\{\\hat{{\\bm{x}}}_{i}\\}</annotation></semantics></math>, then it is processed by a FSQ encoder <math alttext=\"\\text{Enc}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mtext>Enc</mtext><mi>f</mi></msub><annotation encoding=\"application/x-tex\">\\text{Enc}_{f}</annotation></semantics></math> and projected into a low-dimension space with size <math alttext=\"d^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>d</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">d^{\\prime}</annotation></semantics></math>, where <math alttext=\"d^{\\prime}\\ll d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msup><mi>d</mi><mo>&#8242;</mo></msup><mo>&#8810;</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d^{\\prime}\\ll d</annotation></semantics></math>. The output of each channel <math alttext=\"r\\in[1,d^{\\prime}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><msup><mi>d</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in[1,d^{\\prime}]</annotation></semantics></math> for input frame <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is rounded to an integer <math alttext=\"h_{i,r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>h</mi><mrow><mi>i</mi><mo>,</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">h_{i,r}</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The acoustic encoder is optimized with masked prediction loss. We encourage the Chunk SSL model to reconstruct the masked frames based on the context information provided.\nMore specifically, we mask half consecutive frames in every extended chunk. The start position of the masking frame is randomly chosen from <math alttext=\"[0,\\frac{C}{4}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mfrac><mi>C</mi><mn>4</mn></mfrac><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,\\frac{C}{4}]</annotation></semantics></math> at every extended chunk, so we have enough unmasked frames on both sides of the masked frames as context to infer those masked frames.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Assuming the Chunk SSL model outputs for the augmented sequence is <math alttext=\"{\\bm{O}}=\\{{\\bm{o}}_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119926;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119952;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{O}}=\\{{\\bm{o}}_{i}\\}</annotation></semantics></math>. The FSQ index of <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is denoted as <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math> and\n<math alttext=\"{\\mathbf{e}}_{\\mu_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><msub><mi>&#956;</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">{\\mathbf{e}}_{\\mu_{i}}</annotation></semantics></math> is the output embedding of <math alttext=\"\\mu_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math>. The masked prediction loss is defined as</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Tuning</span>: For speech recognition task, the pre-trained models are finetuned and evaluated on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib29\" title=\"\">2015</a>)</cite> dataset. We use two <span class=\"ltx_text ltx_font_typewriter\">dev</span> sets for development and report all results from <span class=\"ltx_text ltx_font_typewriter\">dev</span> and <span class=\"ltx_text ltx_font_typewriter\">test</span> sets.\nFor speech translation task, experiments are conducted on two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib12\" title=\"\">2019</a>)</cite>\nlanguage pairs: English to German (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>DE) and English to Spanish (EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ES). Sequence level knowledge\ndistillation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Rush, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib20\" title=\"\">2016</a>)</cite> is applied to boost the speech translation quality.\nThe models are developed on\nthe <span class=\"ltx_text ltx_font_typewriter\">dev</span> set, and the final results are reported on the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> set.</p>\n\n",
                "matched_terms": [
                    "dev",
                    "dataset",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speech encoder is a Conformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib15\" title=\"\">2020</a>)</cite> based chunk encoder.\nThe encoder is equipped with a relative positional embedding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shaw et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib32\" title=\"\">2018</a>)</cite> and starts with a stacking layer to down-sample the input features by four times.\nTwo model configurations: base and large, have been explored. The base encoder has 12 Conformer layers, input embedding size of 512, 8 attention heads, feedforward layer dimension 2048 and convolutional module kernel size 31. The large encoder has 24 Conformer layers, input embedding size of 768, 16 attention heads, feedforward layer dimension 3072 and convolutional module kernel size 5.\nTransducer is adopted in the fine-tuning experiments.\nThe predictor module is with one Transformer layer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> for speech recognition tasks and two layers for speech translation tasks.\nThe input embedding and feedforward layer dimension are the same as ones in the encoder setting if not mentioned specifically. The joint module is a feedforward layer as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib41\" title=\"\">2020b</a>)</cite> with embedding dimension 1024.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Input speech is represented as 80-dimensional log mel-filterbank coefficients computed every 10ms with a 25ms window. Global channel mean and variance normalization is applied so the trained model could be used for both streaming and offline scenarios.\nWe set the maximum utterance duration to 75 seconds and minimum duration to 5 seconds in pre-training.\nDuring fine-tuning,\na look-ahead chunk&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib33\" title=\"\">2021</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib24\" title=\"\">2021</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib35\" title=\"\">2023</a>)</cite> is utilized and\ndynamic chunk training is employed by default. We alternate between offline training with infinite chunk size, and streaming training with chunk duration sampled from [160, 320, 640, 960, 1280, 1600] ms randomly within each epoch.\nMore details about optimization, such as batch sizes and training schedulers, for different experiments are presented in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A1\" title=\"Appendix A Optimization configures &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nThe target labels are encoded with SentencePiece&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib21\" title=\"\">2018</a>)</cite>. For both speech recognition and speech translation tasks, the vocabulary is an unigram model with size 1024 and full character coverage on the corresponding training text data.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk",
                    "model",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final results are evaluated using an averaged model from checkpoints of the best 10 epochs.\nSpeech recognition experiments are evaluated with WER while speech translation results are measure with scacre BLEU score<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>signature: nrefs:1<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m1\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>case:mixed<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m2\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>eff:no<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m3\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>tok:13a<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m4\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>smooth:exp<math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"footnote2.m5\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math>version:2.4.2</span></span></span>.\nWe report both streaming and offline results.\nFor the streaming decoding, we set the chunk size to 320ms by default if not mentioned specifically. No language model is used in all experiments.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk",
                    "model",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-trained Chunk SSL models are fine-tuned with dynamic chunk training and they could be used for both streaming and offline ASR.\nWe list the offline and streaming recognition results from the Chunk SSL models in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>.\nThe reported literature results are presented in the first part of the tables.\nIt is clear that the chunk SSL pre-trained models could achieve very competitive results compared to other strong baselines.\nThe base and large models outperform the corresponding wav2vec2 base and large models.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the streaming evaluation, the Chunk SSL model with the base configuration excels all models reported in the literature shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>. The large configuration model pushes the WER even lower and it achieves another 0.5 average WER reduction compared to the base configure model. Comparing results in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>,\nthe proposed method significantly reduces the performance gap between the streaming and offline applications. The WER gap between the large configure\nmodels is 0.8 in average for Chunk SSL while the average WER difference is 2.5 for BEST-RQ.\nNote, literature models listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a> are initialized with dedicated pre-trained models either offline or streaming, while a Chunk SSL model could be initialized with the same pre-trained model and operated in both modes.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the speech translation experiments, we evaluate on both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; directions and the results are listed in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. The baselines are initialized with a speech recognition acoustic encoder (&#8220;ASR&#8221; in the column &#8220;Init.&#8221;), which is trained with the English data in the <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> English-Spanish direction. Both &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>Es&#8221; and &#8220;En<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>De&#8221; results show that the encoder initialized with Chunk SSL outperforms the one initialized with a speech recognition encoder trained with <span class=\"ltx_text ltx_font_smallcaps\">Must-C</span> data only. We could draw similar conclusion as the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> experiment that the Chunk SSL could effectively improve speech translation quality for both streaming and offline cases.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "chunk",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a> lists models trained with different codebook sizes with the base configure. The first column gives FSQ vocabulary sizes, the second column provides the Chunk SSL training time (in seconds) for 1000 updates.\n&#8220;levels&#8221; is the FSQ levels for quantization. For example, &#8220;4(<math alttext=\"8\\times 1-5\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mn>8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>3</mn></mrow></mrow><annotation encoding=\"application/x-tex\">8\\times 1-5\\times 3</annotation></semantics></math>)&#8221; in the first row means there are 4 output channels, the first channel is with level 8 and the remaining 3 channels are with level 5. They span a codebook with size 1000.\n&#8220;phn pur.&#8221; stands for phone purity.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "trained",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first two rows listed models trained with FSQ vocabulary size 1000. &#8220;1000*&#8221; means the model is optimized with a full codebook as&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E4\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, while &#8220;1000&#8221; indicates the model is pretrained with the group masked prediction loss following&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#S3.E5\" title=\"In 3.2 Group masked prediction loss &#8227; 3 High Resolution Finite Scalar Quantization &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Eq.</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5</span></a>. The results shows two models\nachieve comparable accuracies\nwith similar pre-training time.\nAs shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, when the codebook size increases, &#8220;phn pur.&#8221; and &#8220;PNMI&#8221; increase steadily.\nBoth metrics show tokens in the codebook with a high resolution have better consistency with phonemes. This confirms our assumption that frames associated with a specific FSQ token are more likely being labelled with the same phoneme if the codebook resolution is high enough.\nOn the other hand, we observe WER is reduced when FSQ codebook size becomes bigger.\nIt is peaked for the codebook size around 6 millions.\nWhen the codebook size gets extremely big, i.e. 791 millions in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>, the &#8220;dev-other&#8221; WER increases from 5.0 to 5.4.\nOur hypothesis is that an extremely large codebook makes the optimization harder, and more training time might be required to get good representation.\nBased on above observations, we conclude that a codebook with high resolution helps Chunk SSL to transfer knowledge to the downstream task. However, an extremely large FSQ codebook might make the pre-training hard and hurt the downstream task.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk",
                    "model",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The next thing we examined is computation cost.\nWe don&#8217;t observe any statistical difference for the FSQ training time when the codebook size changes, since majority of computation time is spent on data input/output, encoder and decoder Transformer/Conformer layer computation. On the other hand, a Chunk SSL training with a large codebook can increase the training time slightly that we find\nabout 10% training time increase when the codebook size grows from 1000 to 6 millions as shown in&#160;<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.T4\" title=\"Table 4 &#8227; 5.1 Main results &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;4</span></a>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "chunk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the base Transducer model to analyze the impact of different chunk sizes on latency and performance. We use Length-Adaptive Average Lagging(LAAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib30\" title=\"\">2022</a>)</cite> as latency scorer, which is evaluated with the Simuleval toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib26\" title=\"\">2020</a>)</cite>. We employ greedy decoding instead of beam search decoding for simplicity in this study.</p>\n\n",
                "matched_terms": [
                    "base",
                    "chunk",
                    "model",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;3</span></a> depicts the model performance (<span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">blue</span>) and latency (<span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">red</span>) under different chunk sizes for speech recognition&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>) and speech translation&#160;(<a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a> and <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.15579v1#S5.F3.sf3\" title=\"Figure 3(c) &#8227; Figure 3 &#8227; 5.3 Latency evaluation &#8227; 5 Experimental Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>). The speech recognition is evaluated on the <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> <span class=\"ltx_text ltx_font_typewriter\">dev-other</span> set and speech translation tasks are evaluated on the corresponding <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> <span class=\"ltx_text ltx_font_typewriter\">dev</span> set.\nIt can be seen that the model is with a small latency (LAAL 0.47 second) but high WER (7.6) when the chunk size is small (160ms). The performance improves steadily when the chunk size increases but at the cost of increasing latency. The speech translation experiments have similar observations. The results indicate the model can switch from streaming mode to offline mode smoothly as the chunk sizes change.\nOther latency evaluation results, such as Average Lagging (AL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib25\" title=\"\">2019</a>)</cite> and Differentiable Average Lagging (DAL)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Arivazhagan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib1\" title=\"\">2019</a>)</cite>, could be found in&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#A2\" title=\"Appendix B Latency Results &#8227; Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#160;<span class=\"ltx_text ltx_ref_tag\">B</span></a></p>\n\n",
                "matched_terms": [
                    "chunk",
                    "dal",
                    "model",
                    "size",
                    "dev",
                    "latency",
                    "laal",
                    "mustc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech self-supervised learning algorithms form contextual representations through learning to predict\nmissing information due to time constraint or masking. <cite class=\"ltx_cite ltx_citemacro_citet\">van&#160;den Oord et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib37\" title=\"\">2018</a>)</cite> propose an auto-regressive based SSL approach to predict the future samples based on the previous samples. Speech SSL methods such as wav2vec2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>, HuBert&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib17\" title=\"\">2021b</a>)</cite> and BEST-RQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib6\" title=\"\">2022</a>)</cite> choose to randomly mask audio span and the model learns to restore those masked regions. Chunk SSL combines both approaches that it masks audio span in the extended chunk\nand recovers masking frame indices in an chunk based auto-regressive manner.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, a chunkwise speech self-supervised learning approach is proposed. The model is trained to\nlearn to reconstruct the masked information in the right most chunk based on unmasked frames in the same chunk and preceding chunks. An efficient copy and append data augmentation method is proposed to\nparallel chunk-wise computation.\nFSQ is employed to generate high resolution discrete tokens and a group based decomposition method is proposed to alleviate the computation challenge.\nThe model is trained with varied chunk durations to fit\nthe different scenarios, hence a pre-trained model could be used for both streaming and offline\napplications. Our experiments show that a model initialized with Chunk SSL could achieve very\ncompetitive offline results and excellent streaming results on <span class=\"ltx_text ltx_font_smallcaps\">Librispeech</span> and two <span class=\"ltx_text ltx_font_smallcaps\">MuST-C</span> translation directions.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "chunk",
                    "model",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The FSQ model is built with learning rate is 2.0e-4 and ExponentialLR scheduler from PyTorch. The effective batch size is 3200 seconds with up to 180,000 updates. The FSQ module is optimized with MSE loss to restore mel-filterbank features from their quantized audio representations. It takes 4 A10 GPUs for 42 hours to build a FSQ model.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training is scheduled as Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> with warmup step 25,000 and maximum learning rate 3e-4. The batch size is 16,000 seconds for the base configure model. We choose a smaller effective batch size for the large configure model with 6,000 seconds due to computation resource limitation. The model is trained up to 400,000 updates. It takes about 10 days for 8 A100 GPUs to build a Chunk SSL encoder with the base configure.</p>\n\n",
                "matched_terms": [
                    "chunk",
                    "base",
                    "model",
                    "size",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">960 hours fine-tuning</span>\nFor the base configure model, we choose the three stages scheduler scheme as&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib4\" title=\"\">2020b</a>)</cite>, i.e., warmup, hold and annealing. The warmup stage takes about 1000 updates with encoder parameters frozen, the hold stage takes about 40,000 updates and the remaining 60,000 updates are in an annealing stage. The peak learning rate in fine-tuning is 2.0e-3.\nThe SpecAugment&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib31\" title=\"\">2019</a>)</cite> data augmentation is applied with 2 frequency maskings of 27, and 10 time maskings at a maximum ratio of 0.05 in fine-tuning experiments.\nThe effective batch size for fine-tuning is 10,000 seconds. For the large configure model, we choose Transformer scheduler&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> since it has less hype-parameters to explore. The warmup step is 5000 with encoder parameters frozen. Similar as the base configure model, the total update step is up to 100,000. The peak learning rate is 5e-4. The effective batch size for the large model is 7200 seconds.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The translation Transducer model is optimized with Transformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15579v1#bib.bib38\" title=\"\">2017</a>)</cite> scheduler with warmup step 10,000 and total update step up to 100,000. The peak learning rate is 5e-4. The effective batch size is 6400 seconds.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            }
        ]
    }
}