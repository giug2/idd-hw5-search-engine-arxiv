{
    "S2.T1": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 1: The architectural design of Uni-MoE-2.0-Omni.",
        "body": "Module\nArchitecture\nParams\n\n\nAudio Encoder\nWhisper-Large-v3\n637M\n\n\nVision Encoder\nSigLIP-So400M\n398M\n\n\nMoE-LLM\nMoE Transformer\n26B\n\n\nMoE-TTS\nMoE Transformer\n1.2BA0.7B\n\n\nTask-DiT\nDense Transformer\n1.5B\n\n\nCodec Decoder\nWavTokenizer-large-600-24k-4096\n442M\n\n\nVAE Decoder\nSD-XL\n49M\n\n\nShared Expert\nMLP\n712M\n\n\nRouted Expert\nMLP\n5.7B\n\n\nActivated Expert\n2 Shared Expert + 0~3 Routed Expert\nMin: 1.5B; Max: 18B",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Module</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Architecture</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Params</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Audio Encoder</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Whisper-Large-v3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">637M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Vision Encoder</td>\n<td class=\"ltx_td ltx_align_center\">SigLIP-So400M</td>\n<td class=\"ltx_td ltx_align_center\">398M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MoE-LLM</td>\n<td class=\"ltx_td ltx_align_center\">MoE Transformer</td>\n<td class=\"ltx_td ltx_align_center\">26B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MoE-TTS</td>\n<td class=\"ltx_td ltx_align_center\">MoE Transformer</td>\n<td class=\"ltx_td ltx_align_center\">1.2BA0.7B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Task-DiT</td>\n<td class=\"ltx_td ltx_align_center\">Dense Transformer</td>\n<td class=\"ltx_td ltx_align_center\">1.5B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Codec Decoder</td>\n<td class=\"ltx_td ltx_align_center\">WavTokenizer-large-600-24k-4096</td>\n<td class=\"ltx_td ltx_align_center\">442M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VAE Decoder</td>\n<td class=\"ltx_td ltx_align_center\">SD-XL</td>\n<td class=\"ltx_td ltx_align_center\">49M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Shared Expert</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MLP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">712M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Routed Expert</td>\n<td class=\"ltx_td ltx_align_center\">MLP</td>\n<td class=\"ltx_td ltx_align_center\">5.7B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Activated Expert</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2 Shared Expert + 0~3 Routed Expert</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Min: 1.5B; Max: 18B</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "unimoe20omni",
            "codec",
            "taskdit",
            "18b",
            "design",
            "shared",
            "transformer",
            "712m",
            "moetts",
            "sdxl",
            "398m",
            "module",
            "activated",
            "architecture",
            "siglipso400m",
            "min",
            "max",
            "audio",
            "encoder",
            "wavtokenizerlarge60024k4096",
            "637m",
            "moe",
            "26b",
            "decoder",
            "15b",
            "vae",
            "params",
            "49m",
            "expert",
            "moellm",
            "dense",
            "mlp",
            "12ba07b",
            "442m",
            "architectural",
            "routed",
            "57b",
            "whisperlargev3",
            "vision"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "moe",
                    "dense",
                    "unimoe20omni",
                    "design",
                    "shared",
                    "routed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "architectural",
                    "architecture",
                    "dense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "architectural",
                    "unimoe20omni",
                    "dense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Unified Modality Encoding &amp; Generation</span>: We design a unified speech encoder that maps diverse audio inputs, including environmental sound, speech, and music, into a shared representation space. For output, a Context-Aware MoE-based TTS module supports dynamic speech synthesis (especially for long speech) and interaction. On the visual side, we employ pre-trained visual encoders to process images and videos, and build a Task-Aware Diffusion Transformer for instruction-guided image generation and editing.</p>\n\n",
                "matched_terms": [
                    "design",
                    "shared",
                    "transformer",
                    "module",
                    "audio",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">MoE-Driven Cross-Modal Fusion</span>: We strategically extend the standard MLP layers to MoE layers. This new MoE architecture incorporates three expert types: null experts for inference-time computation skipping, modality-specific routed experts for storing modality knowledge and processing cross-modal information, and small-size shared experts to facilitate universal information exchange. This design enables efficient computation, specialized modality handling, and effective and stable multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "moe",
                    "design",
                    "mlp",
                    "shared",
                    "architecture",
                    "routed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "moe",
                    "dense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "moe",
                    "shared",
                    "transformer",
                    "routed",
                    "moetts",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The visual understanding module of Uni-MoE-2.0-Omni employs a unified encoding strategy for both images and videos. The overall architecture consists of two major components: a visual encoder and a mapping network (MLP).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "mlp",
                    "module",
                    "architecture",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Visual Encoder</span>. The visual encoder is initialized with SigLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhai2023sigmoid</span>)</cite> vision transformer, which transforms the input image or video frames <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i1.p1.m1\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> into visual features <math alttext=\"Z_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">Z_{V}</annotation></semantics></math>. We adopt the output of the last layer of the transformer as the visual features.</p>\n\n",
                "matched_terms": [
                    "encoder",
                    "vision",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "encoder",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "architecture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "module",
                    "whisperlargev3",
                    "audio",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "module",
                    "whisperlargev3",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{W}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>W</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{W}^{A}</annotation></semantics></math> is the last hidden states of the pre-trained audio encoder adopted from Whisper-large-v3. <span class=\"ltx_text ltx_markedasmath\">MSA</span> and <math alttext=\"h\\text{MCA}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>MCA</mtext></mrow><annotation encoding=\"application/x-tex\">h\\text{MCA}</annotation></semantics></math> denote the multihead self-attention and cross-attention operations, and <span class=\"ltx_text ltx_markedasmath\">LN</span> is the layer norm function. By leveraging the fixed-length query vectors <math alttext=\"X_{Q}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>Q</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">X_{Q}^{A}</annotation></semantics></math>, each 30 seconds of audio encoded by the Whisper encoder is mapped to <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> audio tokens. <math alttext=\"h_{C}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>C</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{C}^{A}</annotation></semantics></math> represents the output of the cross-attention module, which is used to distill the main content of the input audio. After going through all layers of the Whisper decoder, we apply a learnable linear layer for projecting the last output into the representation space of LLM.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "module",
                    "whisperlargev3",
                    "audio",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To process audio longer than Whisper-Large-v3&#8217;s 30-second limit, we employ a chunking mechanism. Long audio is segmented into consecutive 30-second clips, which are batched and processed by the audio understanding module. <span class=\"ltx_text ltx_font_italic\">The resulting features are then concatenated along the time dimensions, enabling the understanding of audio of arbitrary length</span>.</p>\n\n",
                "matched_terms": [
                    "module",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "design",
                    "audio",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies have demonstrated that factual and procedural knowledge is predominantly stored in the feed-forward network (FFN) modules <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-22941</span>)</cite>. The Mixture-of-Experts architecture enables adaptive knowledge retrieval by dynamically activating different FFN modules, which are determined by a router network. Particularly, given <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> expert parameters <math alttext=\"\\{w_{0},...,w_{n-1}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{w_{0},...,w_{n-1}\\}</annotation></semantics></math>, the output of a vanilla MoE for inference is:</p>\n\n",
                "matched_terms": [
                    "expert",
                    "architecture",
                    "moe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These limitations prevent MoE from fully exploiting its adaptive activation capability, ultimately resulting in suboptimal performance.\nTo address these challenges, we propose the Dynamic-Capacity MoE architecture, which incorporates (i) routing gradient estimation to enable differentiable expert selection, (ii) explicit expert role specialization to separately model domain-specific and general knowledge, as well as facilitate knowledge deletion, and (iii) dynamic expert number allocation to adaptively control the amount of parametric knowledge.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "architecture",
                    "moe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central difficulty in training vanilla MoE models arises from the non-differentiable Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> operation used in expert selection, which obstructs gradient propagation to the router and leads to biased optimization. To address this, we migrate the gradient estimation strategy proposed in Grin-MoE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024gringradientinformedmoe</span>)</cite>, which integrates straight-through gradient estimators under the framework of numerical methods for ordinary differential equations (ODEs). This enables end-to-end optimization of both router and experts under sparse activation constraints, improving the stability of router training and allowing more precise token-to-expert assignments. A formal description is given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.SS4\" title=\"A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "moe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance the adaptability of MoE and address the limitation of homogeneous expert types, we explicitly categorize experts into three distinct roles:</p>\n\n",
                "matched_terms": [
                    "expert",
                    "moe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Routed Experts</span>: These are task-specific experts responsible for modeling domain-specific knowledge. They are dynamically activated according to the proposed dynamic capacity routing strategy.</p>\n\n",
                "matched_terms": [
                    "activated",
                    "routed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Shared Experts</span>: These experts capture general, domain-independent knowledge. Unlike routed experts, shared experts are persistently activated for all tokens, ensuring that common knowledge is always available during inference.</p>\n\n",
                "matched_terms": [
                    "activated",
                    "routed",
                    "shared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Vanilla MoE applies a fixed number of experts to every token, ignoring variations in token complexity and knowledge demand.\nWe address this limitation by introducing a dynamic capacity routing strategy, which determines the number of routed experts for each token based on a Top-P sampling.\nFormally, let the router produce a probability vector over routed experts for token <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px3.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "routed",
                    "moe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The set of activated routed experts for token <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px3.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is then:</p>\n\n",
                "matched_terms": [
                    "activated",
                    "routed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a holistic view of our method, Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the complete training procedure in pseudocode form.\nFor simplicity of presentation, we treat null experts as a special case of routed experts in the pseudocode.\nThe algorithm integrates the three proposed components into a unified workflow:\nFirst, <span class=\"ltx_text ltx_font_typewriter\">ActivateExperts</span> function determines expert activation, combining the Top&#8209;P sampling strategy for routed experts with the persistent activation constraint for shared experts;\nSecond, gradient estimation is applied to routed expert outputs to enable end&#8209;to&#8209;end optimization under sparse activation;\nFinally, after computing routed expert contributions, shared expert outputs are computed and incorporated into the MoE representation, injecting general knowledge into the final output.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "moe",
                    "shared",
                    "routed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "design",
                    "module",
                    "architecture",
                    "moetts",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "moe",
                    "dense",
                    "decoder",
                    "module",
                    "architecture",
                    "moetts",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the performance of MoE-TTS in voice timbre control, we adopt a text-context prompt-guided approach to direct the model to synthesize audio with the specified language and timbre. When the model receives an instruction to generate an audio response, the Uni-MoE base model first produces a special token (i.e. <span class=\"ltx_text ltx_font_italic\">&lt;speech start&gt;</span>) indicating the start of speech, . Subsequently, the model generates commands that specify the voice timbre for audio synthesis: fixed timbres include those of Brain, Jenny, and Xiaoxiao, while supported languages cover Chinese and English. Beyond fixed options, the model also enables users to describe desired timbres via natural language; the MoE-TTS module then generates audio with the corresponding timbre based on these descriptive instructions. Upon completion of the timbre prompt, a special token &lt;speech prompt&gt; is generated to signal the end of the prompt. This is followed by the text content that the model needs to transcribe into speech, which is terminated with the special token &lt;speech end&gt;. Through this combination of text-based timbre prompts and target text content, MoE-TTS is guided to perform diverse timbre synthesis.</p>\n\n",
                "matched_terms": [
                    "module",
                    "audio",
                    "moetts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "moetts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a Task-Aware Diffusion Transformer (Task-DiT) that bridges the gap between image understanding and generation. Unlike unified models that suffer from performance trade-offs, our framework preserves the strengths of specialized pre-trained models by connecting them via a lightweight, task-aware bridge.\nAt its core, Task-DiT employs a frozen image generator from PixWizard <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024pixwizard</span>)</cite> to safeguard its high-fidelity synthesis capabilities. To steer this generator, we introduce a query-based conditioning mechanism. Two distinct sets of learnable tokens are processed by a powerful understanding module:</p>\n\n",
                "matched_terms": [
                    "module",
                    "transformer",
                    "taskdit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The enriched features from these tokens are then translated for the generator by dedicated, lightweight projectors. A Task Projector modulates the DiT&#8217;s denoising process based on the command, while a content projector transforms the &lt;IMG[i]&gt; features into a dense conditioning sequence for cross-attention. For image-guided tasks, a Visual Projector aligns source image features encoded by ViT with the DiT&#8217;s conditioning space.\nThis design creates a versatile and efficient channel for task-aware instruction, enabling high-quality, multi-modal image generation without the catastrophic interference typical of end-to-end fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "design",
                    "dense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Expert Warmup with Dense Model: We first pre-trained three specialized expert models focusing on mainstream modalities: speech comprehension, speech generation, and visual comprehension. This stage aims to build the model-specific experts for the following stable MoE-based fine-tuning.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "dense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "architecture",
                    "moe",
                    "shared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "dense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "architecture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "moetts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the pre-training phase of the audio understanding modality, the audio data primarily comprises automatic speech recognition (ASR), audio-caption, and music-caption datasets. Among all pre-training data, ASR datasets constitute the majority, encompassing approximately 15B tokens. In contrast, audio-caption and music-caption datasets are more challenging to acquire than ASR datasets, with a combined total of merely 1B tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "15b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "moe",
                    "codec",
                    "dense",
                    "module",
                    "moetts",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the task projector alignment stage and the visual projector alignment stage, we utilized data encompassing a variety of task types. This included image generation, image editing, low-level computer vision tasks such as deraining and denoising, and conditional generation like Canny-to-image. In total, this data amounted to approximately 2M samples (1.5B tokens).</p>\n\n",
                "matched_terms": [
                    "vision",
                    "15b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "moe",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "design",
                    "audio",
                    "expert",
                    "moe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "architectural",
                    "audio",
                    "architecture",
                    "moe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "module",
                    "design",
                    "architecture",
                    "architectural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "expert",
                    "architecture",
                    "moe",
                    "dense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "moe",
                    "unimoe20omni",
                    "dense",
                    "design",
                    "architecture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To extend it to our Top-<math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p9.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> strategy, we apply the same computation sequentially to each activated expert, sampling without replacement from the routing distribution until the cumulative probability exceeds <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p9.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math>.\nIn this way, the gradient estimation naturally generalizes to multiple experts while remaining consistent with the selection process in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ensuring that the hybrid scaling mechanism is fully compatible with our dynamic-capacity MoE framework.</p>\n\n",
                "matched_terms": [
                    "activated",
                    "expert",
                    "moe"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 2: Overview of multi-stage training pipeline, detailing data composition, token volume, and trainable components for each stage. The total training volume is about 75B tokens, with datasets shared across stages.",
        "body": "Image(5M)\n\n\nVideo(5M)\n\n\nAudio(5M)\n\n\nText(5M)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-1pt;padding-bottom:-1pt;\">Image(5M)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-1pt;padding-bottom:-1pt;\">Video(5M)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-1pt;padding-bottom:-1pt;\">Audio(5M)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-1pt;padding-bottom:-1pt;\">Text(5M)</td>\n</tr>\n</table>\n",
        "informative_terms_identified": [
            "components",
            "composition",
            "shared",
            "data",
            "datasets",
            "audio5m",
            "overview",
            "multistage",
            "token",
            "volume",
            "pipeline",
            "stage",
            "text5m",
            "video5m",
            "total",
            "each",
            "75b",
            "image5m",
            "training",
            "tokens",
            "across",
            "trainable",
            "detailing",
            "stages",
            "about"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "across",
                    "composition",
                    "shared",
                    "data",
                    "training",
                    "tokens",
                    "75b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "across",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "across",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Deep Cross-Modal Alignment</span>: To enable deep and efficient fusion of any modality, we introduce an Omni-Modality 3D RoPE mechanism in the self-attention layers. It encodes the temporal-spatial dimensions of speech, image, text, and video tokens, ensuring seamless alignment and interaction across all input types.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At training and data recipes, we introduce <span class=\"ltx_text ltx_font_bold\">the following training recipe with the data matching</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S3\" title=\"3 Training and Data Recipes &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) :</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training",
                    "data",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "token",
                    "shared",
                    "data",
                    "training",
                    "each",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "total",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Multi-Image</span>. The encoding of multiple images follows a similar procedure to that of a single image. For each image, we independently search the most suitable target resolution, resize it accordingly, and then convert it into the corresponding set of visual patches. Assuming there are <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> images are provided as input, the total number of visual tokens is <math alttext=\"\\sum^{n}_{i=1}((a_{i}\\times b_{i})\\times T)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo lspace=\"0em\" stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>a</mi><mi>i</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>b</mi><mi>i</mi></msub></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sum^{n}_{i=1}((a_{i}\\times b_{i})\\times T)</annotation></semantics></math>, where <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math> and <math alttext=\"b_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m4\" intent=\":literal\"><semantics><msub><mi>b</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">b_{i}</annotation></semantics></math> denote the number of patches along the height and width of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th image.</p>\n\n",
                "matched_terms": [
                    "each",
                    "total",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "total",
                    "tokens",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "training",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{W}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>W</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{W}^{A}</annotation></semantics></math> is the last hidden states of the pre-trained audio encoder adopted from Whisper-large-v3. <span class=\"ltx_text ltx_markedasmath\">MSA</span> and <math alttext=\"h\\text{MCA}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>MCA</mtext></mrow><annotation encoding=\"application/x-tex\">h\\text{MCA}</annotation></semantics></math> denote the multihead self-attention and cross-attention operations, and <span class=\"ltx_text ltx_markedasmath\">LN</span> is the layer norm function. By leveraging the fixed-length query vectors <math alttext=\"X_{Q}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>Q</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">X_{Q}^{A}</annotation></semantics></math>, each 30 seconds of audio encoded by the Whisper encoder is mapped to <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> audio tokens. <math alttext=\"h_{C}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>C</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{C}^{A}</annotation></semantics></math> represents the output of the cross-attention module, which is used to distill the main content of the input audio. After going through all layers of the Whisper decoder, we apply a learnable linear layer for projecting the last output into the representation space of LLM.</p>\n\n",
                "matched_terms": [
                    "each",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "each",
                    "components",
                    "across",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As an illustrative example, consider a video input of length 120 seconds accompanied by audio. We place the visual sequence before the audio sequence and sample the video at a rate of one frame every two seconds. Suppose the RoPE ID of the last text token preceding the visual sequence is <math alttext=\"(x-1,x-1,x-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x-1,x-1,x-1)</annotation></semantics></math>. Then, the RoPE IDs of the first video frame start from <math alttext=\"(x,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x,x)</annotation></semantics></math> and increment row by row and column by column up to <math alttext=\"(x,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> denotes the number of tokens along both the height and width of a video frame. The second frame begins with <math alttext=\"(x+2\\theta,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x,x)</annotation></semantics></math> and increases to <math alttext=\"(x+2\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> is a specific scaling factor for absolute time. Following this procedure, the final frame in the video sequence has an ID of <math alttext=\"(x+118\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>118</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+118\\theta,x+p,x+p)</annotation></semantics></math>.\nSimilarly, assume the RoPE ID of the last token before the audio sequence is <math alttext=\"(y-1,y-1,y-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y-1,y-1,y-1)</annotation></semantics></math>. The first audio segment is then assigned the ID <math alttext=\"(y,y,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y,y,y)</annotation></semantics></math>, which is repeated 20 times to represent the minimum audio unit. The next audio segment receives the ID <math alttext=\"(y+3\\theta,y+3\\theta,y+3\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+3\\theta,y+3\\theta,y+3\\theta)</annotation></semantics></math>. Continuing this process, the final audio segment is assigned the ID <math alttext=\"(y+117\\theta,y+117\\theta,y+117\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+117\\theta,y+117\\theta,y+117\\theta)</annotation></semantics></math>. In this manner, temporal alignment between the visual and audio sequences can also be achieved.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Shared Experts</span>: These experts capture general, domain-independent knowledge. Unlike routed experts, shared experts are persistently activated for all tokens, ensuring that common knowledge is always available during inference.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "shared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Vanilla MoE applies a fixed number of experts to every token, ignoring variations in token complexity and knowledge demand.\nWe address this limitation by introducing a dynamic capacity routing strategy, which determines the number of routed experts for each token based on a Top-P sampling.\nFormally, let the router produce a probability vector over routed experts for token <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px3.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "each",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a holistic view of our method, Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the complete training procedure in pseudocode form.\nFor simplicity of presentation, we treat null experts as a special case of routed experts in the pseudocode.\nThe algorithm integrates the three proposed components into a unified workflow:\nFirst, <span class=\"ltx_text ltx_font_typewriter\">ActivateExperts</span> function determines expert activation, combining the Top&#8209;P sampling strategy for routed experts with the persistent activation constraint for shared experts;\nSecond, gradient estimation is applied to routed expert outputs to enable end&#8209;to&#8209;end optimization under sparse activation;\nFinally, after computing routed expert contributions, shared expert outputs are computed and incorporated into the MoE representation, injecting general knowledge into the final output.</p>\n\n",
                "matched_terms": [
                    "components",
                    "shared",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "each",
                    "stages",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we collected a large-scale multimodal instruction-following dataset to enable the model to understand and process any cross-modal information. The training comprises two phases:</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "shared",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "across",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "across",
                    "pipeline",
                    "stage",
                    "datasets",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "stages",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the pre-training phase of the audio understanding modality, the audio data primarily comprises automatic speech recognition (ASR), audio-caption, and music-caption datasets. Among all pre-training data, ASR datasets constitute the majority, encompassing approximately 15B tokens. In contrast, audio-caption and music-caption datasets are more challenging to acquire than ASR datasets, with a combined total of merely 1B tokens.</p>\n\n",
                "matched_terms": [
                    "total",
                    "datasets",
                    "tokens",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "stage",
                    "datasets",
                    "data",
                    "total",
                    "training",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "tokens",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "volume",
                    "stage",
                    "data",
                    "total",
                    "stages",
                    "training",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the vision modality pre-training stage, the visual data primarily comprised image&#8211;caption and video&#8211;caption datasets. The image&#8211;caption corpus contained approximately 17 million samples (13 billion tokens), whereas the video&#8211;caption data represented a smaller portion, with around 0.1 million samples (0.2 billion tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "datasets",
                    "tokens",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "about",
                    "stage",
                    "datasets",
                    "data",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "datasets",
                    "data",
                    "training",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the task projector alignment stage and the visual projector alignment stage, we utilized data encompassing a variety of task types. This included image generation, image editing, low-level computer vision tasks such as deraining and denoising, and conditional generation like Canny-to-image. In total, this data amounted to approximately 2M samples (1.5B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "total",
                    "tokens",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the caption projector alignment stage, we used high-quality image-caption pairs sourced from a diverse range of data types. This collection included factual, human-annotated descriptions of everyday scenes; detailed, context-rich prompts for complex image generation; and elaborate narratives describing the compositional and spatial relationships between elements. This data accounted for a total of 4.2M samples (3.2B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "total",
                    "tokens",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated and filtered a large-scale dataset from multiple open-source dataset collections, amounting to approximately 10.5M samples (8.1B tokens). This stage aimed to unify the model&#8217;s capabilities under a consistent instruction-following framework. The training data was formatted as instruction-response pairs covering the same core task categories mentioned previously: image generation, image editing, low-level CV enhancements, and conditional image synthesis.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "tokens",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "across",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "tokens",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "stages",
                    "datasets",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "about"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "stages",
                    "across",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "stages",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "stages",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "stages",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "across",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "across",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify that the model&#8217;s enhanced omnimodality understanding capability stems primarily from its audio-text-vision joint coding training on large-scale video data. Future work will focus on scaling this video data and introducing new multimodal positional encoding methods to advance the model&#8217;s comprehension abilities further.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "75b",
                    "tokens",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "stages",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "across",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All open-source training data are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T15\" title=\"Table 15 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T18\" title=\"Table 18 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 3: \nComparison of Uni-MoE-2.0-Omni and variants with other omnimodal models across General, STEM, Doc & OCR, and Video benchmarks.\nAll results presented in this table are evaluated using the lmms-eval (zhang2024lmmsevalrealitycheckevaluation) to ensure consistency and reproducibility. Bold indicates the highest score, and underline indicates the second-highest score for each benchmark.",
        "body": "Benchmark\nUni-MoE-2.0\nUni-MoE-2.0-Omni\nQwen2.5-Omni\nMiniCPM-o 2.6\nBaichuan-Omni-1.5\nMing-Lite-Omni\nMing-Lite-Omni-1.5\n\n\nGeneral\n\n\nMMBench-EN (Dev)\n80.76\n80.50\n75.42\n82.39\n82.82\n82.65\n82.56\n\n\nMMBench-CN (Dev)\n78.69\n79.90\n68.65\n80.50\n68.73\n78.78\n80.07\n\n\nMMStar\n59.72\n59.38\n59.94\n59.59\n60.77\n63.30\n63.80\n\n\nRealWorldQA\n62.22\n63.12\n64.71\n65.23\n66.93\n64.18\n66.01\n\n\nGQA (test-dev)\n61.95\n62.18\n49.55\n59.43\n58.86\n61.68\n61.88\n\n\nMME-RealWorld\n53.73\n53.67\n49.96\n46.94\n52.70\n58.13\n58.55\n\n\nCV-Bench\n75.70\n76.46\n75.82\n75.02\n76.72\n79.49\n79.15\n\n\nSTEM\n\n\nAI2D\n80.93\n81.35\n78.79\n82.55\n79.24\n80.80\n82.67\n\n\nMMMU (Val)\n42.67\n46.67\n44.44\n47.33\n47.11\n51.78\n53.44\n\n\nMMMU-Pro (Standard)\n29.48\n29.65\n36.42\n30.69\n33.93\n33.30\n32.49\n\n\nMMMU-Pro (Vision)\n14.91\n14.51\n13.33\n11.62\n25.90\n12.66\n15.90\n\n\nMathVista (Testmini)\n60.80\n61.30\n56.20\n66.20\n59.50\n69.50\n69.00\n\n\nMathVision (Test)\n40.76\n36.61\n17.14\n14.21\n21.13\n13.29\n25.20\n\n\nLogicVista\n31.47\n32.81\n33.93\n37.05\n32.59\n39.51\n37.05\n\n\nDoc & OCR\n\n\nDocVQA (Test)\n79.75\n79.53\n88.11\n82.09\n89.94\n92.50\n93.55\n\n\nChartQA\n71.64\n73.04\n74.80\n82.80\n83.28\n85.56\n85.80\n\n\nCharXiv (DQ)\n48.80\n47.68\n59.69\n51.32\n41.42\n67.40\n68.15\n\n\nCharXiv (RQ)\n23.20\n24.10\n25.83\n28.60\n30.70\n26.00\n25.83\n\n\nSEED-Bench-2-Plus\n64.21\n64.38\n68.07\n65.26\n66.23\n68.47\n68.25\n\n\nVideo\n\n\nVideo-MME (w/o sub)\n64.85\n66.41\n59.78\n60.78\n59.85\n62.04\n62.56\n\n\nLongVideoBench (Val)\n56.62\n55.35\n53.83\n51.83\n54.00\n54.90\n55.20\n\n\nMVBench\n69.33\n70.53\n61.23\n58.90\n61.12\n66.92\n68.40\n\n\nVSI-Bench\n53.87\n55.97\n19.32\n25.58\n33.89\n36.30\n37.77\n\n\nCharades-Sta\n27.73\n30.62\n29.24\n16.41\n20.41\n8.81\n10.88",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Benchmark</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">MiniCPM-o 2.6<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Baichuan-Omni-1.5<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Ming-Lite-Omni<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Ming-Lite-Omni-1.5<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"8\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">General</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">MMBench-EN (Dev)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">80.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">80.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">75.42</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">82.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">82.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">82.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">82.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">MMBench-CN (Dev)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">78.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">79.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">68.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">80.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">68.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">78.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">80.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">MMStar</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">59.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">59.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">59.94</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">59.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">60.77</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">63.30</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">63.80</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">RealWorldQA</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">62.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">63.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">64.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">65.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">66.93</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">64.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">66.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">GQA (test-dev)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">61.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">62.18</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">49.55</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">59.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">58.86</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">61.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">61.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">MME-RealWorld</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">53.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">53.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">49.96</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">46.94</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">52.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">58.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">58.55</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">CV-Bench</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">75.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">76.46</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">75.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">75.02</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">76.72</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">79.49</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">79.15</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"8\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">STEM</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">AI2D</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">80.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">81.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">78.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">82.55</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">79.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">80.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">82.67</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">MMMU (Val)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">42.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">46.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">44.44</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">47.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">47.11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">51.78</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">53.44</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">MMMU-Pro (Standard)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">29.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">29.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">36.42</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">30.69</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">33.93</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">33.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">32.49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">MMMU-Pro (Vision)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">14.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">14.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">13.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">11.62</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">25.90</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">12.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">15.90</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">MathVista (Testmini)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">60.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">61.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">56.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">66.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">59.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">69.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">69.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">MathVision (Test)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">40.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">36.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">17.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">14.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">21.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">13.29</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">25.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">LogicVista</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">31.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">32.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">33.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">37.05</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">32.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">39.51</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">37.05</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"8\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Doc &amp; OCR</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">DocVQA (Test)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">79.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">79.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">88.11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">82.09</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">89.94</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">92.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">93.55</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">ChartQA</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">71.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">73.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">74.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">82.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">83.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">85.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">85.80</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">CharXiv (DQ)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">48.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">47.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">59.69</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">51.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">41.42</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">67.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">68.15</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">CharXiv (RQ)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">23.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">24.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">25.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">28.60</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">30.70</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">26.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">25.83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">SEED-Bench-2-Plus</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">64.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">64.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">68.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">65.26</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">66.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">68.47</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">68.25</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"8\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Video</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">Video-MME (w/o sub)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">64.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">66.41</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">59.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">60.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">59.85</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">62.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">62.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">LongVideoBench (Val)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">56.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">55.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">53.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">51.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">54.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">54.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">55.20</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">MVBench</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">69.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">70.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">61.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">58.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">61.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">66.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">68.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">VSI-Bench</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">53.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">55.97</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">19.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">25.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">33.89</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">36.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">37.77</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">Charades-Sta</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2pt;padding-bottom:2pt;\">27.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">30.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">29.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2pt;padding-bottom:2pt;\">16.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2pt;padding-bottom:2pt;\">20.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2pt;padding-bottom:2pt;\">8.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2pt;padding-bottom:2pt;\">10.88</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "vsibench",
            "unimoe20omni",
            "presented",
            "mmstar",
            "mathvista",
            "benchmark",
            "val",
            "longvideobench",
            "underline",
            "testmini",
            "video",
            "realworldqa",
            "secondhighest",
            "dev",
            "doc",
            "gqa",
            "testdev",
            "mmerealworld",
            "charxiv",
            "results",
            "mmmu",
            "seedbench2plus",
            "general",
            "mingliteomni15",
            "docvqa",
            "each",
            "ensure",
            "bold",
            "qwen25omni",
            "mvbench",
            "indicates",
            "charadessta",
            "omnimodal",
            "reproducibility",
            "chartqa",
            "score",
            "sub",
            "evaluated",
            "comparison",
            "ocr",
            "stem",
            "mmbenchcn",
            "ai2d",
            "minicpmo",
            "variants",
            "standard",
            "highest",
            "zhang2024lmmsevalrealitycheckevaluation",
            "unimoe20",
            "test",
            "mmbenchen",
            "videomme",
            "consistency",
            "across",
            "baichuanomni15",
            "mingliteomni",
            "all",
            "benchmarks",
            "mathvision",
            "models",
            "mmmupro",
            "lmmseval",
            "cvbench",
            "logicvista",
            "other",
            "26",
            "vision"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
            "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "all",
                    "models",
                    "general",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Deep Cross-Modal Alignment</span>: To enable deep and efficient fusion of any modality, we introduce an Omni-Modality 3D RoPE mechanism in the self-attention layers. It encodes the temporal-spatial dimensions of speech, image, text, and video tokens, ensuring seamless alignment and interaction across all input types.</p>\n\n",
                "matched_terms": [
                    "across",
                    "video",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "reproducibility",
                    "unimoe20omni",
                    "across",
                    "all",
                    "benchmarks",
                    "models",
                    "comparison",
                    "results",
                    "other",
                    "minicpmo",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "each",
                    "ensure",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Visual Encoder</span>. The visual encoder is initialized with SigLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhai2023sigmoid</span>)</cite> vision transformer, which transforms the input image or video frames <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i1.p1.m1\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> into visual features <math alttext=\"Z_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">Z_{V}</annotation></semantics></math>. We adopt the output of the last layer of the transformer as the visual features.</p>\n\n",
                "matched_terms": [
                    "vision",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through these visual understanding modules, we can uniformly encode Single-Image, Multi-Image, and Video inputs, thereby transforming different visual input paradigms into a unified one-dimensional sequence of visual representations. The specific encoding strategies for each paradigm are described as follows:</p>\n\n",
                "matched_terms": [
                    "each",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "vision",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "vision",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{W}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>W</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{W}^{A}</annotation></semantics></math> is the last hidden states of the pre-trained audio encoder adopted from Whisper-large-v3. <span class=\"ltx_text ltx_markedasmath\">MSA</span> and <math alttext=\"h\\text{MCA}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>MCA</mtext></mrow><annotation encoding=\"application/x-tex\">h\\text{MCA}</annotation></semantics></math> denote the multihead self-attention and cross-attention operations, and <span class=\"ltx_text ltx_markedasmath\">LN</span> is the layer norm function. By leveraging the fixed-length query vectors <math alttext=\"X_{Q}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>Q</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">X_{Q}^{A}</annotation></semantics></math>, each 30 seconds of audio encoded by the Whisper encoder is mapped to <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> audio tokens. <math alttext=\"h_{C}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>C</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{C}^{A}</annotation></semantics></math> represents the output of the cross-attention module, which is used to distill the main content of the input audio. After going through all layers of the Whisper decoder, we apply a learnable linear layer for projecting the last output into the representation space of LLM.</p>\n\n",
                "matched_terms": [
                    "each",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "across",
                    "all",
                    "each",
                    "standard",
                    "vision",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Shared Experts</span>: These experts capture general, domain-independent knowledge. Unlike routed experts, shared experts are persistently activated for all tokens, ensuring that common knowledge is always available during inference.</p>\n\n",
                "matched_terms": [
                    "general",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "each",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "all",
                    "results",
                    "ensure"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "ensure",
                    "video",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stem",
                    "general",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "ensure",
                    "video",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "omnimodal",
                    "unimoe20omni",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "vsibench",
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "results",
                    "other",
                    "mvbench",
                    "videomme",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "unimoe20omni",
                    "benchmarks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "results",
                    "standard",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "results",
                    "general",
                    "unimoe20omni",
                    "benchmarks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "unimoe20omni",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Uni-MoE-2.0-Omni proves to be a balanced and effective model for omnimodal TTS, exhibiting particular strength in English synthesis and robust performance across diverse and challenging tasks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "results",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "all",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "score",
                    "models",
                    "other",
                    "benchmark",
                    "highest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "score",
                    "across",
                    "all",
                    "benchmarks",
                    "results",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "score",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "score",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "results",
                    "omnimodal",
                    "across",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "across",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "across",
                    "all",
                    "mmmu",
                    "logicvista",
                    "unimoe20"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "across",
                    "all",
                    "models",
                    "mathvista",
                    "testmini"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "models",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "results",
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All open-source training data are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T15\" title=\"Table 15 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T18\" title=\"Table 18 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>.</p>\n\n",
                "matched_terms": [
                    "presented",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "across",
                    "models",
                    "ocr",
                    "stem",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Visual Understanding</span>. MMBench (EN/CN) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024mmbench</span>)</cite>, MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmstar</span>)</cite>, RealWorldQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">real_world_qa</span>)</cite>, GQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gqa</span>)</cite>, MME-RealWorld <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mme-realworld</span>)</cite>, and CV-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cvbench</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "gqa",
                    "mmerealworld",
                    "cvbench",
                    "general",
                    "mmstar",
                    "realworldqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">STEM Image Reasoning</span>. AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2d</span>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmmu</span>)</cite> and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yue2024mmmupro</span>)</cite> for science reasoning. MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathvista</span>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathvision</span>)</cite> and LogicVista <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">logicvista</span>)</cite> for mathematics reasoning.</p>\n\n",
                "matched_terms": [
                    "mathvision",
                    "mmmupro",
                    "mmmu",
                    "stem",
                    "logicvista",
                    "ai2d",
                    "mathvista"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">OCR &amp; Document Understanding</span>. DocVQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">docvqa</span>)</cite>, ChartQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chartqa</span>)</cite>, CharXiv (DQ/RQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">charxivdq</span>)</cite>, and SEED-Bench-2-Plus <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">seed2plus</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "chartqa",
                    "seedbench2plus",
                    "ocr",
                    "docvqa",
                    "charxiv"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Short Video Understanding</span>. MVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mvbench</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "mvbench",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Long Video Understanding</span>. Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">video_mme</span>)</cite>, LongVideoBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">longvideobench</span>)</cite> and EgoSchema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">egoschema</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "videomme",
                    "video",
                    "longvideobench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video Reasoning</span>. VideoMMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">video_mmmu</span>)</cite> for video knowledge reasoning, VSI-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vsi-bench</span>)</cite> for video spatial reasoning and TOMATO <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tomato</span>)</cite> for video temporal reasoning.</p>\n\n",
                "matched_terms": [
                    "vsibench",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video Temporal Localization</span>. Charades-STA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">charades-sta</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "charadessta",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 4: \nComparison of Uni-MoE-2.0-Omni and variants with other MLLMs across 8 Video benchmarks.\n* denotes the reproduced results. When evaluating Video-MME, the subtitles are not used.\nBold indicates the highest score, and underline indicates the second-highest score for each benchmark.",
        "body": "Model\nVideo-MME\nVideoMMMU\nLongVideoBench\nMVBench\nVSI-Bench\nCharades-Sta\nTOMATO\nEgoSchema\nAvg.\n\n\nVision-Language Models\n\n\nLLaVA-OneVision-7B\n58.2\n33.9\n50.5\n56.7\n-\n-\n-\n60.1\n-\n\n\nLLaVA-Video-7B\n63.3\n36.1\n58.2\n58.6\n-\n-\n-\n57.3\n-\n\n\nNVILA-8B\n64.2\n20.9\n57.7\n68.1\n-\n-\n-\n54.3\n-\n\n\nVideoLLaMA3-7B\n66.2\n47.0\n59.8\n69.7\n-\n-\n-\n63.3\n-\n\n\nInternVL2.5-8B*\n64.1\n46.0\n58.9\n71.9\n34.6\n9.5\n28.0\n51.2\n45.5\n\n\nQwen2.5-VL-7B*\n63.0\n49.6\n57.6\n66.3\n37.7\n40.3\n22.6\n58.4\n49.4\n\n\nOmni Models\n\n\nQwen2.5-Omni*\n59.8\n43.6\n55.1\n61.2\n19.3\n29.2\n25.5\n53.8\n43.5\n\n\nMiniCPM-o 2.6*\n60.8\n37.6\n51.8\n58.9\n25.6\n16.4\n25.0\n43.2\n39.9\n\n\nBaichuan-Omni-1.5*\n59.9\n43.5\n54.0\n61.1\n33.9\n20.4\n25.3\n57.5\n44.4\n\n\nMing-Lite-Omni*\n62.0\n48.8\n54.9\n66.9\n36.3\n8.8\n28.2\n57.0\n45.4\n\n\nMing-Lite-Omni-1.5*\n62.6\n49.3\n55.2\n68.4\n37.8\n10.9\n34.2\n54.5\n46.6\n\n\nOur Models\n\n\nUni-MoE-2.0\n64.9\n39.1\n56.6\n69.3\n53.9\n27.7\n27.0\n52.2\n46.3\n\n\nUni-MoE-2.0-Omni\n66.4\n43.6\n55.4\n70.5\n56.0\n30.6\n27.8\n54.3\n50.6",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Video-MME</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">VideoMMMU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LongVideoBench</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MVBench</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">VSI-Bench</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Charades-Sta</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">TOMATO</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">EgoSchema</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"10\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Vision-Language Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LLaVA-OneVision-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">58.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">33.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">50.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">56.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">60.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LLaVA-Video-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">63.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">36.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">58.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">58.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">57.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">NVILA-8B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">64.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">20.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">57.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">68.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">54.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">VideoLLaMA3-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">66.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">47.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">59.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">69.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">63.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">InternVL2.5-8B*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">64.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">46.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">58.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">71.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">34.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">28.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">51.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">45.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Qwen2.5-VL-7B*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">63.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">49.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">57.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">66.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">37.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">40.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">22.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">58.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">49.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"10\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Omni Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Qwen2.5-Omni*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">59.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">43.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">55.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">61.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">19.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">29.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">25.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">53.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">43.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MiniCPM-o 2.6*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">60.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">37.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">51.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">58.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">25.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">16.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">25.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">43.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">39.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Baichuan-Omni-1.5*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">59.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">43.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">54.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">61.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">33.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">20.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">25.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">57.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">44.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Ming-Lite-Omni*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">62.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">48.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">54.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">66.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">36.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">28.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">57.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">45.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Ming-Lite-Omni-1.5*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">62.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">49.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">55.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">68.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">37.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">10.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">34.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">54.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">46.6</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"10\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Our Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Uni-MoE-2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">64.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">39.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">56.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">69.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">53.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">27.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">27.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">52.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">46.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Uni-MoE-2.0-Omni</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">66.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">43.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">55.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">70.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">56.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">30.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">27.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">54.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.6</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "vsibench",
            "unimoe20omni",
            "qwen25omni",
            "subtitles",
            "mingliteomni",
            "benchmark",
            "longvideobench",
            "used",
            "underline",
            "avg",
            "video",
            "secondhighest",
            "denotes",
            "our",
            "results",
            "mingliteomni15",
            "baichuanomni15",
            "each",
            "videollama37b",
            "charadessta",
            "bold",
            "mvbench",
            "internvl258b",
            "indicates",
            "llavavideo7b",
            "score",
            "mllms",
            "comparison",
            "nvila8b",
            "evaluating",
            "visionlanguage",
            "minicpmo",
            "variants",
            "omni",
            "highest",
            "unimoe20",
            "across",
            "egoschema",
            "benchmarks",
            "videommmu",
            "tomato",
            "models",
            "qwen25vl7b",
            "reproduced",
            "other",
            "when",
            "model",
            "videomme",
            "llavaonevision7b",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "model",
                    "qwen25omni",
                    "our",
                    "avg",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "across",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "when",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Deep Cross-Modal Alignment</span>: To enable deep and efficient fusion of any modality, we introduce an Omni-Modality 3D RoPE mechanism in the self-attention layers. It encodes the temporal-spatial dimensions of speech, image, text, and video tokens, ensuring seamless alignment and interaction across all input types.</p>\n\n",
                "matched_terms": [
                    "across",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "when",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "minicpmo",
                    "models",
                    "qwen25omni",
                    "comparison",
                    "results",
                    "other",
                    "mingliteomni15",
                    "mingliteomni",
                    "baichuanomni15",
                    "our",
                    "model",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "video",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through these visual understanding modules, we can uniformly encode Single-Image, Multi-Image, and Video inputs, thereby transforming different visual input paradigms into a unified one-dimensional sequence of visual representations. The specific encoding strategies for each paradigm are described as follows:</p>\n\n",
                "matched_terms": [
                    "each",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "our",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{W}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>W</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{W}^{A}</annotation></semantics></math> is the last hidden states of the pre-trained audio encoder adopted from Whisper-large-v3. <span class=\"ltx_text ltx_markedasmath\">MSA</span> and <math alttext=\"h\\text{MCA}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>MCA</mtext></mrow><annotation encoding=\"application/x-tex\">h\\text{MCA}</annotation></semantics></math> denote the multihead self-attention and cross-attention operations, and <span class=\"ltx_text ltx_markedasmath\">LN</span> is the layer norm function. By leveraging the fixed-length query vectors <math alttext=\"X_{Q}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>Q</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">X_{Q}^{A}</annotation></semantics></math>, each 30 seconds of audio encoded by the Whisper encoder is mapped to <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> audio tokens. <math alttext=\"h_{C}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>C</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{C}^{A}</annotation></semantics></math> represents the output of the cross-attention module, which is used to distill the main content of the input audio. After going through all layers of the Whisper decoder, we apply a learnable linear layer for projecting the last output into the representation space of LLM.</p>\n\n",
                "matched_terms": [
                    "each",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "each",
                    "across",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As an illustrative example, consider a video input of length 120 seconds accompanied by audio. We place the visual sequence before the audio sequence and sample the video at a rate of one frame every two seconds. Suppose the RoPE ID of the last text token preceding the visual sequence is <math alttext=\"(x-1,x-1,x-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x-1,x-1,x-1)</annotation></semantics></math>. Then, the RoPE IDs of the first video frame start from <math alttext=\"(x,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x,x)</annotation></semantics></math> and increment row by row and column by column up to <math alttext=\"(x,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> denotes the number of tokens along both the height and width of a video frame. The second frame begins with <math alttext=\"(x+2\\theta,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x,x)</annotation></semantics></math> and increases to <math alttext=\"(x+2\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> is a specific scaling factor for absolute time. Following this procedure, the final frame in the video sequence has an ID of <math alttext=\"(x+118\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>118</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+118\\theta,x+p,x+p)</annotation></semantics></math>.\nSimilarly, assume the RoPE ID of the last token before the audio sequence is <math alttext=\"(y-1,y-1,y-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y-1,y-1,y-1)</annotation></semantics></math>. The first audio segment is then assigned the ID <math alttext=\"(y,y,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y,y,y)</annotation></semantics></math>, which is repeated 20 times to represent the minimum audio unit. The next audio segment receives the ID <math alttext=\"(y+3\\theta,y+3\\theta,y+3\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+3\\theta,y+3\\theta,y+3\\theta)</annotation></semantics></math>. Continuing this process, the final audio segment is assigned the ID <math alttext=\"(y+117\\theta,y+117\\theta,y+117\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+117\\theta,y+117\\theta,y+117\\theta)</annotation></semantics></math>. In this manner, temporal alignment between the visual and audio sequences can also be achieved.</p>\n\n",
                "matched_terms": [
                    "denotes",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central difficulty in training vanilla MoE models arises from the non-differentiable Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> operation used in expert selection, which obstructs gradient propagation to the router and leads to biased optimization. To address this, we migrate the gradient estimation strategy proposed in Grin-MoE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024gringradientinformedmoe</span>)</cite>, which integrates straight-through gradient estimators under the framework of numerical methods for ordinary differential equations (ODEs). This enables end-to-end optimization of both router and experts under sparse activation constraints, improving the stability of router training and allowing more precise token-to-expert assignments. A formal description is given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.SS4\" title=\"A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "model",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the performance of MoE-TTS in voice timbre control, we adopt a text-context prompt-guided approach to direct the model to synthesize audio with the specified language and timbre. When the model receives an instruction to generate an audio response, the Uni-MoE base model first produces a special token (i.e. <span class=\"ltx_text ltx_font_italic\">&lt;speech start&gt;</span>) indicating the start of speech, . Subsequently, the model generates commands that specify the voice timbre for audio synthesis: fixed timbres include those of Brain, Jenny, and Xiaoxiao, while supported languages cover Chinese and English. Beyond fixed options, the model also enables users to describe desired timbres via natural language; the MoE-TTS module then generates audio with the corresponding timbre based on these descriptive instructions. Upon completion of the timbre prompt, a special token &lt;speech prompt&gt; is generated to signal the end of the prompt. This is followed by the text content that the model needs to transcribe into speech, which is terminated with the special token &lt;speech end&gt;. Through this combination of text-based timbre prompts and target text content, MoE-TTS is guided to perform diverse timbre synthesis.</p>\n\n",
                "matched_terms": [
                    "when",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a Task-Aware Diffusion Transformer (Task-DiT) that bridges the gap between image understanding and generation. Unlike unified models that suffer from performance trade-offs, our framework preserves the strengths of specialized pre-trained models by connecting them via a lightweight, task-aware bridge.\nAt its core, Task-DiT employs a frozen image generator from PixWizard <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024pixwizard</span>)</cite> to safeguard its high-fidelity synthesis capabilities. To steer this generator, we introduce a query-based conditioning mechanism. Two distinct sets of learnable tokens are processed by a powerful understanding module:</p>\n\n",
                "matched_terms": [
                    "our",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Expert Warmup with Dense Model: We first pre-trained three specialized expert models focusing on mainstream modalities: speech comprehension, speech generation, and visual comprehension. This stage aims to build the model-specific experts for the following stable MoE-based fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "across",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "when",
                    "models",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "used",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "model",
                    "unimoe20omni",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "other",
                    "highest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "vsibench",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "results",
                    "mingliteomni15",
                    "benchmark",
                    "model",
                    "videomme",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "results",
                    "mingliteomni15",
                    "our",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "qwen25omni",
                    "results",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "when",
                    "results",
                    "our",
                    "model",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "models",
                    "results",
                    "mingliteomni",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "unimoe20omni",
                    "mingliteomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Uni-MoE-2.0-Omni proves to be a balanced and effective model for omnimodal TTS, exhibiting particular strength in English synthesis and robust performance across diverse and challenging tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "results",
                    "mingliteomni",
                    "other",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "score",
                    "models",
                    "model",
                    "other",
                    "mingliteomni",
                    "benchmark",
                    "our",
                    "highest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "score",
                    "across",
                    "benchmarks",
                    "results",
                    "baichuanomni15",
                    "our",
                    "omni",
                    "model",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "unimoe20omni",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "score",
                    "models",
                    "benchmark",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "score",
                    "models",
                    "benchmark",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "model",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "our",
                    "results",
                    "across",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "across",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "comparison",
                    "used",
                    "model",
                    "unimoe20"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "unimoe20"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "when",
                    "qwen25omni",
                    "mingliteomni",
                    "our",
                    "used",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "model",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "model",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to express our sincere gratitude to Qi Wang, Qixun Teng, Feifan Wen, and Zitao Li for their contributions to data collection and demo recording. We are thankful to Jinchao Li and Tongshu Bian for their assistance in paper proofreading and video production. We also thank Longyue Wang and Wenhan Luo for their valuable suggestions on paper writing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "across",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Short Video Understanding</span>. MVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mvbench</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "mvbench",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Long Video Understanding</span>. Video-MME <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">video_mme</span>)</cite>, LongVideoBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">longvideobench</span>)</cite> and EgoSchema <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">egoschema</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "egoschema",
                    "videomme",
                    "video",
                    "longvideobench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video Reasoning</span>. VideoMMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">video_mmmu</span>)</cite> for video knowledge reasoning, VSI-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vsi-bench</span>)</cite> for video spatial reasoning and TOMATO <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tomato</span>)</cite> for video temporal reasoning.</p>\n\n",
                "matched_terms": [
                    "tomato",
                    "videommmu",
                    "vsibench",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video Temporal Localization</span>. Charades-STA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">charades-sta</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "charadessta",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "omni",
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hybrid gradient estimator.</span>\nTo balance the stability of router training with the diversity of expert learning, we combine the two estimators above: the first-order gradient is used when the selected expert corresponds to <math alttext=\"\\arg\\max({\\bm{z}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m1\" intent=\":literal\"><semantics><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\arg\\max({\\bm{z}})</annotation></semantics></math>, while the third-order gradient is applied otherwise.\nLet <math alttext=\"\\delta_{\\bm{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#948;</mi><mi>&#119915;</mi></msub><annotation encoding=\"application/x-tex\">\\delta_{\\bm{D}}</annotation></semantics></math> denote the indicator <math alttext=\"\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119915;</mi><mo>=</mo><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))</annotation></semantics></math>. The combined estimator can be written as:\n\n</p>\n\n",
                "matched_terms": [
                    "when",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To extend it to our Top-<math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p9.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> strategy, we apply the same computation sequentially to each activated expert, sampling without replacement from the routing distribution until the cumulative probability exceeds <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p9.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math>.\nIn this way, the gradient estimation naturally generalizes to multiple experts while remaining consistent with the selection process in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ensuring that the hybrid scaling mechanism is fully compatible with our dynamic-capacity MoE framework.</p>\n\n",
                "matched_terms": [
                    "each",
                    "our"
                ]
            }
        ]
    },
    "S4.T5": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 5: \nComparison of Uni-MoE-2.0-Omni and other multimodal models on reasoning and general knowledge benchmarks. All models are tested with direct answers in a zero-shot setting. Bold indicates the highest score, and underline indicates the second-highest score for each benchmark.",
        "body": "Benchmark\nUni-MoE-2.0\nUni-MoE-2.0-Omni\nQwen2.5-Omni*\nMiniCPM-o 2.6*\nBaichuan-Omni 1.5*\nMing-Lite-Omni*\nMing-Lite-Omni-1.5*\n\n\nGPQA Diamond\n30.30\n32.83\n27.27\n30.81\n26.77\n24.24\n31.31\n\n\nGPQA Main\n32.37\n33.48\n23.66\n29.30\n25.46\n32.59\n31.70\n\n\nGPQA Extended\n32.05\n33.15\n22.16\n24.78\n25.00\n34.70\n34.43\n\n\nMMLU-Pro\n36.00\n38.76\n32.23\n29.80\n42.43\n42.72\n44.75\n\n\nAvg.\n32.68\n34.56\n26.33\n28.67\n29.92\n33.56\n35.55",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Benchmark</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">MiniCPM-o 2.6*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Baichuan-Omni 1.5*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Ming-Lite-Omni*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Ming-Lite-Omni-1.5*</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPQA Diamond</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">30.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">32.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">27.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">30.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">26.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">24.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">31.31</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPQA Main</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">32.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">33.48</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">23.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">29.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">25.46</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">32.59</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">31.70</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">GPQA Extended</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">32.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">33.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">22.16</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">24.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">25.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">34.70</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">34.43</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">MMLU-Pro</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">36.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">38.76</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">32.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">29.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">42.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">42.72</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">44.75</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Avg.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">32.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">34.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">26.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">28.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">29.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">33.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">35.55</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "unimoe20omni",
            "main",
            "qwen25omni",
            "mmlupro",
            "extended",
            "mingliteomni",
            "tested",
            "benchmark",
            "underline",
            "avg",
            "reasoning",
            "secondhighest",
            "general",
            "mingliteomni15",
            "multimodal",
            "each",
            "bold",
            "direct",
            "gpqa",
            "indicates",
            "knowledge",
            "score",
            "comparison",
            "minicpmo",
            "zeroshot",
            "highest",
            "answers",
            "unimoe20",
            "all",
            "benchmarks",
            "diamond",
            "models",
            "baichuanomni",
            "other",
            "setting"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "qwen25omni",
                    "multimodal",
                    "avg",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "models",
                    "general",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "baichuanomni",
                    "reasoning",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "reasoning",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">MoE-Driven Cross-Modal Fusion</span>: We strategically extend the standard MLP layers to MoE layers. This new MoE architecture incorporates three expert types: null experts for inference-time computation skipping, modality-specific routed experts for storing modality knowledge and processing cross-modal information, and small-size shared experts to facilitate universal information exchange. This design enables efficient computation, specialized modality handling, and effective and stable multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "all",
                    "benchmarks",
                    "models",
                    "qwen25omni",
                    "comparison",
                    "other",
                    "mingliteomni15",
                    "mingliteomni",
                    "multimodal",
                    "minicpmo",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "each",
                    "multimodal",
                    "knowledge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "models",
                    "main"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "reasoning",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{W}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>W</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{W}^{A}</annotation></semantics></math> is the last hidden states of the pre-trained audio encoder adopted from Whisper-large-v3. <span class=\"ltx_text ltx_markedasmath\">MSA</span> and <math alttext=\"h\\text{MCA}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>MCA</mtext></mrow><annotation encoding=\"application/x-tex\">h\\text{MCA}</annotation></semantics></math> denote the multihead self-attention and cross-attention operations, and <span class=\"ltx_text ltx_markedasmath\">LN</span> is the layer norm function. By leveraging the fixed-length query vectors <math alttext=\"X_{Q}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>Q</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">X_{Q}^{A}</annotation></semantics></math>, each 30 seconds of audio encoded by the Whisper encoder is mapped to <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> audio tokens. <math alttext=\"h_{C}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>C</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{C}^{A}</annotation></semantics></math> represents the output of the cross-attention module, which is used to distill the main content of the input audio. After going through all layers of the Whisper decoder, we apply a learnable linear layer for projecting the last output into the representation space of LLM.</p>\n\n",
                "matched_terms": [
                    "each",
                    "main",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "each",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Homogeneous expert types fail to distinguish between domain-specific and general knowledge, and cannot support operations such as selectively forgetting outdated knowledge.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These limitations prevent MoE from fully exploiting its adaptive activation capability, ultimately resulting in suboptimal performance.\nTo address these challenges, we propose the Dynamic-Capacity MoE architecture, which incorporates (i) routing gradient estimation to enable differentiable expert selection, (ii) explicit expert role specialization to separately model domain-specific and general knowledge, as well as facilitate knowledge deletion, and (iii) dynamic expert number allocation to adaptively control the amount of parametric knowledge.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Shared Experts</span>: These experts capture general, domain-independent knowledge. Unlike routed experts, shared experts are persistently activated for all tokens, ensuring that common knowledge is always available during inference.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "general",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Vanilla MoE applies a fixed number of experts to every token, ignoring variations in token complexity and knowledge demand.\nWe address this limitation by introducing a dynamic capacity routing strategy, which determines the number of routed experts for each token based on a Top-P sampling.\nFormally, let the router produce a probability vector over routed experts for token <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px3.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "each",
                    "knowledge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a holistic view of our method, Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the complete training procedure in pseudocode form.\nFor simplicity of presentation, we treat null experts as a special case of routed experts in the pseudocode.\nThe algorithm integrates the three proposed components into a unified workflow:\nFirst, <span class=\"ltx_text ltx_font_typewriter\">ActivateExperts</span> function determines expert activation, combining the Top&#8209;P sampling strategy for routed experts with the persistent activation constraint for shared experts;\nSecond, gradient estimation is applied to routed expert outputs to enable end&#8209;to&#8209;end optimization under sparse activation;\nFinally, after computing routed expert contributions, shared expert outputs are computed and incorporated into the MoE representation, injecting general knowledge into the final output.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "each",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The enriched features from these tokens are then translated for the generator by dedicated, lightweight projectors. A Task Projector modulates the DiT&#8217;s denoising process based on the command, while a content projector transforms the &lt;IMG[i]&gt; features into a dense conditioning sequence for cross-attention. For image-guided tasks, a Visual Projector aligns source image features encoded by ViT with the DiT&#8217;s conditioning space.\nThis design creates a versatile and efficient channel for task-aware instruction, enabling high-quality, multi-modal image generation without the catastrophic interference typical of end-to-end fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "models",
                    "direct",
                    "reasoning",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "general",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "extended",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "unimoe20omni",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "general",
                    "other",
                    "highest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "mingliteomni15",
                    "benchmark",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "qwen25omni",
                    "multimodal",
                    "other",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "unimoe20omni",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "unimoe20omni",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "general",
                    "unimoe20omni",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "mingliteomni",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "mingliteomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "mingliteomni",
                    "other",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "all",
                    "reasoning",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "score",
                    "models",
                    "answers",
                    "other",
                    "mingliteomni",
                    "benchmark",
                    "highest",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "multimodal",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "unimoe20omni",
                    "score",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "score",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "score",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "main",
                    "models",
                    "comparison",
                    "unimoe20"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "unimoe20",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "reasoning",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "mingliteomni",
                    "reasoning",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "extended",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "multimodal",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "models",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video Reasoning</span>. VideoMMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">video_mmmu</span>)</cite> for video knowledge reasoning, VSI-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vsi-bench</span>)</cite> for video spatial reasoning and TOMATO <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tomato</span>)</cite> for video temporal reasoning.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "reasoning",
                    "benchmarks",
                    "models",
                    "mmlupro",
                    "gpqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For clarity of exposition, we restrict our discussion to the Top-1 MoE setting, and later describe how the approach can be extended to our Dynamic-Capacity MoE. We first consider the Top-1 MoE layer whose output is given by:</p>\n\n",
                "matched_terms": [
                    "extended",
                    "setting"
                ]
            }
        ]
    },
    "S4.T6": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 6: \nComparison of Uni-MoE-2.0-Omni and variants with other omnimodal models across Speech/Audio/Music Understanding benchmarks.\nThe accuracy (ACC) metric is employed to assess results of speech understanding, AQA and MMAU; the CIDER is used to evaluate results of all captioning tasks. We found that Ming-Lite-Omni-1.5 often fails to follow instructions and generates off-topic content, making it difficult to evaluate its performance accurately.",
        "body": "Benchmark\nUni-MoE 2.0\nUni-MoE-2.0-Omni\nQwen2-Audio\nQwen2.5-Omni\nMing-Lite-Omni\n\n\nSpeech Understanding\n\n\nRACE-audio-middle\n90.32\n89.69\n28.27\n92.76\n88.30\n\n\nRACE-audio-high\n87.62\n87.19\n26.95\n87.79\n80.38\n\n\nEHSL-short\n88.00\n90.00\n24.00\n86.00\n82.00\n\n\nEHSL-long\n85.33\n87.33\n15.33\n90.66\n83.33\n\n\nMELD\n40.93\n40.4\n37.92\n16.76\n39.35\n\n\nMMAU-speech\n64.69\n65.00\n-\n70.97\n60.88\n\n\nMMBench-hint-Speech\n97.98\n100\n-\n81.06\n95.34\n\n\nAvg.\n79.27\n79.94\n-\n75.14\n75.65\n\n\nAudio Understanding\n\n\nClothoAQA\n61.76\n61.83\n43.45\n62.29\n53.43\n\n\nClothoV1\n37.9\n33.4\n28.9\n21.2\n6.3\n\n\nClothoV2\n38.1\n33.4\n29.1\n30.1\n6.3\n\n\nAudioCaps\n33.8\n33.6\n40.9\n37.1\n18.5\n\n\nMMAU-Sound\n67.17\n68.06\n-\n71.97\n59.3\n\n\nAvg.\n47.74\n46.05\n-\n44.53\n28.77\n\n\nMusic Understanding\n\n\nMusicCaps\n23.9\n62.4\n21.8\n4.00\n0.5\n\n\nMMAU-Music\n59.3\n56.4\n-\n65.33\n52.23",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Benchmark</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Uni-MoE 2.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen2-Audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Ming-Lite-Omni<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"6\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Speech Understanding</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">RACE-audio-middle</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">90.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">89.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">28.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">92.76</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">RACE-audio-high</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">87.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">26.95</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">87.79</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">EHSL-short</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">88.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">90.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">24.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">EHSL-long</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">87.33</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">15.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">90.66</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">MELD</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">40.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">40.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">37.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">16.76</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">39.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">MMAU-speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">65.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">70.97</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">60.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">MMBench-hint-Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">97.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.06</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">95.34</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Avg.</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">79.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">79.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.65</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"6\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Audio Understanding</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">ClothoAQA</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">61.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">61.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">43.45</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">62.29</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">53.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">ClothoV1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">37.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">33.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">28.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">21.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">6.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">ClothoV2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">38.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">33.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">29.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">30.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">6.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">AudioCaps</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">33.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">33.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">40.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">37.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">18.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">MMAU-Sound</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">67.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">68.06</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">71.97</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Avg.</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">47.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">46.05</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">44.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">28.77</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"6\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Music Understanding</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">MusicCaps</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">23.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">62.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">21.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">MMAU-Music</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">59.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">65.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">52.23</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "fails",
            "unimoe20omni",
            "clothoaqa",
            "speech",
            "employed",
            "raceaudiohigh",
            "musiccaps",
            "benchmark",
            "qwen2audio",
            "found",
            "metric",
            "used",
            "making",
            "difficult",
            "avg",
            "captioning",
            "mmausound",
            "assess",
            "often",
            "accurately",
            "unimoe",
            "audiocaps",
            "mmauspeech",
            "accuracy",
            "results",
            "cider",
            "mingliteomni15",
            "ehsllong",
            "audio",
            "qwen25omni",
            "instructions",
            "evaluate",
            "follow",
            "raceaudiomiddle",
            "omnimodal",
            "clothov2",
            "music",
            "performance",
            "mmbenchhintspeech",
            "comparison",
            "generates",
            "acc",
            "mmaumusic",
            "variants",
            "content",
            "mmau",
            "aqa",
            "across",
            "its",
            "mingliteomni",
            "all",
            "benchmarks",
            "speechaudiomusic",
            "models",
            "understanding",
            "offtopic",
            "clothov1",
            "tasks",
            "other",
            "ehslshort",
            "meld"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "unimoe",
                    "its",
                    "benchmarks",
                    "speech",
                    "understanding",
                    "tasks",
                    "avg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "all",
                    "models",
                    "understanding",
                    "audio",
                    "content"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "all",
                    "understanding",
                    "difficult",
                    "content"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "understanding",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "understanding",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Unified Modality Encoding &amp; Generation</span>: We design a unified speech encoder that maps diverse audio inputs, including environmental sound, speech, and music, into a shared representation space. For output, a Context-Aware MoE-based TTS module supports dynamic speech synthesis (especially for long speech) and interaction. On the visual side, we employ pre-trained visual encoders to process images and videos, and build a Task-Aware Diffusion Transformer for instruction-guided image generation and editing.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Deep Cross-Modal Alignment</span>: To enable deep and efficient fusion of any modality, we introduce an Omni-Modality 3D RoPE mechanism in the self-attention layers. It encodes the temporal-spatial dimensions of speech, image, text, and video tokens, ensuring seamless alignment and interaction across all input types.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "often",
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Language-Centric Hybrid Training for Multimodal Understanding and Generation</span>: To bridge the gap between multimodal understanding and generation tasks&#8212;often treated separately during training&#8212;we propose a hybrid training approach anchored in language generation tasks. By unifying tasks such as image editing, image generation, and speech synthesis within a language generation framework, we break down the inherent barriers between understanding and generation, enabling synergistic enhancement and bidirectional empowerment of both capabilities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tasks",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "unimoe",
                    "all",
                    "benchmarks",
                    "models",
                    "speech",
                    "understanding",
                    "comparison",
                    "results",
                    "mingliteomni15",
                    "tasks",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tasks",
                    "instructions",
                    "audio",
                    "content"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The visual understanding module of Uni-MoE-2.0-Omni employs a unified encoding strategy for both images and videos. The overall architecture consists of two major components: a visual encoder and a mapping network (MLP).</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "its",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This unified encoding of Single-Image, Multi-Image, and Video inputs into a one-dimensional sequence of visual representations enables the transfer of the model&#8217;s capability in high-resolution image understanding to the video domain, thereby facilitating faster convergence and improved performance in video understanding.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "models",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "generates",
                    "audio",
                    "speech",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "all",
                    "understanding",
                    "speech",
                    "comparison",
                    "found"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, the workflow of speech understanding is described as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{W}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>W</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{W}^{A}</annotation></semantics></math> is the last hidden states of the pre-trained audio encoder adopted from Whisper-large-v3. <span class=\"ltx_text ltx_markedasmath\">MSA</span> and <math alttext=\"h\\text{MCA}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>MCA</mtext></mrow><annotation encoding=\"application/x-tex\">h\\text{MCA}</annotation></semantics></math> denote the multihead self-attention and cross-attention operations, and <span class=\"ltx_text ltx_markedasmath\">LN</span> is the layer norm function. By leveraging the fixed-length query vectors <math alttext=\"X_{Q}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>Q</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">X_{Q}^{A}</annotation></semantics></math>, each 30 seconds of audio encoded by the Whisper encoder is mapped to <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> audio tokens. <math alttext=\"h_{C}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>C</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{C}^{A}</annotation></semantics></math> represents the output of the cross-attention module, which is used to distill the main content of the input audio. After going through all layers of the Whisper decoder, we apply a learnable linear layer for projecting the last output into the representation space of LLM.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "used",
                    "content",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To process audio longer than Whisper-Large-v3&#8217;s 30-second limit, we employ a chunking mechanism. Long audio is segmented into consecutive 30-second clips, which are batched and processed by the audio understanding module. <span class=\"ltx_text ltx_font_italic\">The resulting features are then concatenated along the time dimensions, enabling the understanding of audio of arbitrary length</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "follow",
                    "audio",
                    "across",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These limitations prevent MoE from fully exploiting its adaptive activation capability, ultimately resulting in suboptimal performance.\nTo address these challenges, we propose the Dynamic-Capacity MoE architecture, which incorporates (i) routing gradient estimation to enable differentiable expert selection, (ii) explicit expert role specialization to separately model domain-specific and general knowledge, as well as facilitate knowledge deletion, and (iii) dynamic expert number allocation to adaptively control the amount of parametric knowledge.</p>\n\n",
                "matched_terms": [
                    "its",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central difficulty in training vanilla MoE models arises from the non-differentiable Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> operation used in expert selection, which obstructs gradient propagation to the router and leads to biased optimization. To address this, we migrate the gradient estimation strategy proposed in Grin-MoE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024gringradientinformedmoe</span>)</cite>, which integrates straight-through gradient estimators under the framework of numerical methods for ordinary differential equations (ODEs). This enables end-to-end optimization of both router and experts under sparse activation constraints, improving the stability of router training and allowing more precise token-to-expert assignments. A formal description is given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.SS4\" title=\"A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "understanding",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "its",
                    "models",
                    "speech",
                    "audio",
                    "instructions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the performance of MoE-TTS in voice timbre control, we adopt a text-context prompt-guided approach to direct the model to synthesize audio with the specified language and timbre. When the model receives an instruction to generate an audio response, the Uni-MoE base model first produces a special token (i.e. <span class=\"ltx_text ltx_font_italic\">&lt;speech start&gt;</span>) indicating the start of speech, . Subsequently, the model generates commands that specify the voice timbre for audio synthesis: fixed timbres include those of Brain, Jenny, and Xiaoxiao, while supported languages cover Chinese and English. Beyond fixed options, the model also enables users to describe desired timbres via natural language; the MoE-TTS module then generates audio with the corresponding timbre based on these descriptive instructions. Upon completion of the timbre prompt, a special token &lt;speech prompt&gt; is generated to signal the end of the prompt. This is followed by the text content that the model needs to transcribe into speech, which is terminated with the special token &lt;speech end&gt;. Through this combination of text-based timbre prompts and target text content, MoE-TTS is guided to perform diverse timbre synthesis.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "unimoe",
                    "speech",
                    "generates",
                    "content",
                    "audio",
                    "instructions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a Task-Aware Diffusion Transformer (Task-DiT) that bridges the gap between image understanding and generation. Unlike unified models that suffer from performance trade-offs, our framework preserves the strengths of specialized pre-trained models by connecting them via a lightweight, task-aware bridge.\nAt its core, Task-DiT employs a frozen image generator from PixWizard <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024pixwizard</span>)</cite> to safeguard its high-fidelity synthesis capabilities. To steer this generator, we introduce a query-based conditioning mechanism. Two distinct sets of learnable tokens are processed by a powerful understanding module:</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "its",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The enriched features from these tokens are then translated for the generator by dedicated, lightweight projectors. A Task Projector modulates the DiT&#8217;s denoising process based on the command, while a content projector transforms the &lt;IMG[i]&gt; features into a dense conditioning sequence for cross-attention. For image-guided tasks, a Visual Projector aligns source image features encoded by ViT with the DiT&#8217;s conditioning space.\nThis design creates a versatile and efficient channel for task-aware instruction, enabling high-quality, multi-modal image generation without the catastrophic interference typical of end-to-end fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tasks",
                    "content"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Expert Warmup with Dense Model: We first pre-trained three specialized expert models focusing on mainstream modalities: speech comprehension, speech generation, and visual comprehension. This stage aims to build the model-specific experts for the following stable MoE-based fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "its",
                    "performance",
                    "speech",
                    "understanding",
                    "tasks",
                    "audio",
                    "content",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "performance",
                    "all",
                    "results",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "across",
                    "its",
                    "performance",
                    "unimoe",
                    "models",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "speech",
                    "its",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the pre-training phase of the audio understanding modality, the audio data primarily comprises automatic speech recognition (ASR), audio-caption, and music-caption datasets. Among all pre-training data, ASR datasets constitute the majority, encompassing approximately 15B tokens. In contrast, audio-caption and music-caption datasets are more challenging to acquire than ASR datasets, with a combined total of merely 1B tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "understanding",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "tasks",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "used",
                    "content"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "understanding",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "understanding",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "benchmarks",
                    "music",
                    "speech",
                    "understanding",
                    "tasks",
                    "audio",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "performance",
                    "benchmarks",
                    "models",
                    "understanding",
                    "tasks",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "understanding",
                    "results",
                    "mingliteomni15",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "its",
                    "benchmarks",
                    "models",
                    "understanding",
                    "results",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "its",
                    "performance",
                    "benchmarks",
                    "models",
                    "understanding",
                    "results",
                    "mingliteomni15",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "speech",
                    "accuracy",
                    "understanding",
                    "results",
                    "tasks",
                    "audio",
                    "captioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "its",
                    "performance",
                    "benchmarks",
                    "speech",
                    "understanding",
                    "results",
                    "tasks",
                    "audio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "performance",
                    "benchmarks",
                    "models",
                    "audio",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "its",
                    "performance",
                    "models",
                    "speech",
                    "results",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "unimoe20omni",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Uni-MoE-2.0-Omni proves to be a balanced and effective model for omnimodal TTS, exhibiting particular strength in English synthesis and robust performance across diverse and challenging tasks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "its",
                    "benchmarks",
                    "models",
                    "speech",
                    "accuracy",
                    "results",
                    "tasks",
                    "other",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "performance",
                    "models",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "performance",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "its",
                    "performance",
                    "models",
                    "speech",
                    "other",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "performance",
                    "its",
                    "models",
                    "speech",
                    "tasks",
                    "other",
                    "metric",
                    "making"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "all",
                    "benchmarks",
                    "understanding",
                    "results",
                    "audio",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "models",
                    "tasks",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "models",
                    "tasks",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tasks",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "unimoe20omni",
                    "its",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "understanding",
                    "results",
                    "tasks",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "tasks",
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "comparison",
                    "tasks",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "performance",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "generates",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "follow",
                    "models",
                    "understanding",
                    "tasks",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify that the model&#8217;s enhanced omnimodality understanding capability stems primarily from its audio-text-vision joint coding training on large-scale video data. Future work will focus on scaling this video data and introducing new multimodal positional encoding methods to advance the model&#8217;s comprehension abilities further.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "results",
                    "omnimodal",
                    "models",
                    "unimoe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "speech",
                    "understanding",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "across",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "across",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "tasks",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "understanding",
                    "audio",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "models",
                    "across",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "across",
                    "models",
                    "speech",
                    "understanding",
                    "audio",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio X &#8594; Text</span>\nRACE-audio(middle/high) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">race</span>)</cite>, EHSL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite>, MELD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">meld</span>)</cite>, MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmau</span>)</cite>, ClothoAQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clothoaqa</span>)</cite>, ClothoV1 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clotho</span>)</cite>, ClothoV2 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clotho</span>)</cite>, AudioCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audiocaps</span>)</cite>, and MusicCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lpmusic</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "clothov2",
                    "clothoaqa",
                    "audiocaps",
                    "clothov1",
                    "musiccaps",
                    "audio",
                    "meld",
                    "mmau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech &#8594; Speech/Text</span>\nLlamaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llamaq</span>)</cite>, WebQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">webq</span>)</cite>, BigBench Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bigbench</span>)</cite>, and MultiChallenge Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">multichallenge</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            }
        ]
    },
    "S4.T7": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 7: \nComparison of Uni-MoE-2.0-Omni and variants with other omnimodal models across ASR benchmarks.\nThe ASR results presented in this table are evaluated using the word error rate (WER).\nThe LibriSpeech-clean/other-long datasets contain speech samples longer than 3 minutes.",
        "body": "Benchmark\nUni-MoE-2.0\nUni-MoE-2.0-Omni\nQwen2.5-Omni\nMing-Lite-Omni\nMing-Lite-Omni-1.5\nQwen2-Audio\n\n\nASR-EN \\mathbf{\\downarrow}\n\n\nLibriSpeech-clean\n1.73\n1.66\n3.57\n5.36\n1.34\n1.60\n\n\nLibriSpeech-other\n3.26\n3.42\n7.03\n9.89\n2.79\n3.60\n\n\nfleurs-en\n7.78\n7.72\n9.74\n10.16\n8.07\n6.90\n\n\nmls-en\n5.46\n5.39\n6.85\n9.66\n4.04\n5.40\n\n\nCV15-en\n3.63\n4.13\n12.25\n13.75\n7.04\n8.60\n\n\nvoxpopuli\n10.35\n9.43\n9.6\n10.01\n7.13\n6.84\n\n\nLibriSpeech-clean-long\n3.55\n2.04\n7.73\n43.82\n61.86\n11.2\n\n\nLibriSpeech-other-long\n6.12\n4.2\n7.98\n32.2\n61.49\n10.3\n\n\nAvg.\n5.24\n4.75\n8.09\n16.85\n19.22\n6.81\n\n\nASR-ZH\\mathbf{\\downarrow}\n\n\nAishell1\n3.69\n3.23\n2.63\n7.83\n1.33\n1.53\n\n\nAishell2-test-ios\n4.84\n4.94\n23.74\n8.05\n2.45\n2.92\n\n\nAishell2-test-android\n4.84\n4.84\n25.14\n6.19\n2.46\n2.92\n\n\nFleurs-zh\n11.27\n9.58\n8.87\n9.73\n8.36\n7.50\n\n\nCV15-zh\n3.37\n2.97\n11.01\n19.08\n5.96\n6.90\n\n\nAvg.\n5.60\n5.11\n14.28\n10.18\n4.12\n4.35",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Benchmark</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Ming-Lite-Omni<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Ming-Lite-Omni-1.5<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen2-Audio</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"7\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">ASR-EN<span class=\"ltx_text ltx_font_medium\" style=\"--ltx-bg-color:#D9D9D9;\"> <math alttext=\"\\mathbf{\\downarrow}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m4\" intent=\":literal\" style=\"--ltx-bg-color:#D9D9D9;\"><semantics><mo mathbackground=\"#D9D9D9\" stretchy=\"false\" style=\"--ltx-bg-color:#D9D9D9;\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\mathbf{\\downarrow}</annotation></semantics></math></span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">LibriSpeech-clean</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">1.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">1.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">3.57</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">5.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">1.34</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">1.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">LibriSpeech-other</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">3.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">3.42</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">7.03</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">9.89</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">2.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">3.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">fleurs-en</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">7.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">7.72</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">9.74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">10.16</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">8.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">6.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">mls-en</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">5.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">5.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">6.85</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">9.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">4.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">5.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">CV15-en</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">3.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">4.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">12.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">13.75</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">7.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">8.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">voxpopuli</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">10.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">9.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">9.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">10.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">7.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">6.84</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">LibriSpeech-clean-long</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">3.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">2.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">7.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">43.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">61.86</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">11.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">LibriSpeech-other-long</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">6.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">4.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">7.98</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">32.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">61.49</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">10.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">Avg.</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">4.75</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">8.09</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">16.85</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">19.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">6.81</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"7\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">ASR-ZH<math alttext=\"\\mathbf{\\downarrow}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m5\" intent=\":literal\" style=\"--ltx-bg-color:#D9D9D9;\"><semantics><mo mathbackground=\"#D9D9D9\" stretchy=\"false\" style=\"--ltx-bg-color:#D9D9D9;\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\mathbf{\\downarrow}</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">Aishell1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">3.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">3.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">2.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">7.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">1.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">1.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">Aishell2-test-ios</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">4.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">4.94</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">23.74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">8.05</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">2.45</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">2.92</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">Aishell2-test-android</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">4.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">4.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">25.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">6.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">2.46</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">2.92</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">Fleurs-zh</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">11.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">9.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">8.87</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">9.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">8.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">7.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">CV15-zh</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">3.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">2.97</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">11.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">19.08</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">5.96</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2pt;padding-bottom:2pt;\">6.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">Avg.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2pt;padding-bottom:2pt;\">5.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-top:2pt;padding-bottom:2pt;\">5.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2pt;padding-bottom:2pt;\">14.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2pt;padding-bottom:2pt;\">10.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_font_bold\">4.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2pt;padding-bottom:2pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.35</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "unimoe20omni",
            "minutes",
            "asren",
            "wer",
            "presented",
            "speech",
            "mathbfdownarrow",
            "datasets",
            "aishell2testandroid",
            "benchmark",
            "qwen2audio",
            "longer",
            "avg",
            "than",
            "samples",
            "error",
            "rate",
            "asr",
            "librispeechclean",
            "results",
            "librispeechotherlong",
            "contain",
            "librispeechcleanlong",
            "mingliteomni15",
            "cv15zh",
            "qwen25omni",
            "fleurszh",
            "omnimodal",
            "librispeechother",
            "mlsen",
            "evaluated",
            "comparison",
            "aishell2testios",
            "librispeechcleanotherlong",
            "fleursen",
            "asrzhmathbfdownarrow",
            "variants",
            "voxpopuli",
            "unimoe20",
            "cv15en",
            "word",
            "across",
            "mingliteomni",
            "aishell1",
            "benchmarks",
            "models",
            "other"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "wer",
                    "benchmarks",
                    "speech",
                    "avg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Deep Cross-Modal Alignment</span>: To enable deep and efficient fusion of any modality, we introduce an Omni-Modality 3D RoPE mechanism in the self-attention layers. It encodes the temporal-spatial dimensions of speech, image, text, and video tokens, ensuring seamless alignment and interaction across all input types.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "wer",
                    "benchmarks",
                    "models",
                    "speech",
                    "comparison",
                    "results",
                    "librispeechcleanotherlong",
                    "other",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To process audio longer than Whisper-Large-v3&#8217;s 30-second limit, we employ a chunking mechanism. Long audio is segmented into consecutive 30-second clips, which are batched and processed by the audio understanding module. <span class=\"ltx_text ltx_font_italic\">The resulting features are then concatenated along the time dimensions, enabling the understanding of audio of arbitrary length</span>.</p>\n\n",
                "matched_terms": [
                    "longer",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "minutes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Expert Warmup with Dense Model: We first pre-trained three specialized expert models focusing on mainstream modalities: speech comprehension, speech generation, and visual comprehension. This stage aims to build the model-specific experts for the following stable MoE-based fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "results",
                    "omnimodal",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "models",
                    "across",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the pre-training phase of the audio understanding modality, the audio data primarily comprises automatic speech recognition (ASR), audio-caption, and music-caption datasets. Among all pre-training data, ASR datasets constitute the majority, encompassing approximately 15B tokens. In contrast, audio-caption and music-caption datasets are more challenging to acquire than ASR datasets, with a combined total of merely 1B tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr",
                    "datasets",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the vision modality pre-training stage, the visual data primarily comprised image&#8211;caption and video&#8211;caption datasets. The image&#8211;caption corpus contained approximately 17 million samples (13 billion tokens), whereas the video&#8211;caption data represented a smaller portion, with around 0.1 million samples (0.2 billion tokens).</p>\n\n",
                "matched_terms": [
                    "samples",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "samples",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "omnimodal",
                    "speech",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "datasets",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "presented",
                    "results",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "results",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "unimoe20omni",
                    "benchmarks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "word",
                    "librispeechother",
                    "unimoe20omni",
                    "minutes",
                    "across",
                    "error",
                    "wer",
                    "mlsen",
                    "rate",
                    "speech",
                    "asr",
                    "librispeechclean",
                    "aishell1",
                    "benchmarks",
                    "results",
                    "librispeechotherlong",
                    "librispeechcleanlong"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "speech",
                    "datasets",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "word",
                    "omnimodal",
                    "error",
                    "wer",
                    "benchmarks",
                    "rate",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "wer",
                    "models",
                    "speech",
                    "results",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Uni-MoE-2.0-Omni proves to be a balanced and effective model for omnimodal TTS, exhibiting particular strength in English synthesis and robust performance across diverse and challenging tasks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "speech",
                    "results",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "models",
                    "speech",
                    "other",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "models",
                    "speech",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "than",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "than",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "results",
                    "omnimodal",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "across",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "across",
                    "unimoe20"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "models",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "results",
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "across"
                ]
            }
        ]
    },
    "S4.T8": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 8: \nComparison of Uni-MoE-2.0-Omni and variants with other omnimodal models across TTS benchmarks.\nFor English TTS audio results, they are transcribed into text using Whisper. For Chinese TTS audio results, Paraformer is utilized to obtain the transcriptions. Subsequently, these transcriptions are compared with the original text using the WER metric.\nBold indicates the highest score, and underline indicates the second-highest score for each benchmark.",
        "body": "Model\nLibriTTS-clean\nLibriTTS-other\nSEED-en\nSEED-zh\nSEED-hard\nTinyStories-en\nTinyStories-zh\n\n\nFireRedTTS\n-\n-\n1.51\n3.82\n17.45\n-\n-\n\n\nMaskGCT\n-\n-\n2.27\n2.62\n10.27\n-\n-\n\n\nE2 TTS\n-\n-\n1.97\n2.19\n-\n-\n-\n\n\nF5-TTS\n-\n-\n1.56\n1.83\n8.67\n-\n-\n\n\nLlasa\n-\n-\n1.59\n2.97\n11.09\n-\n-\n\n\nCosyVoice\n3.17\n-\n3.39\n3.10\n11.75\n-\n-\n\n\nCosyVoice 2\n-\n-\n1.45\n2.57\n6.83\n-\n-\n\n\nGLM-4-Voice\n5.64\n-\n2.91\n2.10\n-\n-\n-\n\n\nQwen2.5-Omni-7B\n\n5.20\n6.68\n1.73\n1.68\n2.15\n6.20\n8.51\n\n\nMing-Lite-Omni\n\n11.15\n11.33\n2.92\n2.68\n5.52\n15.07\n4.74\n\n\nDense-TTS\n6.51\n6.84\n3.03\n3.41\n3.1\n-\n-\n\n\nUni-MoE-2.0-Omni\n5.85\n7.13\n2.72\n3.10\n2.67\n5.02\n7.02",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LibriTTS-clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LibriTTS-other</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">SEED-en</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">SEED-zh</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">SEED-hard</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">TinyStories-en</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">TinyStories-zh</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">FireRedTTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">17.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MaskGCT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.62</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">10.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">E2 TTS</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.97</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">F5-TTS</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Llasa</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.97</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">11.09</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">CosyVoice</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.17</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">11.75</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">CosyVoice 2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.45</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.57</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">GLM-4-Voice</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.91</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Qwen2.5-Omni-7B<sup class=\"ltx_sup\">&#8727;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">5.20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">6.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">6.20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8.51</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Ming-Lite-Omni<sup class=\"ltx_sup\">&#8727;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">11.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">11.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5.52</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">15.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.74</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Dense-TTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">6.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Uni-MoE-2.0-Omni</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">7.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">5.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">7.02</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "original",
            "transcribed",
            "unimoe20omni",
            "llasa",
            "wer",
            "librittsclean",
            "tinystorieszh",
            "into",
            "chinese",
            "benchmark",
            "metric",
            "underline",
            "cosyvoice",
            "text",
            "secondhighest",
            "utilized",
            "seedhard",
            "f5tts",
            "results",
            "compared",
            "each",
            "audio",
            "bold",
            "indicates",
            "maskgct",
            "glm4voice",
            "omnimodal",
            "they",
            "fireredtts",
            "score",
            "qwen25omni7b",
            "paraformer",
            "comparison",
            "tinystoriesen",
            "transcriptions",
            "variants",
            "seeden",
            "highest",
            "english",
            "across",
            "whisper",
            "mingliteomni",
            "benchmarks",
            "models",
            "tts",
            "seedzh",
            "subsequently",
            "other",
            "obtain",
            "model",
            "librittsother",
            "densetts"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "text",
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "wer",
                    "benchmarks",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "omnimodal",
                    "across",
                    "models",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "into",
                    "omnimodal",
                    "model",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Unified Modality Encoding &amp; Generation</span>: We design a unified speech encoder that maps diverse audio inputs, including environmental sound, speech, and music, into a shared representation space. For output, a Context-Aware MoE-based TTS module supports dynamic speech synthesis (especially for long speech) and interaction. On the visual side, we employ pre-trained visual encoders to process images and videos, and build a Task-Aware Diffusion Transformer for instruction-guided image generation and editing.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "into",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Deep Cross-Modal Alignment</span>: To enable deep and efficient fusion of any modality, we introduce an Omni-Modality 3D RoPE mechanism in the self-attention layers. It encodes the temporal-spatial dimensions of speech, image, text, and video tokens, ensuring seamless alignment and interaction across all input types.</p>\n\n",
                "matched_terms": [
                    "text",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "into",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "wer",
                    "benchmarks",
                    "models",
                    "comparison",
                    "tinystoriesen",
                    "results",
                    "other",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "text",
                    "into",
                    "each",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Mapping Network</span>. The mapping network is composed of a two-layer MLP and a 2D average pooling layer. Specifically, the MLP takes the visual features as input and projects them into the representation space of the language model, thereby producing a one-dimensional sequence of visual features <math alttext=\"H_{V}=p(Z_{V})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>V</mi></msub><mo>=</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>V</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">H_{V}=p(Z_{V})</annotation></semantics></math>. Subsequently, the average pooling layer performs length compression along both spatial dimensions, enabling more efficient training.</p>\n\n",
                "matched_terms": [
                    "subsequently",
                    "model",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through these visual understanding modules, we can uniformly encode Single-Image, Multi-Image, and Video inputs, thereby transforming different visual input paradigms into a unified one-dimensional sequence of visual representations. The specific encoding strategies for each paradigm are described as follows:</p>\n\n",
                "matched_terms": [
                    "each",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "original",
                    "into",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Multi-Image</span>. The encoding of multiple images follows a similar procedure to that of a single image. For each image, we independently search the most suitable target resolution, resize it accordingly, and then convert it into the corresponding set of visual patches. Assuming there are <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> images are provided as input, the total number of visual tokens is <math alttext=\"\\sum^{n}_{i=1}((a_{i}\\times b_{i})\\times T)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo lspace=\"0em\" stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>a</mi><mi>i</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>b</mi><mi>i</mi></msub></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sum^{n}_{i=1}((a_{i}\\times b_{i})\\times T)</annotation></semantics></math>, where <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math> and <math alttext=\"b_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m4\" intent=\":literal\"><semantics><msub><mi>b</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">b_{i}</annotation></semantics></math> denote the number of patches along the height and width of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th image.</p>\n\n",
                "matched_terms": [
                    "each",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "they",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "subsequently",
                    "audio",
                    "into",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{W}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>W</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{W}^{A}</annotation></semantics></math> is the last hidden states of the pre-trained audio encoder adopted from Whisper-large-v3. <span class=\"ltx_text ltx_markedasmath\">MSA</span> and <math alttext=\"h\\text{MCA}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>MCA</mtext></mrow><annotation encoding=\"application/x-tex\">h\\text{MCA}</annotation></semantics></math> denote the multihead self-attention and cross-attention operations, and <span class=\"ltx_text ltx_markedasmath\">LN</span> is the layer norm function. By leveraging the fixed-length query vectors <math alttext=\"X_{Q}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>Q</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">X_{Q}^{A}</annotation></semantics></math>, each 30 seconds of audio encoded by the Whisper encoder is mapped to <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> audio tokens. <math alttext=\"h_{C}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>C</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{C}^{A}</annotation></semantics></math> represents the output of the cross-attention module, which is used to distill the main content of the input audio. After going through all layers of the Whisper decoder, we apply a learnable linear layer for projecting the last output into the representation space of LLM.</p>\n\n",
                "matched_terms": [
                    "each",
                    "audio",
                    "into",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To process audio longer than Whisper-Large-v3&#8217;s 30-second limit, we employ a chunking mechanism. Long audio is segmented into consecutive 30-second clips, which are batched and processed by the audio understanding module. <span class=\"ltx_text ltx_font_italic\">The resulting features are then concatenated along the time dimensions, enabling the understanding of audio of arbitrary length</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "text",
                    "original",
                    "across",
                    "into",
                    "each",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As an illustrative example, consider a video input of length 120 seconds accompanied by audio. We place the visual sequence before the audio sequence and sample the video at a rate of one frame every two seconds. Suppose the RoPE ID of the last text token preceding the visual sequence is <math alttext=\"(x-1,x-1,x-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x-1,x-1,x-1)</annotation></semantics></math>. Then, the RoPE IDs of the first video frame start from <math alttext=\"(x,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x,x)</annotation></semantics></math> and increment row by row and column by column up to <math alttext=\"(x,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> denotes the number of tokens along both the height and width of a video frame. The second frame begins with <math alttext=\"(x+2\\theta,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x,x)</annotation></semantics></math> and increases to <math alttext=\"(x+2\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> is a specific scaling factor for absolute time. Following this procedure, the final frame in the video sequence has an ID of <math alttext=\"(x+118\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>118</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+118\\theta,x+p,x+p)</annotation></semantics></math>.\nSimilarly, assume the RoPE ID of the last token before the audio sequence is <math alttext=\"(y-1,y-1,y-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y-1,y-1,y-1)</annotation></semantics></math>. The first audio segment is then assigned the ID <math alttext=\"(y,y,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y,y,y)</annotation></semantics></math>, which is repeated 20 times to represent the minimum audio unit. The next audio segment receives the ID <math alttext=\"(y+3\\theta,y+3\\theta,y+3\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+3\\theta,y+3\\theta,y+3\\theta)</annotation></semantics></math>. Continuing this process, the final audio segment is assigned the ID <math alttext=\"(y+117\\theta,y+117\\theta,y+117\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+117\\theta,y+117\\theta,y+117\\theta)</annotation></semantics></math>. In this manner, temporal alignment between the visual and audio sequences can also be achieved.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "tts",
                    "subsequently",
                    "into",
                    "each",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the performance of MoE-TTS in voice timbre control, we adopt a text-context prompt-guided approach to direct the model to synthesize audio with the specified language and timbre. When the model receives an instruction to generate an audio response, the Uni-MoE base model first produces a special token (i.e. <span class=\"ltx_text ltx_font_italic\">&lt;speech start&gt;</span>) indicating the start of speech, . Subsequently, the model generates commands that specify the voice timbre for audio synthesis: fixed timbres include those of Brain, Jenny, and Xiaoxiao, while supported languages cover Chinese and English. Beyond fixed options, the model also enables users to describe desired timbres via natural language; the MoE-TTS module then generates audio with the corresponding timbre based on these descriptive instructions. Upon completion of the timbre prompt, a special token &lt;speech prompt&gt; is generated to signal the end of the prompt. This is followed by the text content that the model needs to transcribe into speech, which is terminated with the special token &lt;speech end&gt;. Through this combination of text-based timbre prompts and target text content, MoE-TTS is guided to perform diverse timbre synthesis.</p>\n\n",
                "matched_terms": [
                    "text",
                    "english",
                    "subsequently",
                    "into",
                    "chinese",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The enriched features from these tokens are then translated for the generator by dedicated, lightweight projectors. A Task Projector modulates the DiT&#8217;s denoising process based on the command, while a content projector transforms the &lt;IMG[i]&gt; features into a dense conditioning sequence for cross-attention. For image-guided tasks, a Visual Projector aligns source image features encoded by ViT with the DiT&#8217;s conditioning space.\nThis design creates a versatile and efficient channel for task-aware instruction, enabling high-quality, multi-modal image generation without the catastrophic interference typical of end-to-end fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the leftmost panel of the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S3.F5\" title=\"Figure 5 &#8227; Supervised Fine-tuning &#8227; 3.1 Training Recipe: From LLMs to OLMs &#8227; 3 Training and Data Recipes &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the goal of pre-training is to enable the LLM to comprehend multimodal data, such as images, videos, and speech. This is achieved by mapping the representations of these modalities into the LLM&#8217;s linguistic space, using paired data of multimodal inputs and their corresponding text description.</p>\n\n",
                "matched_terms": [
                    "text",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Expert Warmup with Dense Model: We first pre-trained three specialized expert models focusing on mainstream modalities: speech comprehension, speech generation, and visual comprehension. This stage aims to build the model-specific experts for the following stable MoE-based fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "into",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "tts",
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "into",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "english",
                    "into",
                    "chinese",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "utilized",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "text",
                    "omnimodal",
                    "unimoe20omni",
                    "benchmarks",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "compared",
                    "other",
                    "highest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "results",
                    "benchmark",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "results",
                    "compared",
                    "other",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "text",
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "english",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "other",
                    "chinese",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "english",
                    "unimoe20omni",
                    "across",
                    "wer",
                    "benchmarks",
                    "results",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "results",
                    "compared",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "english",
                    "unimoe20omni",
                    "wer",
                    "models",
                    "librittsclean",
                    "seedhard",
                    "benchmark",
                    "results",
                    "chinese",
                    "densetts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "english",
                    "unimoe20omni",
                    "compared",
                    "chinese",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Uni-MoE-2.0-Omni proves to be a balanced and effective model for omnimodal TTS, exhibiting particular strength in English synthesis and robust performance across diverse and challenging tasks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "english",
                    "unimoe20omni",
                    "across",
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "results",
                    "compared",
                    "other",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "score",
                    "models",
                    "model",
                    "other",
                    "benchmark",
                    "highest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "text",
                    "unimoe20omni",
                    "across",
                    "models",
                    "other",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "score",
                    "across",
                    "benchmarks",
                    "results",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "models",
                    "they",
                    "unimoe20omni",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "original",
                    "unimoe20omni",
                    "score",
                    "models",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "original",
                    "unimoe20omni",
                    "score",
                    "models",
                    "benchmark",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "score",
                    "compared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "model",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "results",
                    "audio",
                    "across",
                    "omnimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "model",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "original",
                    "models",
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "original",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "model",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models",
                    "tts",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "model",
                    "compared"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "results",
                    "omnimodal",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "text",
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "into",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "audio",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "across",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio X &#8594; Text</span>\nRACE-audio(middle/high) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">race</span>)</cite>, EHSL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite>, MELD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">meld</span>)</cite>, MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmau</span>)</cite>, ClothoAQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clothoaqa</span>)</cite>, ClothoV1 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clotho</span>)</cite>, ClothoV2 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clotho</span>)</cite>, AudioCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audiocaps</span>)</cite>, and MusicCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lpmusic</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio"
                ]
            }
        ]
    },
    "S4.T9": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 9: \nComparison of Uni-MoE-2.0-Omni and variants with other omnimodal models across Speech QA benchmarks.\nWe evaluate the models performance by employing existence matching to assess the ACC of the answers. Some results (models w/o ) are from the MOSS speech and GLM-4-voice, where a lower ts\\Delta_{t-s} value indicates better performance.",
        "body": "Model\nLlamaQA\nWebQA\nBigBench Audio\nMultiChallenge Audio\n\n\nsts\\rightarrow t\nsss\\rightarrow s\nts\\Delta_{t-s}\nsts\\rightarrow t\nsss\\rightarrow s\nts\\Delta_{t-s}\nsts\\rightarrow t\nsss\\rightarrow s\nts\\Delta_{t-s}\nsts\\rightarrow t\nsss\\rightarrow s\nts\\Delta_{t-s}\n\n\nSpectron-1B\n21.9\n-\n-\n6.1\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nSpeechGPT-7B\n21.6\n-\n-\n6.5\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nFreeze-Omni-7B\n72\n-\n-\n44.73\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nMoshi-7B\n62.3\n21.0\n41.3\n26.6\n9.2\n17.4\n-\n-\n-\n-\n-\n-\n\n\nLLaMA-Omni-7B\n67.7\n49.0\n18.7\n33.4\n23.7\n9.7\n-\n-\n-\n-\n-\n-\n\n\nLLaMA-Omni2-7B\n70.3\n60.7\n9.6\n34.5\n31.3\n3.2\n-\n-\n-\n-\n-\n-\n\n\nVITA-1.5-7B\n76.7\n-\n-\n42.7\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nStream-Omni-8B\n76.3\n65.0\n11.3\n44.2\n27.5\n16.7\n-\n-\n-\n-\n-\n-\n\n\nOpenOmni-7B\n74.6\n67.2\n7.4\n44.5\n28.9\n15.6\n-\n-\n-\n-\n-\n-\n\n\nNExT-Omni-7B\n78.4\n66.4\n12\n45.6\n28.3\n17.3\n-\n-\n-\n-\n-\n-\n\n\nStep-Audio2-mini-7B\n-\n-\n-\n-\n-\n-\n50.90\n47.50\n3.40\n13.64\n8.08\n5.56\n\n\nKimi-Audio-7B\n-\n-\n-\n-\n-\n-\n59.40\n51.00\n8.40\n7.07\n1.01\n6.06\n\n\nGLM-4-Voice-8B\n74.33\n65.67\n8.66\n45.90\n43.20\n2.70\n44.80\n42.70\n2.10\n9.09\n6.06\n3.03\n\n\nQwen2.5-Omni-7B\n\n77.33\n77.33\n0.00\n48.28\n48.28\n0.00\n58.1\n53.8\n4.3\n13.13\n10.61\n2.52\n\n\nMing-Lite-Omni\n\n80.33\n63.66\n16.67\n53.79\n44.19\n9.60\n53.3\n26.6\n26.7\n27.27\n23.23\n4.04\n\n\nUni-MoE-2.0-Omni\n75.33\n75.33\n0.00\n45.13\n43.95\n1.18\n49.2\n44.7\n4.5\n10.61\n9.6\n1.01",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LlamaQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">WebQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">BigBench Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MultiChallenge Audio</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s\\rightarrow t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo stretchy=\"false\">&#8594;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">s\\rightarrow t</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s\\rightarrow s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m2\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo stretchy=\"false\">&#8594;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s\\rightarrow s</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"\\Delta_{t-s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m3\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#916;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\Delta_{t-s}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s\\rightarrow t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m4\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo stretchy=\"false\">&#8594;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">s\\rightarrow t</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s\\rightarrow s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m5\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo stretchy=\"false\">&#8594;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s\\rightarrow s</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"\\Delta_{t-s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m6\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#916;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\Delta_{t-s}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s\\rightarrow t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m7\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo stretchy=\"false\">&#8594;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">s\\rightarrow t</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s\\rightarrow s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m8\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo stretchy=\"false\">&#8594;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s\\rightarrow s</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"\\Delta_{t-s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m9\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#916;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\Delta_{t-s}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s\\rightarrow t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m10\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo stretchy=\"false\">&#8594;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">s\\rightarrow t</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s\\rightarrow s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m11\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo stretchy=\"false\">&#8594;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s\\rightarrow s</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"\\Delta_{t-s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m12\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#916;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\Delta_{t-s}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Spectron-1B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">21.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">SpeechGPT-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">21.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Freeze-Omni-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">72</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">44.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Moshi-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">62.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">21.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">41.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">26.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">17.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LLaMA-Omni-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">67.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">49.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">18.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">33.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">23.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LLaMA-Omni2-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">70.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">60.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">34.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">31.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">VITA-1.5-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">76.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">42.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Stream-Omni-8B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">76.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">65.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">11.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">44.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">27.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">16.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">OpenOmni-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">74.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">67.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">7.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">44.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">28.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">15.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">NExT-Omni-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">78.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">66.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">45.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">28.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">17.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Step-Audio2-mini-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">50.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">47.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">13.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8.08</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">5.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Kimi-Audio-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">59.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">51.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">7.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6.06</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">GLM-4-Voice-8B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">74.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">65.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">8.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">45.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">43.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">44.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">42.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.10</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9.09</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">6.06</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Qwen2.5-Omni-7B<sup class=\"ltx_sup\">&#8727;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">77.33</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">77.33</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">48.28</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">48.28</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">58.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">53.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">13.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">10.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.52</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Ming-Lite-Omni<sup class=\"ltx_sup\">&#8727;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">80.33</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">63.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">16.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">53.79</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">44.19</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">53.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">26.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">26.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">27.27</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">23.23</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Uni-MoE-2.0-Omni</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">75.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">75.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">45.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">43.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">49.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">44.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">10.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">9.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.01</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "unimoe20omni",
            "kimiaudio7b",
            "speechgpt7b",
            "multichallenge",
            "speech",
            "sssrightarrow",
            "lower",
            "tsdeltats",
            "nextomni7b",
            "models",
            "where",
            "vita157b",
            "assess",
            "value",
            "existence",
            "streamomni8b",
            "results",
            "freezeomni7b",
            "audio",
            "indicates",
            "evaluate",
            "glm4voice",
            "omnimodal",
            "openomni7b",
            "matching",
            "spectron1b",
            "performance",
            "stsrightarrow",
            "llamaomni7b",
            "qwen25omni7b",
            "some",
            "llamaqa",
            "comparison",
            "acc",
            "moss",
            "variants",
            "from",
            "answers",
            "llamaomni27b",
            "across",
            "webqa",
            "glm4voice8b",
            "mingliteomni",
            "benchmarks",
            "models",
            "moshi7b",
            "better",
            "bigbench",
            "other",
            "employing",
            "model",
            "stepaudio2mini7b"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "matching",
                    "performance",
                    "across",
                    "benchmarks",
                    "speech",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "models",
                    "from",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "from",
                    "where",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model",
                    "unimoe20omni",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Unified Modality Encoding &amp; Generation</span>: We design a unified speech encoder that maps diverse audio inputs, including environmental sound, speech, and music, into a shared representation space. For output, a Context-Aware MoE-based TTS module supports dynamic speech synthesis (especially for long speech) and interaction. On the visual side, we employ pre-trained visual encoders to process images and videos, and build a Task-Aware Diffusion Transformer for instruction-guided image generation and editing.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Deep Cross-Modal Alignment</span>: To enable deep and efficient fusion of any modality, we introduce an Omni-Modality 3D RoPE mechanism in the self-attention layers. It encodes the temporal-spatial dimensions of speech, image, text, and video tokens, ensuring seamless alignment and interaction across all input types.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "models",
                    "speech",
                    "comparison",
                    "results",
                    "lower",
                    "other",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "where",
                    "models",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "where",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This unified encoding of Single-Image, Multi-Image, and Video inputs into a one-dimensional sequence of visual representations enables the transfer of the model&#8217;s capability in high-resolution image understanding to the video domain, thereby facilitating faster convergence and improved performance in video understanding.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{W}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>W</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{W}^{A}</annotation></semantics></math> is the last hidden states of the pre-trained audio encoder adopted from Whisper-large-v3. <span class=\"ltx_text ltx_markedasmath\">MSA</span> and <math alttext=\"h\\text{MCA}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>MCA</mtext></mrow><annotation encoding=\"application/x-tex\">h\\text{MCA}</annotation></semantics></math> denote the multihead self-attention and cross-attention operations, and <span class=\"ltx_text ltx_markedasmath\">LN</span> is the layer norm function. By leveraging the fixed-length query vectors <math alttext=\"X_{Q}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>Q</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">X_{Q}^{A}</annotation></semantics></math>, each 30 seconds of audio encoded by the Whisper encoder is mapped to <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> audio tokens. <math alttext=\"h_{C}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>C</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{C}^{A}</annotation></semantics></math> represents the output of the cross-attention module, which is used to distill the main content of the input audio. After going through all layers of the Whisper decoder, we apply a learnable linear layer for projecting the last output into the representation space of LLM.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "where",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As an illustrative example, consider a video input of length 120 seconds accompanied by audio. We place the visual sequence before the audio sequence and sample the video at a rate of one frame every two seconds. Suppose the RoPE ID of the last text token preceding the visual sequence is <math alttext=\"(x-1,x-1,x-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x-1,x-1,x-1)</annotation></semantics></math>. Then, the RoPE IDs of the first video frame start from <math alttext=\"(x,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x,x)</annotation></semantics></math> and increment row by row and column by column up to <math alttext=\"(x,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> denotes the number of tokens along both the height and width of a video frame. The second frame begins with <math alttext=\"(x+2\\theta,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x,x)</annotation></semantics></math> and increases to <math alttext=\"(x+2\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> is a specific scaling factor for absolute time. Following this procedure, the final frame in the video sequence has an ID of <math alttext=\"(x+118\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>118</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+118\\theta,x+p,x+p)</annotation></semantics></math>.\nSimilarly, assume the RoPE ID of the last token before the audio sequence is <math alttext=\"(y-1,y-1,y-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y-1,y-1,y-1)</annotation></semantics></math>. The first audio segment is then assigned the ID <math alttext=\"(y,y,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y,y,y)</annotation></semantics></math>, which is repeated 20 times to represent the minimum audio unit. The next audio segment receives the ID <math alttext=\"(y+3\\theta,y+3\\theta,y+3\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+3\\theta,y+3\\theta,y+3\\theta)</annotation></semantics></math>. Continuing this process, the final audio segment is assigned the ID <math alttext=\"(y+117\\theta,y+117\\theta,y+117\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+117\\theta,y+117\\theta,y+117\\theta)</annotation></semantics></math>. In this manner, temporal alignment between the visual and audio sequences can also be achieved.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "where",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These limitations prevent MoE from fully exploiting its adaptive activation capability, ultimately resulting in suboptimal performance.\nTo address these challenges, we propose the Dynamic-Capacity MoE architecture, which incorporates (i) routing gradient estimation to enable differentiable expert selection, (ii) explicit expert role specialization to separately model domain-specific and general knowledge, as well as facilitate knowledge deletion, and (iii) dynamic expert number allocation to adaptively control the amount of parametric knowledge.</p>\n\n",
                "matched_terms": [
                    "from",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central difficulty in training vanilla MoE models arises from the non-differentiable Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> operation used in expert selection, which obstructs gradient propagation to the router and leads to biased optimization. To address this, we migrate the gradient estimation strategy proposed in Grin-MoE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024gringradientinformedmoe</span>)</cite>, which integrates straight-through gradient estimators under the framework of numerical methods for ordinary differential equations (ODEs). This enables end-to-end optimization of both router and experts under sparse activation constraints, improving the stability of router training and allowing more precise token-to-expert assignments. A formal description is given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.SS4\" title=\"A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Null Experts</span>: These are &#8220;empty&#8221; experts whose output is identically zero. They serve as a mechanism for selective forgetting, effectively removing outdated or irrelevant knowledge from the model&#8217;s output. Null experts are also dynamically activated via the dynamic capacity routing strategy.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the performance of MoE-TTS in voice timbre control, we adopt a text-context prompt-guided approach to direct the model to synthesize audio with the specified language and timbre. When the model receives an instruction to generate an audio response, the Uni-MoE base model first produces a special token (i.e. <span class=\"ltx_text ltx_font_italic\">&lt;speech start&gt;</span>) indicating the start of speech, . Subsequently, the model generates commands that specify the voice timbre for audio synthesis: fixed timbres include those of Brain, Jenny, and Xiaoxiao, while supported languages cover Chinese and English. Beyond fixed options, the model also enables users to describe desired timbres via natural language; the MoE-TTS module then generates audio with the corresponding timbre based on these descriptive instructions. Upon completion of the timbre prompt, a special token &lt;speech prompt&gt; is generated to signal the end of the prompt. This is followed by the text content that the model needs to transcribe into speech, which is terminated with the special token &lt;speech end&gt;. Through this combination of text-based timbre prompts and target text content, MoE-TTS is guided to perform diverse timbre synthesis.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a Task-Aware Diffusion Transformer (Task-DiT) that bridges the gap between image understanding and generation. Unlike unified models that suffer from performance trade-offs, our framework preserves the strengths of specialized pre-trained models by connecting them via a lightweight, task-aware bridge.\nAt its core, Task-DiT employs a frozen image generator from PixWizard <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024pixwizard</span>)</cite> to safeguard its high-fidelity synthesis capabilities. To steer this generator, we introduce a query-based conditioning mechanism. Two distinct sets of learnable tokens are processed by a powerful understanding module:</p>\n\n",
                "matched_terms": [
                    "from",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Image Tokens (&lt;IMG[i]&gt;): Capture the rich semantic essence of the desired output from Uni-MoE-2.0-Omni, forming a compressed scene representation.</p>\n\n",
                "matched_terms": [
                    "from",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The enriched features from these tokens are then translated for the generator by dedicated, lightweight projectors. A Task Projector modulates the DiT&#8217;s denoising process based on the command, while a content projector transforms the &lt;IMG[i]&gt; features into a dense conditioning sequence for cross-attention. For image-guided tasks, a Visual Projector aligns source image features encoded by ViT with the DiT&#8217;s conditioning space.\nThis design creates a versatile and efficient channel for task-aware instruction, enabling high-quality, multi-modal image generation without the catastrophic interference typical of end-to-end fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "from",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Expert Warmup with Dense Model: We first pre-trained three specialized expert models focusing on mainstream modalities: speech comprehension, speech generation, and visual comprehension. This stage aims to build the model-specific experts for the following stable MoE-based fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "from",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "performance",
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "models",
                    "from",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "speech",
                    "from",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the pre-training phase of the audio understanding modality, the audio data primarily comprises automatic speech recognition (ASR), audio-caption, and music-caption datasets. Among all pre-training data, ASR datasets constitute the majority, encompassing approximately 15B tokens. In contrast, audio-caption and music-caption datasets are more challenging to acquire than ASR datasets, with a combined total of merely 1B tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "models",
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "where",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated and filtered a large-scale dataset from multiple open-source dataset collections, amounting to approximately 10.5M samples (8.1B tokens). This stage aimed to unify the model&#8217;s capabilities under a consistent instruction-following framework. The training data was formatted as instruction-response pairs covering the same core task categories mentioned previously: image generation, image editing, low-level CV enhancements, and conditional image synthesis.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "omnimodal",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "benchmarks",
                    "speech",
                    "audio",
                    "model",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "performance",
                    "benchmarks",
                    "some",
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "models",
                    "results",
                    "other",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "benchmarks",
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "models",
                    "speech",
                    "other",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "speech",
                    "from",
                    "results",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "benchmarks",
                    "speech",
                    "from",
                    "results",
                    "lower",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "performance",
                    "benchmarks",
                    "models",
                    "from",
                    "audio",
                    "model",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "models",
                    "speech",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speech",
                    "unimoe20omni",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Uni-MoE-2.0-Omni proves to be a balanced and effective model for omnimodal TTS, exhibiting particular strength in English synthesis and robust performance across diverse and challenging tasks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "performance",
                    "models",
                    "speech",
                    "model",
                    "other",
                    "where",
                    "answers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "performance",
                    "models",
                    "speech",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "results",
                    "audio",
                    "model",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "performance",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "lower",
                    "models",
                    "unimoe20omni",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "models",
                    "better",
                    "where",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "lower",
                    "models",
                    "model",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "better",
                    "model",
                    "unimoe20omni",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "from",
                    "better",
                    "results",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "value",
                    "where",
                    "model",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "comparison",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "models",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "from",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "from",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify that the model&#8217;s enhanced omnimodality understanding capability stems primarily from its audio-text-vision joint coding training on large-scale video data. Future work will focus on scaling this video data and introducing new multimodal positional encoding methods to advance the model&#8217;s comprehension abilities further.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "models",
                    "from",
                    "results",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "matching",
                    "across",
                    "benchmarks",
                    "models",
                    "speech",
                    "from",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "audio",
                    "models",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "across",
                    "models",
                    "speech",
                    "audio",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech &#8594; Speech/Text</span>\nLlamaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llamaq</span>)</cite>, WebQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">webq</span>)</cite>, BigBench Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bigbench</span>)</cite>, and MultiChallenge Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">multichallenge</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "webqa",
                    "multichallenge",
                    "llamaqa",
                    "speech",
                    "bigbench",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Equation <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.E5\" title=\"Equation 5 &#8227; A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> rewrites the expectation over <math alttext=\"{\\bm{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m7\" intent=\":literal\"><semantics><mi>&#119915;</mi><annotation encoding=\"application/x-tex\">{\\bm{D}}</annotation></semantics></math> as a weighted sum over experts, where each term is the loss contribution from a single expert multiplied by its routing probability.\nFor notational simplicity, we denote <math alttext=\"{\\bm{p}}=\\texttt{Softmax}({\\bm{z}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m8\" intent=\":literal\"><semantics><mrow><mi>&#119953;</mi><mo>=</mo><mrow><mtext class=\"ltx_mathvariant_monospace\">Softmax</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{p}}=\\texttt{Softmax}({\\bm{z}})</annotation></semantics></math>. The gradient of <math alttext=\"\\mathcal{L}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m9\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><annotation encoding=\"application/x-tex\">\\mathcal{L}</annotation></semantics></math> with respect to <math alttext=\"{\\bm{z}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m10\" intent=\":literal\"><semantics><mi>&#119963;</mi><annotation encoding=\"application/x-tex\">{\\bm{z}}</annotation></semantics></math> can be written as:</p>\n\n",
                "matched_terms": [
                    "where",
                    "from"
                ]
            }
        ]
    },
    "S4.T10": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 10: \nComparison of Uni-MoE-2.0-Omni and variants with other omnimodal models across Speech-Image/Video QA benchmarks.\nFor Speech-Image QA tasks, we evaluate the models performance using the ACC metric. For Speech-Video QA tasks, we adopt GPT-based evaluation to assess their performance.",
        "body": "Model\nSpeech-Image QA\nSpeech-Video QA\nAvg.\n\n\nA-OK-VQA Speech (Reasoning)\nVQAv2 Speech\nActivityNet Speech\n\n\ns,its,i\\rightarrow t\ns,iss,i\\rightarrow s\ns,its,i\\rightarrow t\ns,iss,i\\rightarrow s\ns,vts,v\\rightarrow t\ns,vss,v\\rightarrow s\n\n\nQwen2.5-Omni-7B*\n58.55\n51.65\n76.84\n71.01\n60.00\n58.91\n62.83\n\n\nMing-Lite-Omni*\n65.08\n42.69\n81.21\n30.43\n58.43\n0.03\n55.57\n\n\nUni-MoE-2.0-Omni\n65.73\n52.58\n78.03\n71.15\n60.16\n57.94\n64.27",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speech-Image QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speech-Video QA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">A-OK-VQA Speech (Reasoning)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">VQAv2 Speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">ActivityNet Speech</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s,i\\rightarrow t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow><mo stretchy=\"false\">&#8594;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">s,i\\rightarrow t</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s,i\\rightarrow s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow><mo stretchy=\"false\">&#8594;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s,i\\rightarrow s</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s,i\\rightarrow t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow><mo stretchy=\"false\">&#8594;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">s,i\\rightarrow t</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s,i\\rightarrow s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m4\" intent=\":literal\"><semantics><mrow><mrow><mi>s</mi><mo>,</mo><mi>i</mi></mrow><mo stretchy=\"false\">&#8594;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s,i\\rightarrow s</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s,v\\rightarrow t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>s</mi><mo>,</mo><mi>v</mi></mrow><mo stretchy=\"false\">&#8594;</mo><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">s,v\\rightarrow t</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"s,v\\rightarrow s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>s</mi><mo>,</mo><mi>v</mi></mrow><mo stretchy=\"false\">&#8594;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">s,v\\rightarrow s</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Qwen2.5-Omni-7B*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">58.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">51.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">76.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">71.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">60.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">58.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">62.83</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Ming-Lite-Omni*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">65.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">42.69</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">81.21</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">30.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">58.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.03</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">55.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Uni-MoE-2.0-Omni</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">65.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">52.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">78.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">71.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">60.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">57.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">64.27</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "aokvqa",
            "unimoe20omni",
            "speechimagevideo",
            "speech",
            "mingliteomni",
            "metric",
            "models",
            "avg",
            "reasoning",
            "activitynet",
            "assess",
            "evaluation",
            "adopt",
            "sitsirightarrow",
            "evaluate",
            "svssvrightarrow",
            "omnimodal",
            "performance",
            "svtsvrightarrow",
            "their",
            "comparison",
            "acc",
            "speechvideo",
            "qwen25omni7b",
            "variants",
            "across",
            "vqav2",
            "benchmarks",
            "sissirightarrow",
            "models",
            "speechimage",
            "tasks",
            "other",
            "model",
            "gptbased"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "evaluation",
                    "benchmarks",
                    "speech",
                    "model",
                    "tasks",
                    "avg",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model",
                    "across",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model",
                    "across",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model",
                    "reasoning",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Deep Cross-Modal Alignment</span>: To enable deep and efficient fusion of any modality, we introduce an Omni-Modality 3D RoPE mechanism in the self-attention layers. It encodes the temporal-spatial dimensions of speech, image, text, and video tokens, ensuring seamless alignment and interaction across all input types.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Language-Centric Hybrid Training for Multimodal Understanding and Generation</span>: To bridge the gap between multimodal understanding and generation tasks&#8212;often treated separately during training&#8212;we propose a hybrid training approach anchored in language generation tasks. By unifying tasks such as image editing, image generation, and speech synthesis within a language generation framework, we break down the inherent barriers between understanding and generation, enabling synergistic enhancement and bidirectional empowerment of both capabilities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "evaluation",
                    "benchmarks",
                    "models",
                    "speech",
                    "comparison",
                    "speechimage",
                    "tasks",
                    "mingliteomni",
                    "other",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "speech",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This unified encoding of Single-Image, Multi-Image, and Video inputs into a one-dimensional sequence of visual representations enables the transfer of the model&#8217;s capability in high-resolution image understanding to the video domain, thereby facilitating faster convergence and improved performance in video understanding.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "adopt",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "speech",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These limitations prevent MoE from fully exploiting its adaptive activation capability, ultimately resulting in suboptimal performance.\nTo address these challenges, we propose the Dynamic-Capacity MoE architecture, which incorporates (i) routing gradient estimation to enable differentiable expert selection, (ii) explicit expert role specialization to separately model domain-specific and general knowledge, as well as facilitate knowledge deletion, and (iii) dynamic expert number allocation to adaptively control the amount of parametric knowledge.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the performance of MoE-TTS in voice timbre control, we adopt a text-context prompt-guided approach to direct the model to synthesize audio with the specified language and timbre. When the model receives an instruction to generate an audio response, the Uni-MoE base model first produces a special token (i.e. <span class=\"ltx_text ltx_font_italic\">&lt;speech start&gt;</span>) indicating the start of speech, . Subsequently, the model generates commands that specify the voice timbre for audio synthesis: fixed timbres include those of Brain, Jenny, and Xiaoxiao, while supported languages cover Chinese and English. Beyond fixed options, the model also enables users to describe desired timbres via natural language; the MoE-TTS module then generates audio with the corresponding timbre based on these descriptive instructions. Upon completion of the timbre prompt, a special token &lt;speech prompt&gt; is generated to signal the end of the prompt. This is followed by the text content that the model needs to transcribe into speech, which is terminated with the special token &lt;speech end&gt;. Through this combination of text-based timbre prompts and target text content, MoE-TTS is guided to perform diverse timbre synthesis.</p>\n\n",
                "matched_terms": [
                    "adopt",
                    "speech",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a Task-Aware Diffusion Transformer (Task-DiT) that bridges the gap between image understanding and generation. Unlike unified models that suffer from performance trade-offs, our framework preserves the strengths of specialized pre-trained models by connecting them via a lightweight, task-aware bridge.\nAt its core, Task-DiT employs a frozen image generator from PixWizard <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024pixwizard</span>)</cite> to safeguard its high-fidelity synthesis capabilities. To steer this generator, we introduce a query-based conditioning mechanism. Two distinct sets of learnable tokens are processed by a powerful understanding module:</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The enriched features from these tokens are then translated for the generator by dedicated, lightweight projectors. A Task Projector modulates the DiT&#8217;s denoising process based on the command, while a content projector transforms the &lt;IMG[i]&gt; features into a dense conditioning sequence for cross-attention. For image-guided tasks, a Visual Projector aligns source image features encoded by ViT with the DiT&#8217;s conditioning space.\nThis design creates a versatile and efficient channel for task-aware instruction, enabling high-quality, multi-modal image generation without the catastrophic interference typical of end-to-end fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the leftmost panel of the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S3.F5\" title=\"Figure 5 &#8227; Supervised Fine-tuning &#8227; 3.1 Training Recipe: From LLMs to OLMs &#8227; 3 Training and Data Recipes &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the goal of pre-training is to enable the LLM to comprehend multimodal data, such as images, videos, and speech. This is achieved by mapping the representations of these modalities into the LLM&#8217;s linguistic space, using paired data of multimodal inputs and their corresponding text description.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Expert Warmup with Dense Model: We first pre-trained three specialized expert models focusing on mainstream modalities: speech comprehension, speech generation, and visual comprehension. This stage aims to build the model-specific experts for the following stable MoE-based fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "tasks",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "performance",
                    "tasks",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "models",
                    "models",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "model",
                    "omnimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "across",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "performance",
                    "models",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "benchmarks",
                    "speech",
                    "tasks",
                    "model",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "performance",
                    "benchmarks",
                    "models",
                    "tasks",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "performance",
                    "evaluation",
                    "benchmarks",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "models",
                    "other",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "benchmarks",
                    "models",
                    "their",
                    "tasks",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "models",
                    "speech",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "speech",
                    "tasks",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "benchmarks",
                    "speech",
                    "tasks",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "performance",
                    "evaluation",
                    "benchmarks",
                    "models",
                    "model",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "models",
                    "speech",
                    "qwen25omni7b",
                    "mingliteomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "speech",
                    "qwen25omni7b",
                    "mingliteomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Uni-MoE-2.0-Omni proves to be a balanced and effective model for omnimodal TTS, exhibiting particular strength in English synthesis and robust performance across diverse and challenging tasks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "tasks",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "benchmarks",
                    "models",
                    "speech",
                    "tasks",
                    "mingliteomni",
                    "other",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "aokvqa",
                    "performance",
                    "speech",
                    "speechimage",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "performance",
                    "models",
                    "speech",
                    "speechvideo",
                    "mingliteomni",
                    "other",
                    "model",
                    "activitynet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "performance",
                    "models",
                    "speech",
                    "tasks",
                    "other",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "performance",
                    "evaluation",
                    "benchmarks",
                    "model",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "models",
                    "their",
                    "tasks",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "performance",
                    "models",
                    "tasks",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "model",
                    "unimoe20omni",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "omnimodal",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "across",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "evaluation",
                    "models",
                    "their",
                    "comparison",
                    "tasks",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "evaluation",
                    "models",
                    "models",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "model",
                    "mingliteomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "speech",
                    "tasks",
                    "models",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "reasoning",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "tasks",
                    "reasoning",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "models",
                    "across",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "across",
                    "evaluate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Vision + Speech &#8594; Speech/Text</span>\nA-OK-VQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">aokvqa</span>)</cite>, VQAv2 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vqa</span>)</cite>, and ActivityNet  <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anet</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "aokvqa",
                    "speech",
                    "vqav2",
                    "activitynet"
                ]
            }
        ]
    },
    "S4.T11": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 11: \nComparison of Uni-MoE-2.0-Omni and other MLLMs across 4 Omnimodal understanding benchmarks.\n* denotes the reproduced results. Bold indicates the highest score, and underline indicates the second-highest score for each benchmark.",
        "body": "Method\nWorldSense\nStreamingBench (Omni)\nOmniVideoBench\nOmniBench\nAvg.\n\n\nVision-Language Models\n\n\nLLaVA-OneVision-7B\n37.737.7\n40.840.8\n-\n-\n-\n\n\nLLaVA-Video-7B\n40.240.2\n41.741.7\n-\n-\n-\n\n\nQwen2.5-VL-7B\n38.338.3\n45.045.0\n29.829.8\n-\n-\n\n\nOmni Models\n\n\nUnified-IO-2 XL\n24.724.7\n-\n-\n38.038.0\n-\n\n\nUnified-IO-2 XXL\n25.925.9\n-\n-\n34.034.0\n-\n\n\nVideoLLaMA 2\n25.425.4\n35.935.9\n29.229.2\n-\n-\n\n\nQwen2.5-Omni-7B*\n43.143.1\n47.147.1\n29.829.8\n26.226.2\n36.636.6\n\n\nMiniCPM-o 2.6*\n43.243.2\n51.0\n34.734.7\n36.736.7\n41.441.4\n\n\nBaichuan-Omni-1.5*\n42.542.5\n47.147.1\n35.0\n42.942.9\n41.9\n\n\nMing-Lite-Omni*\n42.242.2\n38.838.8\n33.333.3\n43.543.5\n39.439.4\n\n\nMing-Lite-Omni-1.5*\n43.5\n40.440.4\n32.132.1\n47.7\n40.940.9\n\n\nOur Models\n\n\nUni-MoE-2.0\n42.842.8\n39.839.8\n34.334.3\n46.546.5\n40.840.8\n\n\nUni-MoE-2.0-Omni\n44.7\n48.1\n35.1\n47.1\n43.7",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">WorldSense</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">StreamingBench (Omni)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">OmniVideoBench</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">OmniBench</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Vision-Language Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LLaVA-OneVision-7B</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"37.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m1\" intent=\":literal\"><semantics><mn>37.7</mn><annotation encoding=\"application/x-tex\">37.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"40.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m2\" intent=\":literal\"><semantics><mn>40.8</mn><annotation encoding=\"application/x-tex\">40.8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">LLaVA-Video-7B</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"40.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m3\" intent=\":literal\"><semantics><mn>40.2</mn><annotation encoding=\"application/x-tex\">40.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"41.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m4\" intent=\":literal\"><semantics><mn>41.7</mn><annotation encoding=\"application/x-tex\">41.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-VL-7B</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"38.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m5\" intent=\":literal\"><semantics><mn>38.3</mn><annotation encoding=\"application/x-tex\">38.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"45.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m6\" intent=\":literal\"><semantics><mn>45.0</mn><annotation encoding=\"application/x-tex\">45.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"29.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m7\" intent=\":literal\"><semantics><mn>29.8</mn><annotation encoding=\"application/x-tex\">29.8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Omni Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Unified-IO-2 XL</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"24.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m8\" intent=\":literal\"><semantics><mn>24.7</mn><annotation encoding=\"application/x-tex\">24.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"38.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m9\" intent=\":literal\"><semantics><mn>38.0</mn><annotation encoding=\"application/x-tex\">38.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Unified-IO-2 XXL</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"25.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m10\" intent=\":literal\"><semantics><mn>25.9</mn><annotation encoding=\"application/x-tex\">25.9</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"34.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m11\" intent=\":literal\"><semantics><mn>34.0</mn><annotation encoding=\"application/x-tex\">34.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VideoLLaMA 2</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"25.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m12\" intent=\":literal\"><semantics><mn>25.4</mn><annotation encoding=\"application/x-tex\">25.4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"35.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m13\" intent=\":literal\"><semantics><mn>35.9</mn><annotation encoding=\"application/x-tex\">35.9</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"29.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m14\" intent=\":literal\"><semantics><mn>29.2</mn><annotation encoding=\"application/x-tex\">29.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-Omni-7B*</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"43.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m15\" intent=\":literal\"><semantics><mn>43.1</mn><annotation encoding=\"application/x-tex\">43.1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"47.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m16\" intent=\":literal\"><semantics><mn>47.1</mn><annotation encoding=\"application/x-tex\">47.1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"29.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m17\" intent=\":literal\"><semantics><mn>29.8</mn><annotation encoding=\"application/x-tex\">29.8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"26.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m18\" intent=\":literal\"><semantics><mn>26.2</mn><annotation encoding=\"application/x-tex\">26.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"36.6\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m19\" intent=\":literal\"><semantics><mn>36.6</mn><annotation encoding=\"application/x-tex\">36.6</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiniCPM-o 2.6*</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"43.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m20\" intent=\":literal\"><semantics><mn>43.2</mn><annotation encoding=\"application/x-tex\">43.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">51.0</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"34.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m21\" intent=\":literal\"><semantics><mn>34.7</mn><annotation encoding=\"application/x-tex\">34.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"36.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m22\" intent=\":literal\"><semantics><mn>36.7</mn><annotation encoding=\"application/x-tex\">36.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"41.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m23\" intent=\":literal\"><semantics><mn>41.4</mn><annotation encoding=\"application/x-tex\">41.4</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Baichuan-Omni-1.5*</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"42.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m24\" intent=\":literal\"><semantics><mn>42.5</mn><annotation encoding=\"application/x-tex\">42.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"47.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m25\" intent=\":literal\"><semantics><mn>47.1</mn><annotation encoding=\"application/x-tex\">47.1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">35.0</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"42.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m26\" intent=\":literal\"><semantics><mn>42.9</mn><annotation encoding=\"application/x-tex\">42.9</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">41.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Ming-Lite-Omni*</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"42.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m27\" intent=\":literal\"><semantics><mn>42.2</mn><annotation encoding=\"application/x-tex\">42.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"38.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m28\" intent=\":literal\"><semantics><mn>38.8</mn><annotation encoding=\"application/x-tex\">38.8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"33.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m29\" intent=\":literal\"><semantics><mn>33.3</mn><annotation encoding=\"application/x-tex\">33.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"43.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m30\" intent=\":literal\"><semantics><mn>43.5</mn><annotation encoding=\"application/x-tex\">43.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"39.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m31\" intent=\":literal\"><semantics><mn>39.4</mn><annotation encoding=\"application/x-tex\">39.4</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Ming-Lite-Omni-1.5*</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">43.5</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"40.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m32\" intent=\":literal\"><semantics><mn>40.4</mn><annotation encoding=\"application/x-tex\">40.4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"32.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m33\" intent=\":literal\"><semantics><mn>32.1</mn><annotation encoding=\"application/x-tex\">32.1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">47.7</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"40.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m34\" intent=\":literal\"><semantics><mn>40.9</mn><annotation encoding=\"application/x-tex\">40.9</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Our Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Uni-MoE-2.0</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"42.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m35\" intent=\":literal\"><semantics><mn>42.8</mn><annotation encoding=\"application/x-tex\">42.8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"39.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m36\" intent=\":literal\"><semantics><mn>39.8</mn><annotation encoding=\"application/x-tex\">39.8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"34.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m37\" intent=\":literal\"><semantics><mn>34.3</mn><annotation encoding=\"application/x-tex\">34.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"46.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m38\" intent=\":literal\"><semantics><mn>46.5</mn><annotation encoding=\"application/x-tex\">46.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"40.8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T11.m39\" intent=\":literal\"><semantics><mn>40.8</mn><annotation encoding=\"application/x-tex\">40.8</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Uni-MoE-2.0-Omni</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">44.7</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">48.1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">35.1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">47.1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">43.7</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "unimoe20omni",
            "omnibench",
            "mingliteomni",
            "benchmark",
            "underline",
            "avg",
            "xxl",
            "secondhighest",
            "denotes",
            "results",
            "worldsense",
            "mingliteomni15",
            "baichuanomni15",
            "each",
            "bold",
            "unifiedio2",
            "indicates",
            "omnimodal",
            "llavavideo7b",
            "method",
            "score",
            "mllms",
            "comparison",
            "videollama",
            "qwen25omni7b",
            "visionlanguage",
            "minicpmo",
            "omni",
            "highest",
            "omnivideobench",
            "unimoe20",
            "across",
            "streamingbench",
            "benchmarks",
            "models",
            "understanding",
            "qwen25vl7b",
            "reproduced",
            "other",
            "our",
            "llavaonevision7b"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "method",
                    "benchmarks",
                    "understanding",
                    "our",
                    "avg"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "understanding",
                    "across",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "understanding",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "our",
                    "omnimodal",
                    "understanding",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "our",
                    "omnimodal",
                    "understanding",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "minicpmo",
                    "models",
                    "understanding",
                    "comparison",
                    "results",
                    "worldsense",
                    "other",
                    "mingliteomni15",
                    "mingliteomni",
                    "baichuanomni15",
                    "our",
                    "omnivideobench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "each",
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The visual understanding module of Uni-MoE-2.0-Omni employs a unified encoding strategy for both images and videos. The overall architecture consists of two major components: a visual encoder and a mapping network (MLP).</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through these visual understanding modules, we can uniformly encode Single-Image, Multi-Image, and Video inputs, thereby transforming different visual input paradigms into a unified one-dimensional sequence of visual representations. The specific encoding strategies for each paradigm are described as follows:</p>\n\n",
                "matched_terms": [
                    "each",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "method",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "each",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a holistic view of our method, Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the complete training procedure in pseudocode form.\nFor simplicity of presentation, we treat null experts as a special case of routed experts in the pseudocode.\nThe algorithm integrates the three proposed components into a unified workflow:\nFirst, <span class=\"ltx_text ltx_font_typewriter\">ActivateExperts</span> function determines expert activation, combining the Top&#8209;P sampling strategy for routed experts with the persistent activation constraint for shared experts;\nSecond, gradient estimation is applied to routed expert outputs to enable end&#8209;to&#8209;end optimization under sparse activation;\nFinally, after computing routed expert contributions, shared expert outputs are computed and incorporated into the MoE representation, injecting general knowledge into the final output.</p>\n\n",
                "matched_terms": [
                    "our",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "our",
                    "understanding",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "each",
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a Task-Aware Diffusion Transformer (Task-DiT) that bridges the gap between image understanding and generation. Unlike unified models that suffer from performance trade-offs, our framework preserves the strengths of specialized pre-trained models by connecting them via a lightweight, task-aware bridge.\nAt its core, Task-DiT employs a frozen image generator from PixWizard <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024pixwizard</span>)</cite> to safeguard its high-fidelity synthesis capabilities. To steer this generator, we introduce a query-based conditioning mechanism. Two distinct sets of learnable tokens are processed by a powerful understanding module:</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "results",
                    "omnimodal",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "omnimodal",
                    "understanding",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "understanding",
                    "other",
                    "highest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "understanding",
                    "results",
                    "mingliteomni15",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "understanding",
                    "results",
                    "other",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "understanding",
                    "results",
                    "mingliteomni15",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "understanding",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "understanding",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "understanding",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "omnimodal",
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "models",
                    "results",
                    "qwen25omni7b",
                    "mingliteomni",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "our",
                    "qwen25omni7b",
                    "unimoe20omni",
                    "mingliteomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Uni-MoE-2.0-Omni proves to be a balanced and effective model for omnimodal TTS, exhibiting particular strength in English synthesis and robust performance across diverse and challenging tasks.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "results",
                    "mingliteomni",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "our",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "score",
                    "models",
                    "other",
                    "mingliteomni",
                    "benchmark",
                    "our",
                    "highest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "unimoe20omni",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "score",
                    "models",
                    "benchmark",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "score",
                    "models",
                    "benchmark",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "our",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "across",
                    "understanding",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "results",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "our",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "across",
                    "unimoe20"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "our",
                    "mingliteomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "omnimodal",
                    "understanding",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "results",
                    "omnimodal",
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "omnimodal",
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "understanding",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "understanding",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "understanding",
                    "our",
                    "omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video&amp;Audio</span>. WorldSense <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hong2025worldsense</span>)</cite>, OmniVideoBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">omnivideobench</span>)</cite> and StreamingBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">streamingbench</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "worldsense",
                    "streamingbench",
                    "omnivideobench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "understanding",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To extend it to our Top-<math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p9.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> strategy, we apply the same computation sequentially to each activated expert, sampling without replacement from the routing distribution until the cumulative probability exceeds <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p9.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math>.\nIn this way, the gradient estimation naturally generalizes to multiple experts while remaining consistent with the selection process in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ensuring that the hybrid scaling mechanism is fully compatible with our dynamic-capacity MoE framework.</p>\n\n",
                "matched_terms": [
                    "each",
                    "our"
                ]
            }
        ]
    },
    "S4.T12": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 12: \nComparison of models across Image Generation and Image Edition benchmarks.",
        "body": "Model\nImage generation\nImage edition\n\n\n\nWise \\uparrow\n\nFID \\downarrow\n\nGEdit-Bench \\uparrow\n\nEmu Edit \\uparrow\nMagicBrush\n\n\n\nCLIPimg \\uparrow\n\n\nDINOimg \\uparrow\n\n\nCLIPout \\uparrow\n\n\n\nImage Generation Models\n\n\nSDXL\n0.48\n13.63\n-\n-\n-\n-\n-\n\n\nPixWizard\n0.43\n11.99\n3.20\n0.039\n0.907\n0.811\n0.298\n\n\nQwen-Image\n0.63\n25.37\n7.42\n0.127\n0.920\n0.800\n0.313\n\n\nOmni Models\n\n\nJanusPro-7B\n0.41\n19.82\n-\n-\n-\n-\n-\n\n\nBagel\n0.49\n25.47\n6.52\n0.124\n0.921\n0.844\n0.310\n\n\nOmniGen\n0.40\n29.32\n5.61\n0.091\n0.907\n0.802\n0.298\n\n\nOmniGen2\n0.45\n31.65\n6.10\n0.103\n0.899\n0.794\n0.307\n\n\nMing-Lite-Omni\n0.54\n16.86\n5.55\n0.052\n0.845\n0.683\n0.299\n\n\nMing-Lite-Omni-1.5\n0.52\n32.39\n6.09\n0.094\n0.910\n0.822\n0.309\n\n\nOur Models\n\n\nUni-MoE-2.0-Omni\n0.44\n18.04\n6.02\n0.076\n0.789\n0.590\n0.288\n\n\nUni-MoE-2.0-Image\n0.46\n18.95\n6.00\n0.080\n0.854\n0.714\n0.293",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"3\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Image generation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Image edition</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">Wise</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T12.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">FID</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T12.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">GEdit-Bench</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T12.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">Emu Edit</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T12.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">MagicBrush</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">CLIPimg</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T12.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">DINOimg</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T12.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">CLIPout</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T12.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"8\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Image Generation Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">SDXL</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">13.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">PixWizard</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">11.99</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">3.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">0.039</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.907</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.811</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.298</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Qwen-Image</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">25.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">7.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">0.127</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.920</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.800</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.313</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"8\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Omni Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">JanusPro-7B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">19.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Bagel</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">25.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">6.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">0.124</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.921</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.844</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.310</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">OmniGen</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">29.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">5.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">0.091</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.907</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.802</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.298</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">OmniGen2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">31.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">6.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">0.103</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.899</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.794</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.307</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Ming-Lite-Omni</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">16.86</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">5.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">0.052</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.845</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.683</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.299</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Ming-Lite-Omni-1.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">32.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">6.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">0.094</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.910</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.822</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.309</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"8\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Our Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Uni-MoE-2.0-Omni</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">18.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">6.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">0.076</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.789</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.590</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.288</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Uni-MoE-2.0-Image</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">18.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\">6.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">0.080</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\">0.854</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\">0.714</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\">0.293</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "unimoe20omni",
            "omnigen",
            "clipimg",
            "unimoe20image",
            "mingliteomni",
            "sdxl",
            "magicbrush",
            "uparrow",
            "januspro7b",
            "image",
            "mingliteomni15",
            "fid",
            "edit",
            "dinoimg",
            "edition",
            "omnigen2",
            "downarrow",
            "generation",
            "comparison",
            "qwenimage",
            "wise",
            "omni",
            "pixwizard",
            "across",
            "benchmarks",
            "models",
            "geditbench",
            "bagel",
            "our",
            "emu",
            "model",
            "clipout"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "generation",
                    "image",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "across",
                    "omnigen",
                    "generation",
                    "bagel",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "model",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "generation",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Unified Modality Encoding &amp; Generation</span>: We design a unified speech encoder that maps diverse audio inputs, including environmental sound, speech, and music, into a shared representation space. For output, a Context-Aware MoE-based TTS module supports dynamic speech synthesis (especially for long speech) and interaction. On the visual side, we employ pre-trained visual encoders to process images and videos, and build a Task-Aware Diffusion Transformer for instruction-guided image generation and editing.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Deep Cross-Modal Alignment</span>: To enable deep and efficient fusion of any modality, we introduce an Omni-Modality 3D RoPE mechanism in the self-attention layers. It encodes the temporal-spatial dimensions of speech, image, text, and video tokens, ensuring seamless alignment and interaction across all input types.</p>\n\n",
                "matched_terms": [
                    "image",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Language-Centric Hybrid Training for Multimodal Understanding and Generation</span>: To bridge the gap between multimodal understanding and generation tasks&#8212;often treated separately during training&#8212;we propose a hybrid training approach anchored in language generation tasks. By unifying tasks such as image editing, image generation, and speech synthesis within a language generation framework, we break down the inherent barriers between understanding and generation, enabling synergistic enhancement and bidirectional empowerment of both capabilities.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "omnigen",
                    "downarrow",
                    "benchmarks",
                    "models",
                    "generation",
                    "comparison",
                    "geditbench",
                    "mingliteomni15",
                    "mingliteomni",
                    "image",
                    "bagel",
                    "our",
                    "qwenimage",
                    "model",
                    "pixwizard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "image",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a Task-Aware Diffusion Transformer (Task-DiT) that bridges the gap between image understanding and generation. Unlike unified models that suffer from performance trade-offs, our framework preserves the strengths of specialized pre-trained models by connecting them via a lightweight, task-aware bridge.\nAt its core, Task-DiT employs a frozen image generator from PixWizard <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024pixwizard</span>)</cite> to safeguard its high-fidelity synthesis capabilities. To steer this generator, we introduce a query-based conditioning mechanism. Two distinct sets of learnable tokens are processed by a powerful understanding module:</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "image",
                    "our",
                    "pixwizard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Image Tokens (&lt;IMG[i]&gt;): Capture the rich semantic essence of the desired output from Uni-MoE-2.0-Omni, forming a compressed scene representation.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The enriched features from these tokens are then translated for the generator by dedicated, lightweight projectors. A Task Projector modulates the DiT&#8217;s denoising process based on the command, while a content projector transforms the &lt;IMG[i]&gt; features into a dense conditioning sequence for cross-attention. For image-guided tasks, a Visual Projector aligns source image features encoded by ViT with the DiT&#8217;s conditioning space.\nThis design creates a versatile and efficient channel for task-aware instruction, enabling high-quality, multi-modal image generation without the catastrophic interference typical of end-to-end fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Expert Warmup with Dense Model: We first pre-trained three specialized expert models focusing on mainstream modalities: speech comprehension, speech generation, and visual comprehension. This stage aims to build the model-specific experts for the following stable MoE-based fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "model",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "model",
                    "image",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "model",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the task projector alignment stage and the visual projector alignment stage, we utilized data encompassing a variety of task types. This included image generation, image editing, low-level computer vision tasks such as deraining and denoising, and conditional generation like Canny-to-image. In total, this data amounted to approximately 2M samples (1.5B tokens).</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the caption projector alignment stage, we used high-quality image-caption pairs sourced from a diverse range of data types. This collection included factual, human-annotated descriptions of everyday scenes; detailed, context-rich prompts for complex image generation; and elaborate narratives describing the compositional and spatial relationships between elements. This data accounted for a total of 4.2M samples (3.2B tokens).</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated and filtered a large-scale dataset from multiple open-source dataset collections, amounting to approximately 10.5M samples (8.1B tokens). This stage aimed to unify the model&#8217;s capabilities under a consistent instruction-following framework. The training data was formatted as instruction-response pairs covering the same core task categories mentioned previously: image generation, image editing, low-level CV enhancements, and conditional image synthesis.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "generation",
                    "image",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "unimoe20omni",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "mingliteomni15",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "mingliteomni15",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "model",
                    "unimoe20omni",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "unimoe20omni",
                    "mingliteomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "unimoe20omni",
                    "mingliteomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Uni-MoE-2.0-Omni proves to be a balanced and effective model for omnimodal TTS, exhibiting particular strength in English synthesis and robust performance across diverse and challenging tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "mingliteomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "models",
                    "mingliteomni",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "our",
                    "omni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "januspro7b",
                    "models",
                    "generation",
                    "image",
                    "bagel",
                    "our",
                    "qwenimage",
                    "fid",
                    "wise",
                    "pixwizard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "edition",
                    "unimoe20omni",
                    "our",
                    "clipimg",
                    "models",
                    "geditbench",
                    "unimoe20image",
                    "image",
                    "magicbrush",
                    "emu",
                    "qwenimage",
                    "model",
                    "pixwizard",
                    "edit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "omnigen2",
                    "our",
                    "models",
                    "generation",
                    "unimoe20image",
                    "fid",
                    "qwenimage",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "omnigen2",
                    "image",
                    "our",
                    "qwenimage",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "our",
                    "generation",
                    "image",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "image",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "mingliteomni",
                    "bagel",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "model",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "generation",
                    "image",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "across",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "omni",
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "edition",
                    "across",
                    "models",
                    "generation",
                    "image",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>. Wise <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">niu2025wise</span>)</cite> and Coco30K <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2014microsoft</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "wise",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>. GEdit-Bench-EN <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025step1x</span>)</cite>, Emu Edit Test <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheynin2024emu</span>)</cite> and MagicBrush <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023magicbrush</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "edition",
                    "image",
                    "magicbrush",
                    "emu",
                    "edit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "generation",
                    "across"
                ]
            }
        ]
    },
    "S4.T13": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 13: \nComparison of models across Controllable Generation and Low-Level Image Restoration benchmarks.",
        "body": "Model\nControllable Generation\nLow-Level Image Restoration\n\n\nCanny-to-Image\nDepth-to-Image\nDerain\nDenoise\n\n\n\nF1-Score \\uparrow\n\n\nFID \\downarrow\n\n\nCLIP-S \\uparrow\n\n\nRMSE \\downarrow\n\n\nFID \\downarrow\n\n\nCLIP-S \\uparrow\n\n\nPSNR \\uparrow\n\n\nSSIM \\uparrow\n\n\nPSNR \\uparrow\n\n\nSSIM \\uparrow\n\n\n\nImage Generation Models\n\n\nPixWizard\n0.24\n18.32\n28.88\n42.61\n23.41\n27.59\n24.62\n0.77\n27.75\n0.81\n\n\nQwen-Image\n0.47\n37.59\n27.45\n51.23\n27.54\n24.81\n26.37\n0.80\n22.19\n0.46\n\n\nOmni Models\n\n\nBagel\n0.17\n130.44\n24.36\n64.85\n39.40\n27.53\n17.14\n0.59\n18.14\n0.25\n\n\nOmniGen\n0.42\n32.15\n29.25\n32.09\n16.98\n29.81\n13.57\n0.22\n17.05\n0.19\n\n\nOmniGen2\n0.16\n45.67\n27.35\n59.57\n52.30\n26.47\n22.22\n0.77\n16.78\n0.41\n\n\nMing-Lite-Omni\n0.17\n154.95\n21.83\n85.71\n126.31\n20.33\n11.63\n0.21\n17.61\n0.51\n\n\nMing-Lite-Omni-1.5\n0.20\n187.42\n21.67\n87.53\n134.01\n20.78\n16.01\n0.38\n14.35\n0.13\n\n\nOur Models\n\n\nUni-MoE-2.0-Omni\n0.24\n20.23\n28.58\n42.41\n27.45\n27.00\n25.41\n0.82\n25.70\n0.48\n\n\nUni-MoE-2.0-Image\n0.24\n18.23\n29.25\n44.23\n21.91\n27.60\n25.69\n0.82\n26.01\n0.47",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"3\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"6\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Canny-to-Image</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Depth-to-Image</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Derain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Denoise</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">F1-Score</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T13.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">FID</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T13.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">CLIP-S</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T13.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">RMSE</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T13.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">FID</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T13.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">CLIP-S</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T13.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">PSNR</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T13.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">SSIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T13.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">PSNR</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T13.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">SSIM</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T13.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"11\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Image Generation Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">PixWizard</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">18.32</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">28.88</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">42.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">23.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">27.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">24.62</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.77</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">27.75</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.81</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Qwen-Image</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.47</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">37.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">27.45</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">51.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">27.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">24.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">26.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">22.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.46</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"11\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Omni Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Bagel</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">130.44</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">24.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">64.85</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">39.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">27.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">17.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">18.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">OmniGen</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.42</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">32.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">29.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">32.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">16.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">29.81</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">13.57</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">17.05</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">OmniGen2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.16</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">45.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">27.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">59.57</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">52.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">26.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">22.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.77</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">16.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.41</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Ming-Lite-Omni</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">154.95</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">21.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">85.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">126.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">20.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">11.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">17.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.51</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Ming-Lite-Omni-1.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">187.42</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">21.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">87.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">134.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">20.78</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">16.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">14.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.13</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"11\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Our Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Uni-MoE-2.0-Omni</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">20.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">28.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">42.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">27.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">27.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">25.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">25.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 4.0pt;\">0.48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 4.0pt;\">Uni-MoE-2.0-Image</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\">0.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">18.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">29.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\">44.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">21.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">27.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">25.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">26.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 4.0pt;\">0.47</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "unimoe20omni",
            "f1score",
            "lowlevel",
            "omnigen",
            "ssim",
            "unimoe20image",
            "mingliteomni",
            "uparrow",
            "depthtoimage",
            "clips",
            "psnr",
            "derain",
            "image",
            "mingliteomni15",
            "fid",
            "omnigen2",
            "downarrow",
            "generation",
            "comparison",
            "denoise",
            "qwenimage",
            "omni",
            "pixwizard",
            "controllable",
            "across",
            "restoration",
            "benchmarks",
            "models",
            "cannytoimage",
            "rmse",
            "bagel",
            "our",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "lowlevel",
                    "benchmarks",
                    "generation",
                    "image",
                    "our",
                    "model",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "across",
                    "omnigen",
                    "generation",
                    "bagel",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "model",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "generation",
                    "unimoe20omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Unified Modality Encoding &amp; Generation</span>: We design a unified speech encoder that maps diverse audio inputs, including environmental sound, speech, and music, into a shared representation space. For output, a Context-Aware MoE-based TTS module supports dynamic speech synthesis (especially for long speech) and interaction. On the visual side, we employ pre-trained visual encoders to process images and videos, and build a Task-Aware Diffusion Transformer for instruction-guided image generation and editing.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Deep Cross-Modal Alignment</span>: To enable deep and efficient fusion of any modality, we introduce an Omni-Modality 3D RoPE mechanism in the self-attention layers. It encodes the temporal-spatial dimensions of speech, image, text, and video tokens, ensuring seamless alignment and interaction across all input types.</p>\n\n",
                "matched_terms": [
                    "image",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Language-Centric Hybrid Training for Multimodal Understanding and Generation</span>: To bridge the gap between multimodal understanding and generation tasks&#8212;often treated separately during training&#8212;we propose a hybrid training approach anchored in language generation tasks. By unifying tasks such as image editing, image generation, and speech synthesis within a language generation framework, we break down the inherent barriers between understanding and generation, enabling synergistic enhancement and bidirectional empowerment of both capabilities.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "lowlevel",
                    "omnigen",
                    "downarrow",
                    "benchmarks",
                    "models",
                    "generation",
                    "comparison",
                    "image",
                    "mingliteomni",
                    "mingliteomni15",
                    "bagel",
                    "our",
                    "qwenimage",
                    "model",
                    "pixwizard",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "clips",
                    "generation",
                    "image",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "image",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "clips"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a Task-Aware Diffusion Transformer (Task-DiT) that bridges the gap between image understanding and generation. Unlike unified models that suffer from performance trade-offs, our framework preserves the strengths of specialized pre-trained models by connecting them via a lightweight, task-aware bridge.\nAt its core, Task-DiT employs a frozen image generator from PixWizard <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024pixwizard</span>)</cite> to safeguard its high-fidelity synthesis capabilities. To steer this generator, we introduce a query-based conditioning mechanism. Two distinct sets of learnable tokens are processed by a powerful understanding module:</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "image",
                    "our",
                    "pixwizard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Task Tokens (&lt;TASK[i]&gt;): Encode high-level commands (e.g., text-to-image, editing, low-level image processing) to specify the generative mode.</p>\n\n",
                "matched_terms": [
                    "image",
                    "lowlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Image Tokens (&lt;IMG[i]&gt;): Capture the rich semantic essence of the desired output from Uni-MoE-2.0-Omni, forming a compressed scene representation.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The enriched features from these tokens are then translated for the generator by dedicated, lightweight projectors. A Task Projector modulates the DiT&#8217;s denoising process based on the command, while a content projector transforms the &lt;IMG[i]&gt; features into a dense conditioning sequence for cross-attention. For image-guided tasks, a Visual Projector aligns source image features encoded by ViT with the DiT&#8217;s conditioning space.\nThis design creates a versatile and efficient channel for task-aware instruction, enabling high-quality, multi-modal image generation without the catastrophic interference typical of end-to-end fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Expert Warmup with Dense Model: We first pre-trained three specialized expert models focusing on mainstream modalities: speech comprehension, speech generation, and visual comprehension. This stage aims to build the model-specific experts for the following stable MoE-based fine-tuning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "model",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "model",
                    "image",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "model",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the task projector alignment stage and the visual projector alignment stage, we utilized data encompassing a variety of task types. This included image generation, image editing, low-level computer vision tasks such as deraining and denoising, and conditional generation like Canny-to-image. In total, this data amounted to approximately 2M samples (1.5B tokens).</p>\n\n",
                "matched_terms": [
                    "cannytoimage",
                    "generation",
                    "image",
                    "lowlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the caption projector alignment stage, we used high-quality image-caption pairs sourced from a diverse range of data types. This collection included factual, human-annotated descriptions of everyday scenes; detailed, context-rich prompts for complex image generation; and elaborate narratives describing the compositional and spatial relationships between elements. This data accounted for a total of 4.2M samples (3.2B tokens).</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated and filtered a large-scale dataset from multiple open-source dataset collections, amounting to approximately 10.5M samples (8.1B tokens). This stage aimed to unify the model&#8217;s capabilities under a consistent instruction-following framework. The training data was formatted as instruction-response pairs covering the same core task categories mentioned previously: image generation, image editing, low-level CV enhancements, and conditional image synthesis.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "lowlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "lowlevel",
                    "benchmarks",
                    "generation",
                    "image",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "unimoe20omni",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "mingliteomni15",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "benchmarks",
                    "models",
                    "mingliteomni15",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "model",
                    "unimoe20omni",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "unimoe20omni",
                    "mingliteomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "unimoe20omni",
                    "mingliteomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Uni-MoE-2.0-Omni proves to be a balanced and effective model for omnimodal TTS, exhibiting particular strength in English synthesis and robust performance across diverse and challenging tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "models",
                    "mingliteomni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "models",
                    "mingliteomni",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "models",
                    "unimoe20omni",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "benchmarks",
                    "our",
                    "omni",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "our",
                    "models",
                    "generation",
                    "image",
                    "bagel",
                    "fid",
                    "qwenimage",
                    "pixwizard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "models",
                    "unimoe20image",
                    "image",
                    "our",
                    "qwenimage",
                    "model",
                    "pixwizard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "omnigen2",
                    "f1score",
                    "depthtoimage",
                    "cannytoimage",
                    "models",
                    "generation",
                    "rmse",
                    "unimoe20image",
                    "our",
                    "qwenimage",
                    "fid",
                    "model",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "omnigen2",
                    "lowlevel",
                    "restoration",
                    "ssim",
                    "denoise",
                    "psnr",
                    "derain",
                    "image",
                    "our",
                    "qwenimage",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "across",
                    "lowlevel",
                    "generation",
                    "image",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "image",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "mingliteomni",
                    "bagel",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "our",
                    "model",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "lowlevel",
                    "generation",
                    "image",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "unimoe20omni",
                    "across",
                    "lowlevel",
                    "restoration",
                    "benchmarks",
                    "models",
                    "generation",
                    "image",
                    "model",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "across",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "omni",
                    "models",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "across",
                    "restoration",
                    "models",
                    "generation",
                    "image",
                    "our",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>. Wise <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">niu2025wise</span>)</cite> and Coco30K <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2014microsoft</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>. MultiGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024controlnet++</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>. Rain100L <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2017deep</span>)</cite> and SIDD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">abdelhamed2018high</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "restoration",
                    "image",
                    "lowlevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "generation",
                    "across"
                ]
            }
        ]
    },
    "S4.T14": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 14: \nComparison of Uni-MoE-2.0-Thinking and Base (non-thinking) version.\nBold indicates the highest score, and underline indicates the second-highest score for each benchmark.",
        "body": "Method\nMathVista (testmini)\nMathVerse (vision)\nLogicVista (testmini)\nMMMU (val)\nAvg.\n\n\nNon-Thinking\n\n\nUni-MoE-2.0\n60.80\n17.26\n31.47\n42.67\n38.05\n\n\nUni-MoE-2.0-Base\n61.30\n18.15\n32.81\n46.67\n39.73\n\n\nThinking\n\n\nUni-MoE-2.0-ColdStart\n55.50\n19.54\n28.35\n39.67\n35.77\n\n\nUni-MoE-2.0-GSPO\n58.90\n21.19\n33.71\n47.11\n40.23\n\n\nUni-MoE-2.0-DPO\n63.90\n22.97\n34.82\n45.78\n41.87",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MathVista (testmini)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MathVerse (vision)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">LogicVista (testmini)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">MMMU (val)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Non-Thinking</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Uni-MoE-2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">60.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">17.26</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">31.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">42.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">38.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Uni-MoE-2.0-Base</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">61.30</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">18.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">32.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">46.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">39.73</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#D9D9D9;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#D9D9D9;\">Thinking</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Uni-MoE-2.0-ColdStart</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">55.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">19.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">28.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">39.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">35.77</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Uni-MoE-2.0-GSPO</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">58.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">21.19</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">33.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">47.11</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">40.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Uni-MoE-2.0-DPO</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">63.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">22.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">34.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">45.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">41.87</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "base",
            "nonthinking",
            "mathvista",
            "benchmark",
            "val",
            "underline",
            "testmini",
            "avg",
            "secondhighest",
            "thinking",
            "version",
            "mmmu",
            "each",
            "bold",
            "indicates",
            "method",
            "score",
            "unimoe20gspo",
            "comparison",
            "unimoe20dpo",
            "highest",
            "unimoe20",
            "unimoe20base",
            "mathverse",
            "unimoe20coldstart",
            "unimoe20thinking",
            "logicvista",
            "vision"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "base",
                    "avg",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "each",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "method",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "each",
                    "vision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "base",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "highest",
                    "score",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "score",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "base",
                    "version",
                    "score",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "unimoe20gspo",
                    "mathverse",
                    "mmmu",
                    "logicvista",
                    "unimoe20"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "unimoe20gspo",
                    "testmini",
                    "mathvista"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "base",
                    "version"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">STEM Image Reasoning</span>. AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2d</span>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmmu</span>)</cite> and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yue2024mmmupro</span>)</cite> for science reasoning. MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathvista</span>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathvision</span>)</cite> and LogicVista <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">logicvista</span>)</cite> for mathematics reasoning.</p>\n\n",
                "matched_terms": [
                    "mmmu",
                    "logicvista",
                    "mathvista"
                ]
            }
        ]
    },
    "A1.T15": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 15: The list of image data used during our training.",
        "body": "Stage\nDataset\n\n\nPretrain\nPixelProse-RedCaps (pixelprose)\n\n\n\nPixelProse-CommonPool (pixelprose)\n\n\n\nGRIT (grit)\n\n\n\nCC3M (cc3m)\n\n\n\nLLaVA-Pretrain (llava-pretrain-gen)\n\n\n\n\n\n\nSFT\n\n&\n\nAnnealing\n \nCambrian-10M (cambrian-10m)\n\n\n\nLLaVA-OneVision (Llava-onevision)\n\n\n\nDocmatix (docmatix)\n\n\n\nPixmo-Docx (pixmo)\n\n\n\nPixmo-Points (pixmo)\n\n\n\nPixmo-Point-Explanations (pixmo)\n\n\n\nPixmo-Ask-Model-Anything (pixmo)\n\n\n\nLatex-OCR\n\n\nLatex-Formulas\n\n\nArxiv-OCR-v0.2\n\n\nMMK12 (mmk12)\n\n\n\nV* (v*)\n\n\n\nVision-R1-cold (vision-r1)\n\n\n\n\nMultimodal-Cold-Start (mmcoldstart)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Stage</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Dataset</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"5\"><span class=\"ltx_text ltx_font_italic\">Pretrain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">PixelProse-RedCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pixelprose</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">PixelProse-CommonPool <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pixelprose</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">GRIT <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">grit</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">CC3M <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cc3m</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LLaVA-Pretrain <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llava-pretrain-gen</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"13\"><span class=\"ltx_text ltx_font_italic\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">SFT</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">&amp;</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Annealing</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Cambrian-10M <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cambrian-10m</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LLaVA-OneVision <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Llava-onevision</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Docmatix <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">docmatix</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Pixmo-Docx <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pixmo</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Pixmo-Points <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pixmo</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Pixmo-Point-Explanations <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pixmo</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Pixmo-Ask-Model-Anything <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pixmo</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Latex-OCR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Latex-Formulas</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Arxiv-OCR-v0.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MMK12 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmk12</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">V* <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">v*</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Vision-R1-cold <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vision-r1</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Multimodal-Cold-Start <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmcoldstart</span>)</cite>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "pixmoaskmodelanything",
            "pixelproseredcaps",
            "multimodalcoldstart",
            "grit",
            "data",
            "pixelprosecommonpool",
            "cambrian10m",
            "pixmopointexplanations",
            "docmatix",
            "used",
            "visionr1cold",
            "llavapretrain",
            "sft",
            "latexformulas",
            "stage",
            "pixmopoints",
            "pixmo",
            "image",
            "visionr1",
            "llavapretraingen",
            "llavaonevision",
            "pixelprose",
            "mmk12",
            "during",
            "training",
            "cc3m",
            "mmcoldstart",
            "dataset",
            "latexocr",
            "pretrain",
            "arxivocrv02",
            "list",
            "annealing",
            "pixmodocx",
            "our"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">All open-source training data are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T15\" title=\"Table 15 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T18\" title=\"Table 18 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "our",
                    "image",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At training and data recipes, we introduce <span class=\"ltx_text ltx_font_bold\">the following training recipe with the data matching</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S3\" title=\"3 Training and Data Recipes &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) :</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Language-Centric Hybrid Training for Multimodal Understanding and Generation</span>: To bridge the gap between multimodal understanding and generation tasks&#8212;often treated separately during training&#8212;we propose a hybrid training approach anchored in language generation tasks. By unifying tasks such as image editing, image generation, and speech synthesis within a language generation framework, we break down the inherent barriers between understanding and generation, enabling synergistic enhancement and bidirectional empowerment of both capabilities.</p>\n\n",
                "matched_terms": [
                    "during",
                    "image",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "our",
                    "image",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "image",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central difficulty in training vanilla MoE models arises from the non-differentiable Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> operation used in expert selection, which obstructs gradient propagation to the router and leads to biased optimization. To address this, we migrate the gradient estimation strategy proposed in Grin-MoE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024gringradientinformedmoe</span>)</cite>, which integrates straight-through gradient estimators under the framework of numerical methods for ordinary differential equations (ODEs). This enables end-to-end optimization of both router and experts under sparse activation constraints, improving the stability of router training and allowing more precise token-to-expert assignments. A formal description is given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.SS4\" title=\"A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a holistic view of our method, Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the complete training procedure in pseudocode form.\nFor simplicity of presentation, we treat null experts as a special case of routed experts in the pseudocode.\nThe algorithm integrates the three proposed components into a unified workflow:\nFirst, <span class=\"ltx_text ltx_font_typewriter\">ActivateExperts</span> function determines expert activation, combining the Top&#8209;P sampling strategy for routed experts with the persistent activation constraint for shared experts;\nSecond, gradient estimation is applied to routed expert outputs to enable end&#8209;to&#8209;end optimization under sparse activation;\nFinally, after computing routed expert contributions, shared expert outputs are computed and incorporated into the MoE representation, injecting general knowledge into the final output.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset",
                    "pretrain",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a Task-Aware Diffusion Transformer (Task-DiT) that bridges the gap between image understanding and generation. Unlike unified models that suffer from performance trade-offs, our framework preserves the strengths of specialized pre-trained models by connecting them via a lightweight, task-aware bridge.\nAt its core, Task-DiT employs a frozen image generator from PixWizard <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024pixwizard</span>)</cite> to safeguard its high-fidelity synthesis capabilities. To steer this generator, we introduce a query-based conditioning mechanism. Two distinct sets of learnable tokens are processed by a powerful understanding module:</p>\n\n",
                "matched_terms": [
                    "our",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we collected a large-scale multimodal instruction-following dataset to enable the model to understand and process any cross-modal information. The training comprises two phases:</p>\n\n",
                "matched_terms": [
                    "stage",
                    "dataset",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "during",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "image",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "image",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "annealing",
                    "image",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "used",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the vision modality pre-training stage, the visual data primarily comprised image&#8211;caption and video&#8211;caption datasets. The image&#8211;caption corpus contained approximately 17 million samples (13 billion tokens), whereas the video&#8211;caption data represented a smaller portion, with around 0.1 million samples (0.2 billion tokens).</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "image",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "during",
                    "annealing",
                    "image",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the task projector alignment stage and the visual projector alignment stage, we utilized data encompassing a variety of task types. This included image generation, image editing, low-level computer vision tasks such as deraining and denoising, and conditional generation like Canny-to-image. In total, this data amounted to approximately 2M samples (1.5B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "image",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the caption projector alignment stage, we used high-quality image-caption pairs sourced from a diverse range of data types. This collection included factual, human-annotated descriptions of everyday scenes; detailed, context-rich prompts for complex image generation; and elaborate narratives describing the compositional and spatial relationships between elements. This data accounted for a total of 4.2M samples (3.2B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "used",
                    "image",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated and filtered a large-scale dataset from multiple open-source dataset collections, amounting to approximately 10.5M samples (8.1B tokens). This stage aimed to unify the model&#8217;s capabilities under a consistent instruction-following framework. The training data was formatted as instruction-response pairs covering the same core task categories mentioned previously: image generation, image editing, low-level CV enhancements, and conditional image synthesis.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "image",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "stage",
                    "image",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "image",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "during",
                    "image",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "our",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "our",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "our",
                    "image",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "our",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "our",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "used",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "our",
                    "used",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "during",
                    "image",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify that the model&#8217;s enhanced omnimodality understanding capability stems primarily from its audio-text-vision joint coding training on large-scale video data. Future work will focus on scaling this video data and introducing new multimodal positional encoding methods to advance the model&#8217;s comprehension abilities further.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "data",
                    "during",
                    "annealing",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "image",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to express our sincere gratitude to Qi Wang, Qixun Teng, Feifan Wen, and Zitao Li for their contributions to data collection and demo recording. We are thankful to Jinchao Li and Tongshu Bian for their assistance in paper proofreading and video production. We also thank Longyue Wang and Wenhan Luo for their valuable suggestions on paper writing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "our",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "our",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hybrid gradient estimator.</span>\nTo balance the stability of router training with the diversity of expert learning, we combine the two estimators above: the first-order gradient is used when the selected expert corresponds to <math alttext=\"\\arg\\max({\\bm{z}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m1\" intent=\":literal\"><semantics><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\arg\\max({\\bm{z}})</annotation></semantics></math>, while the third-order gradient is applied otherwise.\nLet <math alttext=\"\\delta_{\\bm{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#948;</mi><mi>&#119915;</mi></msub><annotation encoding=\"application/x-tex\">\\delta_{\\bm{D}}</annotation></semantics></math> denote the indicator <math alttext=\"\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119915;</mi><mo>=</mo><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))</annotation></semantics></math>. The combined estimator can be written as:\n\n</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            }
        ]
    },
    "A1.T16": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 16: The list of video data used during our training. * indicates that only a subset of the dataset was employed.",
        "body": "Stage\nDataset\n\n\nPretrain\nValley-Pretrain (valley)\n\n\n\nShareGPT4Video (chen2024sharegpt4video)\n\n\n\nVideoVista-Event (li2024videovista)\n\n\n\n\n\n\nSFT\n\n&\n\nAnnealing\n \nVideoChat2-IT* (mvbench)\n\n\n\nLLaVA-Video-178K (llava-video-178k)\n\n\n\nVideoVista-Train (li2024videovista)\n\n\n\nVideoGPT-Plus (videoGPTplus)\n\n\n\nfineVideo (FineVideo)\n\n\n\nTimeChat-Online (timechatonline)\n\n\n\nCharades-STA (charades-sta)\n\n\n\nCinePile (cinepile)\n\n\n\nSF20K (sf20k)\n\n\n\nNeptune (neptune24)\n\n\n\nEgoTaskQA (jia2022egotaskqa)\n\n\n\nFunQA (xie2024funqa)\n\n\n\nVript (yang2024vript)\n\n\n\nTarsier2-Recap (tarsier2)\n\n\n\nInternVideo2-Vid-Text* (wang2024internvideo2)\n\n\n\nSR-91K* (ouyang2025sr_91k)\n\n\n\nVideoVista2-CoT (li2024videovista)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Stage</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Dataset</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text ltx_font_italic\">Pretrain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Valley-Pretrain <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">valley</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">ShareGPT4Video <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024sharegpt4video</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">VideoVista-Event <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024videovista</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"17\"><span class=\"ltx_text ltx_font_italic\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">SFT</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">&amp;</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Annealing</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">VideoChat2-IT* <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mvbench</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LLaVA-Video-178K <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llava-video-178k</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">VideoVista-Train <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024videovista</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">VideoGPT-Plus <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">videoGPTplus</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">fineVideo <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">FineVideo</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">TimeChat-Online <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">timechatonline</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Charades-STA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">charades-sta</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">CinePile <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cinepile</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">SF20K <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sf20k</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Neptune <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">neptune24</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">EgoTaskQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jia2022egotaskqa</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">FunQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xie2024funqa</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Vript <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024vript</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Tarsier2-Recap <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tarsier2</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">InternVideo2-Vid-Text* <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024internvideo2</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">SR-91K* <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ouyang2025sr_91k</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">VideoVista2-CoT <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024videovista</span>)</cite>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "videogptplus",
            "data",
            "timechatonline",
            "videovista2cot",
            "ouyang2025sr91k",
            "employed",
            "xie2024funqa",
            "used",
            "videochat2it",
            "egotaskqa",
            "video",
            "sft",
            "vript",
            "tarsier2",
            "cinepile",
            "stage",
            "only",
            "sr91k",
            "yang2024vript",
            "charadessta",
            "mvbench",
            "indicates",
            "sharegpt4video",
            "wang2024internvideo2",
            "tarsier2recap",
            "li2024videovista",
            "neptune",
            "during",
            "llavavideo178k",
            "sf20k",
            "training",
            "internvideo2vidtext",
            "valleypretrain",
            "finevideo",
            "dataset",
            "videovistatrain",
            "pretrain",
            "jia2022egotaskqa",
            "list",
            "videovistaevent",
            "subset",
            "valley",
            "annealing",
            "funqa",
            "neptune24",
            "our",
            "chen2024sharegpt4video"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "only",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At training and data recipes, we introduce <span class=\"ltx_text ltx_font_bold\">the following training recipe with the data matching</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S3\" title=\"3 Training and Data Recipes &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) :</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Language-Centric Hybrid Training for Multimodal Understanding and Generation</span>: To bridge the gap between multimodal understanding and generation tasks&#8212;often treated separately during training&#8212;we propose a hybrid training approach anchored in language generation tasks. By unifying tasks such as image editing, image generation, and speech synthesis within a language generation framework, we break down the inherent barriers between understanding and generation, enabling synergistic enhancement and bidirectional empowerment of both capabilities.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "only",
                    "data",
                    "training",
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "during",
                    "only",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central difficulty in training vanilla MoE models arises from the non-differentiable Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> operation used in expert selection, which obstructs gradient propagation to the router and leads to biased optimization. To address this, we migrate the gradient estimation strategy proposed in Grin-MoE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024gringradientinformedmoe</span>)</cite>, which integrates straight-through gradient estimators under the framework of numerical methods for ordinary differential equations (ODEs). This enables end-to-end optimization of both router and experts under sparse activation constraints, improving the stability of router training and allowing more precise token-to-expert assignments. A formal description is given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.SS4\" title=\"A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a holistic view of our method, Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the complete training procedure in pseudocode form.\nFor simplicity of presentation, we treat null experts as a special case of routed experts in the pseudocode.\nThe algorithm integrates the three proposed components into a unified workflow:\nFirst, <span class=\"ltx_text ltx_font_typewriter\">ActivateExperts</span> function determines expert activation, combining the Top&#8209;P sampling strategy for routed experts with the persistent activation constraint for shared experts;\nSecond, gradient estimation is applied to routed expert outputs to enable end&#8209;to&#8209;end optimization under sparse activation;\nFinally, after computing routed expert contributions, shared expert outputs are computed and incorporated into the MoE representation, injecting general knowledge into the final output.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset",
                    "pretrain",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we collected a large-scale multimodal instruction-following dataset to enable the model to understand and process any cross-modal information. The training comprises two phases:</p>\n\n",
                "matched_terms": [
                    "stage",
                    "dataset",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "annealing",
                    "subset",
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "used",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the vision modality pre-training stage, the visual data primarily comprised image&#8211;caption and video&#8211;caption datasets. The image&#8211;caption corpus contained approximately 17 million samples (13 billion tokens), whereas the video&#8211;caption data represented a smaller portion, with around 0.1 million samples (0.2 billion tokens).</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "video",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "only",
                    "data",
                    "during",
                    "annealing",
                    "subset",
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the task projector alignment stage and the visual projector alignment stage, we utilized data encompassing a variety of task types. This included image generation, image editing, low-level computer vision tasks such as deraining and denoising, and conditional generation like Canny-to-image. In total, this data amounted to approximately 2M samples (1.5B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the caption projector alignment stage, we used high-quality image-caption pairs sourced from a diverse range of data types. This collection included factual, human-annotated descriptions of everyday scenes; detailed, context-rich prompts for complex image generation; and elaborate narratives describing the compositional and spatial relationships between elements. This data accounted for a total of 4.2M samples (3.2B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "used",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated and filtered a large-scale dataset from multiple open-source dataset collections, amounting to approximately 10.5M samples (8.1B tokens). This stage aimed to unify the model&#8217;s capabilities under a consistent instruction-following framework. The training data was formatted as instruction-response pairs covering the same core task categories mentioned previously: image generation, image editing, low-level CV enhancements, and conditional image synthesis.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "stage",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "our",
                    "mvbench",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "our",
                    "only",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "only",
                    "data",
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "during",
                    "only",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "indicates",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "used",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "our",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "during",
                    "only",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify that the model&#8217;s enhanced omnimodality understanding capability stems primarily from its audio-text-vision joint coding training on large-scale video data. Future work will focus on scaling this video data and introducing new multimodal positional encoding methods to advance the model&#8217;s comprehension abilities further.</p>\n\n",
                "matched_terms": [
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "indicates",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "data",
                    "during",
                    "annealing",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to express our sincere gratitude to Qi Wang, Qixun Teng, Feifan Wen, and Zitao Li for their contributions to data collection and demo recording. We are thankful to Jinchao Li and Tongshu Bian for their assistance in paper proofreading and video production. We also thank Longyue Wang and Wenhan Luo for their valuable suggestions on paper writing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All open-source training data are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T15\" title=\"Table 15 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T18\" title=\"Table 18 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Short Video Understanding</span>. MVBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mvbench</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "mvbench",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video Temporal Localization</span>. Charades-STA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">charades-sta</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "charadessta",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hybrid gradient estimator.</span>\nTo balance the stability of router training with the diversity of expert learning, we combine the two estimators above: the first-order gradient is used when the selected expert corresponds to <math alttext=\"\\arg\\max({\\bm{z}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m1\" intent=\":literal\"><semantics><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\arg\\max({\\bm{z}})</annotation></semantics></math>, while the third-order gradient is applied otherwise.\nLet <math alttext=\"\\delta_{\\bm{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#948;</mi><mi>&#119915;</mi></msub><annotation encoding=\"application/x-tex\">\\delta_{\\bm{D}}</annotation></semantics></math> denote the indicator <math alttext=\"\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119915;</mi><mo>=</mo><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))</annotation></semantics></math>. The combined estimator can be written as:\n\n</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            }
        ]
    },
    "A1.T17": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 17: The list of audio data used during our training.",
        "body": "Stage\nDataset\n\n\nPretrain\nMultilingual-LibriSpeech-English (mls)\n\n\n\nGigaSpeech-L (giga)\n\n\n\nCommonVoice-English (commonvoice)\n\n\n\nWavCaps (wavcaps)\n\n\n\nClothoV1 (clotho)\n\n\n\nMELD (meld)\n\n\n\nMusicBench (musicbench)\n\n\n\nLP-MusicCaps (lpmusic)\n\n\n\n\n\n\nSFT\n\n&\n\nAnnealing\n \nCommonVoice-English (commonvoice)\n\n\n\nWavCaps (wavcaps)\n\n\n\nClothoV1 (clotho)\n\n\n\nMELD (meld)\n\n\n\nMusicBench (musicbench)\n\n\n\nLP-MusicCaps (lpmusic)\n\n\n\nClothoAQA (clothoaqa)\n\n\n\nAudioCaps (audiocaps)\n\n\n\nMELD (meld)\n\n\n\nASVP-ESD (asvp_esd)\n\n\n\nCREMA-D (crema_d)\n\n\n\nEMOV (emov)\n\n\n\nESD (esd)\n\n\n\nJL-Coprus (jlcorpus)\n\n\n\nRAVDESS (ravdess)\n\n\n\nMusicInstruct (MusiLingo)\n\n\n\nLibriSpeech-Long (librispeech)\n\n\n\nRACE-Audio (race)\n\n\n\nAishell1 (aishell1)\n\n\n\nAishell3 (aishell3)\n\n\n\nMozilla-CommonVoice17 (commonvoice)\n\n\n\nPeoples-Speech (people)\n\n\n\nGigaSpeech-M (giga)\n\n\n\nLibriSpeech (librispeech)\n\n\n\nAVQA (avqa)\n\n\n\nMusic-AVQA (musicavqa)\n\n\n\nClothoV2 (clotho)\n\n\n\nUltra-Chat-Audio (ultra)\n\n\n\nBelle-Audio (belle)\n\n\n\nOpenhermes-Audio (OpenHermes)\n\n\n\nmoss-Audio (moss)\n\n\n\nalpaca-Audio (alpaca)\n\n\n\nLlava-150k-Audio (llava-pretrain-gen)\n\n\n\nPixmo-Audio (pixmo)\n\n\n\n\nLlava-video-180k-Audio (llavavid)\n\n\n\n\nTinyStories-en-Audio (tinystories)\n\n\n\n\nTinyStories-zh-Audio (tinystories)\n\n\n\n\nVCCM (vccm)\n\n\n\n\nSpeechCraft (speechcraft)\n\n\n\n\nStream-Omni-Instruct-Audio (stream)\n\n\n\n\nVoice-Assistant-Audio (miniomni)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Stage</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Dataset</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"8\"><span class=\"ltx_text ltx_font_italic\">Pretrain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Multilingual-LibriSpeech-English <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mls</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">GigaSpeech-L <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">giga</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">CommonVoice-English <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">commonvoice</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">WavCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wavcaps</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">ClothoV1 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clotho</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MELD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">meld</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MusicBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">musicbench</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LP-MusicCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lpmusic</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"34\"><span class=\"ltx_text ltx_font_italic\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">SFT</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">&amp;</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Annealing</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">CommonVoice-English <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">commonvoice</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">WavCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wavcaps</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">ClothoV1 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clotho</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MELD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">meld</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MusicBench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">musicbench</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LP-MusicCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lpmusic</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">ClothoAQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clothoaqa</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AudioCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audiocaps</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MELD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">meld</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">ASVP-ESD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">asvp_esd</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">CREMA-D <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">crema_d</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">EMOV <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">emov</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">ESD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">esd</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">JL-Coprus <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jlcorpus</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RAVDESS <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ravdess</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MusicInstruct <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">MusiLingo</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LibriSpeech-Long <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">librispeech</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RACE-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">race</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Aishell1 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">aishell1</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Aishell3 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">aishell3</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Mozilla-CommonVoice17 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">commonvoice</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Peoples-Speech <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">people</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">GigaSpeech-M <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">giga</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">librispeech</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AVQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">avqa</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Music-AVQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">musicavqa</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">ClothoV2 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clotho</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Ultra-Chat-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ultra</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Belle-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">belle</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Openhermes-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">OpenHermes</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">moss-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">moss</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">alpaca-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">alpaca</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Llava-150k-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llava-pretrain-gen</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Pixmo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pixmo</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">Llava-video-180k-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llavavid</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">TinyStories-en-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tinystories</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">TinyStories-zh-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tinystories</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">VCCM <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vccm</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">SpeechCraft <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">speechcraft</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">Stream-Omni-Instruct-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stream</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Voice-Assistant-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">miniomni</span>)</cite>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "ravdess",
            "race",
            "belleaudio",
            "jlcorpus",
            "musicbench",
            "clothoaqa",
            "musicavqa",
            "speechcraft",
            "vccm",
            "data",
            "commonvoice",
            "mossaudio",
            "musicinstruct",
            "esd",
            "giga",
            "used",
            "raceaudio",
            "sft",
            "gigaspeechl",
            "llavavideo180kaudio",
            "peoplesspeech",
            "stage",
            "audiocaps",
            "wavcaps",
            "aishell3",
            "lpmusiccaps",
            "llava150kaudio",
            "avqa",
            "pixmo",
            "stream",
            "llavapretraingen",
            "audio",
            "ultra",
            "clothov2",
            "voiceassistantaudio",
            "gigaspeechm",
            "musilingo",
            "librispeechlong",
            "streamomniinstructaudio",
            "openhermesaudio",
            "mozillacommonvoice17",
            "jlcoprus",
            "alpaca",
            "pixmoaudio",
            "during",
            "miniomni",
            "mls",
            "lpmusic",
            "moss",
            "emov",
            "training",
            "tinystories",
            "clotho",
            "dataset",
            "people",
            "pretrain",
            "aishell1",
            "librispeech",
            "multilinguallibrispeechenglish",
            "ultrachataudio",
            "list",
            "llavavid",
            "clothov1",
            "openhermes",
            "annealing",
            "commonvoiceenglish",
            "belle",
            "tinystoriesenaudio",
            "asvpesd",
            "our",
            "alpacaaudio",
            "tinystorieszhaudio",
            "cremad",
            "meld"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At training and data recipes, we introduce <span class=\"ltx_text ltx_font_bold\">the following training recipe with the data matching</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S3\" title=\"3 Training and Data Recipes &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) :</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Language-Centric Hybrid Training for Multimodal Understanding and Generation</span>: To bridge the gap between multimodal understanding and generation tasks&#8212;often treated separately during training&#8212;we propose a hybrid training approach anchored in language generation tasks. By unifying tasks such as image editing, image generation, and speech synthesis within a language generation framework, we break down the inherent barriers between understanding and generation, enabling synergistic enhancement and bidirectional empowerment of both capabilities.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "during",
                    "audio",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{W}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>W</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{W}^{A}</annotation></semantics></math> is the last hidden states of the pre-trained audio encoder adopted from Whisper-large-v3. <span class=\"ltx_text ltx_markedasmath\">MSA</span> and <math alttext=\"h\\text{MCA}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>MCA</mtext></mrow><annotation encoding=\"application/x-tex\">h\\text{MCA}</annotation></semantics></math> denote the multihead self-attention and cross-attention operations, and <span class=\"ltx_text ltx_markedasmath\">LN</span> is the layer norm function. By leveraging the fixed-length query vectors <math alttext=\"X_{Q}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>Q</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">X_{Q}^{A}</annotation></semantics></math>, each 30 seconds of audio encoded by the Whisper encoder is mapped to <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> audio tokens. <math alttext=\"h_{C}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>C</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{C}^{A}</annotation></semantics></math> represents the output of the cross-attention module, which is used to distill the main content of the input audio. After going through all layers of the Whisper decoder, we apply a learnable linear layer for projecting the last output into the representation space of LLM.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central difficulty in training vanilla MoE models arises from the non-differentiable Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> operation used in expert selection, which obstructs gradient propagation to the router and leads to biased optimization. To address this, we migrate the gradient estimation strategy proposed in Grin-MoE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024gringradientinformedmoe</span>)</cite>, which integrates straight-through gradient estimators under the framework of numerical methods for ordinary differential equations (ODEs). This enables end-to-end optimization of both router and experts under sparse activation constraints, improving the stability of router training and allowing more precise token-to-expert assignments. A formal description is given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.SS4\" title=\"A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a holistic view of our method, Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the complete training procedure in pseudocode form.\nFor simplicity of presentation, we treat null experts as a special case of routed experts in the pseudocode.\nThe algorithm integrates the three proposed components into a unified workflow:\nFirst, <span class=\"ltx_text ltx_font_typewriter\">ActivateExperts</span> function determines expert activation, combining the Top&#8209;P sampling strategy for routed experts with the persistent activation constraint for shared experts;\nSecond, gradient estimation is applied to routed expert outputs to enable end&#8209;to&#8209;end optimization under sparse activation;\nFinally, after computing routed expert contributions, shared expert outputs are computed and incorporated into the MoE representation, injecting general knowledge into the final output.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "pretrain",
                    "training",
                    "our",
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "during",
                    "audio",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we collected a large-scale multimodal instruction-following dataset to enable the model to understand and process any cross-modal information. The training comprises two phases:</p>\n\n",
                "matched_terms": [
                    "stage",
                    "dataset",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "during",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the pre-training phase of the audio understanding modality, the audio data primarily comprises automatic speech recognition (ASR), audio-caption, and music-caption datasets. Among all pre-training data, ASR datasets constitute the majority, encompassing approximately 15B tokens. In contrast, audio-caption and music-caption datasets are more challenging to acquire than ASR datasets, with a combined total of merely 1B tokens.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "during",
                    "training",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "annealing",
                    "training",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "training",
                    "audio",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the vision modality pre-training stage, the visual data primarily comprised image&#8211;caption and video&#8211;caption datasets. The image&#8211;caption corpus contained approximately 17 million samples (13 billion tokens), whereas the video&#8211;caption data represented a smaller portion, with around 0.1 million samples (0.2 billion tokens).</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "during",
                    "annealing",
                    "training",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the task projector alignment stage and the visual projector alignment stage, we utilized data encompassing a variety of task types. This included image generation, image editing, low-level computer vision tasks such as deraining and denoising, and conditional generation like Canny-to-image. In total, this data amounted to approximately 2M samples (1.5B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the caption projector alignment stage, we used high-quality image-caption pairs sourced from a diverse range of data types. This collection included factual, human-annotated descriptions of everyday scenes; detailed, context-rich prompts for complex image generation; and elaborate narratives describing the compositional and spatial relationships between elements. This data accounted for a total of 4.2M samples (3.2B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "used",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated and filtered a large-scale dataset from multiple open-source dataset collections, amounting to approximately 10.5M samples (8.1B tokens). This stage aimed to unify the model&#8217;s capabilities under a consistent instruction-following framework. The training data was formatted as instruction-response pairs covering the same core task categories mentioned previously: image generation, image editing, low-level CV enhancements, and conditional image synthesis.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "stage",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio",
                    "raceaudio",
                    "aishell1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "data",
                    "during",
                    "training",
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "during",
                    "audio",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "used",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "our",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify that the model&#8217;s enhanced omnimodality understanding capability stems primarily from its audio-text-vision joint coding training on large-scale video data. Future work will focus on scaling this video data and introducing new multimodal positional encoding methods to advance the model&#8217;s comprehension abilities further.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "data",
                    "during",
                    "annealing",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to express our sincere gratitude to Qi Wang, Qixun Teng, Feifan Wen, and Zitao Li for their contributions to data collection and demo recording. We are thankful to Jinchao Li and Tongshu Bian for their assistance in paper proofreading and video production. We also thank Longyue Wang and Wenhan Luo for their valuable suggestions on paper writing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All open-source training data are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T15\" title=\"Table 15 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T18\" title=\"Table 18 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "our",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio X &#8594; Text</span>\nRACE-audio(middle/high) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">race</span>)</cite>, EHSL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite>, MELD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">meld</span>)</cite>, MMAU <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmau</span>)</cite>, ClothoAQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clothoaqa</span>)</cite>, ClothoV1 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clotho</span>)</cite>, ClothoV2 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">clotho</span>)</cite>, AudioCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">audiocaps</span>)</cite>, and MusicCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lpmusic</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "clothov2",
                    "race",
                    "clothoaqa",
                    "audiocaps",
                    "clothov1",
                    "lpmusic",
                    "audio",
                    "clotho",
                    "meld"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hybrid gradient estimator.</span>\nTo balance the stability of router training with the diversity of expert learning, we combine the two estimators above: the first-order gradient is used when the selected expert corresponds to <math alttext=\"\\arg\\max({\\bm{z}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m1\" intent=\":literal\"><semantics><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\arg\\max({\\bm{z}})</annotation></semantics></math>, while the third-order gradient is applied otherwise.\nLet <math alttext=\"\\delta_{\\bm{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#948;</mi><mi>&#119915;</mi></msub><annotation encoding=\"application/x-tex\">\\delta_{\\bm{D}}</annotation></semantics></math> denote the indicator <math alttext=\"\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119915;</mi><mo>=</mo><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))</annotation></semantics></math>. The combined estimator can be written as:\n\n</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            }
        ]
    },
    "A1.T18": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 18: The list of video data used during our training.",
        "body": "Stage\nDataset\n\n\n\n\n\nSFT\n\n&\n\nAnnealing\n \nOpenOrca-GPT4 (OpenOrca)\n\n\n\nOpenOrca-Chinese-GPT4 (OpenOrca)\n\n\n\nDAPO-Math (yu2025dapo)\n\n\n\nNemotron-Post-Training-Dataset-v1 (Nemotron)\n\n\n\nMixture-of-Thoughts (Mixture-of-Thoughts)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Stage</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Dataset</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"5\"><span class=\"ltx_text ltx_font_italic\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">SFT</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">&amp;</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Annealing</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">OpenOrca-GPT4 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">OpenOrca</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">OpenOrca-Chinese-GPT4 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">OpenOrca</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DAPO-Math <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2025dapo</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Nemotron-Post-Training-Dataset-v1 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Nemotron</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Mixture-of-Thoughts <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Mixture-of-Thoughts</span>)</cite>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "data",
            "openorcachinesegpt4",
            "mixtureofthoughts",
            "used",
            "sft",
            "video",
            "yu2025dapo",
            "nemotronposttrainingdatasetv1",
            "stage",
            "dapomath",
            "openorcagpt4",
            "during",
            "nemotron",
            "training",
            "dataset",
            "list",
            "openorca",
            "annealing",
            "our"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">All open-source training data are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T15\" title=\"Table 15 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T18\" title=\"Table 18 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At training and data recipes, we introduce <span class=\"ltx_text ltx_font_bold\">the following training recipe with the data matching</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S3\" title=\"3 Training and Data Recipes &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) :</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Language-Centric Hybrid Training for Multimodal Understanding and Generation</span>: To bridge the gap between multimodal understanding and generation tasks&#8212;often treated separately during training&#8212;we propose a hybrid training approach anchored in language generation tasks. By unifying tasks such as image editing, image generation, and speech synthesis within a language generation framework, we break down the inherent barriers between understanding and generation, enabling synergistic enhancement and bidirectional empowerment of both capabilities.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central difficulty in training vanilla MoE models arises from the non-differentiable Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> operation used in expert selection, which obstructs gradient propagation to the router and leads to biased optimization. To address this, we migrate the gradient estimation strategy proposed in Grin-MoE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024gringradientinformedmoe</span>)</cite>, which integrates straight-through gradient estimators under the framework of numerical methods for ordinary differential equations (ODEs). This enables end-to-end optimization of both router and experts under sparse activation constraints, improving the stability of router training and allowing more precise token-to-expert assignments. A formal description is given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.SS4\" title=\"A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a holistic view of our method, Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the complete training procedure in pseudocode form.\nFor simplicity of presentation, we treat null experts as a special case of routed experts in the pseudocode.\nThe algorithm integrates the three proposed components into a unified workflow:\nFirst, <span class=\"ltx_text ltx_font_typewriter\">ActivateExperts</span> function determines expert activation, combining the Top&#8209;P sampling strategy for routed experts with the persistent activation constraint for shared experts;\nSecond, gradient estimation is applied to routed expert outputs to enable end&#8209;to&#8209;end optimization under sparse activation;\nFinally, after computing routed expert contributions, shared expert outputs are computed and incorporated into the MoE representation, injecting general knowledge into the final output.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we collected a large-scale multimodal instruction-following dataset to enable the model to understand and process any cross-modal information. The training comprises two phases:</p>\n\n",
                "matched_terms": [
                    "stage",
                    "dataset",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "annealing",
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "used",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the vision modality pre-training stage, the visual data primarily comprised image&#8211;caption and video&#8211;caption datasets. The image&#8211;caption corpus contained approximately 17 million samples (13 billion tokens), whereas the video&#8211;caption data represented a smaller portion, with around 0.1 million samples (0.2 billion tokens).</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "video",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "during",
                    "annealing",
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the task projector alignment stage and the visual projector alignment stage, we utilized data encompassing a variety of task types. This included image generation, image editing, low-level computer vision tasks such as deraining and denoising, and conditional generation like Canny-to-image. In total, this data amounted to approximately 2M samples (1.5B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the caption projector alignment stage, we used high-quality image-caption pairs sourced from a diverse range of data types. This collection included factual, human-annotated descriptions of everyday scenes; detailed, context-rich prompts for complex image generation; and elaborate narratives describing the compositional and spatial relationships between elements. This data accounted for a total of 4.2M samples (3.2B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "used",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated and filtered a large-scale dataset from multiple open-source dataset collections, amounting to approximately 10.5M samples (8.1B tokens). This stage aimed to unify the model&#8217;s capabilities under a consistent instruction-following framework. The training data was formatted as instruction-response pairs covering the same core task categories mentioned previously: image generation, image editing, low-level CV enhancements, and conditional image synthesis.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "stage",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "annealing",
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "used",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "our",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify that the model&#8217;s enhanced omnimodality understanding capability stems primarily from its audio-text-vision joint coding training on large-scale video data. Future work will focus on scaling this video data and introducing new multimodal positional encoding methods to advance the model&#8217;s comprehension abilities further.</p>\n\n",
                "matched_terms": [
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "data",
                    "during",
                    "annealing",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to express our sincere gratitude to Qi Wang, Qixun Teng, Feifan Wen, and Zitao Li for their contributions to data collection and demo recording. We are thankful to Jinchao Li and Tongshu Bian for their assistance in paper proofreading and video production. We also thank Longyue Wang and Wenhan Luo for their valuable suggestions on paper writing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hybrid gradient estimator.</span>\nTo balance the stability of router training with the diversity of expert learning, we combine the two estimators above: the first-order gradient is used when the selected expert corresponds to <math alttext=\"\\arg\\max({\\bm{z}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m1\" intent=\":literal\"><semantics><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\arg\\max({\\bm{z}})</annotation></semantics></math>, while the third-order gradient is applied otherwise.\nLet <math alttext=\"\\delta_{\\bm{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#948;</mi><mi>&#119915;</mi></msub><annotation encoding=\"application/x-tex\">\\delta_{\\bm{D}}</annotation></semantics></math> denote the indicator <math alttext=\"\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119915;</mi><mo>=</mo><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))</annotation></semantics></math>. The combined estimator can be written as:\n\n</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            }
        ]
    },
    "A1.T19": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 19: The list of visual generation data used during our training. An asterisk (*) indicates that only a subset of the dataset was employed.",
        "body": "Task\nDataset\n\n\n\n\n\nImage Generation\n\n(4.81M)\n \nFlickr30k (plummer2015flickr30k)\n\n\n\nCoco* (lin2014microsoft)\n\n\n\nLLaVA-Pretrain (llava-pretrain-gen)\n\n\n\nJourneyDB* (sun2023journeydb)\n\n\n\nReasonGen (zhang2025reasongen)\n\n\n\nOpenGPT-4o-Image (chen2025opengpt)\n\n\n\nScaleCap (xing2025scalecap)\n\n\n\n\nTextAtlas5M* (wang2025textatlas5m)\n\n\n\n\n\n\nImage Edition\n\n(5.68M)\n \nInstructPix2pix* (brooks2023instructpix2pix)\n\n\n\nMagicBrush (zhang2023magicbrush)\n\n\n\nHQ-Edit (hui2024hq)\n\n\n\nUltraEdit* (zhao2024ultraedit)\n\n\n\nOpenGPT-4o-Image (chen2025opengpt)\n\n\n\nNHR-Edit (kuprashevich2025nohumansrequired)\n\n\n\n\n\n\nControllable Generation\n\n(0.50M)\n \nMultiGen-20M* (li2024controlnet++)\n\n\n\n\n\n\nImage Restoration\n\n(0.36M)\n \nDenseHaze (ancuti2019dense)\n\n\n\nNH-HAZE (ancuti2020nh)\n\n\n\nReside-6K (li2018benchmarking)\n\n\n\nWeather-Stream (zhang2023weatherstream)\n\n\n\nOutdoor-Rain (li2019heavy)\n\n\n\nRain1400 (fu2017removing)\n\n\n\nRainDS (quan2021removing)\n\n\n\nRainDrop (qian2018attentive)\n\n\n\nRealSnow (zhu2023learning)\n\n\n\nSnow100K (liu2018desnownet)\n\n\n\nLOL-v2 (yang2021sparse)\n\n\n\nCLWD (liu2021wdnet)\n\n\n\nDIV2K (agustsson2017ntire)\n\n\n\nFlickr2K (wang2023ntire)\n\n\n\nGoPro (nah2017deep)\n\n\n\nRealBlur (rim2020real)\n\n\n\nRealSR (cai2019toward)\n\n\n\nDPDD (abuolaim2020defocus)\n\n\n\nSIDD (abdelhamed2018high)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Task</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Dataset</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"7\"><span class=\"ltx_text ltx_font_italic\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Image Generation</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">(4.81M)</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Flickr30k <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">plummer2015flickr30k</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Coco* <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2014microsoft</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LLaVA-Pretrain <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">llava-pretrain-gen</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">JourneyDB* <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sun2023journeydb</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">ReasonGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025reasongen</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">OpenGPT-4o-Image <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025opengpt</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">ScaleCap <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xing2025scalecap</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\">TextAtlas5M* <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025textatlas5m</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"6\"><span class=\"ltx_text ltx_font_italic\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Image Edition</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">(5.68M)</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">InstructPix2pix* <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">brooks2023instructpix2pix</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MagicBrush <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023magicbrush</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">HQ-Edit <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hui2024hq</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">UltraEdit* <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2024ultraedit</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">OpenGPT-4o-Image <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025opengpt</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">NHR-Edit <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuprashevich2025nohumansrequired</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Controllable Generation</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">(0.50M)</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MultiGen-20M* <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024controlnet++</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"19\"><span class=\"ltx_text ltx_font_italic\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Image Restoration</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">(0.36M)</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">DenseHaze <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ancuti2019dense</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">NH-HAZE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ancuti2020nh</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Reside-6K <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2018benchmarking</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Weather-Stream <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023weatherstream</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Outdoor-Rain <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2019heavy</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Rain1400 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fu2017removing</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RainDS <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">quan2021removing</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RainDrop <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2018attentive</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RealSnow <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhu2023learning</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Snow100K <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2018desnownet</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LOL-v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2021sparse</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">CLWD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2021wdnet</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DIV2K <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">agustsson2017ntire</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Flickr2K <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023ntire</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">GoPro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nah2017deep</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RealBlur <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rim2020real</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RealSR <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cai2019toward</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DPDD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">abuolaim2020defocus</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">SIDD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">abdelhamed2018high</span>)</cite>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "rain1400",
            "snow100k",
            "abuolaim2020defocus",
            "magicbrush",
            "llavapretrain",
            "xing2025scalecap",
            "chen2025opengpt",
            "image",
            "quan2021removing",
            "brooks2023instructpix2pix",
            "li2019heavy",
            "nhhaze",
            "nhredit",
            "481m",
            "zhang2023weatherstream",
            "dataset",
            "subset",
            "nah2017deep",
            "realsnow",
            "asterisk",
            "wang2025textatlas5m",
            "generation",
            "plummer2015flickr30k",
            "hqedit",
            "050m",
            "div2k",
            "wang2023ntire",
            "task",
            "zhang2023magicbrush",
            "opengpt4oimage",
            "visual",
            "raindrop",
            "rainds",
            "clwd",
            "scalecap",
            "weatherstream",
            "abdelhamed2018high",
            "568m",
            "lolv2",
            "only",
            "multigen20m",
            "ancuti2019dense",
            "sun2023journeydb",
            "llavapretraingen",
            "indicates",
            "edition",
            "hui2024hq",
            "liu2018desnownet",
            "journeydb",
            "instructpix2pix",
            "flickr2k",
            "kuprashevich2025nohumansrequired",
            "fu2017removing",
            "cai2019toward",
            "dpdd",
            "coco",
            "realsr",
            "data",
            "employed",
            "gopro",
            "used",
            "reasongen",
            "agustsson2017ntire",
            "yang2021sparse",
            "flickr30k",
            "sidd",
            "qian2018attentive",
            "zhao2024ultraedit",
            "lin2014microsoft",
            "textatlas5m",
            "ancuti2020nh",
            "li2018benchmarking",
            "reside6k",
            "during",
            "zhang2025reasongen",
            "ultraedit",
            "realblur",
            "training",
            "li2024controlnet",
            "zhu2023learning",
            "036m",
            "controllable",
            "restoration",
            "densehaze",
            "list",
            "liu2021wdnet",
            "outdoorrain",
            "rim2020real",
            "our"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "data",
                    "image",
                    "training",
                    "our",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "only",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "task",
                    "generation",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "our",
                    "generation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "our",
                    "generation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Unified Modality Encoding &amp; Generation</span>: We design a unified speech encoder that maps diverse audio inputs, including environmental sound, speech, and music, into a shared representation space. For output, a Context-Aware MoE-based TTS module supports dynamic speech synthesis (especially for long speech) and interaction. On the visual side, we employ pre-trained visual encoders to process images and videos, and build a Task-Aware Diffusion Transformer for instruction-guided image generation and editing.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At training and data recipes, we introduce <span class=\"ltx_text ltx_font_bold\">the following training recipe with the data matching</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S3\" title=\"3 Training and Data Recipes &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) :</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Language-Centric Hybrid Training for Multimodal Understanding and Generation</span>: To bridge the gap between multimodal understanding and generation tasks&#8212;often treated separately during training&#8212;we propose a hybrid training approach anchored in language generation tasks. By unifying tasks such as image editing, image generation, and speech synthesis within a language generation framework, we break down the inherent barriers between understanding and generation, enabling synergistic enhancement and bidirectional empowerment of both capabilities.</p>\n\n",
                "matched_terms": [
                    "during",
                    "generation",
                    "image",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "data",
                    "image",
                    "training",
                    "our",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "only",
                    "task",
                    "generation",
                    "data",
                    "image",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Visual Encoder</span>. The visual encoder is initialized with SigLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhai2023sigmoid</span>)</cite> vision transformer, which transforms the input image or video frames <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i1.p1.m1\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> into visual features <math alttext=\"Z_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">Z_{V}</annotation></semantics></math>. We adopt the output of the last layer of the transformer as the visual features.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Mapping Network</span>. The mapping network is composed of a two-layer MLP and a 2D average pooling layer. Specifically, the MLP takes the visual features as input and projects them into the representation space of the language model, thereby producing a one-dimensional sequence of visual features <math alttext=\"H_{V}=p(Z_{V})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>V</mi></msub><mo>=</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>V</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">H_{V}=p(Z_{V})</annotation></semantics></math>. Subsequently, the average pooling layer performs length compression along both spatial dimensions, enabling more efficient training.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Multi-Image</span>. The encoding of multiple images follows a similar procedure to that of a single image. For each image, we independently search the most suitable target resolution, resize it accordingly, and then convert it into the corresponding set of visual patches. Assuming there are <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> images are provided as input, the total number of visual tokens is <math alttext=\"\\sum^{n}_{i=1}((a_{i}\\times b_{i})\\times T)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo lspace=\"0em\" stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>a</mi><mi>i</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>b</mi><mi>i</mi></msub></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sum^{n}_{i=1}((a_{i}\\times b_{i})\\times T)</annotation></semantics></math>, where <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math> and <math alttext=\"b_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m4\" intent=\":literal\"><semantics><msub><mi>b</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">b_{i}</annotation></semantics></math> denote the number of patches along the height and width of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th image.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This unified encoding of Single-Image, Multi-Image, and Video inputs into a one-dimensional sequence of visual representations enables the transfer of the model&#8217;s capability in high-resolution image understanding to the video domain, thereby facilitating faster convergence and improved performance in video understanding.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "during",
                    "only",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central difficulty in training vanilla MoE models arises from the non-differentiable Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> operation used in expert selection, which obstructs gradient propagation to the router and leads to biased optimization. To address this, we migrate the gradient estimation strategy proposed in Grin-MoE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024gringradientinformedmoe</span>)</cite>, which integrates straight-through gradient estimators under the framework of numerical methods for ordinary differential equations (ODEs). This enables end-to-end optimization of both router and experts under sparse activation constraints, improving the stability of router training and allowing more precise token-to-expert assignments. A formal description is given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.SS4\" title=\"A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a holistic view of our method, Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the complete training procedure in pseudocode form.\nFor simplicity of presentation, we treat null experts as a special case of routed experts in the pseudocode.\nThe algorithm integrates the three proposed components into a unified workflow:\nFirst, <span class=\"ltx_text ltx_font_typewriter\">ActivateExperts</span> function determines expert activation, combining the Top&#8209;P sampling strategy for routed experts with the persistent activation constraint for shared experts;\nSecond, gradient estimation is applied to routed expert outputs to enable end&#8209;to&#8209;end optimization under sparse activation;\nFinally, after computing routed expert contributions, shared expert outputs are computed and incorporated into the MoE representation, injecting general knowledge into the final output.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "our",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a Task-Aware Diffusion Transformer (Task-DiT) that bridges the gap between image understanding and generation. Unlike unified models that suffer from performance trade-offs, our framework preserves the strengths of specialized pre-trained models by connecting them via a lightweight, task-aware bridge.\nAt its core, Task-DiT employs a frozen image generator from PixWizard <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024pixwizard</span>)</cite> to safeguard its high-fidelity synthesis capabilities. To steer this generator, we introduce a query-based conditioning mechanism. Two distinct sets of learnable tokens are processed by a powerful understanding module:</p>\n\n",
                "matched_terms": [
                    "our",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Task Tokens (&lt;TASK[i]&gt;): Encode high-level commands (e.g., text-to-image, editing, low-level image processing) to specify the generative mode.</p>\n\n",
                "matched_terms": [
                    "task",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The enriched features from these tokens are then translated for the generator by dedicated, lightweight projectors. A Task Projector modulates the DiT&#8217;s denoising process based on the command, while a content projector transforms the &lt;IMG[i]&gt; features into a dense conditioning sequence for cross-attention. For image-guided tasks, a Visual Projector aligns source image features encoded by ViT with the DiT&#8217;s conditioning space.\nThis design creates a versatile and efficient channel for task-aware instruction, enabling high-quality, multi-modal image generation without the catastrophic interference typical of end-to-end fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "generation",
                    "image",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we collected a large-scale multimodal instruction-following dataset to enable the model to understand and process any cross-modal information. The training comprises two phases:</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Expert Warmup with Dense Model: We first pre-trained three specialized expert models focusing on mainstream modalities: speech comprehension, speech generation, and visual comprehension. This stage aims to build the model-specific experts for the following stable MoE-based fine-tuning.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "visual",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "during",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "image",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "during",
                    "generation",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "subset",
                    "image",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the vision modality pre-training stage, the visual data primarily comprised image&#8211;caption and video&#8211;caption datasets. The image&#8211;caption corpus contained approximately 17 million samples (13 billion tokens), whereas the video&#8211;caption data represented a smaller portion, with around 0.1 million samples (0.2 billion tokens).</p>\n\n",
                "matched_terms": [
                    "during",
                    "visual",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "image",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "only",
                    "data",
                    "during",
                    "subset",
                    "image",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the task projector alignment stage and the visual projector alignment stage, we utilized data encompassing a variety of task types. This included image generation, image editing, low-level computer vision tasks such as deraining and denoising, and conditional generation like Canny-to-image. In total, this data amounted to approximately 2M samples (1.5B tokens).</p>\n\n",
                "matched_terms": [
                    "visual",
                    "task",
                    "generation",
                    "data",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the caption projector alignment stage, we used high-quality image-caption pairs sourced from a diverse range of data types. This collection included factual, human-annotated descriptions of everyday scenes; detailed, context-rich prompts for complex image generation; and elaborate narratives describing the compositional and spatial relationships between elements. This data accounted for a total of 4.2M samples (3.2B tokens).</p>\n\n",
                "matched_terms": [
                    "used",
                    "generation",
                    "image",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated and filtered a large-scale dataset from multiple open-source dataset collections, amounting to approximately 10.5M samples (8.1B tokens). This stage aimed to unify the model&#8217;s capabilities under a consistent instruction-following framework. The training data was formatted as instruction-response pairs covering the same core task categories mentioned previously: image generation, image editing, low-level CV enhancements, and conditional image synthesis.</p>\n\n",
                "matched_terms": [
                    "task",
                    "generation",
                    "data",
                    "image",
                    "training",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "dataset",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "during",
                    "image",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "our",
                    "only",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "only",
                    "data",
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "only",
                    "task",
                    "data",
                    "during",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "our",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "our",
                    "visual",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "our",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "restoration",
                    "generation",
                    "image",
                    "our",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "our",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "edition",
                    "our",
                    "image",
                    "training",
                    "magicbrush"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "generation",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "restoration",
                    "image",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "our",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training",
                    "indicates",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "used",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "generation",
                    "image",
                    "our",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "our",
                    "generation",
                    "controllable",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "only",
                    "generation",
                    "during",
                    "image",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify that the model&#8217;s enhanced omnimodality understanding capability stems primarily from its audio-text-vision joint coding training on large-scale video data. Future work will focus on scaling this video data and introducing new multimodal positional encoding methods to advance the model&#8217;s comprehension abilities further.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "indicates",
                    "generation",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "restoration",
                    "generation",
                    "data",
                    "image",
                    "training",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to express our sincere gratitude to Qi Wang, Qixun Teng, Feifan Wen, and Zitao Li for their contributions to data collection and demo recording. We are thankful to Jinchao Li and Tongshu Bian for their assistance in paper proofreading and video production. We also thank Longyue Wang and Wenhan Luo for their valuable suggestions on paper writing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All open-source training data are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T15\" title=\"Table 15 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T18\" title=\"Table 18 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "our",
                    "visual",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "our",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "edition",
                    "restoration",
                    "visual",
                    "generation",
                    "image",
                    "our",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>. Wise <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">niu2025wise</span>)</cite> and Coco30K <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2014microsoft</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image",
                    "lin2014microsoft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>. GEdit-Bench-EN <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025step1x</span>)</cite>, Emu Edit Test <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sheynin2024emu</span>)</cite> and MagicBrush <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023magicbrush</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "magicbrush",
                    "edition",
                    "image",
                    "zhang2023magicbrush"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>. MultiGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024controlnet++</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "li2024controlnet",
                    "generation",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>. Rain100L <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2017deep</span>)</cite> and SIDD <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">abdelhamed2018high</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "restoration",
                    "sidd",
                    "abdelhamed2018high",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "our",
                    "visual",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hybrid gradient estimator.</span>\nTo balance the stability of router training with the diversity of expert learning, we combine the two estimators above: the first-order gradient is used when the selected expert corresponds to <math alttext=\"\\arg\\max({\\bm{z}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m1\" intent=\":literal\"><semantics><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\arg\\max({\\bm{z}})</annotation></semantics></math>, while the third-order gradient is applied otherwise.\nLet <math alttext=\"\\delta_{\\bm{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#948;</mi><mi>&#119915;</mi></msub><annotation encoding=\"application/x-tex\">\\delta_{\\bm{D}}</annotation></semantics></math> denote the indicator <math alttext=\"\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119915;</mi><mo>=</mo><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))</annotation></semantics></math>. The combined estimator can be written as:\n\n</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            }
        ]
    },
    "A1.T20": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 20: The list of video data used during our training. * indicates that only a subset of the dataset was employed.",
        "body": "Stage\nDataset\n\n\n\n\n\nCold\n\nStart\n \nMixture-of-Thoughts (Mixture-of-Thoughts)\n\n\n\nVision-R1-cold (vision-r1)\n\n\n\nMultimodal-Cold-Start (mmcoldstart)\n\n\n\nVideoVista-2-LongCoT (chen2025videovista_cultural)\n\n\n\nGSPO\nMMPR-Tiny* (wang2025internvl3)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">Stage</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Dataset</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text ltx_font_italic\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Cold</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Start</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Mixture-of-Thoughts <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Mixture-of-Thoughts</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Vision-R1-cold <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vision-r1</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Multimodal-Cold-Start <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmcoldstart</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">VideoVista-2-LongCoT <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2025videovista_cultural</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">GSPO</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">MMPR-Tiny* <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2025internvl3</span>)</cite>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "start",
            "multimodalcoldstart",
            "videovista2longcot",
            "data",
            "employed",
            "mixtureofthoughts",
            "used",
            "visionr1cold",
            "mmprtiny",
            "video",
            "cold",
            "stage",
            "only",
            "visionr1",
            "wang2025internvl3",
            "indicates",
            "chen2025videovistacultural",
            "during",
            "training",
            "mmcoldstart",
            "dataset",
            "gspo",
            "list",
            "subset",
            "our"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "only",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At training and data recipes, we introduce <span class=\"ltx_text ltx_font_bold\">the following training recipe with the data matching</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S3\" title=\"3 Training and Data Recipes &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) :</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Language-Centric Hybrid Training for Multimodal Understanding and Generation</span>: To bridge the gap between multimodal understanding and generation tasks&#8212;often treated separately during training&#8212;we propose a hybrid training approach anchored in language generation tasks. By unifying tasks such as image editing, image generation, and speech synthesis within a language generation framework, we break down the inherent barriers between understanding and generation, enabling synergistic enhancement and bidirectional empowerment of both capabilities.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "only",
                    "data",
                    "training",
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "during",
                    "only",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As an illustrative example, consider a video input of length 120 seconds accompanied by audio. We place the visual sequence before the audio sequence and sample the video at a rate of one frame every two seconds. Suppose the RoPE ID of the last text token preceding the visual sequence is <math alttext=\"(x-1,x-1,x-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x-1,x-1,x-1)</annotation></semantics></math>. Then, the RoPE IDs of the first video frame start from <math alttext=\"(x,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x,x)</annotation></semantics></math> and increment row by row and column by column up to <math alttext=\"(x,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> denotes the number of tokens along both the height and width of a video frame. The second frame begins with <math alttext=\"(x+2\\theta,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x,x)</annotation></semantics></math> and increases to <math alttext=\"(x+2\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> is a specific scaling factor for absolute time. Following this procedure, the final frame in the video sequence has an ID of <math alttext=\"(x+118\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>118</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+118\\theta,x+p,x+p)</annotation></semantics></math>.\nSimilarly, assume the RoPE ID of the last token before the audio sequence is <math alttext=\"(y-1,y-1,y-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y-1,y-1,y-1)</annotation></semantics></math>. The first audio segment is then assigned the ID <math alttext=\"(y,y,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y,y,y)</annotation></semantics></math>, which is repeated 20 times to represent the minimum audio unit. The next audio segment receives the ID <math alttext=\"(y+3\\theta,y+3\\theta,y+3\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+3\\theta,y+3\\theta,y+3\\theta)</annotation></semantics></math>. Continuing this process, the final audio segment is assigned the ID <math alttext=\"(y+117\\theta,y+117\\theta,y+117\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+117\\theta,y+117\\theta,y+117\\theta)</annotation></semantics></math>. In this manner, temporal alignment between the visual and audio sequences can also be achieved.</p>\n\n",
                "matched_terms": [
                    "start",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central difficulty in training vanilla MoE models arises from the non-differentiable Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> operation used in expert selection, which obstructs gradient propagation to the router and leads to biased optimization. To address this, we migrate the gradient estimation strategy proposed in Grin-MoE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024gringradientinformedmoe</span>)</cite>, which integrates straight-through gradient estimators under the framework of numerical methods for ordinary differential equations (ODEs). This enables end-to-end optimization of both router and experts under sparse activation constraints, improving the stability of router training and allowing more precise token-to-expert assignments. A formal description is given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.SS4\" title=\"A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a holistic view of our method, Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the complete training procedure in pseudocode form.\nFor simplicity of presentation, we treat null experts as a special case of routed experts in the pseudocode.\nThe algorithm integrates the three proposed components into a unified workflow:\nFirst, <span class=\"ltx_text ltx_font_typewriter\">ActivateExperts</span> function determines expert activation, combining the Top&#8209;P sampling strategy for routed experts with the persistent activation constraint for shared experts;\nSecond, gradient estimation is applied to routed expert outputs to enable end&#8209;to&#8209;end optimization under sparse activation;\nFinally, after computing routed expert contributions, shared expert outputs are computed and incorporated into the MoE representation, injecting general knowledge into the final output.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we collected a large-scale multimodal instruction-following dataset to enable the model to understand and process any cross-modal information. The training comprises two phases:</p>\n\n",
                "matched_terms": [
                    "stage",
                    "dataset",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data",
                    "subset",
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "used",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the vision modality pre-training stage, the visual data primarily comprised image&#8211;caption and video&#8211;caption datasets. The image&#8211;caption corpus contained approximately 17 million samples (13 billion tokens), whereas the video&#8211;caption data represented a smaller portion, with around 0.1 million samples (0.2 billion tokens).</p>\n\n",
                "matched_terms": [
                    "during",
                    "stage",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "video",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "only",
                    "data",
                    "during",
                    "subset",
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the task projector alignment stage and the visual projector alignment stage, we utilized data encompassing a variety of task types. This included image generation, image editing, low-level computer vision tasks such as deraining and denoising, and conditional generation like Canny-to-image. In total, this data amounted to approximately 2M samples (1.5B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the caption projector alignment stage, we used high-quality image-caption pairs sourced from a diverse range of data types. This collection included factual, human-annotated descriptions of everyday scenes; detailed, context-rich prompts for complex image generation; and elaborate narratives describing the compositional and spatial relationships between elements. This data accounted for a total of 4.2M samples (3.2B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "used",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated and filtered a large-scale dataset from multiple open-source dataset collections, amounting to approximately 10.5M samples (8.1B tokens). This stage aimed to unify the model&#8217;s capabilities under a consistent instruction-following framework. The training data was formatted as instruction-response pairs covering the same core task categories mentioned previously: image generation, image editing, low-level CV enhancements, and conditional image synthesis.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "stage",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "our",
                    "only",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "only",
                    "data",
                    "during",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "during",
                    "only",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "our",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tracked expert activation at four representative layers (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F9\" title=\"Figure 9 &#8227; Computational Cost &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>). Expert usage remains stable in the shallowest (Layer 0) and deepest (Layer 27) layers, while middle layers (Layers 9 and 18) show clear changes during training. This indicates that most refinement of expert specialization happens in the middle of the network, where both routing patterns and expert parameters are actively optimized. A key trend in these middle layers is the steady increase in activation of the null expert (Expert 5), meaning the model learns to skip tokens that no longer need processing at intermediate stages. This behavior confirms the value of the null expert for efficient computation and shows that our training strategy effectively improves inference efficiency.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "indicates",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "used",
                    "start",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "gspo",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "our",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "our",
                    "training",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "during",
                    "only",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify that the model&#8217;s enhanced omnimodality understanding capability stems primarily from its audio-text-vision joint coding training on large-scale video data. Future work will focus on scaling this video data and introducing new multimodal positional encoding methods to advance the model&#8217;s comprehension abilities further.</p>\n\n",
                "matched_terms": [
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "indicates",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "during",
                    "training",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "video",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to express our sincere gratitude to Qi Wang, Qixun Teng, Feifan Wen, and Zitao Li for their contributions to data collection and demo recording. We are thankful to Jinchao Li and Tongshu Bian for their assistance in paper proofreading and video production. We also thank Longyue Wang and Wenhan Luo for their valuable suggestions on paper writing.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All open-source training data are presented in Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T15\" title=\"Table 15 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">15</span></a>-<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T18\" title=\"Table 18 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "our",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hybrid gradient estimator.</span>\nTo balance the stability of router training with the diversity of expert learning, we combine the two estimators above: the first-order gradient is used when the selected expert corresponds to <math alttext=\"\\arg\\max({\\bm{z}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m1\" intent=\":literal\"><semantics><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\arg\\max({\\bm{z}})</annotation></semantics></math>, while the third-order gradient is applied otherwise.\nLet <math alttext=\"\\delta_{\\bm{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#948;</mi><mi>&#119915;</mi></msub><annotation encoding=\"application/x-tex\">\\delta_{\\bm{D}}</annotation></semantics></math> denote the indicator <math alttext=\"\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119915;</mi><mo>=</mo><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))</annotation></semantics></math>. The combined estimator can be written as:\n\n</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            }
        ]
    },
    "A1.T21": {
        "source_file": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data",
        "caption": "Table 21: The prompt setting for thinking models.",
        "body": "Prompt For Visual Understanding\n\n\n\n\n\n\nSYSTEM: You are Uni-MoE-v2, a helpful multi-modal model. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analyzing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion. Now, try to solve the following question through the above guidelines.\n\n\n\n\n\n\nUSER: Question: {question}\n\n\n\n\n\n\nPrompt For Visual Generation\n\n\n\n\n\n\nSYSTEM: You should first think step by step about how to construct the image, including background, objects, colors, lighting, and style. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think>1. Position a man seated indoors, capturing him from the upper chest to just above the chin, focusing on  </think> <answer> Here is the image: [TASK0][TASK1][TASK2][IMG0][IMG1][IMG31] </answer>, which means your output should start with <think> and end with </answer>.\n\n\n\n\n\n\nUSER: Image generation: {prompt}",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:341.4pt;\">Prompt For Visual Understanding</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:341.4pt;\"><span class=\"ltx_text ltx_font_bold\">SYSTEM:</span> You are Uni-MoE-v2, a helpful multi-modal model. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: &lt;think&gt; Thought section &lt;/think&gt; Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analyzing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion. Now, try to solve the following question through the above guidelines.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:341.4pt;\"><span class=\"ltx_text ltx_font_bold\">USER:</span> Question: {question}</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:341.4pt;\">Prompt For Visual Generation</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:341.4pt;\"><span class=\"ltx_text ltx_font_bold\">SYSTEM:</span> You should first think step by step about how to construct the image, including background, objects, colors, lighting, and style. The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt;1. Position a man seated indoors, capturing him from the upper chest to just above the chin, focusing on &#8230; &lt;/think&gt; &lt;answer&gt; Here is the image: [TASK0][TASK1][TASK2][IMG0][IMG1]&#8230;&#8230;[IMG31] &lt;/answer&gt;, which means your output should start with &lt;think&gt; and end with &lt;/answer&gt;.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:341.4pt;\"><span class=\"ltx_text ltx_font_bold\">USER:</span> Image generation: {prompt}</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "him",
            "considerations",
            "refining",
            "try",
            "reasoning",
            "chest",
            "findings",
            "errors",
            "ideas",
            "reflection",
            "reach",
            "image",
            "objects",
            "system",
            "helpful",
            "providing",
            "within",
            "seated",
            "previous",
            "brainstorming",
            "following",
            "tags",
            "engaging",
            "focusing",
            "models",
            "before",
            "iteration",
            "chin",
            "requires",
            "develop",
            "model",
            "construct",
            "lighting",
            "output",
            "reflections",
            "detailed",
            "respectively",
            "prompt",
            "please",
            "revisiting",
            "wellconsidered",
            "here",
            "end",
            "solve",
            "reassessment",
            "just",
            "capturing",
            "conclusion",
            "background",
            "accuracy",
            "cycle",
            "precise",
            "enclosed",
            "each",
            "current",
            "answer",
            "such",
            "means",
            "thoroughly",
            "step",
            "generation",
            "involves",
            "process",
            "role",
            "include",
            "final",
            "verifying",
            "accurate",
            "necessary",
            "start",
            "main",
            "visual",
            "upper",
            "logical",
            "explorations",
            "present",
            "question",
            "unimoev2",
            "exploring",
            "structure",
            "various",
            "correct",
            "guidelines",
            "needed",
            "attempts",
            "deem",
            "position",
            "analysis",
            "any",
            "two",
            "from",
            "questions",
            "your",
            "specified",
            "first",
            "man",
            "sections",
            "comprehensive",
            "user",
            "based",
            "through",
            "about",
            "setting",
            "style",
            "summarizing",
            "analyzing",
            "including",
            "assistant",
            "concise",
            "into",
            "response",
            "solutions",
            "thinking",
            "which",
            "new",
            "colors",
            "multimodal",
            "indoors",
            "think",
            "you",
            "exploration",
            "thought",
            "how",
            "systematic",
            "think1",
            "now",
            "systematically",
            "relevant",
            "backtracing",
            "solution",
            "above",
            "understanding",
            "task0task1task2img0img1img31",
            "detail",
            "steps",
            "format"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee&#8217;s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the dense LLM, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech.\n<span class=\"ltx_text ltx_font_italic\">Architecturally</span>, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer.\n<span class=\"ltx_text ltx_font_italic\">For training</span>, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning.\n<span class=\"ltx_text ltx_font_italic\">Data-wise</span>, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues.\nExtensive evaluation across 85 benchmarks demonstrates that our model achieves state-of-the-art or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (WER reduced by up to 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>\n\n",
                "matched_terms": [
                    "present",
                    "based",
                    "through",
                    "include",
                    "understanding",
                    "generation",
                    "following",
                    "new",
                    "image",
                    "multimodal",
                    "from",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pursuit of Artificial Intelligence (AI) has long been guided by a vision of creating systems that can perceive, reason, and interact with the world with human-like breadth and fluency.\nRecently, the field of artificial intelligence has been rapidly advancing towards omnimodal systems, e.g., Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Ming-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini25</span>)</cite>, and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpt4o_system</span>)</cite>, which aim to seamlessly integrate and process a diverse array of data types, including text, images, audio, and beyond, within a single, unified large model.\nThese omnimodal models represent a pivotal step towards more general and versatile AI, promising models capable of richer understanding and more natural interaction with the complex, multi-sensory world <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-04921</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2504-21277</span>)</cite>. This is not only for pushing the frontiers of AI research but also for enabling a wide range of real-world applications, from advanced human-computer interfaces <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/XieZCLZCHCSLLXZ24</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-12326</span>)</cite> and content creation tools <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/nips/SchickDDRLHZCS23</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-23278</span>)</cite> to super-intelligence AI assistants that can comprehend and reason across all modalities.</p>\n\n",
                "matched_terms": [
                    "within",
                    "step",
                    "models",
                    "including",
                    "which",
                    "understanding",
                    "process",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial forays, predominantly from well-resourced industrial labs <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ming_omni</span>)</cite>, have demonstrated impressive capabilities. However, the architectural and algorithmic path to a genuinely comprehensive omnimodal large model is fraught with complexity. <span class=\"ltx_text ltx_framed ltx_framed_underline\">A central challenge</span> lies in the inherent tension between two core competencies: deep context understanding and high-fidelity content generation. Many existing systems are architected with a bias, excelling either as powerful context understanding and reasoning (e.g., audio-only Qwen-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen_omni</span>)</cite>, Baichuan-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-15368</span>)</cite>) that lack generative faculties or as generative powerhouses (e.g., OmniGen <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2506-18871</span>)</cite>, BAGEL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025bagel</span>)</cite>, Janus-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2501-17811</span>)</cite>, Show-o <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/iclr/XieMBZWLGCYS25</span>)</cite>) confined to a narrow set of modalities.\nUnderlying this issue is the inefficient scaling of model capacity, where simply expanding dense transformers <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2307-10802</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_unimoe</span>)</cite> proves computationally prohibitive and difficult to balance dozens of task types with multiple modal interactions.\nThe challenge shows <span class=\"ltx_text ltx_framed ltx_framed_underline\">two critical shortcomings</span>: a lack of robust and efficient compositional reasoning architecture across multiple modalities, and significant instability when training on heterogeneous data at scale. Hence, the field still lacks a computationally efficient and unified architecture that achieves a synthesis of comprehensive understanding and versatile generation for all modalities.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation",
                    "comprehensive",
                    "two",
                    "from",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">This work is anchored in a language-centric perspective to develop an efficient omnimodal large model capable of unified multimodal understanding, reasoning, and generation</span>. Our approach is predicated on the view that language, serving as a structured representation of the world and a mediator between modalities, provides the cornerstone for effective multimodal integration and transition. This architectural choice is motivated by established advantages of LLMs-based multimodal models <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2306-13549</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2505-09777</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:conf/wacv/CuiMCYZLCLYLGLTCZLYMCWZ24</span>)</cite>, including great training stability and straightforward scalability to new scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "including",
                    "generation",
                    "understanding",
                    "new",
                    "multimodal",
                    "develop",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, we build <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, a fully open-source, diverse MoE-based Omnimodal Large Model (OLM). Evolved from the dense Qwen2.5-7B language model, this work demonstrates an efficient pathway to scale a LLM into a powerful and comprehensive OLM.\nThrough progressive architectural evolution and optimized training strategies, we have successfully transformed the base model into an omnimodal model, achieving the crucial transition from mere multimodal understanding to seamless integration of both understanding and generation.\nOur work is founded on <span class=\"ltx_text ltx_font_bold\">three key architectural contributions</span> (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2\" title=\"2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation",
                    "model",
                    "comprehensive",
                    "into",
                    "multimodal",
                    "from",
                    "through"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Unified Modality Encoding &amp; Generation</span>: We design a unified speech encoder that maps diverse audio inputs, including environmental sound, speech, and music, into a shared representation space. For output, a Context-Aware MoE-based TTS module supports dynamic speech synthesis (especially for long speech) and interaction. On the visual side, we employ pre-trained visual encoders to process images and videos, and build a Task-Aware Diffusion Transformer for instruction-guided image generation and editing.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "including",
                    "generation",
                    "into",
                    "image",
                    "process",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Deep Cross-Modal Alignment</span>: To enable deep and efficient fusion of any modality, we introduce an Omni-Modality 3D RoPE mechanism in the self-attention layers. It encodes the temporal-spatial dimensions of speech, image, text, and video tokens, ensuring seamless alignment and interaction across all input types.</p>\n\n",
                "matched_terms": [
                    "any",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">MoE-Driven Cross-Modal Fusion</span>: We strategically extend the standard MLP layers to MoE layers. This new MoE architecture incorporates three expert types: null experts for inference-time computation skipping, modality-specific routed experts for storing modality knowledge and processing cross-modal information, and small-size shared experts to facilitate universal information exchange. This design enables efficient computation, specialized modality handling, and effective and stable multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "new",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Progressive Omnimodal Training Optimization</span>:\nTo mitigate the instability often encountered when training MoE-based omnimodal large models, we designed a progressive training strategy. This approach sequentially advances through: cross-modal alignment &#8594; expert warm-up &#8594; MoE fine-tuning and reinforcement learning &#8594; generative training. This process efficiently scales dense LLMs into MoE-based omnimodal large models with minimal data requirements, while ensuring convergence stability during iterative reinforcement training (GSPO-DPO) in omnimodal data environments.</p>\n\n",
                "matched_terms": [
                    "models",
                    "process",
                    "into",
                    "through"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Language-Centric Hybrid Training for Multimodal Understanding and Generation</span>: To bridge the gap between multimodal understanding and generation tasks&#8212;often treated separately during training&#8212;we propose a hybrid training approach anchored in language generation tasks. By unifying tasks such as image editing, image generation, and speech synthesis within a language generation framework, we break down the inherent barriers between understanding and generation, enabling synergistic enhancement and bidirectional empowerment of both capabilities.</p>\n\n",
                "matched_terms": [
                    "such",
                    "within",
                    "understanding",
                    "generation",
                    "image",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The comprehensive evaluation of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> across 85 multimodal benchmarks reveals that Uni-MoE 2.0 further pushes the boundaries of omnimodal intelligence. In comparison to leading omnimodal models, e.g., Qwen2.5-Omni (trained with 1.2T Tokens), Ming-Lite-Omni-1.0/1.5, Baichuan-Omni-1.5, MiniCPM-o 2.6, and other task-specific models, our model mainly achieves superior results in video understanding and reasoning (averaging + 4% than Ming-Lite-Omni-1.5 on 8 benchmarks), omnimodality comprehension (averaging + 7% than Qwen2.5-Omni on 4 benchmarks, including popular OmniVideoBench and WorldSense), long speech understanding (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 3.5% lower WER than Qwen2.5-Omni on LibriSpeech-clean/other-long) and generation (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> 1% lower WER on TinyStories-en), and AudioVisual tasks (averaging + 4% than Qwen2.5-Omni on Speech-Image QA (Reasoning)). Furthermore, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> exhibits competitive performance in most image generation and editing tasks, outperforming strong generative models (e.g., OmniGen and BAGEL) in image editing, controllable generation and low-level image processing tasks, e.g., + 0.5% than Ming-Lite-Omni on GEdit-Bench and suppressing previous SOTA Qwen-Image and PixWizard on 6 metrics.\nWe open-source all training and inference codes (GitHub), five model checkpoints (Hugging Face), and data lists (Appendix) to support reproducibility and further research.</p>\n\n",
                "matched_terms": [
                    "previous",
                    "models",
                    "generation",
                    "including",
                    "understanding",
                    "comprehensive",
                    "image",
                    "multimodal",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model processes multimodal inputs&#8212;including audio, images, text, and video&#8212;through a unified tokenization scheme, as illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F2\" title=\"Figure 2 &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. As detailed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS2\" title=\"2.2 Uni-Perception &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, audio is segmented into 30-second clips, with each clip represented by a sequence of 200 tokens (20 tokens/3s). High-resolution images are encoded using a sliding window method, where each 384&#215;384 patch is independently tokenized. To enhance the model&#8217;s comprehension of multimodal data (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS3\" title=\"2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>), we introduce Omni-Modality 3D RoPE, a mechanism that temporally aligns inputs from speech, text, video, and images. Within the MoE layer, we employ diverse experts to enable seamless switching between specialized modules. This is stabilized by null experts to realize a token skipping layer, modal-specific routed experts that ensure balanced training, and enhanced by small-size shared experts (only 1/8 of routed experts) that facilitate cross-modal knowledge transfer. For generation tasks (Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.SS4\" title=\"2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>), the model utilizes dedicated control tokens. Audio Generation tokens govern language and role attributes, which are leveraged by context-aware MoE-TTS modules. Similarly, Image Generation tokens, composed of task and content instructions, bridge the foundation model to a task-aware diffusion transformer, enabling end-to-end image generation and editing.</p>\n\n",
                "matched_terms": [
                    "detailed",
                    "within",
                    "generation",
                    "which",
                    "image",
                    "into",
                    "role",
                    "multimodal",
                    "each",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The visual understanding module of Uni-MoE-2.0-Omni employs a unified encoding strategy for both images and videos. The overall architecture consists of two major components: a visual encoder and a mapping network (MLP).</p>\n\n",
                "matched_terms": [
                    "two",
                    "visual",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Visual Encoder</span>. The visual encoder is initialized with SigLIP <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhai2023sigmoid</span>)</cite> vision transformer, which transforms the input image or video frames <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i1.p1.m1\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> into visual features <math alttext=\"Z_{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>V</mi></msub><annotation encoding=\"application/x-tex\">Z_{V}</annotation></semantics></math>. We adopt the output of the last layer of the transformer as the visual features.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "which",
                    "into",
                    "image",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Mapping Network</span>. The mapping network is composed of a two-layer MLP and a 2D average pooling layer. Specifically, the MLP takes the visual features as input and projects them into the representation space of the language model, thereby producing a one-dimensional sequence of visual features <math alttext=\"H_{V}=p(Z_{V})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>H</mi><mi>V</mi></msub><mo>=</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>Z</mi><mi>V</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">H_{V}=p(Z_{V})</annotation></semantics></math>. Subsequently, the average pooling layer performs length compression along both spatial dimensions, enabling more efficient training.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "model",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through these visual understanding modules, we can uniformly encode Single-Image, Multi-Image, and Video inputs, thereby transforming different visual input paradigms into a unified one-dimensional sequence of visual representations. The specific encoding strategies for each paradigm are described as follows:</p>\n\n",
                "matched_terms": [
                    "visual",
                    "understanding",
                    "into",
                    "each",
                    "through"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Single-Image</span>. For a given single image with arbitrary resolution, we preprocess the input while preserving its aspect ratio. Suppose the original resolution of the input image is <math alttext=\"(h,w)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h,w)</annotation></semantics></math>. We traverse a set of candidate resolutions and select a target resolution <math alttext=\"(h^{\\prime},w^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>,</mo><msup><mi>w</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(h^{\\prime},w^{\\prime})</annotation></semantics></math> that is closest to the original resolution while requiring minimal padding. The image is then resized so that either its height or width matches the target resolution, while the other dimension is padded with blank pixels to reach the target resolution. Each candidate resolution is constrained such that both its height and width are integer multiples of the vision encoder patch size <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>, ensuring compatibility with the vision encoder input specification. After this preprocessing, the image can be partitioned into <math alttext=\"a\\times b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">a\\times b</annotation></semantics></math> visual patches, where <math alttext=\"a=h^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><msup><mi>h</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">a=h^{\\prime}/p</annotation></semantics></math> and <math alttext=\"b=w^{\\prime}/p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><msup><mi>w</mi><mo>&#8242;</mo></msup><mo>/</mo><mi>p</mi></mrow></mrow><annotation encoding=\"application/x-tex\">b=w^{\\prime}/p</annotation></semantics></math>, with each patch of size <math alttext=\"p\\times p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p\\times p</annotation></semantics></math>. Assuming there are <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> tokens each visual patch, the total number of visual tokens is <math alttext=\"(a\\times b)\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i1.p1.m9\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>b</mi></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">(a\\times b)\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "such",
                    "visual",
                    "reach",
                    "into",
                    "image",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Multi-Image</span>. The encoding of multiple images follows a similar procedure to that of a single image. For each image, we independently search the most suitable target resolution, resize it accordingly, and then convert it into the corresponding set of visual patches. Assuming there are <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> images are provided as input, the total number of visual tokens is <math alttext=\"\\sum^{n}_{i=1}((a_{i}\\times b_{i})\\times T)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo lspace=\"0em\" stretchy=\"false\">(</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>a</mi><mi>i</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>b</mi><mi>i</mi></msub></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sum^{n}_{i=1}((a_{i}\\times b_{i})\\times T)</annotation></semantics></math>, where <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math> and <math alttext=\"b_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m4\" intent=\":literal\"><semantics><msub><mi>b</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">b_{i}</annotation></semantics></math> denote the number of patches along the height and width of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th image.</p>\n\n",
                "matched_terms": [
                    "each",
                    "visual",
                    "into",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Video</span>. For video data, we adopt the minimum resolution accepted by the vision encoder <math alttext=\"(p,p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(p,p)</annotation></semantics></math> as the target resolution. Each frame of the video is directly resized to this resolution. Regarding frame selection, we uniformly sample the original video at a rate of one frame per second, yielding <math alttext=\"f_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">f_{s}</annotation></semantics></math> sampled frames. If the number of sampled frames is less than a predefined lower bound <math alttext=\"f_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">f_{l}</annotation></semantics></math> or greater than an upper bound <math alttext=\"f_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">f_{u}</annotation></semantics></math>, we uniformly resample the video to satisfy these constraints.\nConsequently, the final number of frames is determined as <math alttext=\"f_{n}=min(max(f_{s},f_{l}),f_{u})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>=</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mi>s</mi></msub><mo>,</mo><msub><mi>f</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><msub><mi>f</mi><mi>u</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f_{n}=min(max(f_{s},f_{l}),f_{u})</annotation></semantics></math>, and the total number of visual tokens is <math alttext=\"f_{n}\\times T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i3.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">f_{n}\\times T</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "visual",
                    "upper",
                    "final"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This unified encoding of Single-Image, Multi-Image, and Video inputs into a one-dimensional sequence of visual representations enables the transfer of the model&#8217;s capability in high-resolution image understanding to the video domain, thereby facilitating faster convergence and improved performance in video understanding.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "understanding",
                    "into",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech understanding plays a crucial and indispensable role in speech dialogue models. However, for the speech input of most speech dialogue models, they merely conduct analysis and compilation on semantic information, thus paying relatively less attention to prosodic features such as intonation and timbre information. A model has the potential to comprehensively understand multiple types of audio information in a unified manner through a larger-scale and powerful encoding structure in terms of comprehension ability. This architecture is comprises of two main components:</p>\n\n",
                "matched_terms": [
                    "such",
                    "main",
                    "models",
                    "understanding",
                    "model",
                    "analysis",
                    "role",
                    "structure",
                    "two",
                    "through"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio Encoder</span>. We adopt the Whisper-Large-v3 encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">whisper</span>)</cite> as our speech encoding module and conduct training on a diverse set of audio datasets. Specifically, input audio signals <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> first undergo resampling to a unified sample rate of 16000 Hz; subsequently, these resampled audio signals are fed into the Whisper encoder, which generates audio features <math alttext=\"Z_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">Z_{A}</annotation></semantics></math>. Notably, the audio features output by this encoder not only encapsulate rich intrinsic information from the input audio but also integrate the generalized speech representation capabilities acquired by the pre-trained Whisper encoder during its prior training phase.</p>\n\n",
                "matched_terms": [
                    "which",
                    "into",
                    "from",
                    "output",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Audio-Language Mapping</span>.Through comparison, it is found that by using the decoder module of Whisper-Large-v3 as the Qformer and adopting a mapping method of 400 tokens per minute, not only can speech information be efficiently extracted, but also a great deal of paralinguistic semantic information, such as timbre, intonation, and emotion, can be obtained. We utilize all decoder layers of Whisper-Large-v3, with the number of query tokens in the Qformer set to 200 (20 tokens/3s). These types of information are ultimately mapped to the textual dimension through the projection layer, enabling the model to effectively utilise this information for understanding and reasoning.</p>\n\n",
                "matched_terms": [
                    "such",
                    "model",
                    "understanding",
                    "through",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"h_{W}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>W</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{W}^{A}</annotation></semantics></math> is the last hidden states of the pre-trained audio encoder adopted from Whisper-large-v3. <span class=\"ltx_text ltx_markedasmath\">MSA</span> and <math alttext=\"h\\text{MCA}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>MCA</mtext></mrow><annotation encoding=\"application/x-tex\">h\\text{MCA}</annotation></semantics></math> denote the multihead self-attention and cross-attention operations, and <span class=\"ltx_text ltx_markedasmath\">LN</span> is the layer norm function. By leveraging the fixed-length query vectors <math alttext=\"X_{Q}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>X</mi><mi>Q</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">X_{Q}^{A}</annotation></semantics></math>, each 30 seconds of audio encoded by the Whisper encoder is mapped to <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m6\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> audio tokens. <math alttext=\"h_{C}^{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>C</mi><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">h_{C}^{A}</annotation></semantics></math> represents the output of the cross-attention module, which is used to distill the main content of the input audio. After going through all layers of the Whisper decoder, we apply a learnable linear layer for projecting the last output into the representation space of LLM.</p>\n\n",
                "matched_terms": [
                    "main",
                    "output",
                    "which",
                    "into",
                    "each",
                    "from",
                    "through"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To process audio longer than Whisper-Large-v3&#8217;s 30-second limit, we employ a chunking mechanism. Long audio is segmented into consecutive 30-second clips, which are batched and processed by the audio understanding module. <span class=\"ltx_text ltx_font_italic\">The resulting features are then concatenated along the time dimensions, enabling the understanding of audio of arbitrary length</span>.</p>\n\n",
                "matched_terms": [
                    "process",
                    "into",
                    "which",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the M-RoPE design in Qwen2-VL <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qwen2-VL</span>)</cite>, we propose Omni-Modality 3D RoPE to enable efficient positional modeling across text, audio, image, and video modalities. Similar to M-RoPE, we decompose the original rotary embedding into three components corresponding to the temporal, height, and width dimensions. For textual inputs, these components share the same position IDs, rendering Omni-Modality 3D RoPE functionally equivalent to the standard 1D-RoPE. For audio inputs, we align the temporal position IDs with absolute time. Specifically, we define 20 tokens as the minimum temporal unit, which corresponds to a duration of 3 seconds. Instead of adopting the original single-step increment of position IDs, we replace it with the variation rate of absolute time. For image inputs, the temporal IDs of visual tokens remain fixed, while the height and width IDs are assigned according to the spatial locations of tokens within the original image. Unlike M-RoPE, the height and width dimensions are traversed in a patch-wise manner, such that all tokens within a single vision patch are enumerated before proceeding to the next patch. For video inputs, the temporal IDs of each frame are incremented according to <span class=\"ltx_text ltx_framed ltx_framed_underline\">absolute time</span>, while the height and width components follow the same ID assignment pattern as images.</p>\n\n",
                "matched_terms": [
                    "such",
                    "within",
                    "visual",
                    "which",
                    "before",
                    "position",
                    "image",
                    "into",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As an illustrative example, consider a video input of length 120 seconds accompanied by audio. We place the visual sequence before the audio sequence and sample the video at a rate of one frame every two seconds. Suppose the RoPE ID of the last text token preceding the visual sequence is <math alttext=\"(x-1,x-1,x-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>x</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x-1,x-1,x-1)</annotation></semantics></math>. Then, the RoPE IDs of the first video frame start from <math alttext=\"(x,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x,x)</annotation></semantics></math> and increment row by row and column by column up to <math alttext=\"(x,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> denotes the number of tokens along both the height and width of a video frame. The second frame begins with <math alttext=\"(x+2\\theta,x,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x,x)</annotation></semantics></math> and increases to <math alttext=\"(x+2\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+2\\theta,x+p,x+p)</annotation></semantics></math>, where <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> is a specific scaling factor for absolute time. Following this procedure, the final frame in the video sequence has an ID of <math alttext=\"(x+118\\theta,x+p,x+p)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>x</mi><mo>+</mo><mrow><mn>118</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo>,</mo><mrow><mi>x</mi><mo>+</mo><mi>p</mi></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x+118\\theta,x+p,x+p)</annotation></semantics></math>.\nSimilarly, assume the RoPE ID of the last token before the audio sequence is <math alttext=\"(y-1,y-1,y-1)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>y</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y-1,y-1,y-1)</annotation></semantics></math>. The first audio segment is then assigned the ID <math alttext=\"(y,y,y)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y,y,y)</annotation></semantics></math>, which is repeated 20 times to represent the minimum audio unit. The next audio segment receives the ID <math alttext=\"(y+3\\theta,y+3\\theta,y+3\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m11\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+3\\theta,y+3\\theta,y+3\\theta)</annotation></semantics></math>. Continuing this process, the final audio segment is assigned the ID <math alttext=\"(y+117\\theta,y+117\\theta,y+117\\theta)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p2.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>+</mo><mrow><mn>117</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#952;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(y+117\\theta,y+117\\theta,y+117\\theta)</annotation></semantics></math>. In this manner, temporal alignment between the visual and audio sequences can also be achieved.</p>\n\n",
                "matched_terms": [
                    "start",
                    "visual",
                    "which",
                    "following",
                    "before",
                    "final",
                    "process",
                    "two",
                    "from",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies have demonstrated that factual and procedural knowledge is predominantly stored in the feed-forward network (FFN) modules <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">DBLP:journals/corr/abs-2503-22941</span>)</cite>. The Mixture-of-Experts architecture enables adaptive knowledge retrieval by dynamically activating different FFN modules, which are determined by a router network. Particularly, given <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> expert parameters <math alttext=\"\\{w_{0},...,w_{n-1}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>w</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>w</mi><mrow><mi>n</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{w_{0},...,w_{n-1}\\}</annotation></semantics></math>, the output of a vanilla MoE for inference is:</p>\n\n",
                "matched_terms": [
                    "previous",
                    "output",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These limitations prevent MoE from fully exploiting its adaptive activation capability, ultimately resulting in suboptimal performance.\nTo address these challenges, we propose the Dynamic-Capacity MoE architecture, which incorporates (i) routing gradient estimation to enable differentiable expert selection, (ii) explicit expert role specialization to separately model domain-specific and general knowledge, as well as facilitate knowledge deletion, and (iii) dynamic expert number allocation to adaptively control the amount of parametric knowledge.</p>\n\n",
                "matched_terms": [
                    "from",
                    "model",
                    "which",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A central difficulty in training vanilla MoE models arises from the non-differentiable Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> operation used in expert selection, which obstructs gradient propagation to the router and leads to biased optimization. To address this, we migrate the gradient estimation strategy proposed in Grin-MoE <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024gringradientinformedmoe</span>)</cite>, which integrates straight-through gradient estimators under the framework of numerical methods for ordinary differential equations (ODEs). This enables end-to-end optimization of both router and experts under sparse activation constraints, improving the stability of router training and allowing more precise token-to-expert assignments. A formal description is given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.SS4\" title=\"A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "precise",
                    "from",
                    "which",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Null Experts</span>: These are &#8220;empty&#8221; experts whose output is identically zero. They serve as a mechanism for selective forgetting, effectively removing outdated or irrelevant knowledge from the model&#8217;s output. Null experts are also dynamically activated via the dynamic capacity routing strategy.</p>\n\n",
                "matched_terms": [
                    "from",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This role specialization enables the model to (i) allocate computational resources according to token-specific knowledge demands, (ii) maintain a persistent general-knowledge backbone via shared experts, and (iii) selectively forget outdated or irrelevant knowledge through null experts, thereby improving both adaptability and controllability of MoE-based architectures.</p>\n\n",
                "matched_terms": [
                    "through",
                    "model",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Vanilla MoE applies a fixed number of experts to every token, ignoring variations in token complexity and knowledge demand.\nWe address this limitation by introducing a dynamic capacity routing strategy, which determines the number of routed experts for each token based on a Top-P sampling.\nFormally, let the router produce a probability vector over routed experts for token <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS2.Px3.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "each",
                    "based",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a holistic view of our method, Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the complete training procedure in pseudocode form.\nFor simplicity of presentation, we treat null experts as a special case of routed experts in the pseudocode.\nThe algorithm integrates the three proposed components into a unified workflow:\nFirst, <span class=\"ltx_text ltx_font_typewriter\">ActivateExperts</span> function determines expert activation, combining the Top&#8209;P sampling strategy for routed experts with the persistent activation constraint for shared experts;\nSecond, gradient estimation is applied to routed expert outputs to enable end&#8209;to&#8209;end optimization under sparse activation;\nFinally, after computing routed expert contributions, shared expert outputs are computed and incorporated into the MoE representation, injecting general knowledge into the final output.</p>\n\n",
                "matched_terms": [
                    "final",
                    "output",
                    "into",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach to speech processing employs a dual-strategy architecture to handle input and output efficiently. For speech understanding, we use a continuous encoding method that significantly reduces token consumption (20 tokens/3 seconds). For speech generation, the base model produces both text and control signals. To empower the model with rich audio generation capabilities, including ambient sounds, music, and speech with diverse timbres, emotions, and tones, we integrate the Wavtokenizer <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ji2024wavtokenizer</span>)</cite> and design a context-aware MoE-TTS module for synthetic voice with three styles.</p>\n\n",
                "matched_terms": [
                    "model",
                    "including",
                    "generation",
                    "understanding",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S2.F3\" title=\"Figure 3 &#8227; 2.4 Uni-Generation &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our MoE-TTS module is built upon a Qwen2.5-0.5B model initialized with a MoE structure. This autoregressive module takes input instructions and text tokens to generate the corresponding speech encodings, which are subsequently decoded into waveform audio by the Wavtokenizer decoder. The training process is divided into two key stages: First, we pre-train three separate dense models, each on a single-speaker TTS dataset to capture a unique vocal style. Second, this model is transformed into a MoE architecture by replicating its FFN layers to create four experts, which are then fine-tuned on a diverse, multi-style, and multi-speaker dataset. The overall workflow of MoE-TTS is outlined as follows:</p>\n\n",
                "matched_terms": [
                    "style",
                    "models",
                    "which",
                    "first",
                    "process",
                    "into",
                    "structure",
                    "two",
                    "model",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where we denote the prompt text, target text content representations to <math alttext=\"(P_{1},...,P_{n})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>P</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>P</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(P_{1},...,P_{n})</annotation></semantics></math> and <math alttext=\"(T_{1},...,T_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>T</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(T_{1},...,T_{m})</annotation></semantics></math> respectively. &#8220;MSA&#8221;, &#8220;MoE&#8221; and &#8220;LN&#8221; refer to the multi-head self-attention, mixture of experts and layer normalization. <math alttext=\"X_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">X_{l}</annotation></semantics></math> shows the output of <span class=\"ltx_text ltx_font_italic\">l</span> th block.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "output",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the performance of MoE-TTS in voice timbre control, we adopt a text-context prompt-guided approach to direct the model to synthesize audio with the specified language and timbre. When the model receives an instruction to generate an audio response, the Uni-MoE base model first produces a special token (i.e. <span class=\"ltx_text ltx_font_italic\">&lt;speech start&gt;</span>) indicating the start of speech, . Subsequently, the model generates commands that specify the voice timbre for audio synthesis: fixed timbres include those of Brain, Jenny, and Xiaoxiao, while supported languages cover Chinese and English. Beyond fixed options, the model also enables users to describe desired timbres via natural language; the MoE-TTS module then generates audio with the corresponding timbre based on these descriptive instructions. Upon completion of the timbre prompt, a special token &lt;speech prompt&gt; is generated to signal the end of the prompt. This is followed by the text content that the model needs to transcribe into speech, which is terminated with the special token &lt;speech end&gt;. Through this combination of text-based timbre prompts and target text content, MoE-TTS is guided to perform diverse timbre synthesis.</p>\n\n",
                "matched_terms": [
                    "based",
                    "start",
                    "include",
                    "through",
                    "prompt",
                    "which",
                    "end",
                    "into",
                    "response",
                    "model",
                    "first",
                    "specified"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synthesize long-form speech, we employ a sentence-splitting strategy during both training and inference. In training, lengthy texts are segmented into short phrases at punctuation marks, which are then batched for efficient processing by MoE-TTS. During inference, we ensure contextual coherence by <span class=\"ltx_text ltx_font_italic\">using the previously generated speech segment to guide the synthesis of the next</span>. This split-and-guide approach enables our model to produce coherent and fluent audio clips <span class=\"ltx_text ltx_framed ltx_framed_underline\">exceeding two minutes</span> in duration<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Context-Aware MoE-TTS are available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HIT-TMG/Uni-MoE-TTS\" title=\"\">https://huggingface.co/HIT-TMG/Uni-MoE-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "two",
                    "model",
                    "into",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a Task-Aware Diffusion Transformer (Task-DiT) that bridges the gap between image understanding and generation. Unlike unified models that suffer from performance trade-offs, our framework preserves the strengths of specialized pre-trained models by connecting them via a lightweight, task-aware bridge.\nAt its core, Task-DiT employs a frozen image generator from PixWizard <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2024pixwizard</span>)</cite> to safeguard its high-fidelity synthesis capabilities. To steer this generator, we introduce a query-based conditioning mechanism. Two distinct sets of learnable tokens are processed by a powerful understanding module:</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "generation",
                    "image",
                    "two",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Image Tokens (&lt;IMG[i]&gt;): Capture the rich semantic essence of the desired output from Uni-MoE-2.0-Omni, forming a compressed scene representation.</p>\n\n",
                "matched_terms": [
                    "from",
                    "output",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The enriched features from these tokens are then translated for the generator by dedicated, lightweight projectors. A Task Projector modulates the DiT&#8217;s denoising process based on the command, while a content projector transforms the &lt;IMG[i]&gt; features into a dense conditioning sequence for cross-attention. For image-guided tasks, a Visual Projector aligns source image features encoded by ViT with the DiT&#8217;s conditioning space.\nThis design creates a versatile and efficient channel for task-aware instruction, enabling high-quality, multi-modal image generation without the catastrophic interference typical of end-to-end fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "models",
                    "generation",
                    "into",
                    "image",
                    "multimodal",
                    "process",
                    "from",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in the leftmost panel of the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S3.F5\" title=\"Figure 5 &#8227; Supervised Fine-tuning &#8227; 3.1 Training Recipe: From LLMs to OLMs &#8227; 3 Training and Data Recipes &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the goal of pre-training is to enable the LLM to comprehend multimodal data, such as images, videos, and speech. This is achieved by mapping the representations of these modalities into the LLM&#8217;s linguistic space, using paired data of multimodal inputs and their corresponding text description.</p>\n\n",
                "matched_terms": [
                    "such",
                    "into",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this stage, we collected a large-scale multimodal instruction-following dataset to enable the model to understand and process any cross-modal information. The training comprises two phases:</p>\n\n",
                "matched_terms": [
                    "any",
                    "process",
                    "multimodal",
                    "two",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Expert Warmup with Dense Model: We first pre-trained three specialized expert models focusing on mainstream modalities: speech comprehension, speech generation, and visual comprehension. This stage aims to build the model-specific experts for the following stable MoE-based fine-tuning.</p>\n\n",
                "matched_terms": [
                    "focusing",
                    "visual",
                    "models",
                    "generation",
                    "following",
                    "model",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mixed-Data Fine-tuning for MoE-based Model: We then fine-tuned the model on a mixture of all data types using our proposed MoE architecture. The experts in this MoE layer were initialized from the pre-trained experts above, with a final configuration comprising two speech experts, one visual expert, and one null expert. This setup also integrates the inherent language knowledge of the base LLM and two small, shared experts.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "above",
                    "final",
                    "two",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generative tasks, the model employs specialized decoding strategies. In speech generation, the Dense LLM outputs both dialogue content and acoustic control signals, which are then rendered into audio by a Dense TTS model. For image tasks, the model first verbalizes its reasoning before generating the final output tokens for tasks like captioning or editing. <span class=\"ltx_text ltx_font_italic\">This unified approach allows multimodal understanding and generation tasks to be jointly leveraged during instruction tuning, leading to further performance gains.</span></p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation",
                    "which",
                    "model",
                    "before",
                    "into",
                    "image",
                    "multimodal",
                    "final",
                    "output",
                    "reasoning",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Empirical results indicated that the initial modality-specific expert warmup was insufficient to ensure uniform capability across the diverse spectrum of cross-modal tasks after mixed-data fine-tuning. To further calibrate the model and bridge this performance gap, we conducted a subsequent annealing training phase using a balanced mixture of all data types. Crucially, this annealing process was not confined to the omnimodal foundation model; it was also deployed to fine-tune the specialized sub-modules responsible for image editing/generation and text-to-speech (TTS), promoting stability and proficiency throughout the entire architecture.</p>\n\n",
                "matched_terms": [
                    "model",
                    "process",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To develop the Thinking variant of Uni-MoE 2.0, we adopted a training strategy combining cold-start initialization with online reinforcement learning (GSPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025groupsequencepolicyoptimization</span>)</cite> and Direct Preference Optimization (DPO) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">rafailov2024directpreferenceoptimizationlanguage</span>)</cite>. The cold-start phase aims to stimulate the model&#8217;s foundational reasoning capabilities across multimodal inputs&#8212;including text, images, and video&#8212;while online reinforcement learning enhances the model&#8217;s autonomous exploration and further refines its reasoning under sparse reward signals. However, during training, we observed limited improvement in the accuracy reward, particularly when processing full-modality data. To address this, we introduced a DPO stage to specifically strengthen the model&#8217;s reasoning ability. In this stage, we reuse rollout samples from the online RL phase under an &#8220;LLM as a Judge&#8221; mechanism, selecting those with a logical reasoning process and accurate outcomes as positive examples. Additionally, for samples that yielded zero accuracy during online training, we leverage closed-source commercial models (Gemini-2.5-Flash) as a teacher to generate high-quality reasoning demonstrations, thereby improving the student model, Uni-MoE 2.0. The GSPO-DPO pipeline supports iterative refinement for progressively stronger reasoning performance. A comprehensive treatment of this iterative optimization strategy and associated experimental analysis is provided in VIPO-R1<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Iterative Policy Optimization: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/HITsz-TMG/VerIPO\" title=\"\">https://github.com/HITsz-TMG/VerIPO</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2025veripo</span>)</cite>. The specific datasets are given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.T20\" title=\"Table 20 &#8227; A.1 Training Data List &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">20</span></a> of the Appendix.</p>\n\n",
                "matched_terms": [
                    "thinking",
                    "exploration",
                    "models",
                    "accuracy",
                    "comprehensive",
                    "analysis",
                    "process",
                    "logical",
                    "multimodal",
                    "develop",
                    "from",
                    "model",
                    "reasoning",
                    "accurate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final stage integrates comprehensive omnimodal capabilities by adding speech and image generation. In this phase, the foundational model remains frozen, while the context-aware MoE-TTS and Task-Aware DiT modules are fine-tuned using the corresponding data from the preceding training stages. This approach preserves the model&#8217;s core omnimodal understanding while efficiently scaling its generative abilities to new modalities. Because the foundation model was pre-trained on data related to image and speech generation, this final training stage converges rapidly, and the new generative modules align effectively with the existing model architecture.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation",
                    "comprehensive",
                    "new",
                    "image",
                    "final",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the supervised fine-tuning phase, we reduced the volume of ASR data to 1 billion tokens. To expand the model&#8217;s recognition capabilities&#8212;originally focused on speech, environmental sounds, and music&#8212;toward tasks including speech dialogue, environmental sound question answering, music question answering, and emotion recognition, we collected and constructed a set of relevant datasets for the fine-tuning phase. Furthermore, to activate the model&#8217;s capability to generate special tokens required for speech generation, we incorporated data from speech generation tasks, including TTS tasks and speech dialogue tasks, into the training data. In this stage, the total volume of audio data amounts to 5B tokens.</p>\n\n",
                "matched_terms": [
                    "question",
                    "including",
                    "generation",
                    "into",
                    "from",
                    "relevant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we performed sample-level balancing to ensure that the data from all modalities were controlled to be of comparable scale. For the abundant image data, we conducted quality filtering and selected a subset of high-quality samples. For video data, we selected a subset of samples with clear and semantically meaningful audio tracks, which were then expanded into audio&#8211;visual unified understanding data. Finally, the image training data amount to approximately 5 billion tokens, while the video training data are expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "which",
                    "into",
                    "image",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of the MoE-TTS module is divided into three stages. In the first stage, the pre-training activation stage, data with consistent timbre (covering both Chinese and English) is used; this phase primarily aims to activate the capability of the dense model, initialized with the Qwen-0.5B language model, to convert text content of codec tokens into audio content, with a total data volume of 2B tokens. The second stage incorporates a large amount of data with diverse timbres, including two types: data for same-timbre synthesis (where three timbre categories dominate in quantity) and data for stylized custom timbre synthesis (where timbre is controlled via natural language), with a data volume of 5B tokens. In the final stage, training for MoE model activation is conducted using data of the same quantity as the previous stage, resulting in the final version of the MoE-TTS model.</p>\n\n",
                "matched_terms": [
                    "previous",
                    "including",
                    "into",
                    "final",
                    "two",
                    "model",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated a large collection of open-source instruction-tuning datasets encompassing a wide range of tasks, including general image understanding, STEM reasoning, document understanding, visual grounding, and video description. At this stage, the image instruction-tuning data consisted of roughly 22 billion tokens, while the video instruction-tuning data accounted for about 19 billion tokens.</p>\n\n",
                "matched_terms": [
                    "about",
                    "visual",
                    "including",
                    "understanding",
                    "image",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the annealing stage, we applied sample-level balancing to ensure that data from all modalities were maintained at comparable scales. For the abundant image data, we performed quality filtering to retain only a high-quality subset. For the video data, we selected samples with clear and semantically meaningful audio tracks, which were further extended into audio&#8211;visual unified understanding datasets. Ultimately, the image training data comprised approximately 5 billion tokens, while the video training data were expanded to 21 billion tokens.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "which",
                    "into",
                    "image",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the task projector alignment stage and the visual projector alignment stage, we utilized data encompassing a variety of task types. This included image generation, image editing, low-level computer vision tasks such as deraining and denoising, and conditional generation like Canny-to-image. In total, this data amounted to approximately 2M samples (1.5B tokens).</p>\n\n",
                "matched_terms": [
                    "visual",
                    "generation",
                    "image",
                    "such"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the caption projector alignment stage, we used high-quality image-caption pairs sourced from a diverse range of data types. This collection included factual, human-annotated descriptions of everyday scenes; detailed, context-rich prompts for complex image generation; and elaborate narratives describing the compositional and spatial relationships between elements. This data accounted for a total of 4.2M samples (3.2B tokens).</p>\n\n",
                "matched_terms": [
                    "from",
                    "generation",
                    "detailed",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the supervised instruction-tuning stage, we curated and filtered a large-scale dataset from multiple open-source dataset collections, amounting to approximately 10.5M samples (8.1B tokens). This stage aimed to unify the model&#8217;s capabilities under a consistent instruction-following framework. The training data was formatted as instruction-response pairs covering the same core task categories mentioned previously: image generation, image editing, low-level CV enhancements, and conditional image synthesis.</p>\n\n",
                "matched_terms": [
                    "from",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the annealing stage, we utilized a curated, high-quality dataset of selected samples across various task types. This stage placed particular emphasis on dense captions for image generation, generation processes that incorporate reasoning steps, and various long-tail image editing tasks, such as changing human emotion. This dataset consisted of approximately 3M samples (2.8B tokens).</p>\n\n",
                "matched_terms": [
                    "such",
                    "generation",
                    "steps",
                    "image",
                    "various",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Omni-Modal Fine-tuning stage, we introduce pure textual data to further enhance the model&#8217;s understanding and reasoning capabilities. For this stage, the textual data primarily consists of text-only instructional data and mathematical question&#8211;answer pairs, totalling approximately 1 billion tokens. In the subsequent Omni-Modal Annealing stage, STEM-related data (e.g., codes, math) are incorporated into the training data to further improve model performance, resulting in approximately 4 billion tokens of textual data at this stage.</p>\n\n",
                "matched_terms": [
                    "model",
                    "understanding",
                    "into",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate the capabilities of <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span>, we conducted experiments on 85 benchmarks, including multimodal or omnimodal understanding (image, text, video, cross/tri-modal), audio and music understanding, speech generation, image generation and editing, and low-level image processing tasks. &#8220;Uni-MoE-2.0&#8221; refers to the Uni-MoE-2.0-Omni model without annealing training. &#8220;Uni-MoE-2.0-Base/thinking&#8221; represents the omnimodal understanding and thinking model without speech and image generation abilities.</p>\n\n",
                "matched_terms": [
                    "thinking",
                    "including",
                    "generation",
                    "understanding",
                    "image",
                    "multimodal",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_bold\">Uni-MoE-2.0-Omni</span> achieves performance comparable to other Omnimodal Understanding models on both general image understanding and STEM-related image understanding tasks. In particular, Uni-MoE-2.0-Omni attains the highest performance on the GQA (test-dev) and MathVision (Test) benchmarks. For Doc &amp; OCR tasks, Uni-MoE-2.0-Omni exhibits a certain performance gap compared to the strongest existing Omnimodal Understanding models. We attribute this to the limited availability of Doc &amp; OCR data during pre-training and the consistently low proportion of such data in subsequent training stages. This observation also, to some extent, highlights the scarcity of publicly available Doc &amp; OCR datasets in the current open-source ecosystem.</p>\n\n",
                "matched_terms": [
                    "such",
                    "models",
                    "understanding",
                    "image",
                    "current"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T3\" title=\"Table 3 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrate that the Uni-MoE-2.0-Omni model achieves SOTA performance across multiple video understanding benchmarks. Specifically, Uni-MoE-2.0-Omni surpasses the previous SOTA model Ming-Lite-Omni-1.5 by approximately 3.85% on the long video understanding benchmark Video-MME, and by about <span class=\"ltx_text ltx_font_bold\">18.20% on the spatial reasoning benchmark VSI-Bench</span>.</p>\n\n",
                "matched_terms": [
                    "about",
                    "previous",
                    "understanding",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T4\" title=\"Table 4 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance across a diverse set of video understanding and reasoning benchmarks. Notably, our model achieves state-of-the-art results on Video-MME (66.4) and VSI-Bench (56.0), while maintaining strong competitiveness on MVBench (70.5). Compared to other omnimodal models, Uni-MoE-2.0-Omni shows particularly outstanding performance in visual-spatial reasoning (VSI-Bench) and comprehensive video QA (Video-MME), surpassing Qwen2.5-Omni by significant margins of 36.7% and 6.6% respectively. These results collectively validate that Uni-MoE-2.0-Omni delivers superior video comprehension capabilities, highlighting its robust generalization and reasoning proficiency in multimodal video understanding.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "models",
                    "understanding",
                    "comprehensive",
                    "multimodal",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although the training corpus contains a relatively small amount of text data, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T5\" title=\"Table 5 &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, our Uni-MoE-2.0-Omni series models demonstrate strong language capabilities on complex reasoning and knowledge benchmarks such as GPQA and MMLU-Pro. Their average performance (32.68&#8211;34.56) significantly surpasses that of a comparable competitor, Qwen2.5-Onmi 7B (27.27) and Ming-Lite-Omni-1.5 (31.31), while maintaining competitive results on challenging scientific QA (GPQA Diamond) and comprehensive academic reasoning (MMLU-Pro). These results indicate that Uni-MoE-2.0-Omni not only performs reliably in specialized domain knowledge but also reflects the effectiveness and scalability of its architecture in complex language understanding and reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "such",
                    "models",
                    "understanding",
                    "comprehensive",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T6\" title=\"Table 6 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T7\" title=\"Table 7 &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models and audio large models across Chinese and English ASR, Speech Understanding, Audio Understanding, and Music Understanding benchmarks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Uni-MoE-2.0-Omni demonstrates leading performance across diverse audio tasks. It sets a new standard in English Automatic Speech Recognition (ASR), achieving remarkably low Word Error Rates (WER) of 1.73 on LibriSpeech-clean, 3.26 on LibriSpeech-other, and 5.46 on mls-en. The model also delivers strong results in Mandarin ASR, with WERs of 3.69 on Aishell1 and 4.84 on Aishell2. It is worth emphasising that our model possesses strong long-speech understanding capabilities (with an average duration of 3.6 minutes). Specifically, it achieves a low Word Error Rate (WER) of 2.04 and 4.2 on the LibriSpeech-clean-long and LibriSpeech-other-long test sets, two long-audio benchmarks constructed from short-audio synthesis. Beyond transcription, it excels in semantic understanding, attaining high accuracy on spoken question-answering (RACE-audio: 90.32 middle, 87.82 high) and competitive scores on audio captioning tasks (Uni-MoE-2.0-Omni vs. Qwen2.5-Omni: 46.1 vs. 44.5).</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "understanding",
                    "new",
                    "two",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a performance gap is observed in specialized domains such as Music Understanding when compared to certain benchmarks. We hypothesize that this stems from the relatively lower proportion of high-quality, musically annotated data during pre-training and instruction tuning. This observation not only highlights a specific area for future improvement for our model but also reflects a broader challenge within the multimodal research community: the scarcity of large-scale, open-source music understanding datasets. In summary, Uni-MoE-2.0-Omni proves to be a powerful and versatile model for a wide spectrum of audio-centric tasks, with particularly strong results in speech recognition and general audio comprehension, while its performance in niche domains like music is contingent on the availability of relevant training data.</p>\n\n",
                "matched_terms": [
                    "such",
                    "within",
                    "understanding",
                    "multimodal",
                    "from",
                    "model",
                    "relevant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As summarised in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we evaluate the text-to-speech (TTS) performance of our model against several omnimodal models on standardized benchmarks. The evaluation methodology involves generating audio from text and then transcribing it back to calculate the Word Error Rate (WER); we employ Whisper for English and Paraformer for Chinese outputs.</p>\n\n",
                "matched_terms": [
                    "involves",
                    "from",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate the competitive capability of Uni-MoE-2.0-Omni in Chinese and English speech generation. On the LibriTTS-clean benchmark, it achieves a WER of 5.85, significantly outperforming models like Ming-lite-Omni (11.15) and approaching the performance of the state-of-the-art Qwen2.5-Omni-7B. Furthermore, on the challenging SEED-hard benchmark, Uni-MoE-TTS attains a WER of 2.67, surpassing both Dense-TTS and Ming-Lite-Omni, which underscores its robustness in complex synthesis scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-form speech synthesis, tests on the TinyStory validation sets reveal a nuanced strength profile: Uni-MoE-2.0-Omni excels in English, demonstrating superior prosodic consistency, timbre stability, and linguistic fluency, while Ming-Lite-Omni shows a comparative advantage in Chinese. Overall, our model shows the best comprehensive performance in Chinese and English compared to Ming-Lite-Omni and Qwen2.5-Omni-7B.</p>\n\n",
                "matched_terms": [
                    "comprehensive",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Uni-MoE-2.0-Omni proves to be a balanced and effective model for omnimodal TTS, exhibiting particular strength in English synthesis and robust performance across diverse and challenging tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "conclusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T9\" title=\"Table 9 &#8227; 4.2.2 Text &#8594; Speech &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Uni-MoE-2.0-Omni demonstrates competitive performance against other omnimodal models across Speech QA benchmarks, including LlamaQA, WebQA, BigBench Audio, and MultiChallenge Audio. Notably, Uni-MoE-2.0-Omni achieves remarkable results in several key tasks: in the LlamaQA (s&#8594;s) subtask, it attains an accuracy of 75.33, ranking second only to the top-performing model, which underscores its strong ability to handle speech-to-speech reasoning in this scenario. In the BigBench Audio (s&#8594;s) task, it achieves 44.7, showcasing its robustness in audio-related QA.\nFor some challenging benchmarks, such as MultiChallenge Audio, Uni-MoE-2.0-Omni still maintains a competitive position, indicating its versatility across diverse speech QA scenarios. While there is a performance gap compared to the strongest models like Ming-Lite-Omni on certain tasks, we attribute this to the relatively limited specialized Knowledge QA data during pre-training and the low proportion of such data in subsequent training stages.</p>\n\n",
                "matched_terms": [
                    "such",
                    "models",
                    "accuracy",
                    "including",
                    "which",
                    "position",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T10\" title=\"Table 10 &#8227; 4.2.3 Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Uni-MoE-2.0-Omni demonstrates strong and balanced performance across multimodal question-answering tasks, establishing a compelling advantage over comparable models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Speech-Image QA domain, our model achieves top performance on the challenging A-OK-VQA Speech benchmark, leading all competitors in both text (65.73) and speech (52.58) output modalities. This dual dominance highlights Uni-MoE-2.0-Omni&#8217;s superior capability in integrating visual and auditory information for complex reasoning.</p>\n\n",
                "matched_terms": [
                    "output",
                    "visual",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This strength extends to the Speech-Video QA realm. On the ActivityNet benchmark, Uni-MoE-2.0-Omni achieves the highest score (60.16) for text-based answers (s&#8594;t), demonstrating excellent comprehension of temporal and visual narratives. More importantly, it maintains a very strong performance for direct speech answers (s&#8594;s, 57.94), a task where other models like Ming-Lite-Omni fail catastrophically. This result underscores a key advantage of our model: its unique ability to generate high-quality, contextually accurate responses directly in the speech modality without sacrificing performance, a capability where most other omnimodal models show significant weakness.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "models",
                    "model",
                    "accurate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while other models may excel in a single metric, Uni-MoE-2.0-Omni distinguishes itself through its robust and consistent performance across both text and speech output tasks, making it a more versatile and reliable solution for real-world multimodal applications.</p>\n\n",
                "matched_terms": [
                    "through",
                    "solution",
                    "models",
                    "multimodal",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present in the Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T11\" title=\"Table 11 &#8227; 4.2.4 Vision + Speech &#8594; Speech/Text &#8227; 4.2 Audio Understanding and Speech Generation &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> the performance of the Uni-MoE-2.0-Omni model on four omnimodal benchmarks that require simultaneous understanding of visual and audio information. On WorldSense and OmniVideoBench, which evaluate long video omnimodal comprehension, Uni-MoE-2.0-Omni achieves SOTA performance. On StreamingBench (Omni), which focuses on shorter videos, and OmniBench, which assesses joint image-audio understanding, our model ranks second. Across all evaluation metrics, Uni-MoE-2.0-Omni attains a SOTA overall score of 43.7%, outperforming the second-best model Baichuan-Omni-1.5 (41.9%), by approximately 2%. These results demonstrate that Uni-MoE-2.0-Omni possesses a strong omni-modal understanding capability.</p>\n\n",
                "matched_terms": [
                    "present",
                    "visual",
                    "understanding",
                    "which",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T12\" title=\"Table 12 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T13\" title=\"Table 13 &#8227; 4.3 Omnimodality Understanding &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, our Uni-MoE-2.0-Omni models demonstrate strong and versatile performance. They particularly excel in controllable generation and low-level image restoration, significantly outperforming other omni-models. While specialized generators may lead in pure generation, our models remain highly competitive in complex editing and conditional tasks, showcasing their versatility.</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>\nIn pure image generation, our models are competitive. Notably, Uni-MoE-2.0-Omni (0.44) surpasses the original PixWizard (0.43) on the Wise benchmark. While specialized models like Qwen-Image lead on the Wise, our FID score (18.04) is highly competitive, outperforming several omnimodalmodels. For instance, it is 9.0% lower (better) than JanusPro-7B (19.82) and 29.2% lower than Bagel (25.47).</p>\n\n",
                "matched_terms": [
                    "models",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Edition</span>\nOur models show particularly robust performance in image editing. On GEdit-Bench, Uni-MoE-2.0-Omni achieves a score of 6.02, which is 88.1% higher (better) than the original PixWizard (3.20). On the Emu Edit benchmark, our model&#8217;s score of 0.076 is also 94.8% higher than PixWizard (0.039), demonstrating strong performance in instruction-following edits. While state-of-the-art models like Qwen-Image (7.42 GEdiT, 0.127 Emu) still lead, our model is highly competitive. Furthermore, the Uni-MoE-2.0-Image model (MoE and DiT training version) demonstrates even stronger capabilities on complex tasks, especially on the MagicBrush benchmark, where it significantly improves upon the base model&#8217;s CLIPImg score (0.854 vs. 0.789).</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "image",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Controllable Generation</span>\nThis is a particularly strong area for our models. For Canny-to-Image tasks, our Uni-MoE-2.0-Image model achieves a dramatically better FID score (18.23) compared to both the specialized Qwen-Image (37.59) and the omni-model OmniGen2 (45.67). While its F1-Score (0.24) is lower than Qwen-Image (0.47), it surpasses OmniGen2 (0.16). For Depth-to-Image tasks, our model (27.45 FID, 42.41 RMSE) outperforms both Qwen-Image (27.54 FID, 51.23 RMSE) and OmniGen2 (52.30 FID, 59.57 RMSE) on the key metrics of FID and RMSE, demonstrating superior controllable generation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Level Image Restoration</span>\nIn low-level image restoration tasks, our Uni-MoE-2.0-Omni model shows superior performance. For Derain tasks, its PSNR (25.41) is highly competitive with the specialized Qwen-Image (26.37) and significantly better than OmniGen2 (22.22). Notably, its SSIM (0.82) is superior to both Qwen-Image (0.80) and OmniGen2 (0.77). For Denoise tasks, our model (25.70) outperforms Qwen-Image (22.19) by 15.8% and OmniGen2 (16.78) by 53.1% in terms of PSNR.</p>\n\n",
                "matched_terms": [
                    "model",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand the operational dynamics of our Dynamic-Capacity MoE, we visualise expert routing probabilities across layers for diverse tasks (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F7\" title=\"Figure 7 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). The results reveal that initial layers exhibit nearly identical routing across tasks, indicating common set of experts for low-level feature extraction. In deeper layers, routing patterns diverge, reflecting task-specific specialization, while the probability of selecting the null expert (Expert 5) decreases sharply, suggesting that computation is increasingly concentrated in semantically critical stages. The activation profiles align with the intended expert roles: Expert 1 dominates vision-centric tasks (Image, Video and omnimodal Understanding), Experts 2 and 3 are prominent in audio-related tasks (Audio and omnimodal Understanding), and Expert 4 consistently contributes across tasks as a general-purpose semantic expert. Notably, understanding tasks show dynamic, layer-dependent routing, implying adaptive allocation of computational resources from concrete to abstract representations. In contrast, generation tasks exhibit stable, uniform routing across layers, consistent with a homogeneous refinement process. These observations confirm that our MoE design effectively promotes functional specialization and learns efficient, task-dependent computation pathways.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation",
                    "process",
                    "image",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Analysis of the Top-P routing mechanism (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F8\" title=\"Figure 8 &#8227; 4.5 MoE Analysis &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>) shows a structured allocation of computational budget across layers, measured by the proportion of tokens activating different numbers of experts. The distribution follows a distinct peak&#8211;trough&#8211;peak&#8211;fall pattern: high load in early layers (1&#8211;3) for general-purpose feature extraction, a brief reduction in layers 4&#8211;6, a primary peak in middle-to-deep layers (7&#8211;21) corresponding to complex reasoning and feature integration, and a final decline in layers 21&#8211;27 as processing converges to output. While consistent across tasks, modality-specific differences emerge: temporal inputs such as Video and Audio exhibit a stronger initial peak than static Image, with more tokens activating multiple experts, indicating greater parallel resource needs for spatiotemporal processing. These results suggest that the routing mechanism learns both a global computational structure and fine-grained, modality-aware resource allocation.</p>\n\n",
                "matched_terms": [
                    "such",
                    "analysis",
                    "final",
                    "image",
                    "structure",
                    "output",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.T14\" title=\"Table 14 &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, we present the performance of Uni-MoE-2.0 models at various reinforcement learning stages, as well as their comparison with the original direct-answer models. It can be observed that a significant performance drop in the Uni-MoE-2.0-Cold Start model. We attribute this to the limited amount of Cold-Start data used, which contained complex and diverse tasks that differed substantially from the main evaluation focus on mathematical and scientific reasoning, resulting in a noticeable decline in generalization capability.</p>\n\n",
                "matched_terms": [
                    "present",
                    "start",
                    "main",
                    "models",
                    "which",
                    "various",
                    "from",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After continuing GSPO training based on the Cold-Start model, we find that Uni-MoE-2.0-GSPO shows performance recovery across all evaluation metrics, even surpassing the original Uni-MoE-2.0 model in MathVerse, LogicVista, and MMMU. In particular, MathVerse performance increased by 3.04%.</p>\n\n",
                "matched_terms": [
                    "based",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, after further applying DPO training on top of Uni-MoE-2.0-GSPO, the model exhibits a clear upward trend in performance, especially achieving a 5% improvement on MathVista (testmini) and an average gain of 1.64% across all evaluation metrics. This demonstrates the effectiveness of the DPO stage, showing that using a small amount of DPO data annotated by powerful commercial models can significantly enhance the model&#8217;s reasoning ability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#S4.F10\" title=\"Figure 10 &#8227; 4.6.1 Visual Reasoning &#8227; 4.6 Thinking vs. No-Thinking &#8227; 4 Experiment &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> demonstrates that integrating a structured thinking process markedly improves the faithfulness and coherence of generated images. The baseline model, lacking explicit reasoning, frequently generates semantically inconsistent visuals (e.g., a rodent in an arid habitat or apple trees bearing fruit in winter). Conversely, when guided by a step-by-step reasoning chain, the model successfully parses prompts into constituent visual elements&#8212;such as texture, environment, and seasonal context&#8212;resulting in outputs that are precisely aligned with the prompt&#8217;s semantic intent. Unlike Qwen2.5-Omni and Ming-Lite-Omni, our model incorporates a thinking chain for image generation, similar to the approach used in BAGEL.</p>\n\n",
                "matched_terms": [
                    "thinking",
                    "visual",
                    "generation",
                    "image",
                    "into",
                    "process",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our investigation reveals two key principles for token-based TTS: a dual-rate token strategy and strategic architectural scaling. We find that while a compact 20 tokens per second suffices for audio representation, generation requires 40 tokens per second to capture fine-grained acoustic details. Furthermore, smaller (0.5B) autoregressive models struggle with style conversion, a limitation we address by adopting a MoE architecture. For these models, training on token embeddings from a pre-trained base model proves more effective for capturing vocal style than end-to-end training. Together, these approaches form an effective framework for building efficient and expressive neural spoken systems. Building on these insights, our future work will follow Uni-MoE-Audio, using a single tokenizer for both understanding and generation tasks. This foundation will enable cross-modal training with text and video, facilitating context-aware and voice-preserved applications. Concurrently, we will optimize our MoE architecture with conditional routing for more efficient and controllable multi-speaker synthesis.</p>\n\n",
                "matched_terms": [
                    "capturing",
                    "models",
                    "understanding",
                    "generation",
                    "requires",
                    "two",
                    "from",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate collaborative development, we decoupled the image generation module from the base model. This architectural choice not only simplifies joint training but also enables flexible image generation control through natural language and specialized tokens. While the current text-to-image performance in automated metrics remains limited&#8212;due to the constrained version of our external diffusion model and the scarcity of text&#8211;image paired data&#8212;we observe notable improvements in image editing and low-level image processing tasks. These gains suggest the viability of a language-centric training strategy for integrating diverse image processing capabilities.\nIn future work, we will further refine the architecture for image editing and generation, allowing the base model to perceive and interpret such tasks during pre-training. Nevertheless, the iterative diffusion process for image synthesis and editing will remain externalized. This design reflects our view that multi-step temporal modelling (as in diffusion) operates at a fundamentally different temporal granularity compared to single-step next-token prediction.</p>\n\n",
                "matched_terms": [
                    "such",
                    "through",
                    "generation",
                    "image",
                    "process",
                    "current",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We identify that the model&#8217;s enhanced omnimodality understanding capability stems primarily from its audio-text-vision joint coding training on large-scale video data. Future work will focus on scaling this video data and introducing new multimodal positional encoding methods to advance the model&#8217;s comprehension abilities further.</p>\n\n",
                "matched_terms": [
                    "new",
                    "understanding",
                    "from",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis indicates that a dense model can be directly extended to an MoE architecture. This approach enables the model to achieve more comprehensive capabilities even with limited additional data (75B tokens).\nBuilding on this, our future work will focus on optimizing expert specialization and leveraging large-scale dense model distillation. This strategy will allow us to efficiently construct a unified, fine-grained MoE-based model for omnimodal understanding and generation.</p>\n\n",
                "matched_terms": [
                    "understanding",
                    "generation",
                    "comprehensive",
                    "analysis",
                    "model",
                    "construct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our training approach, which builds upon the progressive strategy from Uni-MoE 1.0, was augmented in version 2.0 with annealing experiments and reinforcement learning. Comparative results demonstrate that the omnimodal large model&#8217;s training is highly sensitive to data recipe, yet our progressive training strategy ensured remarkable stability during the RL phase for MoE-based models. Moving forward, we will refine this iterative RL strategy by introducing distinct RL methods at various training stages to enhance model capabilities periodically.</p>\n\n",
                "matched_terms": [
                    "models",
                    "which",
                    "various",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, Uni-MoE-2.0-Omni represents a significant advancement in the development of open-source, omnimodal large models. By building upon the dense Qwen2.5-7B architecture and introducing key innovations&#8212;a dynamic-capacity MoE design, a progressive training strategy enhanced with iterative reinforcement learning, and a curated multimodal data matching technique&#8212;the model achieves robust capabilities in understanding, reasoning, and generating across text, image, and speech modalities. Notably, we explored a progressive model architecture evolution and training strategy optimization, which can extend dense large language models into efficient MoE-based omnimodal large models, achieving a leap from multimodal understanding to both understanding and generation.\nExtensive evaluations across 85 benchmarks confirm that Uni-MoE-2.0-Omni sets a new state-of-the-art or is highly competitive with leading omnimodal large models, demonstrating particular strengths in video understanding, omnimodal comprehension, long speech understanding and generating, audio-visual, controllable image generation, and low-level image restoration tasks. The commitment to open-sourcing the model&#8217;s code, checkpoints, and data ensures transparency and fosters further innovation in the field of multimodal artificial intelligence.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "generation",
                    "which",
                    "new",
                    "image",
                    "into",
                    "multimodal",
                    "from",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across three categories of capabilities of image understanding: General Visual Understanding, STEM Image Understanding (focusing mainly on Science and Mathematics), and OCR &amp; Document Understanding.</p>\n\n",
                "matched_terms": [
                    "focusing",
                    "visual",
                    "models",
                    "understanding",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">General Visual Understanding</span>. MMBench (EN/CN) <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024mmbench</span>)</cite>, MMStar <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmstar</span>)</cite>, RealWorldQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">real_world_qa</span>)</cite>, GQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gqa</span>)</cite>, MME-RealWorld <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mme-realworld</span>)</cite>, and CV-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cvbench</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">STEM Image Reasoning</span>. AI2D <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ai2d</span>)</cite>, MMMU <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmmu</span>)</cite> and MMMU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yue2024mmmupro</span>)</cite> for science reasoning. MathVista <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathvista</span>)</cite>, MathVision <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mathvision</span>)</cite> and LogicVista <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">logicvista</span>)</cite> for mathematics reasoning.</p>\n\n",
                "matched_terms": [
                    "image",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of video understanding: Short Video Understanding, Long Video Understanding, Video Reasoning and Video Temporal Localization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "understanding",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two benchmarks focused on complex reasoning and knowledge tasks, including the three versions of GPQA <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gpqa</span>)</cite> and the MMLU-Pro <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mmlupro</span>)</cite> dataset.</p>\n\n",
                "matched_terms": [
                    "two",
                    "models",
                    "including",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models on two categories of omni benchmarks that require simultaneous understanding of visual and audio information: Video&amp;Audio and Image&amp;Audio.</p>\n\n",
                "matched_terms": [
                    "two",
                    "visual",
                    "models",
                    "understanding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of visual generation tasks: Image Generation, Image Edition, Controllable Generation and Image Restoration.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "models",
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Image Generation</span>. Wise <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">niu2025wise</span>)</cite> and Coco30K <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lin2014microsoft</span>)</cite>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our models across four categories of capabilities of audio understanding and speech generation: Audio understanding with text reply, text to speech, voice conversation(speech to speech or speech to text), and voice conversation with visual Information(speech and image/video to speech or speech and image/video to text.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "models",
                    "understanding",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For clarity of exposition, we restrict our discussion to the Top-1 MoE setting, and later describe how the approach can be extended to our Dynamic-Capacity MoE. We first consider the Top-1 MoE layer whose output is given by:</p>\n\n",
                "matched_terms": [
                    "first",
                    "setting",
                    "output",
                    "how"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, <math alttext=\"{\\bm{z}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m1\" intent=\":literal\"><semantics><mi>&#119963;</mi><annotation encoding=\"application/x-tex\">{\\bm{z}}</annotation></semantics></math> denotes the router logits, <math alttext=\"\\texttt{Softmax}({\\bm{z}})_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\">Softmax</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{Softmax}({\\bm{z}})_{i}</annotation></semantics></math> is the gating probability for expert <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>, <math alttext=\"{\\bm{D}}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119915;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{D}}_{i}</annotation></semantics></math> is a binary mask indicating whether expert <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is selected.\nLet <math alttext=\"f(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m6\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">f(\\cdot)</annotation></semantics></math> denote the remainder of the network, including the loss function. The training objective can then be expressed as:</p>\n\n",
                "matched_terms": [
                    "including",
                    "here"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Equation <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.E5\" title=\"Equation 5 &#8227; A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> rewrites the expectation over <math alttext=\"{\\bm{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m7\" intent=\":literal\"><semantics><mi>&#119915;</mi><annotation encoding=\"application/x-tex\">{\\bm{D}}</annotation></semantics></math> as a weighted sum over experts, where each term is the loss contribution from a single expert multiplied by its routing probability.\nFor notational simplicity, we denote <math alttext=\"{\\bm{p}}=\\texttt{Softmax}({\\bm{z}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m8\" intent=\":literal\"><semantics><mrow><mi>&#119953;</mi><mo>=</mo><mrow><mtext class=\"ltx_mathvariant_monospace\">Softmax</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">{\\bm{p}}=\\texttt{Softmax}({\\bm{z}})</annotation></semantics></math>. The gradient of <math alttext=\"\\mathcal{L}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m9\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><annotation encoding=\"application/x-tex\">\\mathcal{L}</annotation></semantics></math> with respect to <math alttext=\"{\\bm{z}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m10\" intent=\":literal\"><semantics><mi>&#119963;</mi><annotation encoding=\"application/x-tex\">{\\bm{z}}</annotation></semantics></math> can be written as:</p>\n\n",
                "matched_terms": [
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second equality in Equation <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.E6\" title=\"Equation 6 &#8227; A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> is usually known as baseline subtraction.\nIn the ODE literature, the term <math alttext=\"f\\big({\\bm{p}}_{i}\\cdot Expert({\\bm{x}},{\\bm{w}}_{i})\\big)-f(0)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p1.m11\" intent=\":literal\"><semantics><mrow><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><mrow><msub><mi>&#119953;</mi><mi>i</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mi>E</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119961;</mi><mo>,</mo><msub><mi>&#119960;</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><mi>f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">f\\big({\\bm{p}}_{i}\\cdot Expert({\\bm{x}},{\\bm{w}}_{i})\\big)-f(0)</annotation></semantics></math> can be approximated in various ways. We focus on two common numerical schemes:</p>\n\n",
                "matched_terms": [
                    "two",
                    "various"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We next present two alternative approximations of <math alttext=\"\\nabla_{{\\bm{z}}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mo>&#8711;</mo><mi>&#119963;</mi></msub><annotation encoding=\"application/x-tex\">\\nabla_{{\\bm{z}}}</annotation></semantics></math> based on two numerical schemes.</p>\n\n",
                "matched_terms": [
                    "two",
                    "present",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Third-order (Heun) approximation.</span>\nUsing Heun&#8217;s method, which combines multiple derivative evaluations for higher accuracy, we obtain:</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hybrid gradient estimator.</span>\nTo balance the stability of router training with the diversity of expert learning, we combine the two estimators above: the first-order gradient is used when the selected expert corresponds to <math alttext=\"\\arg\\max({\\bm{z}})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m1\" intent=\":literal\"><semantics><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\arg\\max({\\bm{z}})</annotation></semantics></math>, while the third-order gradient is applied otherwise.\nLet <math alttext=\"\\delta_{\\bm{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m2\" intent=\":literal\"><semantics><msub><mi>&#948;</mi><mi>&#119915;</mi></msub><annotation encoding=\"application/x-tex\">\\delta_{\\bm{D}}</annotation></semantics></math> denote the indicator <math alttext=\"\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>&#948;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119915;</mi><mo>=</mo><mrow><mi>arg</mi><mo lspace=\"0.167em\">&#8289;</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\delta({\\bm{D}}=\\arg\\max({\\bm{z}}))</annotation></semantics></math>. The combined estimator can be written as:\n\n</p>\n\n",
                "matched_terms": [
                    "two",
                    "above"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gradient Estimation Function.</span> Following the hybrid gradient estimation in Equation <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#A1.E7\" title=\"Equation 7 &#8227; A.4 Gradient Estimation Formalization &#8227; Appendix A Appendix &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, the computation can be described as follows.\nFirst, we compute the forward output of the sampled expert <math alttext=\"{\\bm{D}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p6.m1\" intent=\":literal\"><semantics><mi>&#119915;</mi><annotation encoding=\"application/x-tex\">{\\bm{D}}</annotation></semantics></math> weighted by its routing probability:</p>\n\n",
                "matched_terms": [
                    "first",
                    "output",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To extend it to our Top-<math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p9.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> strategy, we apply the same computation sequentially to each activated expert, sampling without replacement from the routing distribution until the cumulative probability exceeds <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SS4.p9.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math>.\nIn this way, the gradient estimation naturally generalizes to multiple experts while remaining consistent with the selection process in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12609v2#algorithm1\" title=\"Algorithm 1 &#8227; Routing Gradient Estimation &#8227; 2.3.2 Dynamic-Capacity MoE &#8227; 2.3 Main Architecture &#8227; 2 Uni-MoE-2.0-Omni &#8227; Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ensuring that the hybrid scaling mechanism is fully compatible with our dynamic-capacity MoE framework.</p>\n\n",
                "matched_terms": [
                    "each",
                    "from",
                    "process"
                ]
            }
        ]
    }
}