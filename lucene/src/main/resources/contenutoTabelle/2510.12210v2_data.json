{
    "S4.T1": {
        "source_file": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation",
        "caption": "Table 1: \nEvaluation results of DiSTAR and other systems LibriSpeech-PC test-clean and SeedTTS test-en. ◆\\blacklozenge denotes the scores reported in DiTAR paper. The boldface and underline indicate the best and the second-best result, respectively. ↑\\uparrow and ↓\\downarrow indicate that lower or higher values are better.",
        "body": "System\n#Params\nWER(%)↓\\downarrow\nSIM↑\\uparrow\nUTMOS↑\\uparrow\n\n\nLibriSpeech test-clean\n\n\nHuman\n-\n1.80\n0.69\n4.10\n\n\nRVQ resynthesized\n-\n1.83\n0.66\n4.23\n\n\nIndexTTS\n0.5B\n2.57\n0.62\n4.35\n\n\nE2TTS (NFE=32)\n0.3B\n2.74\n0.70\n3.47\n\n\nF5TTS-v1 (NFE=32)\n0.3B\n2.02\n0.68\n3.83\n\n\nDiTAR (NFE=10) ◆\n\n0.6B\n2.39\n0.67\n4.22\n\n\nDiSTAR-base (NFE=24)\n0.15B\n1.90\n0.64\n4.29\n\n\nDiSTAR-medium (NFE=24)\n0.3B\n1.66\n0.67\n4.27\n\n\nSeed-TTS test-en\n\n\nHuman\n-\n1.47\n0.73\n3.53\n\n\nRVQ resynthesized\n-\n1.71\n0.70\n3.79\n\n\nIndexTTS\n0.5B\n1.92\n0.61\n3.98\n\n\nE2TTS (NFE=32)\n0.3B\n2.20\n0.71\n3.20\n\n\nF5TTS-v1 (NFE=32)\n0.3B\n1.35\n0.68\n3.66\n\n\nDiTAR (NFE=10) ◆\n\n0.6B\n1.78\n0.64\n4.15\n\n\nDiSTAR-base (NFE=24)\n0.15B\n1.51\n0.64\n3.93\n\n\nDiSTAR-medium (NFE=24)\n0.3B\n1.32\n0.66\n4.05",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">System</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">#Params</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER(%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">UTMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">LibriSpeech test-clean</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Human</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">RVQ resynthesized</th>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">1.83</td>\n<td class=\"ltx_td ltx_align_center\">0.66</td>\n<td class=\"ltx_td ltx_align_center\">4.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">IndexTTS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.35</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">E2TTS (NFE=32)</th>\n<td class=\"ltx_td ltx_align_center\">0.3B</td>\n<td class=\"ltx_td ltx_align_center\">2.74</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.70</span></td>\n<td class=\"ltx_td ltx_align_center\">3.47</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">F5TTS-v1 (NFE=32)</th>\n<td class=\"ltx_td ltx_align_center\">0.3B</td>\n<td class=\"ltx_td ltx_align_center\">2.02</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\">3.83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">DiTAR (NFE=10) <sup class=\"ltx_sup\">&#9670;</sup>\n</th>\n<td class=\"ltx_td ltx_align_center\">0.6B</td>\n<td class=\"ltx_td ltx_align_center\">2.39</td>\n<td class=\"ltx_td ltx_align_center\">0.67</td>\n<td class=\"ltx_td ltx_align_center\">4.22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">DiSTAR-base (NFE=24)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.15B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.29</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">DiSTAR-medium (NFE=24)</th>\n<td class=\"ltx_td ltx_align_center\">0.3B</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.66</span></td>\n<td class=\"ltx_td ltx_align_center\">0.67</td>\n<td class=\"ltx_td ltx_align_center\">4.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">Seed-TTS test-en</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Human</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">RVQ resynthesized</th>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">1.71</td>\n<td class=\"ltx_td ltx_align_center\">0.70</td>\n<td class=\"ltx_td ltx_align_center\">3.79</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">IndexTTS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.98</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">E2TTS (NFE=32)</th>\n<td class=\"ltx_td ltx_align_center\">0.3B</td>\n<td class=\"ltx_td ltx_align_center\">2.20</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center\">3.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">F5TTS-v1 (NFE=32)</th>\n<td class=\"ltx_td ltx_align_center\">0.3B</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">1.35</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\">3.66</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">DiTAR (NFE=10) <sup class=\"ltx_sup\">&#9670;</sup>\n</th>\n<td class=\"ltx_td ltx_align_center\">0.6B</td>\n<td class=\"ltx_td ltx_align_center\">1.78</td>\n<td class=\"ltx_td ltx_align_center\">0.64</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.15</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">DiSTAR-base (NFE=24)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.15B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.93</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">DiSTAR-medium (NFE=24)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.3B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">1.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.05</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "distar",
            "indextts",
            "respectively",
            "secondbest",
            "reported",
            "lower",
            "06b",
            "underline",
            "↑uparrow",
            "denotes",
            "ditar",
            "evaluation",
            "best",
            "015b",
            "nfe10",
            "results",
            "nfe24",
            "testen",
            "resynthesized",
            "paper",
            "distarbase",
            "scores",
            "boldface",
            "system",
            "wer↓downarrow",
            "systems",
            "f5ttsv1",
            "distarmedium",
            "↓downarrow",
            "librispeechpc",
            "rvq",
            "e2tts",
            "params",
            "◆blacklozenge",
            "testclean",
            "03b",
            "sim↑uparrow",
            "higher",
            "result",
            "indicate",
            "librispeech",
            "nfe32",
            "utmos↑uparrow",
            "better",
            "human",
            "05b",
            "seedtts",
            "values",
            "other"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">we show comparison results with SOTA baselines. The main results of objective metrics are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T1\" title=\"Table 1 &#8227; Model settings and baselines. &#8227; 4.1 Experimental Settings &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Subjective results are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T2\" title=\"Table 2 &#8227; 4.2 Zero-Shot TTS &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker/style consistency, while maintaining rich output diversity. Audio samples are provided on <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/w/DiSTAR_demo\" title=\"\">https://anonymous.4open.science/w/DiSTAR_demo</a>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A third, increasingly influential direction embraces multi-codebook discrete representations, typically residual vector quantization (RVQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib30\" title=\"\">2022</a>)</cite>, where several codebooks per frame progressively capture detail <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib8\" title=\"\">2024</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib57\" title=\"\">2025</a>)</cite>. RVQ confers two attractive properties: (i) at a sufficient aggregate bitrate it can reconstruct high-fidelity audio; and (ii) by remaining discrete, it preserves the stability and interpretability of LM-style training and decoding. However, RVQ introduces a second dependency axis beyond temporal order: intra-frame depth &#8211; the strong correlation among codebooks at the same time frame &#8211; is critical for quality. Effective RVQ TTS systems must therefore model time and depth jointly. Previous explorations, such as including flattening codebooks<cite class=\"ltx_cite ltx_citemacro_cite\">Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib4\" title=\"\">2022</a>)</cite>, semantic-to-acoustic hierarchies<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite>, RQ-Transformer<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib59\" title=\"\">2023</a>)</cite>, and delay-pattern scheduling<cite class=\"ltx_cite ltx_citemacro_cite\">Copet et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib11\" title=\"\">2023</a>)</cite>, partially address this coupling but still trade off inference parallelism, long-range consistency, and train/infer efficiency, leaving RVQ&#8217;s full potential under-exploited. This raises a central question: Can we architect a generator that natively models RVQ&#8217;s joint time-depth structure, achieving high quality at reasonable compute?</p>\n\n",
                "matched_terms": [
                    "rvq",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, we introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;(<span class=\"ltx_text ltx_font_bold\">DI</span>ffusion over a <span class=\"ltx_text ltx_font_bold\">S</span>calable <span class=\"ltx_text ltx_font_bold\">T</span>oken <span class=\"ltx_text ltx_font_bold\">A</span>uto<span class=\"ltx_text ltx_font_bold\">R</span>egressive Representation for Speech Generation), a zero-shot TTS framework that operates entirely in the RVQ discrete code space and tightly couples an AR language model with a masked diffusion transformer. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;adopts the patch-wise factorization strategy popularized in next-patch systems <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>)</cite>: it aggregates RVQ tokens into patches. A causal LM drafts the next patch by predicting a compact hidden sketch that captures coarse temporal evolution, after which a discrete masked diffusion Transformer performs parallel infilling within the patch. Inspired by LLaDA <cite class=\"ltx_cite ltx_citemacro_cite\">Nie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib38\" title=\"\">2025</a>)</cite>, the masked diffusion component operates not as a continuous denoiser but as a iterative discrete demasking process over masked positions, thereby resolving multi-codebook (depth) dependencies and supporting efficient parallel synthesis in the RVQ code space.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several design choices in <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yield practical advantages. First, the fully discrete setting preserves an <span class=\"ltx_text ltx_font_typewriter\">[EOS]</span> token for the immediate termination of patch-level generation, eliminating auxiliary duration predictors or stop heads <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>); Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib45\" title=\"\">2018</a>)</cite>. Second, sharing the RVQ code space between the AR sketcher and the masked diffusion refiner allows end-to-end optimization and reduces inter-module mismatch relative to cascaded pipelines <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib16\" title=\"\">2024b</a>)</cite>. Third, decoding achieves robust quality under purely greedy settings, while temperature-based sampling extends to RVQ-level and joint layer&#8211;time strategies, enabling fine-grained control over the diversity&#8211;determinism trade-off. Fourth, pruning the upper RVQ layers at inference controls computation and bitrate to match bandwidth/latency constraints without retraining.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On standard zero-shot TTS benchmarks, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;demonstrates strong robustness, speaker similarity, and naturalness, while maintaining the inference cost close to its continuous counterpart DiTAR and using fewer/comparable parameters than competitive baselines. Audio samples are available on the demo page.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "ditar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot TTS framework that couples an autoregressive drafter with masked diffusion entirely in the discrete RVQ domain, achieving patch-level parallelism and joint modeling of RVQ layer&#8211;time dependencies.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In standard zero-shot benchmarks, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;provides state-of-the-art robustness, speaker similarity, and naturalness with comparable or lower computational cost.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "lower"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot TTS work broadly bifurcates by the target representation. Continuous-latent approaches predict high-information features (mel or codec latents) with diffusion/flow to boost long-range consistency and robust cloning. <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib46\" title=\"\">2023</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib25\" title=\"\">2024</a>)</cite> scale latent and factorized diffusion with speech prompting; <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib29\" title=\"\">2024</a>)</cite> casts text-guided speech infilling as flow matching; <cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib17\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>)</cite> report strong results via continuous flow matching; <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib60\" title=\"\">2025</a>)</cite> simplify training with scalar-latent codecs and Transformer diffusion; <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib32\" title=\"\">2024</a>)</cite> improves time-varying style control; and recent <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib31\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib34\" title=\"\">2024</a>)</cite> explore DiT-based and autoregressive diffusion decoders, while <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>; Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib53\" title=\"\">2024</a>)</cite> place diffusion heads inside causal stacks to retain AR-like controllability. In contrast, discrete-token pipelines discretize speech and leverage AR LMs for stronger in-context prompting and explicit decoding control. <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite> set the Nerual codec LM recipe, with <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib8\" title=\"\">2024</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib51\" title=\"\">2025b</a>)</cite> improving robustness and human parity; token-infilling AR models such as <cite class=\"ltx_cite ltx_citemacro_citep\">(Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib26\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib41\" title=\"\">2024</a>)</cite> advanced editing and wild-data zero-shot; <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>)</cite> uses masked generative codec modeling; and large multilingual systems such as<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib15\" title=\"\">2024a</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib5\" title=\"\">2024</a>)</cite>, broaden coverage and controllability.</p>\n\n",
                "matched_terms": [
                    "results",
                    "human",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S2.F1\" title=\"Figure 1 &#8227; 2.2 Mask Diffusion Models &#8227; 2 Related Works &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, an autoregressive architecture that advances patch by patch while remaining entirely within a discrete residual vector-quantized (RVQ) code domain.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recast long sequences of discrete speech codes into a tiled layout of patches, akin to the DiTAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>)</cite> paradigm. An autoregressive (AR) Transformer governs dependencies across patches, while a masked diffusion Transformer completes the contents within each patch in parallel. In practice, generation advances along the patch index autoregressively, with the next patch produced via conditional masked diffusion.\nLet <math alttext=\"\\mathbf{C}=[\\mathbf{c}_{0},\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{L-1}]\\in\\mathbb{Z}^{L\\times J}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119836;</mi><mn>0</mn></msub><mo>,</mo><msub><mi>&#119836;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119836;</mi><mrow><mi>L</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8484;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>J</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}=[\\mathbf{c}_{0},\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{L-1}]\\in\\mathbb{Z}^{L\\times J}</annotation></semantics></math> denote the sequence of RVQ codes from <math alttext=\"J\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>J</mi><annotation encoding=\"application/x-tex\">J</annotation></semantics></math> quantizers, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the RVQ code sequence length. And let <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m4\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> be the input text.\nWe estimate the <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m5\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> by conditional maximum likelihood over the patch-grouped codes. By the chain rule, the model factorizes as</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq",
                    "ditar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Omega\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.Px3.p3.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#937;</mi><annotation encoding=\"application/x-tex\">\\Omega</annotation></semantics></math> indexes the currently editable sites, <math alttext=\"s_{t}(i)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.Px3.p3.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s_{t}(i)</annotation></semantics></math> denotes per-position confidence scores, <math alttext=\"\\widehat{\\mathbf{C}}^{(k)}_{\\rho_{n}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.Px3.p3.m7\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119810;</mi><mo>^</mo></mover><msub><mi>&#961;</mi><mi>n</mi></msub><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\widehat{\\mathbf{C}}^{(k)}_{\\rho_{n}}</annotation></semantics></math> is produced as a provisional completion at this timestep, and <math alttext=\"\\mathrm{Mask}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.Px3.p3.m8\" intent=\":literal\"><semantics><mrow><mi>Mask</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Mask}(\\cdot,\\cdot)</annotation></semantics></math> replaces the selected entries with the mask token. This schedule enforces a monotone decrease in the masked ratio and mirrors the forward-noising trajectory, aligning the reverse refinement with the corruption process. Throughout decoding we employ classifier-free guidance <cite class=\"ltx_cite ltx_citemacro_citep\">(Ho &amp; Salimans, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib20\" title=\"\">2022</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib33\" title=\"\">2024</a>)</cite> with rescaling, following <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "denotes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S2.F1\" title=\"Figure 1 &#8227; 2.2 Mask Diffusion Models &#8227; 2 Related Works &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> sketches the <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;pipeline. In the spirit of DiTAR, we adopt a blockwise decomposition of the RVQ code stream: a long sequence of discrete tokens is sliced into patch-level units. Dependencies across patches are modeled by a causal autoregressive (AR) language model, whereas a Transformer-based masked diffusion module decodes within-patch content in parallel.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq",
                    "ditar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;dispenses with both an explicit duration predictor and forced alignment. Moreover, unlike prior designs, the RVQ aggregator is not restricted to disjoint patches: overlapping windows are permitted. At each decoding step, the diffusion head predicts, in one shot, a segment whose length matches the aggregator&#8217;s stride on the output stream, ensuring consistency. Additional architectural and training details are provided in the following sections.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we position <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;against strong contemporary systems and report state-of-the-art results.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "results",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models are trained on Emilia <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib19\" title=\"\">2024</a>)</cite>, a large, multilingual, in-the-wild speech corpus curated for scalable speech generation. For this study we use only its English subset, totaling roughly <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>50</mn><annotation encoding=\"application/x-tex\">50</annotation></semantics></math>k hours. Evaluation spans two open benchmarks: (i) LibriSpeech(PC) <cite class=\"ltx_cite ltx_citemacro_citep\">(Meister et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib36\" title=\"\">2023</a>)</cite> test-clean, <math alttext=\"5.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>5.4</mn><annotation encoding=\"application/x-tex\">5.4</annotation></semantics></math> hours from <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math> unique English speakers; we follow the established zero-shot cross-sentence protocol and adopt the subset from F5TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>)</cite> (40 prompts, 1127 utterances), and (ii) SeedTTS test-en, introduced with Seed-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>)</cite>, consisting of 1088 English samples drawn from Common Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib2\" title=\"\">2019</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "librispeechpc",
                    "testen",
                    "seedtts",
                    "testclean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train all models on 64 NVIDIA A100 80GB GPUs. We train <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;of two sizes <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-base (0.15B), and <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-medium (0.3B). Architectural specifics are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A2.SS2\" title=\"B.2 Model Architecture &#8227; Appendix B Implementation Details &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>. Unless otherwise noted, we use a patch size of <math alttext=\"8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>8</mn><annotation encoding=\"application/x-tex\">8</annotation></semantics></math> and condition the diffusion module on a single historical patch. Inference procedures are detailed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S3.SS4.SSS0.Px4\" title=\"Decoding heuristics. &#8227; 3.4 Training and Inference &#8227; 3 DiSTAR &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "distarmedium",
                    "015b",
                    "distarbase",
                    "03b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;exhibits strong overall performance. Across benchmarks it attains the lowest WER, indicating robust synthesis. We assess speaker similarity using both objective and human judgments: SIM and speaker SMOS. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields SIM on par with the best alternatives and leads on SMOS. We attribute these gains in part to reduced sensitivity to high-frequency artifacts in the reference prompt, which preserves cleaner timbral cues during cloning. The same trend is observed in the objective predictor UTMOS and in CMOS listening tests. Notably, relative to continuous-representation systems, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;achieves comparable or superior perceptual quality with a similar or smaller parameter budget. As model capacity grows, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields consistent improvements on objective metrics, closely matching the scaling behavior reported for discrete-token autoregressive systems and indicating a healthy scaling trajectory.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "systems",
                    "best",
                    "human",
                    "reported"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we conduct a detailed analysis of the various components of <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>. Unless specified otherwise, we default to using <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-base with 0.15 billion parameters, a patch size of 8, a stride of 8, and NFE of 24, tested on the LibriSpeech-PC test dataset mentioned above. We discuss the patch size in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A4\" title=\"Appendix D Patch Size. &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> and classifier-guidance free settings in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A3\" title=\"Appendix C Classifier-Free Guidance &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "librispeechpc",
                    "distarbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;under multiple inference strategies (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T3\" title=\"Table 3 &#8227; Decoding Strategies. &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nDeterministic greedy decoding yields the lowest WER, evidencing stable synthesis,\nwhile speaker similarity is slightly lower than with stochastic sampling.\nThis aligns with the standard diversity-determinism trade-off: sampling can\nrecover timbral nuances at the cost of higher variability.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "lower",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;randomly drops the last <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math> RVQ layers. At inference, we simply prune higher RVQ layers to achieve variable bitrate and compute. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.F2\" title=\"Figure 2 &#8227; 4.4 Inference Efficiency and Controllability &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the inference&#8211;quality trade-off under different RVQ pruning depths. As more RVQ layers are retained (hence higher FLOPs), speaker similarity increases markedly, whereas WER changes little and reaches its minimum around six layers. This pattern is consistent with the hypothesis that upper RVQ layers primarily encode acoustic detail rather than linguistic content <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot TTS framework that operates in an RVQ code space and tightly couples an autoregressive Transformer with masked diffusion. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;achieves blockwise parallelism while effectively modeling layer&#8211;time dependencies across RVQ levels and local temporal neighborhoods, without an explicit duration predictor. We further introduced a simple but effective RVQ-aware sampling procedure that stabilizes inference and improves perceptual quality. In combination, these design choices yield SOTA robustness, speaker similarity, and naturalness in zero-shot speech synthesis, while exposing clear levers for controllability at inference time.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results are currently demonstrated under the codec, RVQ configuration, and evaluation setups described in the paper. Performance may depend on the RVQ depth and codebook design. Due to resource constraints, we trained our model solely on a around-50k-hour English corpus<cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib19\" title=\"\">2024</a>)</cite>; broader generalization to multilingual and multi-style settings remains to be evaluated.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "rvq",
                    "results",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize 64 A100 GPUs, each processing a batch size of 36K token frames, and train <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160; for 0.6M steps. The AdamW optimizer is employed with a learning rate of 0.75e-4 for AR, and 1.5e-4 for the other parts, <math alttext=\"\\beta_{t}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mi>t</mi></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{t}=0.9</annotation></semantics></math>, and <math alttext=\"\\beta_{2}=0.99\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.99</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A3.T5\" title=\"Table 5 &#8227; Appendix C Classifier-Free Guidance &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, holding all other factors fixed, nested CFG (Scheme <span class=\"ltx_text ltx_font_bold\">B</span>) and simple single-step CFG (Scheme <span class=\"ltx_text ltx_font_bold\">A</span>) yield nearly identical objective metrics. To reduce computational cost and latency, we therefore adopt the simpler Scheme <span class=\"ltx_text ltx_font_bold\">A</span> (single-step CFG) as the default in the main experiments.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "other"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation",
        "caption": "Table 2: Subjective evaluation results on Seed-TTS test-en dataset. We compare DiTAR with several leading TTS systems based on discrete or/and continuous speech representations.",
        "body": "System\nSMOS\nCMOS\n\n\n\n\nHuman\n3.073.07\n0.000.00\n\n\nFireRedTTS\n2.36±0.582.36_{\\pm 0.58}\n−0.34±0.21-0.34_{\\pm 0.21}\n\n\nCosyVoice 2\n3.07±0.213.07_{\\pm 0.21}\n−0.04±0.17-0.04_{\\pm 0.17}\n\n\nE2TTS\n3.29±0.193.29_{\\pm 0.19}\n−0.08±0.22-0.08_{\\pm 0.22}\n\n\nF5TTS\n3.08±0.203.08_{\\pm 0.20}\n0.01±0.120.01_{\\pm 0.12}\n\n\nDiSTAR\n3.31±0.25\\mathbf{3.31}_{\\pm 0.25}\n0.22±0.13\\mathbf{0.22}_{\\pm 0.13}",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">System</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SMOS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">CMOS</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Human</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mn>3.07</mn><annotation encoding=\"application/x-tex\">3.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.00\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mn>0.00</mn><annotation encoding=\"application/x-tex\">0.00</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">FireRedTTS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.36_{\\pm 0.58}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><msub><mn>2.36</mn><mrow><mo>&#177;</mo><mn>0.58</mn></mrow></msub><annotation encoding=\"application/x-tex\">2.36_{\\pm 0.58}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"-0.34_{\\pm 0.21}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><msub><mn>0.34</mn><mrow><mo>&#177;</mo><mn>0.21</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">-0.34_{\\pm 0.21}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CosyVoice 2</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.07_{\\pm 0.21}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><msub><mn>3.07</mn><mrow><mo>&#177;</mo><mn>0.21</mn></mrow></msub><annotation encoding=\"application/x-tex\">3.07_{\\pm 0.21}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"-0.04_{\\pm 0.17}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><msub><mn>0.04</mn><mrow><mo>&#177;</mo><mn>0.17</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">-0.04_{\\pm 0.17}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">E2TTS</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.29_{\\pm 0.19}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><msub><mn>3.29</mn><mrow><mo>&#177;</mo><mn>0.19</mn></mrow></msub><annotation encoding=\"application/x-tex\">3.29_{\\pm 0.19}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"-0.08_{\\pm 0.22}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><msub><mn>0.08</mn><mrow><mo>&#177;</mo><mn>0.22</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">-0.08_{\\pm 0.22}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">F5TTS</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.08_{\\pm 0.20}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><msub><mn>3.08</mn><mrow><mo>&#177;</mo><mn>0.20</mn></mrow></msub><annotation encoding=\"application/x-tex\">3.08_{\\pm 0.20}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.01_{\\pm 0.12}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><msub><mn>0.01</mn><mrow><mo>&#177;</mo><mn>0.12</mn></mrow></msub><annotation encoding=\"application/x-tex\">0.01_{\\pm 0.12}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">DiSTAR</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><math alttext=\"\\mathbf{3.31}_{\\pm 0.25}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><msub><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">3.31</mn><mrow><mo>&#177;</mo><mn>0.25</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{3.31}_{\\pm 0.25}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><math alttext=\"\\mathbf{0.22}_{\\pm 0.13}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><msub><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.22</mn><mrow><mo>&#177;</mo><mn>0.13</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{0.22}_{\\pm 0.13}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "distar",
            "cmos",
            "leading",
            "speech",
            "subjective",
            "several",
            "331±025mathbf331pm",
            "cosyvoice",
            "022±013mathbf022pm",
            "ditar",
            "evaluation",
            "f5tts",
            "results",
            "continuous",
            "testen",
            "307±021307pm",
            "−008±022008pm",
            "308±020308pm",
            "−034±021034pm",
            "system",
            "smos",
            "−004±017004pm",
            "fireredtts",
            "systems",
            "001±012001pm",
            "e2tts",
            "representations",
            "dataset",
            "236±058236pm",
            "discrete",
            "tts",
            "compare",
            "human",
            "orand",
            "seedtts",
            "329±019329pm",
            "based"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">we show comparison results with SOTA baselines. The main results of objective metrics are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T1\" title=\"Table 1 &#8227; Model settings and baselines. &#8227; 4.1 Experimental Settings &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Subjective results are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T2\" title=\"Table 2 &#8227; 4.2 Zero-Shot TTS &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker/style consistency, while maintaining rich output diversity. Audio samples are provided on <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/w/DiSTAR_demo\" title=\"\">https://anonymous.4open.science/w/DiSTAR_demo</a>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "discrete",
                    "systems",
                    "speech",
                    "tts",
                    "continuous",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot text-to-speech (TTS) aims to synthesize natural, intelligible speech for an unseen speaker, matching voice timbre and style from only a brief prompt while remaining robust over long passages &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib16\" title=\"\">2024b</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>)</cite>. This capability is central to open-domain narration, content creation, and conversational agents <cite class=\"ltx_cite ltx_citemacro_citep\">(Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib23\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib37\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A first mainstream route models speech as a sequence of discrete tokens from a single codebook (e.g., from a neural audio codec) and applies an autoregressive language model (AR) to predict the next token, followed by a vocoder/codec decoder to reconstruct the waveform <cite class=\"ltx_cite ltx_citemacro_cite\">Anastassiou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>); Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib46\" title=\"\">2023</a>)</cite>. This track inherits mature AR modeling and decoding strategies from the Natural Language Processing community: discrete <span class=\"ltx_text ltx_font_typewriter\">[EOS]</span> tokens enable clear termination; maximum-likelihood training is relatively stable compared with Mean Squared/Absolute Error Loss; and decoding hyper-parameters (temperature, top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>/top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>) and perplexity provide interpretable control at inference. However, codec rate sequences are long; exposure bias compounds in thousands of steps; and purely AR decoders struggle with long-range consistency (speaker/style drift) and throughput <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib49\" title=\"\">2024</a>)</cite>. Moreover, when a single discrete layer runs at low bitrate or with limited expressivity, reconstructions tend to lose fidelity and fine detail.</p>\n\n",
                "matched_terms": [
                    "discrete",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A second line of work models continuous speech representations (e.g., mel spectrograms or latents from pre-trained audio variational autoencoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Kingma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib27\" title=\"\">2019</a>)</cite> or self-supervised learning encoders) and generates in the continuous space via diffusion or continuous-domain AR before vocoding to waveform <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>; Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib17\" title=\"\">2024</a>)</cite>. Recent work interleaves AR drafting with <span class=\"ltx_text ltx_font_italic\">next-patch diffusion</span> over continuous latents, balancing quality and compute, and achieving impressive zero-shot cloning and cross-speaker generalization <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>)</cite>. Yet continuous latents introduce practical fragilities: modeling high-dimensional, information-dense features complicates optimization and convergence; systems are sensitive to domain shift; many rely on explicit duration predictors; and pipelines that add reinforcement learning (RL) or intricate aligners raise engineering and tuning burden.</p>\n\n",
                "matched_terms": [
                    "representations",
                    "continuous",
                    "speech",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A third, increasingly influential direction embraces multi-codebook discrete representations, typically residual vector quantization (RVQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib30\" title=\"\">2022</a>)</cite>, where several codebooks per frame progressively capture detail <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib8\" title=\"\">2024</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib57\" title=\"\">2025</a>)</cite>. RVQ confers two attractive properties: (i) at a sufficient aggregate bitrate it can reconstruct high-fidelity audio; and (ii) by remaining discrete, it preserves the stability and interpretability of LM-style training and decoding. However, RVQ introduces a second dependency axis beyond temporal order: intra-frame depth &#8211; the strong correlation among codebooks at the same time frame &#8211; is critical for quality. Effective RVQ TTS systems must therefore model time and depth jointly. Previous explorations, such as including flattening codebooks<cite class=\"ltx_cite ltx_citemacro_cite\">Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib4\" title=\"\">2022</a>)</cite>, semantic-to-acoustic hierarchies<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite>, RQ-Transformer<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib59\" title=\"\">2023</a>)</cite>, and delay-pattern scheduling<cite class=\"ltx_cite ltx_citemacro_cite\">Copet et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib11\" title=\"\">2023</a>)</cite>, partially address this coupling but still trade off inference parallelism, long-range consistency, and train/infer efficiency, leaving RVQ&#8217;s full potential under-exploited. This raises a central question: Can we architect a generator that natively models RVQ&#8217;s joint time-depth structure, achieving high quality at reasonable compute?</p>\n\n",
                "matched_terms": [
                    "discrete",
                    "systems",
                    "tts",
                    "several",
                    "representations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, we introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;(<span class=\"ltx_text ltx_font_bold\">DI</span>ffusion over a <span class=\"ltx_text ltx_font_bold\">S</span>calable <span class=\"ltx_text ltx_font_bold\">T</span>oken <span class=\"ltx_text ltx_font_bold\">A</span>uto<span class=\"ltx_text ltx_font_bold\">R</span>egressive Representation for Speech Generation), a zero-shot TTS framework that operates entirely in the RVQ discrete code space and tightly couples an AR language model with a masked diffusion transformer. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;adopts the patch-wise factorization strategy popularized in next-patch systems <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>)</cite>: it aggregates RVQ tokens into patches. A causal LM drafts the next patch by predicting a compact hidden sketch that captures coarse temporal evolution, after which a discrete masked diffusion Transformer performs parallel infilling within the patch. Inspired by LLaDA <cite class=\"ltx_cite ltx_citemacro_cite\">Nie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib38\" title=\"\">2025</a>)</cite>, the masked diffusion component operates not as a continuous denoiser but as a iterative discrete demasking process over masked positions, thereby resolving multi-codebook (depth) dependencies and supporting efficient parallel synthesis in the RVQ code space.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "discrete",
                    "systems",
                    "speech",
                    "tts",
                    "continuous"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The design gives two main advantages. Compared with continuous next-patch diffusion, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;avoids optimization issues in high-dimensional continuous latents while retaining patch-level parallelism. Compared with single-codebook AR, it models the intra-frame multi-codebook coupling inside each patch, improving coherence and allowing depthwise parallel refinement, which reduces exposure-bias effects without sacrificing the robustness of discrete training.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "continuous",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several design choices in <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yield practical advantages. First, the fully discrete setting preserves an <span class=\"ltx_text ltx_font_typewriter\">[EOS]</span> token for the immediate termination of patch-level generation, eliminating auxiliary duration predictors or stop heads <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>); Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib45\" title=\"\">2018</a>)</cite>. Second, sharing the RVQ code space between the AR sketcher and the masked diffusion refiner allows end-to-end optimization and reduces inter-module mismatch relative to cascaded pipelines <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib16\" title=\"\">2024b</a>)</cite>. Third, decoding achieves robust quality under purely greedy settings, while temperature-based sampling extends to RVQ-level and joint layer&#8211;time strategies, enabling fine-grained control over the diversity&#8211;determinism trade-off. Fourth, pruning the upper RVQ layers at inference controls computation and bitrate to match bandwidth/latency constraints without retraining.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "discrete",
                    "several"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On standard zero-shot TTS benchmarks, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;demonstrates strong robustness, speaker similarity, and naturalness, while maintaining the inference cost close to its continuous counterpart DiTAR and using fewer/comparable parameters than competitive baselines. Audio samples are available on the demo page.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "continuous",
                    "tts",
                    "ditar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot TTS framework that couples an autoregressive drafter with masked diffusion entirely in the discrete RVQ domain, achieving patch-level parallelism and joint modeling of RVQ layer&#8211;time dependencies.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "discrete",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot TTS work broadly bifurcates by the target representation. Continuous-latent approaches predict high-information features (mel or codec latents) with diffusion/flow to boost long-range consistency and robust cloning. <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib46\" title=\"\">2023</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib25\" title=\"\">2024</a>)</cite> scale latent and factorized diffusion with speech prompting; <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib29\" title=\"\">2024</a>)</cite> casts text-guided speech infilling as flow matching; <cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib17\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>)</cite> report strong results via continuous flow matching; <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib60\" title=\"\">2025</a>)</cite> simplify training with scalar-latent codecs and Transformer diffusion; <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib32\" title=\"\">2024</a>)</cite> improves time-varying style control; and recent <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib31\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib34\" title=\"\">2024</a>)</cite> explore DiT-based and autoregressive diffusion decoders, while <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>; Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib53\" title=\"\">2024</a>)</cite> place diffusion heads inside causal stacks to retain AR-like controllability. In contrast, discrete-token pipelines discretize speech and leverage AR LMs for stronger in-context prompting and explicit decoding control. <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite> set the Nerual codec LM recipe, with <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib8\" title=\"\">2024</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib51\" title=\"\">2025b</a>)</cite> improving robustness and human parity; token-infilling AR models such as <cite class=\"ltx_cite ltx_citemacro_citep\">(Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib26\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib41\" title=\"\">2024</a>)</cite> advanced editing and wild-data zero-shot; <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>)</cite> uses masked generative codec modeling; and large multilingual systems such as<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib15\" title=\"\">2024a</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib5\" title=\"\">2024</a>)</cite>, broaden coverage and controllability.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "speech",
                    "tts",
                    "human",
                    "results",
                    "continuous"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Masked diffusion extends discrete-state diffusion to language by formalizing token corruption with structured transition kernels, notably D3PM and multinomial diffusion, which introduced absorbing masks and principled categorical noising <cite class=\"ltx_cite ltx_citemacro_citep\">(Austin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib3\" title=\"\">2021</a>; Hoogeboom et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib21\" title=\"\">2021</a>)</cite>. Recent theory shows that masked-diffusion training objectives reduce to mixtures of masked-LM losses and can be reparameterized to connect tightly with any-order autoregression; this yields simpler sampling and clarifies likelihood bounds <cite class=\"ltx_cite ltx_citemacro_citep\">(Ou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib40\" title=\"\">2024</a>; Shih et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib48\" title=\"\">2022</a>)</cite>. Scaling experiments train masked-diffusion LMs from scratch at LLM scale (e.g., LLaDA), demonstrating competitive perplexity and strong instruction-following after SFT <cite class=\"ltx_cite ltx_citemacro_citep\">(Nie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib38\" title=\"\">2025</a>)</cite>. Beyond pure masking, generalized interpolating discrete diffusion mixes masking with uniform noise, enabling self-correction and compute-matched gains <cite class=\"ltx_cite ltx_citemacro_citep\">(von R&#252;tte et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib54\" title=\"\">2025</a>)</cite>. Parallel advances include simplified/standardized masked diffusion parameterizations, reparameterized discrete diffusion routes and denoisers, and analyses that decouple paradigm (MDM vs. AR) from architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib47\" title=\"\">2024</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib62\" title=\"\">2023</a>)</cite>. Task-level studies further adapt discrete diffusion to conditional long-text generation with improved efficiency <cite class=\"ltx_cite ltx_citemacro_citep\">(Dat et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib12\" title=\"\">2024</a>)</cite>. Collectively, these results position masked discrete diffusion as a competitive LM family with any-order decoding, efficient parallelism, and growing theoretical clarity.</p>\n\n",
                "matched_terms": [
                    "results",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S2.F1\" title=\"Figure 1 &#8227; 2.2 Mask Diffusion Models &#8227; 2 Related Works &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, an autoregressive architecture that advances patch by patch while remaining entirely within a discrete residual vector-quantized (RVQ) code domain.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recast long sequences of discrete speech codes into a tiled layout of patches, akin to the DiTAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>)</cite> paradigm. An autoregressive (AR) Transformer governs dependencies across patches, while a masked diffusion Transformer completes the contents within each patch in parallel. In practice, generation advances along the patch index autoregressively, with the next patch produced via conditional masked diffusion.\nLet <math alttext=\"\\mathbf{C}=[\\mathbf{c}_{0},\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{L-1}]\\in\\mathbb{Z}^{L\\times J}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119836;</mi><mn>0</mn></msub><mo>,</mo><msub><mi>&#119836;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119836;</mi><mrow><mi>L</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8484;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>J</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}=[\\mathbf{c}_{0},\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{L-1}]\\in\\mathbb{Z}^{L\\times J}</annotation></semantics></math> denote the sequence of RVQ codes from <math alttext=\"J\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>J</mi><annotation encoding=\"application/x-tex\">J</annotation></semantics></math> quantizers, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the RVQ code sequence length. And let <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m4\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> be the input text.\nWe estimate the <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m5\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> by conditional maximum likelihood over the patch-grouped codes. By the chain rule, the model factorizes as</p>\n\n",
                "matched_terms": [
                    "distar",
                    "discrete",
                    "speech",
                    "ditar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S2.F1\" title=\"Figure 1 &#8227; 2.2 Mask Diffusion Models &#8227; 2 Related Works &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> sketches the <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;pipeline. In the spirit of DiTAR, we adopt a blockwise decomposition of the RVQ code stream: a long sequence of discrete tokens is sliced into patch-level units. Dependencies across patches are modeled by a causal autoregressive (AR) language model, whereas a Transformer-based masked diffusion module decodes within-patch content in parallel.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "discrete",
                    "ditar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we position <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;against strong contemporary systems and report state-of-the-art results.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "results",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models are trained on Emilia <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib19\" title=\"\">2024</a>)</cite>, a large, multilingual, in-the-wild speech corpus curated for scalable speech generation. For this study we use only its English subset, totaling roughly <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>50</mn><annotation encoding=\"application/x-tex\">50</annotation></semantics></math>k hours. Evaluation spans two open benchmarks: (i) LibriSpeech(PC) <cite class=\"ltx_cite ltx_citemacro_citep\">(Meister et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib36\" title=\"\">2023</a>)</cite> test-clean, <math alttext=\"5.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>5.4</mn><annotation encoding=\"application/x-tex\">5.4</annotation></semantics></math> hours from <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math> unique English speakers; we follow the established zero-shot cross-sentence protocol and adopt the subset from F5TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>)</cite> (40 prompts, 1127 utterances), and (ii) SeedTTS test-en, introduced with Seed-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>)</cite>, consisting of 1088 English samples drawn from Common Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib2\" title=\"\">2019</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "f5tts",
                    "testen",
                    "seedtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use both objective and subjective metrics to evaluate our models. For the objective metrics, we evaluate (i) Word Error Rate (WER) to assess robustness and intelligibility using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib42\" title=\"\">2022</a>)</cite> as our ASR model; (ii) Speaker similarity (SIM) via cosine similarity between the TDNN-based WavLM embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib7\" title=\"\">2022</a>)</cite> extracted from the generated audio and its reference prompt; (iii) UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib43\" title=\"\">2022</a>)</cite>, an automatic predicted mean opinion score (MOS) for speech quality.For the subjective metrics, comparative mean option score (CMOS) and similarity mean option score (SMOS) are used to evaluate naturalness/robustness and similarity, respectively. CMOS is on a scale of -3 to 3, and SMOS is on a scale of 1 to 5.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "smos",
                    "cmos",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;exhibits strong overall performance. Across benchmarks it attains the lowest WER, indicating robust synthesis. We assess speaker similarity using both objective and human judgments: SIM and speaker SMOS. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields SIM on par with the best alternatives and leads on SMOS. We attribute these gains in part to reduced sensitivity to high-frequency artifacts in the reference prompt, which preserves cleaner timbral cues during cloning. The same trend is observed in the objective predictor UTMOS and in CMOS listening tests. Notably, relative to continuous-representation systems, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;achieves comparable or superior perceptual quality with a similar or smaller parameter budget. As model capacity grows, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields consistent improvements on objective metrics, closely matching the scaling behavior reported for discrete-token autoregressive systems and indicating a healthy scaling trajectory.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "smos",
                    "cmos",
                    "systems",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we conduct a detailed analysis of the various components of <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>. Unless specified otherwise, we default to using <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-base with 0.15 billion parameters, a patch size of 8, a stride of 8, and NFE of 24, tested on the LibriSpeech-PC test dataset mentioned above. We discuss the patch size in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A4\" title=\"Appendix D Patch Size. &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> and classifier-guidance free settings in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A3\" title=\"Appendix C Classifier-Free Guidance &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;under multiple inference strategies (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T3\" title=\"Table 3 &#8227; Decoding Strategies. &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nDeterministic greedy decoding yields the lowest WER, evidencing stable synthesis,\nwhile speaker similarity is slightly lower than with stochastic sampling.\nThis aligns with the standard diversity-determinism trade-off: sampling can\nrecover timbral nuances at the cost of higher variability.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "compare"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot TTS framework that operates in an RVQ code space and tightly couples an autoregressive Transformer with masked diffusion. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;achieves blockwise parallelism while effectively modeling layer&#8211;time dependencies across RVQ levels and local temporal neighborhoods, without an explicit duration predictor. We further introduced a simple but effective RVQ-aware sampling procedure that stabilizes inference and improves perceptual quality. In combination, these design choices yield SOTA robustness, speaker similarity, and naturalness in zero-shot speech synthesis, while exposing clear levers for controllability at inference time.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "speech",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results are currently demonstrated under the codec, RVQ configuration, and evaluation setups described in the paper. Performance may depend on the RVQ depth and codebook design. Due to resource constraints, we trained our model solely on a around-50k-hour English corpus<cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib19\" title=\"\">2024</a>)</cite>; broader generalization to multilingual and multi-style settings remains to be evaluated.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A <em class=\"ltx_emph ltx_font_italic\">patch-based</em> autoregressive framework on continuous tokens that couples a causal LM with a bidirectional diffusion transformer (LocDiT) for intra-patch prediction. This divide-and-conquer design balances determinism and diversity and reduces compute versus fully diffusion-based approaches. The paper reports strong zero-shot TTS with fast temperature-based sampling.</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An industrial-level, controllable zero-shot TTS that builds on XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib5\" title=\"\">2024</a>)</cite>-style AR modeling with practical enhancements for stability and control. The system targets faster inference and simpler training while providing fine control over pronunciation, timing, and emotion, with public demos and code.\nWe use the official code and pre-trained checkpoint <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/IndexTeam/Index-TTS\" title=\"\">https://huggingface.co/IndexTeam/Index-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "system",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A foundation TTS framework with a semantic-token LM front-end and a two-stage waveform generator, designed for robust zero-shot cloning and instruction-tuned conversational speech. It emphasizes data/process pipeline design for large-scale training and showcases in-context learning for dubbing and chat.\nWe use the official code and pre-trained checkpoint <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/FireRedTeam/FireRedTTS\" title=\"\">https://huggingface.co/FireRedTeam/FireRedTTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">High-fidelity zero-shot TTS offers clear benefits for accessibility, education, and creative production, yet the ability of <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160; to closely match speaker timbre introduces material risks, such as impersonation and social-engineering fraud, spoofed voice biometrics, non-consensual cloning, and scalable disinformation. To mitigate misuse, we advocate consent-first deployment with strict use-policy gating, the use of robust audio watermarks to support provenance, and a public abuse-reporting channel with prompt triage and access revocation.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "tts"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation",
        "caption": "Table 3: Objective evaluation results between different decoding strategies.",
        "body": "Type\nTtimeT_{\\text{time}}\nTlayerT_{\\text{layer}}\nWER\nSPK\n\n\n\n\nSample\n1\n1\n2.11\n0.626\n\n\nSample\n0.95\n0.8\n1.99\n0.640\n\n\nGreedy\n0.95\n0.8\n1.91\n0.636",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"T_{\\text{time}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mtext>time</mtext></msub><annotation encoding=\"application/x-tex\">T_{\\text{time}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><math alttext=\"T_{\\text{layer}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mtext>layer</mtext></msub><annotation encoding=\"application/x-tex\">T_{\\text{layer}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SPK</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Sample</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.626</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Sample</td>\n<td class=\"ltx_td ltx_align_center\">0.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.8</td>\n<td class=\"ltx_td ltx_align_center\">1.99</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.640</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Greedy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">0.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">0.636</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "greedy",
            "spk",
            "evaluation",
            "decoding",
            "wer",
            "tlayerttextlayer",
            "different",
            "results",
            "objective",
            "sample",
            "strategies",
            "ttimettexttime",
            "between",
            "type"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We compare <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;under multiple inference strategies (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T3\" title=\"Table 3 &#8227; Decoding Strategies. &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nDeterministic greedy decoding yields the lowest WER, evidencing stable synthesis,\nwhile speaker similarity is slightly lower than with stochastic sampling.\nThis aligns with the standard diversity-determinism trade-off: sampling can\nrecover timbral nuances at the cost of higher variability.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker/style consistency, while maintaining rich output diversity. Audio samples are provided on <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/w/DiSTAR_demo\" title=\"\">https://anonymous.4open.science/w/DiSTAR_demo</a>.</p>\n\n",
                "matched_terms": [
                    "between",
                    "decoding",
                    "greedy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A first mainstream route models speech as a sequence of discrete tokens from a single codebook (e.g., from a neural audio codec) and applies an autoregressive language model (AR) to predict the next token, followed by a vocoder/codec decoder to reconstruct the waveform <cite class=\"ltx_cite ltx_citemacro_cite\">Anastassiou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>); Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib46\" title=\"\">2023</a>)</cite>. This track inherits mature AR modeling and decoding strategies from the Natural Language Processing community: discrete <span class=\"ltx_text ltx_font_typewriter\">[EOS]</span> tokens enable clear termination; maximum-likelihood training is relatively stable compared with Mean Squared/Absolute Error Loss; and decoding hyper-parameters (temperature, top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>/top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>) and perplexity provide interpretable control at inference. However, codec rate sequences are long; exposure bias compounds in thousands of steps; and purely AR decoders struggle with long-range consistency (speaker/style drift) and throughput <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib49\" title=\"\">2024</a>)</cite>. Moreover, when a single discrete layer runs at low bitrate or with limited expressivity, reconstructions tend to lose fidelity and fine detail.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "strategies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several design choices in <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yield practical advantages. First, the fully discrete setting preserves an <span class=\"ltx_text ltx_font_typewriter\">[EOS]</span> token for the immediate termination of patch-level generation, eliminating auxiliary duration predictors or stop heads <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>); Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib45\" title=\"\">2018</a>)</cite>. Second, sharing the RVQ code space between the AR sketcher and the masked diffusion refiner allows end-to-end optimization and reduces inter-module mismatch relative to cascaded pipelines <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib16\" title=\"\">2024b</a>)</cite>. Third, decoding achieves robust quality under purely greedy settings, while temperature-based sampling extends to RVQ-level and joint layer&#8211;time strategies, enabling fine-grained control over the diversity&#8211;determinism trade-off. Fourth, pruning the upper RVQ layers at inference controls computation and bitrate to match bandwidth/latency constraints without retraining.</p>\n\n",
                "matched_terms": [
                    "between",
                    "decoding",
                    "greedy",
                    "strategies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop an RVQ-specific sampling method that boosts quality and stability; support for diverse decoding strategies, and on-the-fly bitrate/compute control without retraining.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "strategies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot TTS work broadly bifurcates by the target representation. Continuous-latent approaches predict high-information features (mel or codec latents) with diffusion/flow to boost long-range consistency and robust cloning. <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib46\" title=\"\">2023</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib25\" title=\"\">2024</a>)</cite> scale latent and factorized diffusion with speech prompting; <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib29\" title=\"\">2024</a>)</cite> casts text-guided speech infilling as flow matching; <cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib17\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>)</cite> report strong results via continuous flow matching; <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib60\" title=\"\">2025</a>)</cite> simplify training with scalar-latent codecs and Transformer diffusion; <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib32\" title=\"\">2024</a>)</cite> improves time-varying style control; and recent <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib31\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib34\" title=\"\">2024</a>)</cite> explore DiT-based and autoregressive diffusion decoders, while <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>; Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib53\" title=\"\">2024</a>)</cite> place diffusion heads inside causal stacks to retain AR-like controllability. In contrast, discrete-token pipelines discretize speech and leverage AR LMs for stronger in-context prompting and explicit decoding control. <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite> set the Nerual codec LM recipe, with <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib8\" title=\"\">2024</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib51\" title=\"\">2025b</a>)</cite> improving robustness and human parity; token-infilling AR models such as <cite class=\"ltx_cite ltx_citemacro_citep\">(Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib26\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib41\" title=\"\">2024</a>)</cite> advanced editing and wild-data zero-shot; <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>)</cite> uses masked generative codec modeling; and large multilingual systems such as<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib15\" title=\"\">2024a</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib5\" title=\"\">2024</a>)</cite>, broaden coverage and controllability.</p>\n\n",
                "matched_terms": [
                    "results",
                    "decoding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Masked diffusion extends discrete-state diffusion to language by formalizing token corruption with structured transition kernels, notably D3PM and multinomial diffusion, which introduced absorbing masks and principled categorical noising <cite class=\"ltx_cite ltx_citemacro_citep\">(Austin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib3\" title=\"\">2021</a>; Hoogeboom et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib21\" title=\"\">2021</a>)</cite>. Recent theory shows that masked-diffusion training objectives reduce to mixtures of masked-LM losses and can be reparameterized to connect tightly with any-order autoregression; this yields simpler sampling and clarifies likelihood bounds <cite class=\"ltx_cite ltx_citemacro_citep\">(Ou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib40\" title=\"\">2024</a>; Shih et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib48\" title=\"\">2022</a>)</cite>. Scaling experiments train masked-diffusion LMs from scratch at LLM scale (e.g., LLaDA), demonstrating competitive perplexity and strong instruction-following after SFT <cite class=\"ltx_cite ltx_citemacro_citep\">(Nie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib38\" title=\"\">2025</a>)</cite>. Beyond pure masking, generalized interpolating discrete diffusion mixes masking with uniform noise, enabling self-correction and compute-matched gains <cite class=\"ltx_cite ltx_citemacro_citep\">(von R&#252;tte et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib54\" title=\"\">2025</a>)</cite>. Parallel advances include simplified/standardized masked diffusion parameterizations, reparameterized discrete diffusion routes and denoisers, and analyses that decouple paradigm (MDM vs. AR) from architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib47\" title=\"\">2024</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib62\" title=\"\">2023</a>)</cite>. Task-level studies further adapt discrete diffusion to conditional long-text generation with improved efficiency <cite class=\"ltx_cite ltx_citemacro_citep\">(Dat et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib12\" title=\"\">2024</a>)</cite>. Collectively, these results position masked discrete diffusion as a competitive LM family with any-order decoding, efficient parallelism, and growing theoretical clarity.</p>\n\n",
                "matched_terms": [
                    "results",
                    "decoding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate this with three lightweight decoding tricks: (i) <span class=\"ltx_text ltx_font_bold\">Layer-wise temperature shaping.</span> Cool down deeper RVQ layers so they don&#8217;t dominate early. Concretely, multiply the sampling temperature for layer <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m1\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> by <math alttext=\"T_{\\text{layer}}^{\\,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>T</mi><mtext>layer</mtext><mi>j</mi></msubsup><annotation encoding=\"application/x-tex\">T_{\\text{layer}}^{\\,j}</annotation></semantics></math>. (ii) <span class=\"ltx_text ltx_font_bold\">Position-wise temperature shaping.</span> Within a patch, reduce confidence for farther-ahead positions by scaling the temperature at offset <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> with <math alttext=\"T_{\\text{time}}^{\\,l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>T</mi><mtext>time</mtext><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">T_{\\text{time}}^{\\,l}</annotation></semantics></math> (<math alttext=\"0\\!\\leq\\!l\\!&lt;\\!S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m5\" intent=\":literal\"><semantics><mrow><mn>0</mn><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8804;</mo><mi>l</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&lt;</mo><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">0\\!\\leq\\!l\\!&lt;\\!S</annotation></semantics></math>).\n(iii) <span class=\"ltx_text ltx_font_bold\">Hybrid sampling.</span> Generate the first <math alttext=\"50\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m6\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">50\\%</annotation></semantics></math> of the target positions by sampling, then switch to greedy for the remaining <math alttext=\"50\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m7\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">50\\%</annotation></semantics></math> to trade diversity for stability. In principle, the greedy/sample schedule is a hyperparameter; we adopt a simple half&#8211;half scheme to avoid over-tuning.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "greedy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together these inference strategies keep sampling diverse yet stable, while also enabling high-quality greedy decoding.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "greedy",
                    "strategies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use both objective and subjective metrics to evaluate our models. For the objective metrics, we evaluate (i) Word Error Rate (WER) to assess robustness and intelligibility using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib42\" title=\"\">2022</a>)</cite> as our ASR model; (ii) Speaker similarity (SIM) via cosine similarity between the TDNN-based WavLM embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib7\" title=\"\">2022</a>)</cite> extracted from the generated audio and its reference prompt; (iii) UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib43\" title=\"\">2022</a>)</cite>, an automatic predicted mean opinion score (MOS) for speech quality.For the subjective metrics, comparative mean option score (CMOS) and similarity mean option score (SMOS) are used to evaluate naturalness/robustness and similarity, respectively. CMOS is on a scale of -3 to 3, and SMOS is on a scale of 1 to 5.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "between",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">we show comparison results with SOTA baselines. The main results of objective metrics are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T1\" title=\"Table 1 &#8227; Model settings and baselines. &#8227; 4.1 Experimental Settings &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Subjective results are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T2\" title=\"Table 2 &#8227; 4.2 Zero-Shot TTS &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;exhibits strong overall performance. Across benchmarks it attains the lowest WER, indicating robust synthesis. We assess speaker similarity using both objective and human judgments: SIM and speaker SMOS. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields SIM on par with the best alternatives and leads on SMOS. We attribute these gains in part to reduced sensitivity to high-frequency artifacts in the reference prompt, which preserves cleaner timbral cues during cloning. The same trend is observed in the objective predictor UTMOS and in CMOS listening tests. Notably, relative to continuous-representation systems, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;achieves comparable or superior perceptual quality with a similar or smaller parameter budget. As model capacity grows, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields consistent improvements on objective metrics, closely matching the scaling behavior reported for discrete-token autoregressive systems and indicating a healthy scaling trajectory.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;randomly drops the last <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math> RVQ layers. At inference, we simply prune higher RVQ layers to achieve variable bitrate and compute. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.F2\" title=\"Figure 2 &#8227; 4.4 Inference Efficiency and Controllability &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the inference&#8211;quality trade-off under different RVQ pruning depths. As more RVQ layers are retained (hence higher FLOPs), speaker similarity increases markedly, whereas WER changes little and reaches its minimum around six layers. This pattern is consistent with the hypothesis that upper RVQ layers primarily encode acoustic detail rather than linguistic content <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results are currently demonstrated under the codec, RVQ configuration, and evaluation setups described in the paper. Performance may depend on the RVQ depth and codebook design. Due to resource constraints, we trained our model solely on a around-50k-hour English corpus<cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib19\" title=\"\">2024</a>)</cite>; broader generalization to multilingual and multi-style settings remains to be evaluated.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate the effect of patching, we vary the patch size <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> while keeping the <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-base configuration fixed and adopt greedy decoding to eliminate sampling stochasticity. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A4.T6\" title=\"Table 6 &#8227; Appendix D Patch Size. &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, performance degrades when <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> is either too small or too large. Small patches deprive the masked diffusion of sufficient within-patch context, weakening reconstruction; moreover, the autoregressive horizon is only marginally shortened, so efficiency gains are limited. In contrast, very large patches encourage the refiner to overrely on copy-from-context shortcuts rather than resolving long&#8211;time dependencies, which also harms quality. Balancing these factors, we use <math alttext=\"P=8\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">P=8</annotation></semantics></math> by default as a favorable trade-off between compute and performance.</p>\n\n",
                "matched_terms": [
                    "between",
                    "decoding",
                    "greedy"
                ]
            }
        ]
    },
    "A2.T4": {
        "source_file": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation",
        "caption": "Table 4: Configurations of DiSTAR with different sizes.",
        "body": "Model size\n\n∼0.15\\sim 0.15B\n\n∼0.3\\sim 0.3B\n\n\n\n\nAggregator\nRVQ dim\n32\n48\n\n\nNumber of layers\n4\n4\n\n\nHidden dim\n512\n768\n\n\nNumber of heads\n8\n16\n\n\nFFN dim\n1024\n3072\n\n\nLanguage Model\nNumber of layers\n24\n24\n\n\nHidden dim\n512\n768\n\n\nNumber of heads\n8\n16\n\n\nFFN dim\n1024\n2048\n\n\nDiffusion\nNumber of layers\n16\n16\n\n\nHidden dim\n512\n768\n\n\nNumber of heads\n16\n16\n\n\nFFN dim\n2048\n3072",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\">Model size</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<math alttext=\"\\sim 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T4.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.15</annotation></semantics></math>B</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<math alttext=\"\\sim 0.3\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T4.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.3</annotation></semantics></math>B</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"5\">Aggregator</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">RVQ dim</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Number of layers</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hidden dim</td>\n<td class=\"ltx_td ltx_align_center\">512</td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Number of heads</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FFN dim</td>\n<td class=\"ltx_td ltx_align_center\">1024</td>\n<td class=\"ltx_td ltx_align_center\">3072</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\">Language Model</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Number of layers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hidden dim</td>\n<td class=\"ltx_td ltx_align_center\">512</td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Number of heads</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FFN dim</td>\n<td class=\"ltx_td ltx_align_center\">1024</td>\n<td class=\"ltx_td ltx_align_center\">2048</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" rowspan=\"4\">Diffusion</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Number of layers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hidden dim</td>\n<td class=\"ltx_td ltx_align_center\">512</td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Number of heads</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\">FFN dim</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">2048</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">3072</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "distar",
            "sizes",
            "heads",
            "diffusion",
            "∼03sim",
            "015b",
            "different",
            "hidden",
            "∼015sim",
            "number",
            "rvq",
            "dim",
            "ffn",
            "03b",
            "language",
            "configurations",
            "size",
            "layers",
            "model",
            "aggregator"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A2.T4\" title=\"Table 4 &#8227; B.2 Model Architecture &#8227; Appendix B Implementation Details &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the key hyperparameters of the models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker/style consistency, while maintaining rich output diversity. Audio samples are provided on <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/w/DiSTAR_demo\" title=\"\">https://anonymous.4open.science/w/DiSTAR_demo</a>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "language",
                    "rvq",
                    "diffusion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A first mainstream route models speech as a sequence of discrete tokens from a single codebook (e.g., from a neural audio codec) and applies an autoregressive language model (AR) to predict the next token, followed by a vocoder/codec decoder to reconstruct the waveform <cite class=\"ltx_cite ltx_citemacro_cite\">Anastassiou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>); Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib46\" title=\"\">2023</a>)</cite>. This track inherits mature AR modeling and decoding strategies from the Natural Language Processing community: discrete <span class=\"ltx_text ltx_font_typewriter\">[EOS]</span> tokens enable clear termination; maximum-likelihood training is relatively stable compared with Mean Squared/Absolute Error Loss; and decoding hyper-parameters (temperature, top-<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>/top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>) and perplexity provide interpretable control at inference. However, codec rate sequences are long; exposure bias compounds in thousands of steps; and purely AR decoders struggle with long-range consistency (speaker/style drift) and throughput <cite class=\"ltx_cite ltx_citemacro_cite\">Song et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib49\" title=\"\">2024</a>)</cite>. Moreover, when a single discrete layer runs at low bitrate or with limited expressivity, reconstructions tend to lose fidelity and fine detail.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A third, increasingly influential direction embraces multi-codebook discrete representations, typically residual vector quantization (RVQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib30\" title=\"\">2022</a>)</cite>, where several codebooks per frame progressively capture detail <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib8\" title=\"\">2024</a>; Xiaomi, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib57\" title=\"\">2025</a>)</cite>. RVQ confers two attractive properties: (i) at a sufficient aggregate bitrate it can reconstruct high-fidelity audio; and (ii) by remaining discrete, it preserves the stability and interpretability of LM-style training and decoding. However, RVQ introduces a second dependency axis beyond temporal order: intra-frame depth &#8211; the strong correlation among codebooks at the same time frame &#8211; is critical for quality. Effective RVQ TTS systems must therefore model time and depth jointly. Previous explorations, such as including flattening codebooks<cite class=\"ltx_cite ltx_citemacro_cite\">Borsos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib4\" title=\"\">2022</a>)</cite>, semantic-to-acoustic hierarchies<cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite>, RQ-Transformer<cite class=\"ltx_cite ltx_citemacro_cite\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib59\" title=\"\">2023</a>)</cite>, and delay-pattern scheduling<cite class=\"ltx_cite ltx_citemacro_cite\">Copet et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib11\" title=\"\">2023</a>)</cite>, partially address this coupling but still trade off inference parallelism, long-range consistency, and train/infer efficiency, leaving RVQ&#8217;s full potential under-exploited. This raises a central question: Can we architect a generator that natively models RVQ&#8217;s joint time-depth structure, achieving high quality at reasonable compute?</p>\n\n",
                "matched_terms": [
                    "rvq",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, we introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;(<span class=\"ltx_text ltx_font_bold\">DI</span>ffusion over a <span class=\"ltx_text ltx_font_bold\">S</span>calable <span class=\"ltx_text ltx_font_bold\">T</span>oken <span class=\"ltx_text ltx_font_bold\">A</span>uto<span class=\"ltx_text ltx_font_bold\">R</span>egressive Representation for Speech Generation), a zero-shot TTS framework that operates entirely in the RVQ discrete code space and tightly couples an AR language model with a masked diffusion transformer. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;adopts the patch-wise factorization strategy popularized in next-patch systems <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>)</cite>: it aggregates RVQ tokens into patches. A causal LM drafts the next patch by predicting a compact hidden sketch that captures coarse temporal evolution, after which a discrete masked diffusion Transformer performs parallel infilling within the patch. Inspired by LLaDA <cite class=\"ltx_cite ltx_citemacro_cite\">Nie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib38\" title=\"\">2025</a>)</cite>, the masked diffusion component operates not as a continuous denoiser but as a iterative discrete demasking process over masked positions, thereby resolving multi-codebook (depth) dependencies and supporting efficient parallel synthesis in the RVQ code space.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "language",
                    "rvq",
                    "diffusion",
                    "hidden",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The design gives two main advantages. Compared with continuous next-patch diffusion, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;avoids optimization issues in high-dimensional continuous latents while retaining patch-level parallelism. Compared with single-codebook AR, it models the intra-frame multi-codebook coupling inside each patch, improving coherence and allowing depthwise parallel refinement, which reduces exposure-bias effects without sacrificing the robustness of discrete training.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "diffusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several design choices in <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yield practical advantages. First, the fully discrete setting preserves an <span class=\"ltx_text ltx_font_typewriter\">[EOS]</span> token for the immediate termination of patch-level generation, eliminating auxiliary duration predictors or stop heads <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>); Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib45\" title=\"\">2018</a>)</cite>. Second, sharing the RVQ code space between the AR sketcher and the masked diffusion refiner allows end-to-end optimization and reduces inter-module mismatch relative to cascaded pipelines <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib16\" title=\"\">2024b</a>)</cite>. Third, decoding achieves robust quality under purely greedy settings, while temperature-based sampling extends to RVQ-level and joint layer&#8211;time strategies, enabling fine-grained control over the diversity&#8211;determinism trade-off. Fourth, pruning the upper RVQ layers at inference controls computation and bitrate to match bandwidth/latency constraints without retraining.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "diffusion",
                    "rvq",
                    "layers",
                    "heads"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot TTS framework that couples an autoregressive drafter with masked diffusion entirely in the discrete RVQ domain, achieving patch-level parallelism and joint modeling of RVQ layer&#8211;time dependencies.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq",
                    "diffusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot TTS work broadly bifurcates by the target representation. Continuous-latent approaches predict high-information features (mel or codec latents) with diffusion/flow to boost long-range consistency and robust cloning. <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib46\" title=\"\">2023</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib25\" title=\"\">2024</a>)</cite> scale latent and factorized diffusion with speech prompting; <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib29\" title=\"\">2024</a>)</cite> casts text-guided speech infilling as flow matching; <cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib17\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>)</cite> report strong results via continuous flow matching; <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib60\" title=\"\">2025</a>)</cite> simplify training with scalar-latent codecs and Transformer diffusion; <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib32\" title=\"\">2024</a>)</cite> improves time-varying style control; and recent <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib31\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib34\" title=\"\">2024</a>)</cite> explore DiT-based and autoregressive diffusion decoders, while <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>; Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib53\" title=\"\">2024</a>)</cite> place diffusion heads inside causal stacks to retain AR-like controllability. In contrast, discrete-token pipelines discretize speech and leverage AR LMs for stronger in-context prompting and explicit decoding control. <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite> set the Nerual codec LM recipe, with <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib8\" title=\"\">2024</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib51\" title=\"\">2025b</a>)</cite> improving robustness and human parity; token-infilling AR models such as <cite class=\"ltx_cite ltx_citemacro_citep\">(Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib26\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib41\" title=\"\">2024</a>)</cite> advanced editing and wild-data zero-shot; <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>)</cite> uses masked generative codec modeling; and large multilingual systems such as<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib15\" title=\"\">2024a</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib5\" title=\"\">2024</a>)</cite>, broaden coverage and controllability.</p>\n\n",
                "matched_terms": [
                    "heads",
                    "diffusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Masked diffusion extends discrete-state diffusion to language by formalizing token corruption with structured transition kernels, notably D3PM and multinomial diffusion, which introduced absorbing masks and principled categorical noising <cite class=\"ltx_cite ltx_citemacro_citep\">(Austin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib3\" title=\"\">2021</a>; Hoogeboom et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib21\" title=\"\">2021</a>)</cite>. Recent theory shows that masked-diffusion training objectives reduce to mixtures of masked-LM losses and can be reparameterized to connect tightly with any-order autoregression; this yields simpler sampling and clarifies likelihood bounds <cite class=\"ltx_cite ltx_citemacro_citep\">(Ou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib40\" title=\"\">2024</a>; Shih et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib48\" title=\"\">2022</a>)</cite>. Scaling experiments train masked-diffusion LMs from scratch at LLM scale (e.g., LLaDA), demonstrating competitive perplexity and strong instruction-following after SFT <cite class=\"ltx_cite ltx_citemacro_citep\">(Nie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib38\" title=\"\">2025</a>)</cite>. Beyond pure masking, generalized interpolating discrete diffusion mixes masking with uniform noise, enabling self-correction and compute-matched gains <cite class=\"ltx_cite ltx_citemacro_citep\">(von R&#252;tte et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib54\" title=\"\">2025</a>)</cite>. Parallel advances include simplified/standardized masked diffusion parameterizations, reparameterized discrete diffusion routes and denoisers, and analyses that decouple paradigm (MDM vs. AR) from architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib47\" title=\"\">2024</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib62\" title=\"\">2023</a>)</cite>. Task-level studies further adapt discrete diffusion to conditional long-text generation with improved efficiency <cite class=\"ltx_cite ltx_citemacro_citep\">(Dat et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib12\" title=\"\">2024</a>)</cite>. Collectively, these results position masked discrete diffusion as a competitive LM family with any-order decoding, efficient parallelism, and growing theoretical clarity.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S2.F1\" title=\"Figure 1 &#8227; 2.2 Mask Diffusion Models &#8227; 2 Related Works &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, an autoregressive architecture that advances patch by patch while remaining entirely within a discrete residual vector-quantized (RVQ) code domain.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recast long sequences of discrete speech codes into a tiled layout of patches, akin to the DiTAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>)</cite> paradigm. An autoregressive (AR) Transformer governs dependencies across patches, while a masked diffusion Transformer completes the contents within each patch in parallel. In practice, generation advances along the patch index autoregressively, with the next patch produced via conditional masked diffusion.\nLet <math alttext=\"\\mathbf{C}=[\\mathbf{c}_{0},\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{L-1}]\\in\\mathbb{Z}^{L\\times J}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119836;</mi><mn>0</mn></msub><mo>,</mo><msub><mi>&#119836;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119836;</mi><mrow><mi>L</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8484;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>J</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}=[\\mathbf{c}_{0},\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{L-1}]\\in\\mathbb{Z}^{L\\times J}</annotation></semantics></math> denote the sequence of RVQ codes from <math alttext=\"J\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>J</mi><annotation encoding=\"application/x-tex\">J</annotation></semantics></math> quantizers, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the RVQ code sequence length. And let <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m4\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> be the input text.\nWe estimate the <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m5\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> by conditional maximum likelihood over the patch-grouped codes. By the chain rule, the model factorizes as</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq",
                    "model",
                    "diffusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S2.F1\" title=\"Figure 1 &#8227; 2.2 Mask Diffusion Models &#8227; 2 Related Works &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> sketches the <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;pipeline. In the spirit of DiTAR, we adopt a blockwise decomposition of the RVQ code stream: a long sequence of discrete tokens is sliced into patch-level units. Dependencies across patches are modeled by a causal autoregressive (AR) language model, whereas a Transformer-based masked diffusion module decodes within-patch content in parallel.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "language",
                    "rvq",
                    "diffusion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text prompt is fed to the AR LM and then concatenated with a learned patch embedding obtained by aggregating the relevant RVQ codes. The LM produces a contextual summary <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m1\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, which&#8212;together with a finite history of previously generated tokens&#8212;serves as conditioning for the diffusion model. Training minimizes the cross-entropy objective in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S3.E2\" title=\"In Discrete masked diffusion over the next span. &#8227; 3.1.1 Formulation &#8227; 3.1 Overview &#8227; 3 DiSTAR &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "rvq",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;dispenses with both an explicit duration predictor and forced alignment. Moreover, unlike prior designs, the RVQ aggregator is not restricted to disjoint patches: overlapping windows are permitted. At each decoding step, the diffusion head predicts, in one shot, a segment whose length matches the aggregator&#8217;s stride on the output stream, ensuring consistency. Additional architectural and training details are provided in the following sections.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq",
                    "diffusion",
                    "aggregator"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a lightweight non-autoregressive Transformer encoder to turn frame-level RVQ codes into one vector per patch.\nConcretely, each RVQ layer has its own embedding table, and we adopt factorized embedding parameterization <cite class=\"ltx_cite ltx_citemacro_citep\">(Lan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib28\" title=\"\">2019</a>)</cite>: a narrow embedding (e.g., 32-d) is learned and then lifted to <math alttext=\"d_{\\text{model}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mtext>model</mtext></msub><annotation encoding=\"application/x-tex\">d_{\\text{model}}</annotation></semantics></math> by a layer-specific linear map. A learnable scalar mixes the layer embeddings at each frame into a single continuous vector by weighted summing, giving one hidden vector per RVQ frame.</p>\n\n",
                "matched_terms": [
                    "rvq",
                    "hidden"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the overlapped mode in classic CNNs <cite class=\"ltx_cite ltx_citemacro_citep\">(O&#8217;shea &amp; Nash, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib39\" title=\"\">2015</a>)</cite>, we do not require the stride to equal the aggregation patch size on the input RVQ sequence, thereby allowing overlapping. With patch length <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> and stride <math alttext=\"S\\!\\leq\\!P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8804;</mo><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">S\\!\\leq\\!P</annotation></semantics></math>, we intentionally allow <math alttext=\"S&lt;P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&lt;</mo><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">S&lt;P</annotation></semantics></math> so adjacent patches overlap, which smooths boundaries and provide more information.\nAfter the encoder, each patch vector is obtained by averaging the final hidden states over the last <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> tokens of that patch. The resulting sequence of patch embeddings is then passed to the AR language model.</p>\n\n",
                "matched_terms": [
                    "language",
                    "size",
                    "rvq",
                    "hidden",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To complete each drafted patch, we employ a discrete masked diffusion model that operates in parallel over the tokens of the patch while respecting bidirectional context, akin to LLaDA-style non-causal Transformers.\nConditioning follows the spirit of <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>)</cite>: the diffusion model receives a compact prefix formed by concatenating (i) the autoregressive planner&#8217;s hidden state and (ii) a sliding window of previously generated codes. To avoid scale mismatch, the AR hidden is first passed through a trainable scalar gate before concatenation, which keeps its contribution numerically stable.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "hidden",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The target RVQ streams are linearized across time into a one-dimensional token sequence\n<math alttext=\"\\tilde{\\mathbf{c}}=(\\mathbf{c}_{k}^{\\top},\\mathbf{c}_{k+1}^{\\top},\\ldots)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>&#119836;</mi><mo>~</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119836;</mi><mi>k</mi><mo>&#8868;</mo></msubsup><mo>,</mo><msubsup><mi>&#119836;</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><mo>&#8868;</mo></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{c}}=(\\mathbf{c}_{k}^{\\top},\\mathbf{c}_{k+1}^{\\top},\\ldots)</annotation></semantics></math>\nwith <math alttext=\"\\mathbf{c}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{k}</annotation></semantics></math> denoting the code tuple at frame <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>. For each position, we add a learnable embedding and an RVQ-layer embedding to inject positional/type cues, which breaks symmetry and provides simple priors for the model <cite class=\"ltx_cite ltx_citemacro_citep\">(Dosovitskiy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib14\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "rvq",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, we draw a timestep <math alttext=\"t\\sim\\mathcal{U}(0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">t\\sim\\mathcal{U}(0,1]</annotation></semantics></math> and compute the masking ratio via a cosine schedule <math alttext=\"\\lambda(t)=\\cos\\big(\\frac{1-t}{2}\\pi\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>cos</mi><mo>&#8289;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><mfrac><mrow><mn>1</mn><mo>&#8722;</mo><mi>t</mi></mrow><mn>2</mn></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#960;</mi></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(t)=\\cos\\big(\\frac{1-t}{2}\\pi\\big)</annotation></semantics></math> following &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib6\" title=\"\">2022</a>)</cite>.\nA fraction <math alttext=\"\\lambda(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(t)</annotation></semantics></math> of target tokens in the current patch is replaced by a special <span class=\"ltx_text ltx_font_typewriter\">[MASK]</span> symbol, and the diffusion model learns to reconstruct all masked RVQ tokens simultaneously given the visible tokens and the conditioning prefix. At inference, we use the same cosine schedule to anneal the mask ratio over <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m4\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> iterations using iteratively decoding, after which the process advances to the next patch.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "rvq",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the model resilient to depth reductions, during training we randomly drop the top <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math> RVQ tiers, drawing <math alttext=\"\\ell\\sim\\mathrm{Unif}\\{0,\\dots,L-1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8764;</mo><mrow><mi>Unif</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mrow><mi>L</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\ell\\sim\\mathrm{Unif}\\{0,\\dots,L-1\\}</annotation></semantics></math> where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the total number of layers. The network therefore encounters examples with missing high-depth codes and learns to decode from shallower stacks, enabling test-time bitrate/compute control by simply pruning the last <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math> layers, with no retraining required.</p>\n\n",
                "matched_terms": [
                    "rvq",
                    "number",
                    "model",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate this with three lightweight decoding tricks: (i) <span class=\"ltx_text ltx_font_bold\">Layer-wise temperature shaping.</span> Cool down deeper RVQ layers so they don&#8217;t dominate early. Concretely, multiply the sampling temperature for layer <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m1\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> by <math alttext=\"T_{\\text{layer}}^{\\,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>T</mi><mtext>layer</mtext><mi>j</mi></msubsup><annotation encoding=\"application/x-tex\">T_{\\text{layer}}^{\\,j}</annotation></semantics></math>. (ii) <span class=\"ltx_text ltx_font_bold\">Position-wise temperature shaping.</span> Within a patch, reduce confidence for farther-ahead positions by scaling the temperature at offset <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> with <math alttext=\"T_{\\text{time}}^{\\,l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>T</mi><mtext>time</mtext><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">T_{\\text{time}}^{\\,l}</annotation></semantics></math> (<math alttext=\"0\\!\\leq\\!l\\!&lt;\\!S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m5\" intent=\":literal\"><semantics><mrow><mn>0</mn><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8804;</mo><mi>l</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&lt;</mo><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">0\\!\\leq\\!l\\!&lt;\\!S</annotation></semantics></math>).\n(iii) <span class=\"ltx_text ltx_font_bold\">Hybrid sampling.</span> Generate the first <math alttext=\"50\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m6\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">50\\%</annotation></semantics></math> of the target positions by sampling, then switch to greedy for the remaining <math alttext=\"50\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m7\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">50\\%</annotation></semantics></math> to trade diversity for stability. In principle, the greedy/sample schedule is a hyperparameter; we adopt a simple half&#8211;half scheme to avoid over-tuning.</p>\n\n",
                "matched_terms": [
                    "rvq",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We largely adopt the overall architecture and staged training recipe of <span class=\"ltx_text ltx_font_smallcaps\">Magicodec</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib50\" title=\"\">2025a</a>)</cite>, but replace its single-codebook VQ module with a residual vector quantizer (RVQ). Our RVQ is a Transformer-based streaming codec with approximately <math alttext=\"0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS1.p1.m1\" intent=\":literal\"><semantics><mn>0.3</mn><annotation encoding=\"application/x-tex\">0.3</annotation></semantics></math>B parameters. Under this configuration, a <math alttext=\"24\\,\\mathrm{kHz}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>24</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>kHz</mi></mrow><annotation encoding=\"application/x-tex\">24\\,\\mathrm{kHz}</annotation></semantics></math> waveform is compressed into a discrete token stream at <math alttext=\"64\\,\\mathrm{Hz}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>64</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>Hz</mi></mrow><annotation encoding=\"application/x-tex\">64\\,\\mathrm{Hz}</annotation></semantics></math>. The codec employs <math alttext=\"9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS1.p1.m4\" intent=\":literal\"><semantics><mn>9</mn><annotation encoding=\"application/x-tex\">9</annotation></semantics></math> residual stages (RVQ layers); each stage uses a codebook of size <math alttext=\"65{,}536\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>65</mn><mo>,</mo><mn>536</mn></mrow><annotation encoding=\"application/x-tex\">65{,}536</annotation></semantics></math> with <math alttext=\"16\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS1.p1.m6\" intent=\":literal\"><semantics><mn>16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math>-dimensional code vectors.</p>\n\n",
                "matched_terms": [
                    "rvq",
                    "size",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;comprises (i) an aggregator, (ii) a causal language model (LM), and (iii) a masked diffusion model (MDM), all instantiated with Transformer backbones. The aggregator and MDM are bidirectional RoFormers with rotary positional encodings (RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib52\" title=\"\">2024</a>)</cite>, SwiGLU activations <cite class=\"ltx_cite ltx_citemacro_citep\">(Shazeer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib44\" title=\"\">2020</a>)</cite>, and a Pre-Norm layout using RMSNorm<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Sennrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib61\" title=\"\">2019</a>)</cite>. The AR LM follows the Qwen2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib58\" title=\"\">2024</a>)</cite> style decoder-only architecture and is trained from scratch.\nInput text is converted to phoneme sequences and embedded via a learned lookup table. At inference, consistent with prior practice, we construct a prefix context that includes the text together with a short acoustic prompt.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "language",
                    "diffusion",
                    "model",
                    "aggregator"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train all models on 64 NVIDIA A100 80GB GPUs. We train <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;of two sizes <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-base (0.15B), and <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-medium (0.3B). Architectural specifics are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A2.SS2\" title=\"B.2 Model Architecture &#8227; Appendix B Implementation Details &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>. Unless otherwise noted, we use a patch size of <math alttext=\"8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>8</mn><annotation encoding=\"application/x-tex\">8</annotation></semantics></math> and condition the diffusion module on a single historical patch. Inference procedures are detailed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S3.SS4.SSS0.Px4\" title=\"Decoding heuristics. &#8227; 3.4 Training and Inference &#8227; 3 DiSTAR &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "sizes",
                    "015b",
                    "size",
                    "diffusion",
                    "03b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;exhibits strong overall performance. Across benchmarks it attains the lowest WER, indicating robust synthesis. We assess speaker similarity using both objective and human judgments: SIM and speaker SMOS. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields SIM on par with the best alternatives and leads on SMOS. We attribute these gains in part to reduced sensitivity to high-frequency artifacts in the reference prompt, which preserves cleaner timbral cues during cloning. The same trend is observed in the objective predictor UTMOS and in CMOS listening tests. Notably, relative to continuous-representation systems, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;achieves comparable or superior perceptual quality with a similar or smaller parameter budget. As model capacity grows, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields consistent improvements on objective metrics, closely matching the scaling behavior reported for discrete-token autoregressive systems and indicating a healthy scaling trajectory.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we conduct a detailed analysis of the various components of <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>. Unless specified otherwise, we default to using <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-base with 0.15 billion parameters, a patch size of 8, a stride of 8, and NFE of 24, tested on the LibriSpeech-PC test dataset mentioned above. We discuss the patch size in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A4\" title=\"Appendix D Patch Size. &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> and classifier-guidance free settings in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A3\" title=\"Appendix C Classifier-Free Guidance &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;randomly drops the last <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math> RVQ layers. At inference, we simply prune higher RVQ layers to achieve variable bitrate and compute. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.F2\" title=\"Figure 2 &#8227; 4.4 Inference Efficiency and Controllability &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the inference&#8211;quality trade-off under different RVQ pruning depths. As more RVQ layers are retained (hence higher FLOPs), speaker similarity increases markedly, whereas WER changes little and reaches its minimum around six layers. This pattern is consistent with the hypothesis that upper RVQ layers primarily encode acoustic detail rather than linguistic content <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq",
                    "layers",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot TTS framework that operates in an RVQ code space and tightly couples an autoregressive Transformer with masked diffusion. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;achieves blockwise parallelism while effectively modeling layer&#8211;time dependencies across RVQ levels and local temporal neighborhoods, without an explicit duration predictor. We further introduced a simple but effective RVQ-aware sampling procedure that stabilizes inference and improves perceptual quality. In combination, these design choices yield SOTA robustness, speaker similarity, and naturalness in zero-shot speech synthesis, while exposing clear levers for controllability at inference time.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "rvq",
                    "diffusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results are currently demonstrated under the codec, RVQ configuration, and evaluation setups described in the paper. Performance may depend on the RVQ depth and codebook design. Due to resource constraints, we trained our model solely on a around-50k-hour English corpus<cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib19\" title=\"\">2024</a>)</cite>; broader generalization to multilingual and multi-style settings remains to be evaluated.</p>\n\n",
                "matched_terms": [
                    "rvq",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize 64 A100 GPUs, each processing a batch size of 36K token frames, and train <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160; for 0.6M steps. The AdamW optimizer is employed with a learning rate of 0.75e-4 for AR, and 1.5e-4 for the other parts, <math alttext=\"\\beta_{t}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mi>t</mi></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{t}=0.9</annotation></semantics></math>, and <math alttext=\"\\beta_{2}=0.99\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.99</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate the effect of patching, we vary the patch size <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> while keeping the <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-base configuration fixed and adopt greedy decoding to eliminate sampling stochasticity. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A4.T6\" title=\"Table 6 &#8227; Appendix D Patch Size. &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, performance degrades when <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> is either too small or too large. Small patches deprive the masked diffusion of sufficient within-patch context, weakening reconstruction; moreover, the autoregressive horizon is only marginally shortened, so efficiency gains are limited. In contrast, very large patches encourage the refiner to overrely on copy-from-context shortcuts rather than resolving long&#8211;time dependencies, which also harms quality. Balancing these factors, we use <math alttext=\"P=8\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">P=8</annotation></semantics></math> by default as a favorable trade-off between compute and performance.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "size"
                ]
            }
        ]
    },
    "A3.T5": {
        "source_file": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation",
        "caption": "Table 5: Objective evaluation results between different classifer-free guidance strategies.",
        "body": "Schema\nwcfg,histw_{\\text{cfg,hist}}\nwcfg,arw_{\\text{cfg,ar}}\nwrescalew_{\\text{rescale}}\nWER\nSPK\n\n\n\n\nA\n2\n-\n0.75\n2.12\n0.63\n\n\nA\n1.25\n-\n0.75\n1.74\n0.63\n\n\nA\n0.75\n-\n0.75\n1.99\n0.62\n\n\nB\n0.75\n0.75\n0.75\n1.76\n0.63\n\n\nB\n0.75\n1.25\n0.75\n1.90\n0.63\n\n\nB\n1.25\n1.25\n0.75\n1.88\n0.63",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Schema</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"w_{\\text{cfg,hist}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m1\" intent=\":literal\"><semantics><msub><mi>w</mi><mtext>cfg,hist</mtext></msub><annotation encoding=\"application/x-tex\">w_{\\text{cfg,hist}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"w_{\\text{cfg,ar}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m2\" intent=\":literal\"><semantics><msub><mi>w</mi><mtext>cfg,ar</mtext></msub><annotation encoding=\"application/x-tex\">w_{\\text{cfg,ar}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><math alttext=\"w_{\\text{rescale}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m3\" intent=\":literal\"><semantics><msub><mi>w</mi><mtext>rescale</mtext></msub><annotation encoding=\"application/x-tex\">w_{\\text{rescale}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SPK</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">A</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">A</span></td>\n<td class=\"ltx_td ltx_align_center\">1.25</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.75</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">A</span></td>\n<td class=\"ltx_td ltx_align_center\">0.75</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.75</td>\n<td class=\"ltx_td ltx_align_center\">1.99</td>\n<td class=\"ltx_td ltx_align_center\">0.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">B</span></td>\n<td class=\"ltx_td ltx_align_center\">0.75</td>\n<td class=\"ltx_td ltx_align_center\">1.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.75</td>\n<td class=\"ltx_td ltx_align_center\">1.90</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">1.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">1.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">1.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">0.63</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wcfghistwtextcfghist",
            "spk",
            "evaluation",
            "wrescalewtextrescale",
            "wer",
            "different",
            "results",
            "objective",
            "classiferfree",
            "schema",
            "strategies",
            "wcfgarwtextcfgar",
            "between",
            "guidance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A3.T5\" title=\"Table 5 &#8227; Appendix C Classifier-Free Guidance &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, holding all other factors fixed, nested CFG (Scheme <span class=\"ltx_text ltx_font_bold\">B</span>) and simple single-step CFG (Scheme <span class=\"ltx_text ltx_font_bold\">A</span>) yield nearly identical objective metrics. To reduce computational cost and latency, we therefore adopt the simpler Scheme <span class=\"ltx_text ltx_font_bold\">A</span> (single-step CFG) as the default in the main experiments.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker/style consistency, while maintaining rich output diversity. Audio samples are provided on <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/w/DiSTAR_demo\" title=\"\">https://anonymous.4open.science/w/DiSTAR_demo</a>.</p>\n\n",
                "matched_terms": [
                    "between",
                    "guidance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several design choices in <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yield practical advantages. First, the fully discrete setting preserves an <span class=\"ltx_text ltx_font_typewriter\">[EOS]</span> token for the immediate termination of patch-level generation, eliminating auxiliary duration predictors or stop heads <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>); Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib45\" title=\"\">2018</a>)</cite>. Second, sharing the RVQ code space between the AR sketcher and the masked diffusion refiner allows end-to-end optimization and reduces inter-module mismatch relative to cascaded pipelines <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib16\" title=\"\">2024b</a>)</cite>. Third, decoding achieves robust quality under purely greedy settings, while temperature-based sampling extends to RVQ-level and joint layer&#8211;time strategies, enabling fine-grained control over the diversity&#8211;determinism trade-off. Fourth, pruning the upper RVQ layers at inference controls computation and bitrate to match bandwidth/latency constraints without retraining.</p>\n\n",
                "matched_terms": [
                    "between",
                    "strategies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use both objective and subjective metrics to evaluate our models. For the objective metrics, we evaluate (i) Word Error Rate (WER) to assess robustness and intelligibility using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib42\" title=\"\">2022</a>)</cite> as our ASR model; (ii) Speaker similarity (SIM) via cosine similarity between the TDNN-based WavLM embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib7\" title=\"\">2022</a>)</cite> extracted from the generated audio and its reference prompt; (iii) UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib43\" title=\"\">2022</a>)</cite>, an automatic predicted mean opinion score (MOS) for speech quality.For the subjective metrics, comparative mean option score (CMOS) and similarity mean option score (SMOS) are used to evaluate naturalness/robustness and similarity, respectively. CMOS is on a scale of -3 to 3, and SMOS is on a scale of 1 to 5.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "between",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">we show comparison results with SOTA baselines. The main results of objective metrics are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T1\" title=\"Table 1 &#8227; Model settings and baselines. &#8227; 4.1 Experimental Settings &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Subjective results are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T2\" title=\"Table 2 &#8227; 4.2 Zero-Shot TTS &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;exhibits strong overall performance. Across benchmarks it attains the lowest WER, indicating robust synthesis. We assess speaker similarity using both objective and human judgments: SIM and speaker SMOS. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields SIM on par with the best alternatives and leads on SMOS. We attribute these gains in part to reduced sensitivity to high-frequency artifacts in the reference prompt, which preserves cleaner timbral cues during cloning. The same trend is observed in the objective predictor UTMOS and in CMOS listening tests. Notably, relative to continuous-representation systems, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;achieves comparable or superior perceptual quality with a similar or smaller parameter budget. As model capacity grows, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields consistent improvements on objective metrics, closely matching the scaling behavior reported for discrete-token autoregressive systems and indicating a healthy scaling trajectory.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;under multiple inference strategies (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T3\" title=\"Table 3 &#8227; Decoding Strategies. &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nDeterministic greedy decoding yields the lowest WER, evidencing stable synthesis,\nwhile speaker similarity is slightly lower than with stochastic sampling.\nThis aligns with the standard diversity-determinism trade-off: sampling can\nrecover timbral nuances at the cost of higher variability.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "strategies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;randomly drops the last <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math> RVQ layers. At inference, we simply prune higher RVQ layers to achieve variable bitrate and compute. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.F2\" title=\"Figure 2 &#8227; 4.4 Inference Efficiency and Controllability &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the inference&#8211;quality trade-off under different RVQ pruning depths. As more RVQ layers are retained (hence higher FLOPs), speaker similarity increases markedly, whereas WER changes little and reaches its minimum around six layers. This pattern is consistent with the hypothesis that upper RVQ layers primarily encode acoustic detail rather than linguistic content <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results are currently demonstrated under the codec, RVQ configuration, and evaluation setups described in the paper. Performance may depend on the RVQ depth and codebook design. Due to resource constraints, we trained our model solely on a around-50k-hour English corpus<cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib19\" title=\"\">2024</a>)</cite>; broader generalization to multilingual and multi-style settings remains to be evaluated.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            }
        ]
    },
    "A4.T6": {
        "source_file": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation",
        "caption": "Table 6: Objective evaluation results between different patch size PP of DiSTAR. All results use greedy decoding to eliminate sampling randomness.",
        "body": "Patch size\nWER\nSPK\nUTMOS\n\n\n\n\n2\n4.50\n0.63\n4.26\n\n\n4\n1.85\n0.65\n4.33\n\n\n8\n1.91\n0.64\n4.29",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Patch size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">WER</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SPK</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">UTMOS</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.26</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.33</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">8</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">1.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">4.29</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "distar",
            "greedy",
            "eliminate",
            "all",
            "evaluation",
            "decoding",
            "sampling",
            "size",
            "wer",
            "spk",
            "different",
            "results",
            "objective",
            "randomness",
            "use",
            "utmos",
            "between",
            "patch"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To isolate the effect of patching, we vary the patch size <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> while keeping the <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-base configuration fixed and adopt greedy decoding to eliminate sampling stochasticity. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A4.T6\" title=\"Table 6 &#8227; Appendix D Patch Size. &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, performance degrades when <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> is either too small or too large. Small patches deprive the masked diffusion of sufficient within-patch context, weakening reconstruction; moreover, the autoregressive horizon is only marginally shortened, so efficiency gains are limited. In contrast, very large patches encourage the refiner to overrely on copy-from-context shortcuts rather than resolving long&#8211;time dependencies, which also harms quality. Balancing these factors, we use <math alttext=\"P=8\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">P=8</annotation></semantics></math> by default as a favorable trade-off between compute and performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker/style consistency, while maintaining rich output diversity. Audio samples are provided on <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/w/DiSTAR_demo\" title=\"\">https://anonymous.4open.science/w/DiSTAR_demo</a>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "decoding",
                    "greedy",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this challenge, we introduce <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;(<span class=\"ltx_text ltx_font_bold\">DI</span>ffusion over a <span class=\"ltx_text ltx_font_bold\">S</span>calable <span class=\"ltx_text ltx_font_bold\">T</span>oken <span class=\"ltx_text ltx_font_bold\">A</span>uto<span class=\"ltx_text ltx_font_bold\">R</span>egressive Representation for Speech Generation), a zero-shot TTS framework that operates entirely in the RVQ discrete code space and tightly couples an AR language model with a masked diffusion transformer. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;adopts the patch-wise factorization strategy popularized in next-patch systems <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>)</cite>: it aggregates RVQ tokens into patches. A causal LM drafts the next patch by predicting a compact hidden sketch that captures coarse temporal evolution, after which a discrete masked diffusion Transformer performs parallel infilling within the patch. Inspired by LLaDA <cite class=\"ltx_cite ltx_citemacro_cite\">Nie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib38\" title=\"\">2025</a>)</cite>, the masked diffusion component operates not as a continuous denoiser but as a iterative discrete demasking process over masked positions, thereby resolving multi-codebook (depth) dependencies and supporting efficient parallel synthesis in the RVQ code space.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The design gives two main advantages. Compared with continuous next-patch diffusion, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;avoids optimization issues in high-dimensional continuous latents while retaining patch-level parallelism. Compared with single-codebook AR, it models the intra-frame multi-codebook coupling inside each patch, improving coherence and allowing depthwise parallel refinement, which reduces exposure-bias effects without sacrificing the robustness of discrete training.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several design choices in <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yield practical advantages. First, the fully discrete setting preserves an <span class=\"ltx_text ltx_font_typewriter\">[EOS]</span> token for the immediate termination of patch-level generation, eliminating auxiliary duration predictors or stop heads <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>); Shen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib45\" title=\"\">2018</a>)</cite>. Second, sharing the RVQ code space between the AR sketcher and the masked diffusion refiner allows end-to-end optimization and reduces inter-module mismatch relative to cascaded pipelines <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>); Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib16\" title=\"\">2024b</a>)</cite>. Third, decoding achieves robust quality under purely greedy settings, while temperature-based sampling extends to RVQ-level and joint layer&#8211;time strategies, enabling fine-grained control over the diversity&#8211;determinism trade-off. Fourth, pruning the upper RVQ layers at inference controls computation and bitrate to match bandwidth/latency constraints without retraining.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "greedy",
                    "decoding",
                    "sampling",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop an RVQ-specific sampling method that boosts quality and stability; support for diverse decoding strategies, and on-the-fly bitrate/compute control without retraining.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Zero-shot TTS work broadly bifurcates by the target representation. Continuous-latent approaches predict high-information features (mel or codec latents) with diffusion/flow to boost long-range consistency and robust cloning. <cite class=\"ltx_cite ltx_citemacro_citep\">(Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib46\" title=\"\">2023</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib25\" title=\"\">2024</a>)</cite> scale latent and factorized diffusion with speech prompting; <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib29\" title=\"\">2024</a>)</cite> casts text-guided speech infilling as flow matching; <cite class=\"ltx_cite ltx_citemacro_citep\">(Eskimez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib17\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>)</cite> report strong results via continuous flow matching; <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib60\" title=\"\">2025</a>)</cite> simplify training with scalar-latent codecs and Transformer diffusion; <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib32\" title=\"\">2024</a>)</cite> improves time-varying style control; and recent <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib31\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib34\" title=\"\">2024</a>)</cite> explore DiT-based and autoregressive diffusion decoders, while <cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>; Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib53\" title=\"\">2024</a>)</cite> place diffusion heads inside causal stacks to retain AR-like controllability. In contrast, discrete-token pipelines discretize speech and leverage AR LMs for stronger in-context prompting and explicit decoding control. <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite> set the Nerual codec LM recipe, with <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib8\" title=\"\">2024</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib51\" title=\"\">2025b</a>)</cite> improving robustness and human parity; token-infilling AR models such as <cite class=\"ltx_cite ltx_citemacro_citep\">(Kharitonov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib26\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib41\" title=\"\">2024</a>)</cite> advanced editing and wild-data zero-shot; <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib55\" title=\"\">2024</a>)</cite> uses masked generative codec modeling; and large multilingual systems such as<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib15\" title=\"\">2024a</a>; Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib5\" title=\"\">2024</a>)</cite>, broaden coverage and controllability.</p>\n\n",
                "matched_terms": [
                    "results",
                    "decoding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Masked diffusion extends discrete-state diffusion to language by formalizing token corruption with structured transition kernels, notably D3PM and multinomial diffusion, which introduced absorbing masks and principled categorical noising <cite class=\"ltx_cite ltx_citemacro_citep\">(Austin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib3\" title=\"\">2021</a>; Hoogeboom et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib21\" title=\"\">2021</a>)</cite>. Recent theory shows that masked-diffusion training objectives reduce to mixtures of masked-LM losses and can be reparameterized to connect tightly with any-order autoregression; this yields simpler sampling and clarifies likelihood bounds <cite class=\"ltx_cite ltx_citemacro_citep\">(Ou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib40\" title=\"\">2024</a>; Shih et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib48\" title=\"\">2022</a>)</cite>. Scaling experiments train masked-diffusion LMs from scratch at LLM scale (e.g., LLaDA), demonstrating competitive perplexity and strong instruction-following after SFT <cite class=\"ltx_cite ltx_citemacro_citep\">(Nie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib38\" title=\"\">2025</a>)</cite>. Beyond pure masking, generalized interpolating discrete diffusion mixes masking with uniform noise, enabling self-correction and compute-matched gains <cite class=\"ltx_cite ltx_citemacro_citep\">(von R&#252;tte et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib54\" title=\"\">2025</a>)</cite>. Parallel advances include simplified/standardized masked diffusion parameterizations, reparameterized discrete diffusion routes and denoisers, and analyses that decouple paradigm (MDM vs. AR) from architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib47\" title=\"\">2024</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib62\" title=\"\">2023</a>)</cite>. Task-level studies further adapt discrete diffusion to conditional long-text generation with improved efficiency <cite class=\"ltx_cite ltx_citemacro_citep\">(Dat et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib12\" title=\"\">2024</a>)</cite>. Collectively, these results position masked discrete diffusion as a competitive LM family with any-order decoding, efficient parallelism, and growing theoretical clarity.</p>\n\n",
                "matched_terms": [
                    "results",
                    "decoding",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S2.F1\" title=\"Figure 1 &#8227; 2.2 Mask Diffusion Models &#8227; 2 Related Works &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, an autoregressive architecture that advances patch by patch while remaining entirely within a discrete residual vector-quantized (RVQ) code domain.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recast long sequences of discrete speech codes into a tiled layout of patches, akin to the DiTAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib24\" title=\"\">2025</a>)</cite> paradigm. An autoregressive (AR) Transformer governs dependencies across patches, while a masked diffusion Transformer completes the contents within each patch in parallel. In practice, generation advances along the patch index autoregressively, with the next patch produced via conditional masked diffusion.\nLet <math alttext=\"\\mathbf{C}=[\\mathbf{c}_{0},\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{L-1}]\\in\\mathbb{Z}^{L\\times J}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119810;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119836;</mi><mn>0</mn></msub><mo>,</mo><msub><mi>&#119836;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119836;</mi><mrow><mi>L</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8484;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>J</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{C}=[\\mathbf{c}_{0},\\mathbf{c}_{1},\\ldots,\\mathbf{c}_{L-1}]\\in\\mathbb{Z}^{L\\times J}</annotation></semantics></math> denote the sequence of RVQ codes from <math alttext=\"J\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>J</mi><annotation encoding=\"application/x-tex\">J</annotation></semantics></math> quantizers, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the RVQ code sequence length. And let <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m4\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> be the input text.\nWe estimate the <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;parameters <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m5\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> by conditional maximum likelihood over the patch-grouped codes. By the chain rule, the model factorizes as</p>\n\n",
                "matched_terms": [
                    "distar",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{c}_{&lt;i}=[\\mathbf{c}_{0},\\ldots,\\mathbf{c}_{i-1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#119836;</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119836;</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119836;</mi><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{c}_{&lt;i}=[\\mathbf{c}_{0},\\ldots,\\mathbf{c}_{i-1}]</annotation></semantics></math> collects all previously generated codes. Training minimizes the negative log-likelihood,\nwhile inference realizes the autoregressive step at the patch level and resolves intra-patch tokens via masked diffusion in an iterative parallel decoding pass.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "patch",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At inference, generation proceeds through iterative decoding in parallel over a masked sequence. We begin from an all-masked initialization <math alttext=\"\\dot{\\mathbf{C}}^{(k)}_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.Px3.p2.m1\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119810;</mi><mo>&#729;</mo></mover><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\dot{\\mathbf{C}}^{(k)}_{1}</annotation></semantics></math>. At time <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.Px3.p2.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the masked diffusion Transformer takes <math alttext=\"\\dot{\\mathbf{C}}^{(k)}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.Px3.p2.m3\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119810;</mi><mo>&#729;</mo></mover><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\dot{\\mathbf{C}}^{(k)}_{t}</annotation></semantics></math> as input and outputs categorical distributions for every currently masked position in one shot.\nFrom these distributions we produce a provisional completion <math alttext=\"\\widehat{\\mathbf{C}}^{(k)}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.Px3.p2.m4\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119810;</mi><mo>^</mo></mover><mi>t</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\widehat{\\mathbf{C}}^{(k)}_{t}</annotation></semantics></math> (sampling or choosing modes), and compute per-position confidence scores <math alttext=\"s_{t}(i)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.Px3.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s_{t}(i)</annotation></semantics></math> (e.g., maximum class probability).</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text prompt is fed to the AR LM and then concatenated with a learned patch embedding obtained by aggregating the relevant RVQ codes. The LM produces a contextual summary <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m1\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, which&#8212;together with a finite history of previously generated tokens&#8212;serves as conditioning for the diffusion model. Training minimizes the cross-entropy objective in equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S3.E2\" title=\"In Discrete masked diffusion over the next span. &#8227; 3.1.1 Formulation &#8227; 3.1 Overview &#8227; 3 DiSTAR &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "patch",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;dispenses with both an explicit duration predictor and forced alignment. Moreover, unlike prior designs, the RVQ aggregator is not restricted to disjoint patches: overlapping windows are permitted. At each decoding step, the diffusion head predicts, in one shot, a segment whose length matches the aggregator&#8217;s stride on the output stream, ensuring consistency. Additional architectural and training details are provided in the following sections.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "decoding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a lightweight non-autoregressive Transformer encoder to turn frame-level RVQ codes into one vector per patch.\nConcretely, each RVQ layer has its own embedding table, and we adopt factorized embedding parameterization <cite class=\"ltx_cite ltx_citemacro_citep\">(Lan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib28\" title=\"\">2019</a>)</cite>: a narrow embedding (e.g., 32-d) is learned and then lifted to <math alttext=\"d_{\\text{model}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mtext>model</mtext></msub><annotation encoding=\"application/x-tex\">d_{\\text{model}}</annotation></semantics></math> by a layer-specific linear map. A learnable scalar mixes the layer embeddings at each frame into a single continuous vector by weighted summing, giving one hidden vector per RVQ frame.</p>\n\n",
                "matched_terms": [
                    "patch",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the overlapped mode in classic CNNs <cite class=\"ltx_cite ltx_citemacro_citep\">(O&#8217;shea &amp; Nash, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib39\" title=\"\">2015</a>)</cite>, we do not require the stride to equal the aggregation patch size on the input RVQ sequence, thereby allowing overlapping. With patch length <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> and stride <math alttext=\"S\\!\\leq\\!P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8804;</mo><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">S\\!\\leq\\!P</annotation></semantics></math>, we intentionally allow <math alttext=\"S&lt;P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo>&lt;</mo><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">S&lt;P</annotation></semantics></math> so adjacent patches overlap, which smooths boundaries and provide more information.\nAfter the encoder, each patch vector is obtained by averaging the final hidden states over the last <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> tokens of that patch. The resulting sequence of patch embeddings is then passed to the AR language model.</p>\n\n",
                "matched_terms": [
                    "patch",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, we draw a timestep <math alttext=\"t\\sim\\mathcal{U}(0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">t\\sim\\mathcal{U}(0,1]</annotation></semantics></math> and compute the masking ratio via a cosine schedule <math alttext=\"\\lambda(t)=\\cos\\big(\\frac{1-t}{2}\\pi\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>cos</mi><mo>&#8289;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><mfrac><mrow><mn>1</mn><mo>&#8722;</mo><mi>t</mi></mrow><mn>2</mn></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#960;</mi></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(t)=\\cos\\big(\\frac{1-t}{2}\\pi\\big)</annotation></semantics></math> following &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib6\" title=\"\">2022</a>)</cite>.\nA fraction <math alttext=\"\\lambda(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda(t)</annotation></semantics></math> of target tokens in the current patch is replaced by a special <span class=\"ltx_text ltx_font_typewriter\">[MASK]</span> symbol, and the diffusion model learns to reconstruct all masked RVQ tokens simultaneously given the visible tokens and the conditioning prefix. At inference, we use the same cosine schedule to anneal the mask ratio over <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m4\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> iterations using iteratively decoding, after which the process advances to the next patch.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "patch",
                    "use",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At inference, we observe a <span class=\"ltx_text ltx_font_italic\">tail-first</span> bias: tokens near the end of each patch often receive higher confidence early in decoding.\nVanilla decoding thus induces a mask pattern misaligned with training and degrades performance.\nA likely reason is that, in temporally/casually dependent sequences, non-autoregressive training makes later positions easier (they lean more on preceding context), leading to overconfidence.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate this with three lightweight decoding tricks: (i) <span class=\"ltx_text ltx_font_bold\">Layer-wise temperature shaping.</span> Cool down deeper RVQ layers so they don&#8217;t dominate early. Concretely, multiply the sampling temperature for layer <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m1\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> by <math alttext=\"T_{\\text{layer}}^{\\,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>T</mi><mtext>layer</mtext><mi>j</mi></msubsup><annotation encoding=\"application/x-tex\">T_{\\text{layer}}^{\\,j}</annotation></semantics></math>. (ii) <span class=\"ltx_text ltx_font_bold\">Position-wise temperature shaping.</span> Within a patch, reduce confidence for farther-ahead positions by scaling the temperature at offset <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> with <math alttext=\"T_{\\text{time}}^{\\,l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>T</mi><mtext>time</mtext><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">T_{\\text{time}}^{\\,l}</annotation></semantics></math> (<math alttext=\"0\\!\\leq\\!l\\!&lt;\\!S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m5\" intent=\":literal\"><semantics><mrow><mn>0</mn><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8804;</mo><mi>l</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&lt;</mo><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">0\\!\\leq\\!l\\!&lt;\\!S</annotation></semantics></math>).\n(iii) <span class=\"ltx_text ltx_font_bold\">Hybrid sampling.</span> Generate the first <math alttext=\"50\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m6\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">50\\%</annotation></semantics></math> of the target positions by sampling, then switch to greedy for the remaining <math alttext=\"50\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p2.m7\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">50\\%</annotation></semantics></math> to trade diversity for stability. In principle, the greedy/sample schedule is a hyperparameter; we adopt a simple half&#8211;half scheme to avoid over-tuning.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "sampling",
                    "greedy",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our default setup, <math alttext=\"T_{\\text{layer}}{=}0.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mtext>layer</mtext></msub><mo>=</mo><mn>0.8</mn></mrow><annotation encoding=\"application/x-tex\">T_{\\text{layer}}{=}0.8</annotation></semantics></math>, <math alttext=\"T_{\\text{time}}{=}0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mtext>time</mtext></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">T_{\\text{time}}{=}0.95</annotation></semantics></math>; we use top-<math alttext=\"k{=}50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p3.m3\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">k{=}50</annotation></semantics></math>, top-<math alttext=\"p{=}0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p3.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">p{=}0.9</annotation></semantics></math>, and anneal temperature from <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p3.m5\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math> to <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p3.m6\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math> for sampling.\nFollowing <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib8\" title=\"\">2024</a>)</cite>, we also add a repetition-aware penalty within every <math alttext=\"P_{r}{=}4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.SSS0.Px4.p3.m7\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mi>r</mi></msub><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">P_{r}{=}4</annotation></semantics></math> patches to curb code collapse at the layer level.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together these inference strategies keep sampling diverse yet stable, while also enabling high-quality greedy decoding.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "sampling",
                    "greedy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;comprises (i) an aggregator, (ii) a causal language model (LM), and (iii) a masked diffusion model (MDM), all instantiated with Transformer backbones. The aggregator and MDM are bidirectional RoFormers with rotary positional encodings (RoPE) <cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib52\" title=\"\">2024</a>)</cite>, SwiGLU activations <cite class=\"ltx_cite ltx_citemacro_citep\">(Shazeer, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib44\" title=\"\">2020</a>)</cite>, and a Pre-Norm layout using RMSNorm<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Sennrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib61\" title=\"\">2019</a>)</cite>. The AR LM follows the Qwen2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib58\" title=\"\">2024</a>)</cite> style decoder-only architecture and is trained from scratch.\nInput text is converted to phoneme sequences and embedded via a learned lookup table. At inference, consistent with prior practice, we construct a prefix context that includes the text together with a short acoustic prompt.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we position <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;against strong contemporary systems and report state-of-the-art results.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models are trained on Emilia <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib19\" title=\"\">2024</a>)</cite>, a large, multilingual, in-the-wild speech corpus curated for scalable speech generation. For this study we use only its English subset, totaling roughly <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>50</mn><annotation encoding=\"application/x-tex\">50</annotation></semantics></math>k hours. Evaluation spans two open benchmarks: (i) LibriSpeech(PC) <cite class=\"ltx_cite ltx_citemacro_citep\">(Meister et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib36\" title=\"\">2023</a>)</cite> test-clean, <math alttext=\"5.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>5.4</mn><annotation encoding=\"application/x-tex\">5.4</annotation></semantics></math> hours from <math alttext=\"40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mn>40</mn><annotation encoding=\"application/x-tex\">40</annotation></semantics></math> unique English speakers; we follow the established zero-shot cross-sentence protocol and adopt the subset from F5TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib10\" title=\"\">2025b</a>)</cite> (40 prompts, 1127 utterances), and (ii) SeedTTS test-en, introduced with Seed-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib1\" title=\"\">2024</a>)</cite>, consisting of 1088 English samples drawn from Common Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib2\" title=\"\">2019</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "use",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use both objective and subjective metrics to evaluate our models. For the objective metrics, we evaluate (i) Word Error Rate (WER) to assess robustness and intelligibility using Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib42\" title=\"\">2022</a>)</cite> as our ASR model; (ii) Speaker similarity (SIM) via cosine similarity between the TDNN-based WavLM embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib7\" title=\"\">2022</a>)</cite> extracted from the generated audio and its reference prompt; (iii) UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib43\" title=\"\">2022</a>)</cite>, an automatic predicted mean opinion score (MOS) for speech quality.For the subjective metrics, comparative mean option score (CMOS) and similarity mean option score (SMOS) are used to evaluate naturalness/robustness and similarity, respectively. CMOS is on a scale of -3 to 3, and SMOS is on a scale of 1 to 5.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "objective",
                    "use",
                    "utmos",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train all models on 64 NVIDIA A100 80GB GPUs. We train <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;of two sizes <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-base (0.15B), and <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-medium (0.3B). Architectural specifics are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A2.SS2\" title=\"B.2 Model Architecture &#8227; Appendix B Implementation Details &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>. Unless otherwise noted, we use a patch size of <math alttext=\"8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>8</mn><annotation encoding=\"application/x-tex\">8</annotation></semantics></math> and condition the diffusion module on a single historical patch. Inference procedures are detailed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S3.SS4.SSS0.Px4\" title=\"Decoding heuristics. &#8227; 3.4 Training and Inference &#8227; 3 DiSTAR &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "all",
                    "size",
                    "use",
                    "patch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">we show comparison results with SOTA baselines. The main results of objective metrics are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T1\" title=\"Table 1 &#8227; Model settings and baselines. &#8227; 4.1 Experimental Settings &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Subjective results are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T2\" title=\"Table 2 &#8227; 4.2 Zero-Shot TTS &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;exhibits strong overall performance. Across benchmarks it attains the lowest WER, indicating robust synthesis. We assess speaker similarity using both objective and human judgments: SIM and speaker SMOS. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields SIM on par with the best alternatives and leads on SMOS. We attribute these gains in part to reduced sensitivity to high-frequency artifacts in the reference prompt, which preserves cleaner timbral cues during cloning. The same trend is observed in the objective predictor UTMOS and in CMOS listening tests. Notably, relative to continuous-representation systems, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;achieves comparable or superior perceptual quality with a similar or smaller parameter budget. As model capacity grows, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;yields consistent improvements on objective metrics, closely matching the scaling behavior reported for discrete-token autoregressive systems and indicating a healthy scaling trajectory.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "wer",
                    "utmos",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we conduct a detailed analysis of the various components of <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>. Unless specified otherwise, we default to using <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>-base with 0.15 billion parameters, a patch size of 8, a stride of 8, and NFE of 24, tested on the LibriSpeech-PC test dataset mentioned above. We discuss the patch size in Appendix &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A4\" title=\"Appendix D Patch Size. &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> and classifier-guidance free settings in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A3\" title=\"Appendix C Classifier-Free Guidance &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "patch",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;under multiple inference strategies (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.T3\" title=\"Table 3 &#8227; Decoding Strategies. &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nDeterministic greedy decoding yields the lowest WER, evidencing stable synthesis,\nwhile speaker similarity is slightly lower than with stochastic sampling.\nThis aligns with the standard diversity-determinism trade-off: sampling can\nrecover timbral nuances at the cost of higher variability.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "greedy",
                    "wer",
                    "decoding",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;randomly drops the last <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math> RVQ layers. At inference, we simply prune higher RVQ layers to achieve variable bitrate and compute. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#S4.F2\" title=\"Figure 2 &#8227; 4.4 Inference Efficiency and Controllability &#8227; 4 Experiments &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes the inference&#8211;quality trade-off under different RVQ pruning depths. As more RVQ layers are retained (hence higher FLOPs), speaker similarity increases markedly, whereas WER changes little and reaches its minimum around six layers. This pattern is consistent with the hypothesis that upper RVQ layers primarily encode acoustic detail rather than linguistic content <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib9\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "wer",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>, a zero-shot TTS framework that operates in an RVQ code space and tightly couples an autoregressive Transformer with masked diffusion. <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160;achieves blockwise parallelism while effectively modeling layer&#8211;time dependencies across RVQ levels and local temporal neighborhoods, without an explicit duration predictor. We further introduced a simple but effective RVQ-aware sampling procedure that stabilizes inference and improves perceptual quality. In combination, these design choices yield SOTA robustness, speaker similarity, and naturalness in zero-shot speech synthesis, while exposing clear levers for controllability at inference time.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "sampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results are currently demonstrated under the codec, RVQ configuration, and evaluation setups described in the paper. Performance may depend on the RVQ depth and codebook design. Due to resource constraints, we trained our model solely on a around-50k-hour English corpus<cite class=\"ltx_cite ltx_citemacro_cite\">He et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib19\" title=\"\">2024</a>)</cite>; broader generalization to multilingual and multi-style settings remains to be evaluated.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize 64 A100 GPUs, each processing a batch size of 36K token frames, and train <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160; for 0.6M steps. The AdamW optimizer is employed with a learning rate of 0.75e-4 for AR, and 1.5e-4 for the other parts, <math alttext=\"\\beta_{t}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mi>t</mi></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{t}=0.9</annotation></semantics></math>, and <math alttext=\"\\beta_{2}=0.99\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.99</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#A3.T5\" title=\"Table 5 &#8227; Appendix C Classifier-Free Guidance &#8227; DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, holding all other factors fixed, nested CFG (Scheme <span class=\"ltx_text ltx_font_bold\">B</span>) and simple single-step CFG (Scheme <span class=\"ltx_text ltx_font_bold\">A</span>) yield nearly identical objective metrics. To reduce computational cost and latency, we therefore adopt the simpler Scheme <span class=\"ltx_text ltx_font_bold\">A</span> (single-step CFG) as the default in the main experiments.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A diffusion-transformer (DiT) flow-matching system that addresses E2TTS&#8217;s convergence and robustness issues with a ConvNeXt <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12210v2#bib.bib35\" title=\"\">2022</a>)</cite> text encoder and an inference-time Sway Sampling schedule. Trained at scale (about 100k hours), it demonstrates strong zero-shot cloning, code-switching, and efficient inference, with open-sourced code and checkpoints. It is widely adopted as a strong continuous-latent NAR baseline.\nWe use the official code and pre-trained checkpoint <span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/SWivid/F5-TTS\" title=\"\">https://huggingface.co/SWivid/F5-TTS</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">High-fidelity zero-shot TTS offers clear benefits for accessibility, education, and creative production, yet the ability of <span class=\"ltx_text ltx_font_smallcaps\">DiSTAR</span>&#160; to closely match speaker timbre introduces material risks, such as impersonation and social-engineering fraud, spoofed voice biometrics, non-consensual cloning, and scalable disinformation. To mitigate misuse, we advocate consent-first deployment with strict use-policy gating, the use of robust audio watermarks to support provenance, and a public abuse-reporting channel with prompt triage and access revocation.</p>\n\n",
                "matched_terms": [
                    "distar",
                    "use"
                ]
            }
        ]
    }
}