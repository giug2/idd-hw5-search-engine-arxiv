{
    "S5.T1": {
        "source_file": "BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT",
        "caption": "Table 1: Mora-label error rates [%] of PA recognition. (=†{}^{\\dagger}= ASR output has multiple possible katakana representation, and minimum error rates are shown here. See main text for details.)",
        "body": "eval1\neval2\neval3\nJSUT\n\n\naccent err.\n\n✓\n\n✓\n\n✓\n\n✓\n\n\nWhisper†\n20.6\n24.3\n15.1\n19.7\n13.5\n16.5\n2.8\n6.8\n\n\nMultipa†\n17.2\n–\n19.8\n–\n18.3\n–\n16.2\n–\n\n\nPA-only\n7.2\n11.5\n7.6\n11.9\n7.9\n13.5\n6.6\n13.2\n\n\nMT\n4.4\n7.3\n4.9\n7.8\n5.0\n9.1\n5.8\n9.9\n\n\nMT+Cond.\n4.3\n7.3\n5.2\n8.2\n5.0\n9.3\n6.1\n10.3\n\n\nMT+LF\n4.1\n7.0\n4.8\n7.7\n4.9\n9.1\n6.1\n10.3\n\n\n\nMT+LF with external ASR models\n\n\n\nwith Whisper\n4.3\n7.2\n4.8\n7.6\n4.6\n8.8\n2.1\n6.4\n\n\nwith TT-only\n3.1\n6.0\n4.1\n7.1\n4.1\n8.3\n4.3\n8.6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:80%;\">eval1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:80%;\">eval2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:80%;\">eval3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:80%;\">JSUT</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">accent err.</span></th>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Whisper<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8224;</span></sup></span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">20.6</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">24.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">15.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">19.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">13.5</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">16.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.8</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Multipa<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8224;</span></sup></span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">17.2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">19.8</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">18.3</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">16.2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">PA-only</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.6</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.9</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">13.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.6</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">13.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MT</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.4</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.9</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">9.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">5.8</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">9.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MT+Cond.</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.3</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.2</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MT+LF</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">7.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.8</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">7.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.9</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">9.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"9\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MT+LF</span><span class=\"ltx_text\" style=\"font-size:80%;\"> with external ASR models</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">with Whisper</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.3</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.8</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.6</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">6.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">with TT-only</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">6.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">7.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.1</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">8.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.3</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "representation",
            "paonly",
            "ttonly",
            "text",
            "eval1",
            "output",
            "accent",
            "eval2",
            "moralabel",
            "has",
            "details",
            "external",
            "possible",
            "multipa†",
            "here",
            "katakana",
            "main",
            "whisper",
            "jsut",
            "mtcond",
            "see",
            "eval3",
            "err",
            "multiple",
            "whisper†",
            "minimum",
            "mtlf",
            "asr",
            "rates",
            "models",
            "†dagger",
            "error",
            "recognition"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Tables&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> show the mora-label error rates (MLERs) and CERs of the systems, respectively. MLERs were computed both with and without accent errors.\nWe observed that the generic multilingual speech recognizer (Whisper) was not suitable for our phonemic transcription task.\nEven though our final metrics for Whisper were optimistic, computed by choosing the optimal pronunciations based on the reference labels, the results on the CSJ evaluation sets were not competitive with our tailored model.\nAs discussed previously, this is due to the normalization effect from the language model.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that Whisper was also not competitive in CER.\nThis was because CSJ utterances contain a lot of speaker errors, which Whisper tended to ignore.\nOn the other hand, for JSUT, since this dataset contains read speech without speaker errors, Whisper worked almost perfectly.\nIn that case, if we could choose the pronunciation of each word optimally, the MLER can be very low (6.8%).</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">From the results of the decoding methods (``MT+Cond'' and ``MT+LF''), it was shown that the lattice fusion was effective.\nThe outputs of ``MT+Cond'' suggested that explicit conditioning is fragile if the recognizer cannot recognize textual representation of errors.\nWe observed that neither decoding method was effective in the JSUT task, due to the inaccuracy of TT outputs used to compute TT lattices (see Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nThis issue was mitigated by introducing an external speech recognizer as a source of TT lattices.\nAs shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ``MT+LF with Whisper'' achieved the best result for the JSUT task. Similarly, since our ``TT-only'' model was better than Whisper for CSJ tasks, ``MT+LF with TT-only'' showed the best results for the CSJ tasks.\nThus, we confirmed that ``MT+LF'' approach could further be enhanced if we had an accurate TT recognizer.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper presents methods for building speech recognizers tailored for Japanese speaking assessment tasks.\nSpecifically, we build a speech recognizer that outputs phonemic labels with accent markers.\nAlthough Japanese is resource-rich, there is only a small amount of data for training models to produce accurate phonemic transcriptions that include accent marks.\nWe propose two methods to mitigate data sparsity. First, a multitask training scheme introduces auxiliary loss functions to estimate orthographic text labels and pitch patterns of the input signal, so that utterances with only orthographic annotations can be leveraged in training. The second fuses two estimators, one over phonetic alphabet strings, and the other over text token sequences. To combine these estimates we develop an algorithm based on the finite-state transducer framework.\nOur results indicate that the use of multitask learning and fusion is effective for building an accurate phonemic recognizer.\nWe show that this approach is advantageous compared to the use of generic multilingual recognizers.\nThe relative advantages of the proposed methods were also compared.\nOur proposed methods reduced the average of mora-label error rates from 12.3% to 7.1% over the CSJ core evaluation sets.</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "text",
                    "models",
                    "accent",
                    "moralabel",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Automatic speech recognition (ASR) technology has been evolving to to enable the mapping of speech signals to canonical textual representations under a variety of conditions.\nIn typical ASR systems, speaker errors, such as incorrect accent positions, mispronunciations, fillers, and hesitations, are intentionally discarded.\nAlthough many applications prefer such canonical output rather than a phonetically accurate transcription, in certain cases such as language education, this normalization effect can be viewed as a limitation because it hinders the accurate evaluation of users' speaking proficiency.</span>\n</p>\n\n",
                "matched_terms": [
                    "accent",
                    "output",
                    "has",
                    "recognition",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">There have been several attempts to build universal phonetic transcribers, with a particular focus on extending ASR to low-resource languages </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWhile multilingual models are potentially applicable to speaking skill assessment, assessing language-specific phenomena, such as pitch accent, is also essential for educational purposes.\nIn this paper, we focus on phonemic transcription of Japanese along with pitch accent markers, for the purposes of building a speech recognizer tailored for Japanese speaking assessment.\nConsidering the complexity and irregularity of Japanese pitch accent rules </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, this presents a unique challenge that cannot be solved only by multilingual phonetic recognizers.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "accent",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prior work by </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> developed a phonemic recognizer conditioned on the outputs of external speech recognizers.\nAlthough their method did not include a built-in ASR component, there are several similarities to our proposal.\nTheir system performs both implicit conditioning, employing text tokens as an auxiliary input, and explicit conditioning using text information as constraints on decoding.\nUnlike their implicit conditioning method, our approach does not require reliable text labels because it solves text and phonetic alphabet (PA) recognition simultaneously in a multitask learning framework.\nFurther, as discussed in the following section, one can view our decoding method as an extension of their explicit conditioning method that can robustly handle weak text constraints.\nFor a conventional ASR task, the iterative refinement method is proposed in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in order to estimate PA and text token sequences jointly.\nHowever, this method does not estimate accent locations, and does not utilize a dictionary to enhance accuracy in small data scenarios.</span>\n</p>\n\n",
                "matched_terms": [
                    "external",
                    "text",
                    "accent",
                    "recognition",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The technical contributions in this paper are twofold: First, a multitask training method that is effective for pitch-accent recognition; second, a decoding method that decodes an optimal PA sequence robustly by considering both text tokens and PA estimation results.\nThese two novel methods are implemented in a streamable ASR model, and comparative experiments were conducted to confirm the effectiveness of the proposed methods.\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2 System Design &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates our proposed method. The next three sections describe the components in the figure.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "recognition",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Conventionally, language models are integrated into ASR systems in order to improve the output quality leveraging the frequencies of phrases and words.\nHowever, since one of our goals is to detect errors in speech that manifest as infrequent fluctuations in token sequences, such corrections by language models may conflict with this objective.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "output",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Connectionist Temporal Classification (CTC) can be a solution to this problem.\nUnlike other ASR architectures, such as RNN-T </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and LAS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a CTC model does not include a module for capturing mutual dependency within the output sequences.\nTherefore, it is expected that CTC models are incapable of correcting erroneous utterances into canonical ones.\nWhile it has been reported that even CTC can implicitly learn an internal language model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CTC is still considered the best choice if correction needs to be minimized.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "output",
                    "has",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For our purposes, a training set annotated with PAs and accent markers is required.\nThe transcription in the dataset must be faithful to the original speech, including errors.\nCSJ includes manual annotations for such speech variation. By training on this dataset, one can obtain a recognizer that can detect such errors represented in PAs.\nA subset of CSJ (called ``core'') provides mora-wise phonemic annotations in katakana and the perceived location of accent;</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Japanese is a <em class=\"ltx_emph ltx_font_italic\">mora-timed</em> language, a mora being a prosodic unit corresponding to a short vowel (V) or a single post-vocalic consonant (C) (Syllable-initial consonants do not count for moraic purposes).\nSyllables of the form (C)V are monomoraic, and those\nof the form (C)VV or (C)VC are bimoraic.\nEach mora ideally occupies about the same amount of time, though see <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib13\" title=\"\">13</a>]</cite> for a detailed review of the situation for Japanese.</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nhowever, this subset is only a small fraction of the entire dataset,\nconsisting of only 23,683 training utterances, or less than 6% of the entire CSJ training dataset.\nAlthough Japanese is considered to be a resource-rich language, given the sparsity of accent-annotated datasets, it is still not straightforward to build an accurate recognizer out of the available data.\nThe next sections describe practical remedies for this limitation.</span>\n</p>\n\n",
                "matched_terms": [
                    "katakana",
                    "accent",
                    "see"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The sparsity of phonetic labels with accent information is the central challenge in developing speech recognizers tailored for Japanese speaking assessment.\nThe first remedy we employ is multitask learning.\nOrthographic transcription is cheaper to obtain compared to phonemic annotation, especially if the phonemic annotation also requires accent annotation.\nSince text transcriptions and phonemic annotations are expected to be mutually dependent, solving two tasks simultaneously can help learning a common representation of speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "representation",
                    "text",
                    "accent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">On the other hand, estimated pitch information is also cheap to obtain via a high-quality fundamental frequency ( </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> ) extraction algorithm.\nSince Japanese has lexical pitch accent, pitch information is also expected to be highly correlated with the target labels.\nThus, in this paper, we train a transformer with three estimation tasks: one for PAs, one for text tokens (TTs), and one for estimated  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> classes.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "accent",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our PA tokenizer assumes a katakana transcription of the utterance.\nIf an accent is placed on a mora, this is denoted by appending an apostrophe to the mora label (e.g. </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">&#12461;&#12517;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> /k</span>\n  <sup class=\"ltx_sup\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">j</span>\n  </sup>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">W/ with accent is denoted as </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">&#12461;&#12517;'</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nThe long-vowel marker (</span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">&#12540;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) is replaced by a copy of the preceding vowel.\nFor example, the </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">&#12461;&#12517;&#12540;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> /k</span>\n  <sup class=\"ltx_sup\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">j</span>\n  </sup>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">W/ is replaced by </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">&#12461;&#12517;&#12454;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> /k</span>\n  <sup class=\"ltx_sup\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">j</span>\n  </sup>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">W W/.\nSimilarly, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">&#12461;&#12517;'&#12540;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is replaced by </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">&#12461;&#12517;'&#12454;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe collect all tokens from the CSJ core training set, resulting in 243 unique output tokens.</span>\n</p>\n\n",
                "matched_terms": [
                    "accent",
                    "katakana",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Let </span>\n  <math alttext=\"Y\\in\\mathbb{R}^{T\\times K}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">Y</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">K</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">Y\\in\\mathbb{R}^{T\\times K}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> be an array of log probabilities of each class in CTC output layer, including the special blank symbol </span>\n  <math alttext=\"\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#981;</mi>\n      <annotation encoding=\"application/x-tex\">\\phi</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"y_{t,k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">k</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{t,k}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the log-probability of label </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> emitted at time </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor each utterance, a confusion network over the CTC labels can be constructed as in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4 Lattice Fusion &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(a).\nIn confusion networks, each non-final state represents the timestamp </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and outgoing arcs from state </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are defined for each possible </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The arc labeled as </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from state </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is designed to have a weight representing the negative log probability as </span>\n  <math alttext=\"w=-y_{t,k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">w</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">y</mi>\n            <mrow>\n              <mi mathsize=\"0.900em\">t</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mi mathsize=\"0.900em\">k</mi>\n            </mrow>\n          </msub>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">w=-y_{t,k}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nLet </span>\n  <math alttext=\"\\mathcal{S}^{\\mathrm{P}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m12\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119982;</mi>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">P</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{S}^{\\mathrm{P}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> be the confusion network constructed by this procedure.</span>\n</p>\n\n",
                "matched_terms": [
                    "possible",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The CTC output labels must be post-processed by removing the same-label repetition and the blank </span>\n  <math alttext=\"\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#981;</mi>\n      <annotation encoding=\"application/x-tex\">\\phi</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> symbols.\nThis post-processing can be represented as a transducer as in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4 Lattice Fusion &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(b). By applying the composition operator to the confusion network </span>\n  <math alttext=\"\\mathcal{S}^{\\mathrm{P}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119982;</mi>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">P</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{S}^{\\mathrm{P}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the CTC postprocessing FST </span>\n  <math alttext=\"\\mathcal{B}^{\\mathrm{P}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8492;</mi>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">P</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{B}^{\\mathrm{P}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and projecting the FST to the ouput labels, the lattice </span>\n  <math alttext=\"\\mathcal{L}^{\\mathrm{P}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">P</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}^{\\mathrm{P}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that represents a probability distribution over all possible PA sequences can be computed.\nFor computational efficiency, we further applied a lattice pruning method and further optimization for maximizing the efficiency of the subsequent procedures, as follows:\n</span>\n  <math alttext=\"\\mathcal{L}^{\\mathrm{P}}=\\mathrm{Opt}(\\mathrm{\\pi_{O}}(\\mathcal{S}^{\\mathrm{P}}\\circ\\mathcal{B}^{\\mathrm{P}}))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">P</mi>\n        </msup>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">Opt</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mrow>\n              <msub>\n                <mi mathsize=\"0.900em\">&#960;</mi>\n                <mi mathsize=\"0.900em\" mathvariant=\"normal\">O</mi>\n              </msub>\n              <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n              <mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                <mrow>\n                  <msup>\n                    <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119982;</mi>\n                    <mi mathsize=\"0.900em\" mathvariant=\"normal\">P</mi>\n                  </msup>\n                  <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#8728;</mo>\n                  <msup>\n                    <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8492;</mi>\n                    <mi mathsize=\"0.900em\" mathvariant=\"normal\">P</mi>\n                  </msup>\n                </mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n              </mrow>\n            </mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}^{\\mathrm{P}}=\\mathrm{Opt}(\\mathrm{\\pi_{O}}(\\mathcal{S}^{\\mathrm{P}}\\circ\\mathcal{B}^{\\mathrm{P}}))</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nHere, </span>\n  <math alttext=\"\\circ\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8728;</mo>\n      <annotation encoding=\"application/x-tex\">\\circ</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the composition operator on FSTs, </span>\n  <math alttext=\"\\mathrm{\\pi_{O}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#960;</mi>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">O</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathrm{\\pi_{O}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a projection function that obtains an FSA from the FST by removing its input labels, and </span>\n  <math alttext=\"\\mathrm{Opt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">Opt</mi>\n      <annotation encoding=\"application/x-tex\">\\mathrm{Opt}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is an optimization procedure that prunes, removes </span>\n  <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#1013;</mi>\n      <annotation encoding=\"application/x-tex\">\\epsilon</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, determinizes, and minimizes </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the input, in this order, over the log semiring.\nFollowing the procedure, a compact representation of PA distribution can be obtained from the output log-probabilities of the PA-CTC task.</span>\n</p>\n\n",
                "matched_terms": [
                    "representation",
                    "possible",
                    "output",
                    "here"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our TT-to-PA FST is based on UniDic </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nOf 876,803 UniDic entries, 873,647 were selected by discarding entries with null pronunciations or unknown PAs.\nLexicon FST construction followed </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nUsing the TT-to-PA converter represented as an FST, a lattice representing distribution over PA sequences can also be obtained by converting the TT lattice to a PA lattice.\nThis conversion can be done via FST composition.\nLet </span>\n  <math alttext=\"\\mathcal{D}^{\\mathrm{}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n        <mi/>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\mathrm{}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> be an TT-to-PA FST, a PA lattice induced by the TT lattice can be computed as: </span>\n  <math alttext=\"\\mathcal{L^{\\prime}}^{\\mathrm{T2P}}=\\mathrm{Opt}(\\mathrm{\\pi_{O}}(\\mathcal{L}^{\\mathrm{T}}\\circ\\mathcal{D}^{\\mathrm{}}))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mmultiscripts>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n          <mrow/>\n          <mo mathsize=\"0.900em\">&#8242;</mo>\n          <mrow/>\n          <mi mathsize=\"0.900em\">T2P</mi>\n        </mmultiscripts>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">Opt</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mrow>\n              <msub>\n                <mi mathsize=\"0.900em\">&#960;</mi>\n                <mi mathsize=\"0.900em\" mathvariant=\"normal\">O</mi>\n              </msub>\n              <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n              <mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                <mrow>\n                  <msup>\n                    <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n                    <mi mathsize=\"0.900em\" mathvariant=\"normal\">T</mi>\n                  </msup>\n                  <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#8728;</mo>\n                  <msup>\n                    <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n                    <mi/>\n                  </msup>\n                </mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n              </mrow>\n            </mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L^{\\prime}}^{\\mathrm{T2P}}=\\mathrm{Opt}(\\mathrm{\\pi_{O}}(\\mathcal{L}^{\\mathrm{T}}\\circ\\mathcal{D}^{\\mathrm{}}))</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFurthermore, since the dictionary </span>\n  <math alttext=\"\\mathcal{D}^{\\mathrm{}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n        <mi/>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\mathrm{}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> does not provide relative probabilities when a word has multiple pronunciation, it is necessary to accumulate PA weights to </span>\n  <math alttext=\"\\mathcal{L^{\\prime}}^{\\mathrm{T2P}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mmultiscripts>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mrow/>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n        <mrow/>\n        <mi mathsize=\"0.900em\">T2P</mi>\n      </mmultiscripts>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L^{\\prime}}^{\\mathrm{T2P}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe PA probability can be taken from </span>\n  <math alttext=\"\\mathcal{L}^{\\mathrm{P}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">P</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}^{\\mathrm{P}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe pronunciation-weighted lattice obtained in this way can be expressed as:\n</span>\n  <math alttext=\"\\mathcal{L}^{\\mathrm{T2P}}=\\mathrm{Norm}(\\mathcal{L}^{\\mathrm{P}}\\circ\\mathcal{L^{\\prime}}^{\\mathrm{T2P}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n          <mi mathsize=\"0.900em\">T2P</mi>\n        </msup>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">Norm</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mrow>\n              <msup>\n                <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n                <mi mathsize=\"0.900em\" mathvariant=\"normal\">P</mi>\n              </msup>\n              <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#8728;</mo>\n              <mmultiscripts>\n                <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n                <mrow/>\n                <mo mathsize=\"0.900em\">&#8242;</mo>\n                <mrow/>\n                <mi mathsize=\"0.900em\">T2P</mi>\n              </mmultiscripts>\n            </mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}^{\\mathrm{T2P}}=\\mathrm{Norm}(\\mathcal{L}^{\\mathrm{P}}\\circ\\mathcal{L^{\\prime}}^{\\mathrm{T2P}})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nHere, </span>\n  <math alttext=\"\\mathrm{Norm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">Norm</mi>\n      <annotation encoding=\"application/x-tex\">\\mathrm{Norm}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the normalization operator that normalizes the weights of outgoing arcs for each state to satisfy the sum-to-one constraint.</span>\n</p>\n\n",
                "matched_terms": [
                    "multiple",
                    "has",
                    "here"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For analyzing the performances of the proposed systems, we compared three variants of the method: the full proposed method (MT+LF), a variant without lattice fusion (MT), and a variant with neither lattice fusion nor multitask learning (PA-only).\nFurther, instead of lattice fusion, we implemented explicit conditioning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using our recognizers TT and PA output results (MT+Cond.).\nFor LF systems, the MT+LF system computed the TT lattices using the TT outputs of the MT model.\nWe additionally compared LF method with other sources of TT lattices. Whisper, and our ``TT-only'' variant were used as external TT lattice sources. For LF with Whisper, the lattice was constructed to have only a single path corresponding to the recognition result from Whisper.\nAs side results, we also compare character error rates (CERs) of the MT model, Whisper, and our model without PA and  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> outputs (TT-only).</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "paonly",
                    "external",
                    "mtcond",
                    "ttonly",
                    "output",
                    "mtlf",
                    "whisper",
                    "error",
                    "recognition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As baseline systems, we adapted Whisper </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">whisper-1</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model from OpenAI) and Multipa </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Since neither system is able to output our target PA directly, we made adapters that produce multiple PA outputs corresponding to the Whisper output text, or Multipa output IPAs. The minimum error rates among all possible outputs from the adapter were used as a final metric for the systems with such adapters. For the Whisper adapter, since our adapter is based on UniDic, the adapter can also output PAs with accent markers based on the accent annotation given for each word in UniDic.</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "text",
                    "possible",
                    "output",
                    "accent",
                    "multiple",
                    "minimum",
                    "whisper",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For evaluation, two datasets with accent annotations were employed.\nSince our model training is dependent on CSJ, core subsets of CSJ evaluation sets were used as our primary evaluation sets (CSJ eval1/ eval2/ eval3). In addition to CSJ, ``basic5000'' from the JSUT corpora was used </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This dataset (JSUT) contains read utterances from a single speaker.\nCSJ's spontaneous and multi-speaker properties align more to our objective; however, JSUT is also important for probing the behavior of our methods in out-of-domain settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "eval3",
                    "eval1",
                    "accent",
                    "eval2",
                    "jsut"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Comparing PA-only and MT, it was shown that training with the noncore subset was important.\nOur method could successfully leverage the noncore subset which lacks accent annotations.\nThe improvements made by MT were shown to be transferrable to the out-of-domain JSUT task where MLERs reduced from 13.2% to 9.9%.</span>\n</p>\n\n",
                "matched_terms": [
                    "paonly",
                    "accent",
                    "jsut"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we proposed and evaluated methods for building tailored speech recognizers for Japanese speaking assessment.\nThe proposed recognizer is equipped with phonetic alphabet and text token decoders, and the phonetic alphabet results are decoded using a lattice fusion technique that integrates the recognition results from text token decoders with using a pronunciation dictionary.\nThe recognizer is trained on CSJ, which contains phonetic annotations of mispronunciations, hesitations and accents.\nOur model training uses a multitask learning scheme consisting of phonetic transcription prediction, text-token prediction, and  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> pattern classification.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "recognition"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT",
        "caption": "Table 2: Character error rates [%] of text transcription.",
        "body": "eval1\neval2\neval3\nJSUT\n\n\nWhisper\n18.9\n17.2\n15.1\n8.3\n\n\n\n\nTT-only\n4.6\n5.4\n5.6\n17.8\n\n\nMT\n6.3\n6.8\n7.8\n22.6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">eval1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">eval2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">eval3</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">JSUT</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Whisper</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">18.9</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">17.2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">15.1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.3</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">TT-only</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">17.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MT</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">22.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "rates",
            "transcription",
            "character",
            "ttonly",
            "eval3",
            "text",
            "eval1",
            "eval2",
            "whisper",
            "error",
            "jsut"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Tables&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> show the mora-label error rates (MLERs) and CERs of the systems, respectively. MLERs were computed both with and without accent errors.\nWe observed that the generic multilingual speech recognizer (Whisper) was not suitable for our phonemic transcription task.\nEven though our final metrics for Whisper were optimistic, computed by choosing the optimal pronunciations based on the reference labels, the results on the CSJ evaluation sets were not competitive with our tailored model.\nAs discussed previously, this is due to the normalization effect from the language model.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that Whisper was also not competitive in CER.\nThis was because CSJ utterances contain a lot of speaker errors, which Whisper tended to ignore.\nOn the other hand, for JSUT, since this dataset contains read speech without speaker errors, Whisper worked almost perfectly.\nIn that case, if we could choose the pronunciation of each word optimally, the MLER can be very low (6.8%).</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">From the results of the decoding methods (``MT+Cond'' and ``MT+LF''), it was shown that the lattice fusion was effective.\nThe outputs of ``MT+Cond'' suggested that explicit conditioning is fragile if the recognizer cannot recognize textual representation of errors.\nWe observed that neither decoding method was effective in the JSUT task, due to the inaccuracy of TT outputs used to compute TT lattices (see Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nThis issue was mitigated by introducing an external speech recognizer as a source of TT lattices.\nAs shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ``MT+LF with Whisper'' achieved the best result for the JSUT task. Similarly, since our ``TT-only'' model was better than Whisper for CSJ tasks, ``MT+LF with TT-only'' showed the best results for the CSJ tasks.\nThus, we confirmed that ``MT+LF'' approach could further be enhanced if we had an accurate TT recognizer.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper presents methods for building speech recognizers tailored for Japanese speaking assessment tasks.\nSpecifically, we build a speech recognizer that outputs phonemic labels with accent markers.\nAlthough Japanese is resource-rich, there is only a small amount of data for training models to produce accurate phonemic transcriptions that include accent marks.\nWe propose two methods to mitigate data sparsity. First, a multitask training scheme introduces auxiliary loss functions to estimate orthographic text labels and pitch patterns of the input signal, so that utterances with only orthographic annotations can be leveraged in training. The second fuses two estimators, one over phonetic alphabet strings, and the other over text token sequences. To combine these estimates we develop an algorithm based on the finite-state transducer framework.\nOur results indicate that the use of multitask learning and fusion is effective for building an accurate phonemic recognizer.\nWe show that this approach is advantageous compared to the use of generic multilingual recognizers.\nThe relative advantages of the proposed methods were also compared.\nOur proposed methods reduced the average of mora-label error rates from 12.3% to 7.1% over the CSJ core evaluation sets.</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "text",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The sparsity of phonetic labels with accent information is the central challenge in developing speech recognizers tailored for Japanese speaking assessment.\nThe first remedy we employ is multitask learning.\nOrthographic transcription is cheaper to obtain compared to phonemic annotation, especially if the phonemic annotation also requires accent annotation.\nSince text transcriptions and phonemic annotations are expected to be mutually dependent, solving two tasks simultaneously can help learning a common representation of speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "transcription",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For analyzing the performances of the proposed systems, we compared three variants of the method: the full proposed method (MT+LF), a variant without lattice fusion (MT), and a variant with neither lattice fusion nor multitask learning (PA-only).\nFurther, instead of lattice fusion, we implemented explicit conditioning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using our recognizers TT and PA output results (MT+Cond.).\nFor LF systems, the MT+LF system computed the TT lattices using the TT outputs of the MT model.\nWe additionally compared LF method with other sources of TT lattices. Whisper, and our ``TT-only'' variant were used as external TT lattice sources. For LF with Whisper, the lattice was constructed to have only a single path corresponding to the recognition result from Whisper.\nAs side results, we also compare character error rates (CERs) of the MT model, Whisper, and our model without PA and  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> outputs (TT-only).</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "character",
                    "ttonly",
                    "whisper",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As baseline systems, we adapted Whisper </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">whisper-1</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model from OpenAI) and Multipa </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Since neither system is able to output our target PA directly, we made adapters that produce multiple PA outputs corresponding to the Whisper output text, or Multipa output IPAs. The minimum error rates among all possible outputs from the adapter were used as a final metric for the systems with such adapters. For the Whisper adapter, since our adapter is based on UniDic, the adapter can also output PAs with accent markers based on the accent annotation given for each word in UniDic.</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "text",
                    "whisper",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For evaluation, two datasets with accent annotations were employed.\nSince our model training is dependent on CSJ, core subsets of CSJ evaluation sets were used as our primary evaluation sets (CSJ eval1/ eval2/ eval3). In addition to CSJ, ``basic5000'' from the JSUT corpora was used </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This dataset (JSUT) contains read utterances from a single speaker.\nCSJ's spontaneous and multi-speaker properties align more to our objective; however, JSUT is also important for probing the behavior of our methods in out-of-domain settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "eval2",
                    "eval1",
                    "jsut",
                    "eval3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we proposed and evaluated methods for building tailored speech recognizers for Japanese speaking assessment.\nThe proposed recognizer is equipped with phonetic alphabet and text token decoders, and the phonetic alphabet results are decoded using a lattice fusion technique that integrates the recognition results from text token decoders with using a pronunciation dictionary.\nThe recognizer is trained on CSJ, which contains phonetic annotations of mispronunciations, hesitations and accents.\nOur model training uses a multitask learning scheme consisting of phonetic transcription prediction, text-token prediction, and  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> pattern classification.</span>\n</p>\n\n",
                "matched_terms": [
                    "transcription",
                    "text"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT",
        "caption": "Table 3: Mora-label error rates [%] of each task combination. ``noncore'' shows whether the noncore subset was used in the training.",
        "body": "noncore\neval1\neval2\neval3\nJSUT\n\n\n\n\nPA-only\n–\n11.5\n11.9\n13.5\n13.2\n\n\nPA + TT\n✔\n7.7\n8.1\n9.5\n10.1\n\n\nPA +  fo\\boldsymbol{f_{o}}\n–\n11.8\n11.9\n14.0\n13.7\n\n\nPA +  fo\\boldsymbol{f_{o}}\n✔\n11.8\n12.4\n13.9\n12.5\n\n\nPA + TT +  fo\\boldsymbol{f_{o}} (MT)\n✔\n7.3\n7.8\n9.1\n9.9",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">noncore</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">eval1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">eval2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">eval3</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">JSUT</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">PA-only</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">13.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">13.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">PA + TT</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10004;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">PA +  <math alttext=\"\\boldsymbol{f_{o}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>o</mi></msub><annotation encoding=\"application/x-tex\">\\boldsymbol{f_{o}}</annotation></semantics></math></span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">14.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">13.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">PA +  <math alttext=\"\\boldsymbol{f_{o}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>o</mi></msub><annotation encoding=\"application/x-tex\">\\boldsymbol{f_{o}}</annotation></semantics></math></span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10004;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">12.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">13.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">12.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">PA + TT +  <math alttext=\"\\boldsymbol{f_{o}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>o</mi></msub><annotation encoding=\"application/x-tex\">\\boldsymbol{f_{o}}</annotation></semantics></math> (MT)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10004;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.9</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "rates",
            "each",
            "paonly",
            "task",
            "subset",
            "training",
            "eval3",
            "eval1",
            "jsut",
            "noncore",
            "combination",
            "used",
            "whether",
            "foboldsymbolfo",
            "eval2",
            "moralabel",
            "error",
            "shows"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further analyze how each task contributes in multitask training, comparative experiments over each combination of auxiliary task were conducted.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, most of the advantage of multitask training is attributable to the TT task.\nThe  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> task was not effective if it was only an auxiliary task (PA+ </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> rows).\nHowever,  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> yields a considerable gain when it was combined with TT,\nsuggesting that  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> can be used to regularize the TT task.\nSince TT labels can only have limited information on accents, adding  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p5.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in conjunction with TT was important to obtain a reliable recognizer that can also leverage prosodic information.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper presents methods for building speech recognizers tailored for Japanese speaking assessment tasks.\nSpecifically, we build a speech recognizer that outputs phonemic labels with accent markers.\nAlthough Japanese is resource-rich, there is only a small amount of data for training models to produce accurate phonemic transcriptions that include accent marks.\nWe propose two methods to mitigate data sparsity. First, a multitask training scheme introduces auxiliary loss functions to estimate orthographic text labels and pitch patterns of the input signal, so that utterances with only orthographic annotations can be leveraged in training. The second fuses two estimators, one over phonetic alphabet strings, and the other over text token sequences. To combine these estimates we develop an algorithm based on the finite-state transducer framework.\nOur results indicate that the use of multitask learning and fusion is effective for building an accurate phonemic recognizer.\nWe show that this approach is advantageous compared to the use of generic multilingual recognizers.\nThe relative advantages of the proposed methods were also compared.\nOur proposed methods reduced the average of mora-label error rates from 12.3% to 7.1% over the CSJ core evaluation sets.</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "moralabel",
                    "error",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The training of phonetically accurate recognizers often suffers from the limited availability of accurate phonetic or phonemic transcriptions.\nAlthough Japanese is a relatively resource-rich language, the largest multi-speaker dataset with hand-annotated pitch accent is, to the best of our knowledge, the ``core'' subset of the Corpus of Spontaneous Japanese (CSJ) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, containing only 45 hours of speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "subset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For our purposes, a training set annotated with PAs and accent markers is required.\nThe transcription in the dataset must be faithful to the original speech, including errors.\nCSJ includes manual annotations for such speech variation. By training on this dataset, one can obtain a recognizer that can detect such errors represented in PAs.\nA subset of CSJ (called ``core'') provides mora-wise phonemic annotations in katakana and the perceived location of accent;</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Japanese is a <em class=\"ltx_emph ltx_font_italic\">mora-timed</em> language, a mora being a prosodic unit corresponding to a short vowel (V) or a single post-vocalic consonant (C) (Syllable-initial consonants do not count for moraic purposes).\nSyllables of the form (C)V are monomoraic, and those\nof the form (C)VV or (C)VC are bimoraic.\nEach mora ideally occupies about the same amount of time, though see <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib13\" title=\"\">13</a>]</cite> for a detailed review of the situation for Japanese.</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nhowever, this subset is only a small fraction of the entire dataset,\nconsisting of only 23,683 training utterances, or less than 6% of the entire CSJ training dataset.\nAlthough Japanese is considered to be a resource-rich language, given the sparsity of accent-annotated datasets, it is still not straightforward to build an accurate recognizer out of the available data.\nThe next sections describe practical remedies for this limitation.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "training",
                    "subset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Text tokens (TTs) are character-based. We collected all Unicode points from the CSJ training dataset (core and non-core) after NFKC normalization, resulting in 2,309 distinct tokens.</span>\n</p>\n\n",
                "matched_terms": [
                    "noncore",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The objective of the  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> classifier task is to ensure that pitch information is propagated through the transformer, allowing the model to estimate  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> contours.\nFor each utterance, the Harvest algorithm</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is applied to estimate  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> each 10 ms.\nSince pitch accent is based not on the pitch itself, but rather its trajectory, the task is designed to estimate predefined classes that indicate if  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is going up or down.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For analyzing the performances of the proposed systems, we compared three variants of the method: the full proposed method (MT+LF), a variant without lattice fusion (MT), and a variant with neither lattice fusion nor multitask learning (PA-only).\nFurther, instead of lattice fusion, we implemented explicit conditioning </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using our recognizers TT and PA output results (MT+Cond.).\nFor LF systems, the MT+LF system computed the TT lattices using the TT outputs of the MT model.\nWe additionally compared LF method with other sources of TT lattices. Whisper, and our ``TT-only'' variant were used as external TT lattice sources. For LF with Whisper, the lattice was constructed to have only a single path corresponding to the recognition result from Whisper.\nAs side results, we also compare character error rates (CERs) of the MT model, Whisper, and our model without PA and  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> outputs (TT-only).</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "paonly",
                    "used",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For training, the CSJ dataset is augmented with speed perturbation </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> followed by a time-domain variant of SpecAugment </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor speed perturbation, 20% of samples were modified to have 90% rate, and another 20% of samples were modified to have 110% rate.\nThe time-domain SpecAugment is developed to apply masking as in the original SpecAugment but in the raw-waveform domain.\nThis variant implements time-masking on the raw waveform, and the spectral masking on the (full utterance) discrete Fourier transform domain.\nThe hyper-parameters for SpecAugment were set as follows: The number of time-domain masks was </span>\n  <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">10</mn>\n      <annotation encoding=\"application/x-tex\">10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the number of frequency-domain masks was </span>\n  <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2</mn>\n      <annotation encoding=\"application/x-tex\">2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the maximum length of time-domain masks was </span>\n  <math alttext=\"0.05T\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">0.05</mn>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">T</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">0.05T</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where </span>\n  <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">T</mi>\n      <annotation encoding=\"application/x-tex\">T</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the length of the input utterance, and the maximum length of frequency domain mask was </span>\n  <math alttext=\"0.3F\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">0.3</mn>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">F</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">0.3F</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">F</mi>\n      <annotation encoding=\"application/x-tex\">F</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the bandwidth of the input signal in mel.\nThe task weights were preliminarily set to 0.3, 0.6, and 0.1 for the PA, TT, and  </span>\n  <math alttext=\"f_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">o</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{o}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> tasks respectively. We did not tune the task weights.</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As baseline systems, we adapted Whisper </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">whisper-1</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model from OpenAI) and Multipa </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Since neither system is able to output our target PA directly, we made adapters that produce multiple PA outputs corresponding to the Whisper output text, or Multipa output IPAs. The minimum error rates among all possible outputs from the adapter were used as a final metric for the systems with such adapters. For the Whisper adapter, since our adapter is based on UniDic, the adapter can also output PAs with accent markers based on the accent annotation given for each word in UniDic.</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "each",
                    "used",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For evaluation, two datasets with accent annotations were employed.\nSince our model training is dependent on CSJ, core subsets of CSJ evaluation sets were used as our primary evaluation sets (CSJ eval1/ eval2/ eval3). In addition to CSJ, ``basic5000'' from the JSUT corpora was used </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This dataset (JSUT) contains read utterances from a single speaker.\nCSJ's spontaneous and multi-speaker properties align more to our objective; however, JSUT is also important for probing the behavior of our methods in out-of-domain settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "eval3",
                    "eval1",
                    "eval2",
                    "used",
                    "jsut"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Tables&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> show the mora-label error rates (MLERs) and CERs of the systems, respectively. MLERs were computed both with and without accent errors.\nWe observed that the generic multilingual speech recognizer (Whisper) was not suitable for our phonemic transcription task.\nEven though our final metrics for Whisper were optimistic, computed by choosing the optimal pronunciations based on the reference labels, the results on the CSJ evaluation sets were not competitive with our tailored model.\nAs discussed previously, this is due to the normalization effect from the language model.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that Whisper was also not competitive in CER.\nThis was because CSJ utterances contain a lot of speaker errors, which Whisper tended to ignore.\nOn the other hand, for JSUT, since this dataset contains read speech without speaker errors, Whisper worked almost perfectly.\nIn that case, if we could choose the pronunciation of each word optimally, the MLER can be very low (6.8%).</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "each",
                    "task",
                    "shows",
                    "moralabel",
                    "error",
                    "jsut"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Comparing PA-only and MT, it was shown that training with the noncore subset was important.\nOur method could successfully leverage the noncore subset which lacks accent annotations.\nThe improvements made by MT were shown to be transferrable to the out-of-domain JSUT task where MLERs reduced from 13.2% to 9.9%.</span>\n</p>\n\n",
                "matched_terms": [
                    "paonly",
                    "training",
                    "task",
                    "subset",
                    "noncore",
                    "jsut"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">From the results of the decoding methods (``MT+Cond'' and ``MT+LF''), it was shown that the lattice fusion was effective.\nThe outputs of ``MT+Cond'' suggested that explicit conditioning is fragile if the recognizer cannot recognize textual representation of errors.\nWe observed that neither decoding method was effective in the JSUT task, due to the inaccuracy of TT outputs used to compute TT lattices (see Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nThis issue was mitigated by introducing an external speech recognizer as a source of TT lattices.\nAs shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20655v1#S5.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 5.2 Discussion &#8227; 5 Experiments &#8227; BUILDING TAILORED SPEECH RECOGNIZERS FOR JAPANESE SPEAKING ASSESSMENT\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ``MT+LF with Whisper'' achieved the best result for the JSUT task. Similarly, since our ``TT-only'' model was better than Whisper for CSJ tasks, ``MT+LF with TT-only'' showed the best results for the CSJ tasks.\nThus, we confirmed that ``MT+LF'' approach could further be enhanced if we had an accurate TT recognizer.</span>\n</p>\n\n",
                "matched_terms": [
                    "used",
                    "task",
                    "jsut"
                ]
            }
        ]
    }
}