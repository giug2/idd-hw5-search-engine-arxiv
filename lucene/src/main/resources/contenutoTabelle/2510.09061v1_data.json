{
    "S3.T1": {
        "caption": "Table 1: Any-to-any voice conversion results. Blue indicates the best performance, Underline indicates second best. Subjective evaluation results showing MOS and SMOS scores, along with 95% confidence intervals.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Objective Evaluation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Subjective Evaluation</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SECS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">NISQA<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">MOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">B-MOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">FreeVC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">4.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\textbf{{\\color[rgb]{0,0,1}3.60}}\\pm\\textbf{{\\color[rgb]{0,0,1}0.26}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#0000FF\" style=\"--ltx-fg-color:#0000FF;\">3.60</mtext><mo>&#177;</mo><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#0000FF\" style=\"--ltx-fg-color:#0000FF;\">0.26</mtext></mrow><annotation encoding=\"application/x-tex\">\\textbf{{\\color[rgb]{0,0,1}3.60}}\\pm\\textbf{{\\color[rgb]{0,0,1}0.26}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.01\\pm 0.28\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mrow><mn>3.01</mn><mo>&#177;</mo><mn>0.28</mn></mrow><annotation encoding=\"application/x-tex\">3.01\\pm 0.28</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math alttext=\"\\underline{3.31}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><munder accentunder=\"true\"><mn>3.31</mn><mo stretchy=\"true\">&#175;</mo></munder><annotation encoding=\"application/x-tex\">\\underline{3.31}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">KNN-VC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">78.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\underline{0.62}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m11\" intent=\":literal\"><semantics><munder accentunder=\"true\"><mn>0.62</mn><mo stretchy=\"true\">&#175;</mo></munder><annotation encoding=\"application/x-tex\">\\underline{0.62}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.17\\pm 0.23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m12\" intent=\":literal\"><semantics><mrow><mn>3.17</mn><mo>&#177;</mo><mn>0.23</mn></mrow><annotation encoding=\"application/x-tex\">3.17\\pm 0.23</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.89\\pm 0.21\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m13\" intent=\":literal\"><semantics><mrow><mn>2.89</mn><mo>&#177;</mo><mn>0.21</mn></mrow><annotation encoding=\"application/x-tex\">2.89\\pm 0.21</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Diff-Hier</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">81.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.87\\pm 0.28\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m14\" intent=\":literal\"><semantics><mrow><mn>2.87</mn><mo>&#177;</mo><mn>0.28</mn></mrow><annotation encoding=\"application/x-tex\">2.87\\pm 0.28</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.42\\pm 0.25\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m15\" intent=\":literal\"><semantics><mrow><mn>3.42</mn><mo>&#177;</mo><mn>0.25</mn></mrow><annotation encoding=\"application/x-tex\">3.42\\pm 0.25</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">DDDM-VC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\underline{81.86}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m16\" intent=\":literal\"><semantics><munder accentunder=\"true\"><mn>81.86</mn><mo stretchy=\"true\">&#175;</mo></munder><annotation encoding=\"application/x-tex\">\\underline{81.86}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.89\\pm 0.28\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m17\" intent=\":literal\"><semantics><mrow><mn>2.89</mn><mo>&#177;</mo><mn>0.28</mn></mrow><annotation encoding=\"application/x-tex\">2.89\\pm 0.28</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\textbf{{\\color[rgb]{0,0,1}3.61}}\\pm\\textbf{{\\color[rgb]{0,0,1}0.23}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m18\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#0000FF\" style=\"--ltx-fg-color:#0000FF;\">3.61</mtext><mo>&#177;</mo><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#0000FF\" style=\"--ltx-fg-color:#0000FF;\">0.23</mtext></mrow><annotation encoding=\"application/x-tex\">\\textbf{{\\color[rgb]{0,0,1}3.61}}\\pm\\textbf{{\\color[rgb]{0,0,1}0.23}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Facodec</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">81.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\underline{2.08}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m19\" intent=\":literal\"><semantics><munder accentunder=\"true\"><mn>2.08</mn><mo stretchy=\"true\">&#175;</mo></munder><annotation encoding=\"application/x-tex\">\\underline{2.08}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.49\\pm 0.29\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m20\" intent=\":literal\"><semantics><mrow><mn>2.49</mn><mo>&#177;</mo><mn>0.29</mn></mrow><annotation encoding=\"application/x-tex\">2.49\\pm 0.29</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.66\\pm 0.27\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m21\" intent=\":literal\"><semantics><mrow><mn>2.66</mn><mo>&#177;</mo><mn>0.27</mn></mrow><annotation encoding=\"application/x-tex\">2.66\\pm 0.27</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">O_O-VC (Ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">86.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">1.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><math alttext=\"\\underline{4.04}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m22\" intent=\":literal\"><semantics><munder accentunder=\"true\"><mn>4.04</mn><mo stretchy=\"true\">&#175;</mo></munder><annotation encoding=\"application/x-tex\">\\underline{4.04}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><math alttext=\"\\underline{3.42\\pm 0.24}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m23\" intent=\":literal\"><semantics><munder accentunder=\"true\"><mrow><mn>3.42</mn><mo>&#177;</mo><mn>0.24</mn></mrow><mo stretchy=\"true\">&#175;</mo></munder><annotation encoding=\"application/x-tex\">\\underline{3.42\\pm 0.24}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><math alttext=\"\\underline{3.48\\pm 0.23}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m24\" intent=\":literal\"><semantics><munder accentunder=\"true\"><mrow><mn>3.48</mn><mo>&#177;</mo><mn>0.23</mn></mrow><mo stretchy=\"true\">&#175;</mo></munder><annotation encoding=\"application/x-tex\">\\underline{3.48\\pm 0.23}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_markedasmath ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">3.45</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wer↓downarrow",
            "second",
            "knnvc",
            "blue",
            "208¯underline208",
            "evaluation",
            "diffhier",
            "8186¯underline8186",
            "dddmvc",
            "301±028301pm",
            "smos",
            "underline",
            "mos",
            "361±023textbfcolorrgb001361pmtextbfcolorrgb001023",
            "cer↓downarrow",
            "nisqa↑uparrow",
            "showing",
            "freevc",
            "voice",
            "facodec",
            "objective",
            "249±029249pm",
            "secs↑uparrow",
            "360±026textbfcolorrgb001360pmtextbfcolorrgb001026",
            "scores",
            "performance",
            "anytoany",
            "404¯underline404",
            "287±028287pm",
            "062¯underline062",
            "348±023¯underline348pm",
            "results",
            "indicates",
            "confidence",
            "bmos↑uparrow",
            "smos↑uparrow",
            "289±021289pm",
            "ours",
            "266±027266pm",
            "intervals",
            "along",
            "model",
            "342±024¯underline342pm",
            "conversion",
            "best",
            "317±023317pm",
            "289±028289pm",
            "mos↑uparrow",
            "subjective",
            "oovc",
            "331¯underline331",
            "342±025342pm"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate our model in a zero-shot setting, where the target speaker is unseen during training. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S3.T1\" title=\"Table 1 &#8227; 3.2.4 Objective Function &#8227; 3.2 Model Overview &#8227; 3 Methodology &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> demonstrate that our model achieves the best performance in terms of content consistency, with the lowest WER and CER. Furthermore, our model achieves the second highest MOS, only slightly behind FreeVC. This can be attributed to the fact that FreeVC is trained on a high-quality speech dataset, whereas our model is fine-tuned and adapted on LibriSpeech, which is of comparatively lower quality, leading to decreased performance. Despite FreeVC&#8217;s strong MOS, it performs notably worse in terms of speaker similarity and content intelligibility compared to our model. Although DDDM-VC achieves the highest Similarity Mean Opinion Score (SMOS), its speech quality is comparatively poor. Overall, our model achieves the best intelligibility while maintaining a strong balance between naturalness (MOS) and speaker similarity (SMOS), outperforming recent systems in a zero-shot scenario.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Traditional voice conversion (VC) methods typically attempt to separate speaker identity and linguistic information into distinct representations, which are then combined to reconstruct the audio. However, effectively disentangling these factors remains challenging, often leading to information loss during training. In this paper, we propose a new approach that leverages synthetic speech data generated by a high-quality, pretrained multispeaker text-to-speech (TTS) model. Specifically, synthetic data pairs that share the same linguistic content but differ in speaker identity are used as input-output pairs to train the voice conversion model. This enables the model to learn a direct mapping between source and target voices, effectively capturing speaker-specific characteristics while preserving linguistic content. Additionally, we introduce a flexible training strategy for any-to-any voice conversion that generalizes well to unseen speakers and new languages, enhancing adaptability and performance in zero-shot scenarios. Our experiments show that our proposed method achieves a 16.35% relative reduction in word error rate and a 5.91% improvement in speaker cosine similarity, outperforming several state-of-the-art methods. Voice conversion samples can be accessed at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://oovc-emnlp-2025.github.io/\" title=\"\">https://oovc-emnlp-2025.github.io/</a></p>\n\n",
                "matched_terms": [
                    "anytoany",
                    "model",
                    "conversion",
                    "voice",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion</span>\n</p>\n\n",
                "matched_terms": [
                    "anytoany",
                    "conversion",
                    "oovc",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Voice conversion specifically aims to transform a source speaker&#8217;s voice to match a target speaker while preserving the original linguistic content. This is typically done by disentangling speech into content and speaker identity representations, which are combined during training to reconstruct the audio. At inference time, the source content is paired with a target speaker embedding to generate the converted speech.\n</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies have primarily focused on feature disentanglement methods and audio reconstruction in voice conversion systems. However, feature disentanglement remains a challenging task and training models to reconstruct the audio may not be well suited for the voice conversion objective, which inherently involves transforming speech from one speaker to another. To address these limitations, we propose a novel training strategy that leverages synthetic data generated by a high-quality multi-speaker text-to-speech (TTS) system to directly establish input-output mappings for voice conversion, bypassing the need for traditional reconstruction-based approaches. Despite the high fidelity of synthetic data, such TTS systems are typically constrained to a fixed set of speakers, limiting their applicability to any-to-any voice conversion, particularly for unseen speakers. To mitigate this issue, we introduce a training framework that promotes generalization to unseen speakers without relying on additional text or speaker labels, thereby enhancing the system&#8217;s adaptability and performance in zero-shot voice conversion scenarios. In summary, we make the following contributions:</p>\n\n",
                "matched_terms": [
                    "anytoany",
                    "conversion",
                    "voice",
                    "objective",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Synthetic data for voice conversion training: We propose the use of synthetic data generated by a high-quality multi-speaker TTS system to train voice conversion models. This approach eliminates the need for audio reconstruction and feature disentanglement, enabling direct learning of input-output mappings.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Improved generalization: We introduce a training strategy that allows the model to generalize to unseen speakers or unseen languages, making it well suited for zero-shot voice conversion.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "model",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of voice conversion (VC) is to transform the voice of a source speaker into that of a target speaker while preserving the original linguistic content. Achieving this requires an effective decomposition of speech signals into distinct components such as linguistic content, speaker timbre, and prosodic characteristics. Early VC systems were typically trained as speech-to-speech models on parallel datasets, where multiple speakers uttered the same sentences, defining the task as a sequence-to-sequence problem <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib39\" title=\"\">2015</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib3\" title=\"\">2014</a>)</cite>. Recent VC approaches have focused on reconstructing speech using disentangled representations of linguistic content and speaker identity.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A common strategy is to leverage pretrained automatic speech recognition (ASR) models to extract phonetic features such as PPGs, which provide a speaker-independent representation of the input speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib40\" title=\"\">2016</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib24\" title=\"\">2021</a>; Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib41\" title=\"\">2018</a>)</cite>. Specifically, speaker information is obtained using a pretrained speaker verification (SV) model. The speaker embeddings are combined with the content features during decoding, allowing the system to generate speech in the target speaker&#8217;s voice. Some approaches leverage hidden text representations from pretrained multispeaker text-to-speech (TTS) models, using them either as semantic features or as target representations for learning a mapping from audio to text <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib31\" title=\"\">2020</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib44\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite their advantages, text-based methods suffer from several limitations. PPGs and other textual representations often fail to capture fine-grained attributes such as accent, prosody, and speaker-independent speaking style. As a result, these systems often produce speech that lacks expressiveness and sounds overly neutral <cite class=\"ltx_cite ltx_citemacro_citep\">(Hussain et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib16\" title=\"\">2023</a>)</cite>. Although ASR-based disentanglement methods have shown progress in separating speaker and content information, their reliance on textual supervision and limited prosodic modeling remain significant challenges for achieving natural and expressive voice conversion across diverse speakers and languages.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work has introduced k-nearest neighbor (kNN)-based voice conversion methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Baas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib1\" title=\"\">2023</a>)</cite>, offering a simpler alternative to traditional feature disentanglement approaches. These methods operate directly on frame-level self-supervised representations extracted from both source and target speech, which encode both phonetic and speaker-specific information. Voice conversion is performed by replacing each frame of the source with its nearest neighbor from the target set, followed by vocoder-based synthesis.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Diffusion models have shown exceptional performance in generative tasks across a variety of domains, including images, videos, and audio. In speech processing, diffusion models have been successfully applied to tasks such as audio generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib19\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib4\" title=\"\">2021a</a>)</cite> and text-to-speech (TTS) synthesis <cite class=\"ltx_cite ltx_citemacro_citep\">(Popov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib33\" title=\"\">2021</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib13\" title=\"\">2022a</a>)</cite>. Furthermore, diffusion models have been investigated for VC tasks with the aim of enhancing the conversion process. In particular, diffusion-based VC models <cite class=\"ltx_cite ltx_citemacro_citep\">(Popov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib34\" title=\"\">2022</a>)</cite> have demonstrated high performance in zero-shot speaker adaptation through iterative sampling processes. While recent works such as Diff-HierVC <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib7\" title=\"\">2023</a>)</cite> and DDDM-VC <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib8\" title=\"\">2024</a>)</cite> have further improved zero-shot VC performance through source-filter disentanglement and disentangled denoising processes, the audio quality of diffusion models is still limited.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "performance",
                    "dddmvc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In text-free VC systems, content and speaker identity are often not fully disentangled. As a result, speaker information can leak into the content representation, which undermines the system&#8217;s ability to perform clean speaker conversion. This leakage reduces the system&#8217;s generalization to unseen voices or speaking styles and often results in converted speech that retains characteristics of the source speaker.</p>\n\n",
                "matched_terms": [
                    "results",
                    "conversion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As an alternative solution, synthetic data offers several advantages for voice conversion. When both source and target audio are generated from the same linguistic content, it provides a clean and direct supervisory signal. This shared content allows precise frame-level alignment between source and target audio, enabling more stable and fine-grained learning of the conversion function. Unlike traditional approaches that rely on symbolic representations (e.g., phonemes or characters), synthetic data eliminates the need for such intermediates and avoids issues like label noise commonly found in real-data training. Furthermore, with controlled or predefined durations, speaker-independent features are inherently aligned across domains, removing the need for forced alignment algorithms used in previous work <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib31\" title=\"\">2020</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib44\" title=\"\">2021</a>)</cite>. Unlike methods that map audio to hidden text representations from multispeaker TTS models, our approach directly maps source audio to target audio, simplifying the learning process. This enables one-to-one frame alignment, allowing the model to focus more effectively on speaker transformation while preserving linguistic content. Finally, synthetic data enables the creation of diverse speaker pairs with uniform content, supporting the learning of generalizable speaker conversion mappings. Motivated by these advantages, this work is the first to propose using synthetic data as a training paradigm for voice conversion models.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "model",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon these advantages, we propose a synthetic data strategy to improve the disentanglement of speaker and content representations in voice conversion. Instead of relying on real-world utterances, we generate high-quality synthetic pairs with identical linguistic content but varying speaker identities. This provides ideal supervision for isolating speaker-independent content.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt VITS <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib18\" title=\"\">2021</a>)</cite> as the backbone of our TTS system because it combines variational inference, flows, and adversarial learning to generate high-quality speech with precise duration control and the ability to sample two audios with different speakers conditioned on a shared latent linguistic representation. This enables the creation of large-scale, controllable training data that improves disentanglement, reduces speaker leakage, and enhances voice conversion robustness.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This process generates pairs of utterances that share the same linguistic latent features, duration, and prosody, while differing only in speaker identity. Such pairs provide a clean and consistent training signal for voice conversion. Standard VITS, however, produces utterances independently, which often leads to mismatches in duration and rhythm.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After generating synthetic data consisting of source and target speech pairs for supervised training, these utterances are directly utilized as input-output pairs for the voice conversion model. As the backbone architecture, we adopt a VITS-base model. Following the design of FreeVC <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib22\" title=\"\">2023</a>)</cite>, our model structure retains its core components. However, we note that while the source and target speech pairs share the same underlying linguistic content, the target speech is conditioned on a different speaker identity, which primarily manifests in variations in pitch. To avoid mismatch between input and output during training, we incorporate the fundamental frequency (<math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math>) of the target speech as an additional conditioning feature when decoding the final audio. The general model pipeline is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Synthetic Data Strategy &#8227; 3 Methodology &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "model",
                    "freevc",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 2:</span> Fine-tune the model using real large multispeaker speech recordings corpus with a reconstruction-based objective, the input and output of the model is the same audio. During this phase, WavLM and content extractor is frozen to preserve speaker-independent representations learned in phase 1. This fine-tuning phase helps the model adapt to new speakers without the need for transcripts and improves the fidelity of linguistic content. Additionally, the model becomes more versatile and can easily adapt to different domains, such as various languages or accents.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the methodology proposed in <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib22\" title=\"\">2023</a>)</cite>, we formulate the objective function by combining losses from conditional variational autoencoders (CVAE) and generative adversarial networks (GAN) <cite class=\"ltx_cite ltx_citemacro_citep\">(Mao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib27\" title=\"\">2017</a>; Larsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib20\" title=\"\">2016</a>)</cite>. CVAE-related losses include the KL divergence loss <math alttext=\"L_{\\text{kl}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>kl</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{kl}}</annotation></semantics></math>, which measures the discrepancy between the prior and posterior distributions of the flow-based model, and a phase-dependent reconstruction/conversion loss, either <math alttext=\"L_{\\text{rec}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>rec</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{rec}}</annotation></semantics></math> in phase 2 or <math alttext=\"L_{\\text{cv}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m3\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>cv</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{cv}}</annotation></semantics></math> in phase 1, defined as the L1 distance between the predicted and target mel-spectrograms. GAN-related losses include the adversarial loss for the discriminator <math alttext=\"L_{\\text{adv}}(D)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext>adv</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>D</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L_{\\text{adv}}(D)</annotation></semantics></math>, the adversarial loss for the generator <math alttext=\"L_{\\text{adv}}(G)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext>adv</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L_{\\text{adv}}(G)</annotation></semantics></math>, and the feature matching loss <math alttext=\"L_{\\text{fm}}(G)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext>fm</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L_{\\text{fm}}(G)</annotation></semantics></math>. We further incorporate two distillation losses into the total objective. The final loss function is defined as:</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For phase 1 of training, we use synthetic speech generated by a publicly available pretrained VITS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jaywalnut310/vits\" title=\"\">https://github.com/jaywalnut310/vits</a></span></span></span>. Specifically, we adopt the model released in the official repository. The amount of synthetic data corresponds to the VCTK training set used in the original VITS implementation. For each sample, the target audio is synthesized using the ground-truth text and speaker ID, while the source audio is generated by sampling a different random speaker ID. In phase 2, we fine-tune the model on the LibriSpeech dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib30\" title=\"\">2015</a>)</cite>, using the train-clean-360 and train-clean-100 subsets, totaling approximately 460 hours of speech from 1,172 speakers. Evaluation is conducted on the test-clean subset under any-to-any voice conversion scenarios.</p>\n\n",
                "matched_terms": [
                    "anytoany",
                    "model",
                    "conversion",
                    "voice",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare our method with several recent state-of-the-art voice conversion models, including FreeVC <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib22\" title=\"\">2023</a>)</cite>, DDDM-VC <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib8\" title=\"\">2024</a>)</cite>, Diff-HierVC <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib7\" title=\"\">2023</a>)</cite>, FaCodec (NaturalSpeech 3) <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib17\" title=\"\">2024</a>)</cite>, and KNN-VC <cite class=\"ltx_cite ltx_citemacro_citep\">(Baas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib1\" title=\"\">2023</a>)</cite>. For all baselines, we use official publicly released pretrained models. For KNN-VC, we use an 8-minute real speech segment as the reference pool for nearest-neighbor retrieval.</p>\n\n",
                "matched_terms": [
                    "knnvc",
                    "facodec",
                    "voice",
                    "freevc",
                    "conversion",
                    "dddmvc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective Evaluation</span>: We evaluate system performance using four objective metrics: Character Error Rate (CER), Word Error Rate (WER), Speaker Encoder Cosine Similarity (SECS), and Objective Naturalness. CER and WER assess intelligibility between the source and converted speech, using the HuBERT model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-large-ls960-ft\" title=\"\">https://huggingface.co/facebook/hubert-large-ls960-ft</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib11\" title=\"\">2021</a>)</cite>. SECS measures speaker similarity using the cosine similarity between embeddings extracted by Resemblyzer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/resemble-ai/Resemblyzer\" title=\"\">https://github.com/resemble-ai/Resemblyzer</a></span></span></span>. Naturalness is assessed using NISQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Mittag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib28\" title=\"\">2021</a>)</cite>, which estimates perceptual speech quality without reference audio. We compute these metrics on 1,000 randomly sampled audio pairs from LibriSpeech test-clean.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "evaluation",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Evaluation</span>: For human evaluation, we use Mean Opinion Score (MOS) and Speaker Similarity Mean Opinion Score (SMOS). MOS rates naturalness, while SMOS rates speaker similarity, both on a 1-5 scale. We randomly select 30 audio pairs from the objective set, each evaluated by three different annotators, resulting in a total of 540 labeled audio samples. A total of 12 volunteer listeners participate in the evaluation. Final scores are calculated by averaging the ratings across annotators for each pair to ensure reliability. In voice conversion systems, both MOS and SMOS help evaluate model quality. To provide an overall comparison, we introduce a new metric called balance-MOS (B-MOS), defined as the average of these two scores.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "model",
                    "voice",
                    "conversion",
                    "objective",
                    "evaluation",
                    "subjective",
                    "smos",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct an ablation study by modifying or removing key modules to evaluate their individual contributions, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Evaluation Metrics &#8227; 4 Experiment Setup &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. We observe that removing the use of synthetic data, the F0 encoder, or phase 2 fine-tuning each leads to a noticeable drop in intelligibility, highlighting the importance of all three components. Eliminating phase 2 fine-tuning also causes a significant reduction in speaker similarity, likely due to the limited speaker diversity in the phase 1 dataset. However, since the phase 1 data is of higher quality, the phase 2 adaptation may slightly reduce speech quality. These findings demonstrate that using a synthetic dataset in phase 1 can achieve speech quality comparable to real data (competitive NISQA with FreeVC), while our phase 2 adaptation enables the model to generalize effectively to new datasets and unseen speakers without transcript labels.</p>\n\n",
                "matched_terms": [
                    "model",
                    "freevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to intelligibility and naturalness, we also assess pitch preservation by reporting the F0 Pearson Correlation Coefficient (F0-PCC) <cite class=\"ltx_cite ltx_citemacro_citep\">(Benesty et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib2\" title=\"\">2009</a>)</cite>, which is computed between the F0 contours of the source and converted audio. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Zero-Shot Voice Conversion &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our system achieves the highest F0-PCC, outperforming recent models that explicitly condition on F0 such as Diff-HierVC, as well as the same backbone model FreeVC without F0 conditioning. These results highlight the effectiveness of the proposed F0 encoder in maintaining accurate pitch contours and demonstrate strong pitch consistency across converted speech.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "freevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also quantitatively evaluate how effectively the prior encoder removes speaker information by comparing our model to FreeVC, which shares the same backbone architecture. Our goal is to demonstrate that training with synthetic data significantly improves the removal of speaker identity from source audio. To assess this, we use three clustering evaluation metrics: Adjusted Rand Index (ARI), Normalized Mutual Information (NMI) and Silhouette Score. The ARI measures the similarity between predicted clusters and true speaker labels, adjusted for chance. A lower ARI indicates that the clusters do not correspond well to speaker identities, suggesting better speaker information removal. NMI measures the amount of shared information between the predicted and true clusters; lower values indicate weaker correlation and thus stronger speaker anonymization. The Silhouette Score reflects how well each embedding fits within its cluster compared to the others. Lower scores imply that the model&#8217;s embeddings are not tightly grouped by speaker, further indicating that speaker identity has been suppressed. The quantitative results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Ablation Study &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Our model, trained with synthetic data, consistently achieves lower scores across all metrics, demonstrating its improved ability to remove speaker-specific information compared to FreeVC.</p>\n\n",
                "matched_terms": [
                    "model",
                    "freevc",
                    "evaluation",
                    "results",
                    "indicates",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For intuitive visualization, we use t-SNE to plot the speaker-independent features in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F2\" title=\"Figure 2 &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Our model&#8217;s embeddings are more evenly dispersed across speakers (different colors), indicating greater speaker independence. In contrast, FreeVC shows noticeable clustering, such as for speakers <math alttext=\"6563\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p4.m1\" intent=\":literal\"><semantics><mn>6563</mn><annotation encoding=\"application/x-tex\">6563</annotation></semantics></math> and <math alttext=\"5192\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p4.m2\" intent=\":literal\"><semantics><mn>5192</mn><annotation encoding=\"application/x-tex\">5192</annotation></semantics></math>, which indicates that its features preserve more speaker-specific information.</p>\n\n",
                "matched_terms": [
                    "freevc",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the adaptability of our approach to new languages by applying the model to speech data from previously unseen linguistic domains. In this experiment, we fine-tune the model in phase 2 using speech from three languages: Chinese (ZH), Italian (IT), and Vietnamese (VI). We use AISHELL-3 for Chinese <cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib38\" title=\"\">2021</a>)</cite>, the same dataset as <cite class=\"ltx_cite ltx_citemacro_citep\">(Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib42\" title=\"\">2025</a>)</cite> for Vietnamese, and the Multilingual LibriSpeech (MLS) training subset for Italian <cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib35\" title=\"\">2020</a>)</cite>. We reserve a portion of each training set as a test set and pair 400 utterances for evaluation. To measure content intelligibility, we use language-specific ASR tools: FunASR <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib9\" title=\"\">2023</a>)</cite> with paraformer-zh <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib10\" title=\"\">2022</a>)</cite> for Chinese, Chunkformer-large-vi <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib21\" title=\"\">2025</a>)</cite> for Vietnamese, and Whisper-large <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib36\" title=\"\">2023</a>)</cite> for Italian. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Adaptation to New Languages &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that Phase 2 fine-tuning boosts performance and enables language adaptation using only audio, without requiring labeled data. It also proves that our speaker-independent features still retain some accent or language information.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The heatmap displays a clear diagonal of high similarity values, indicating strong frame-level alignment between the source and target audio. Furthermore, the top-1 cosine similarity alignment path, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F5.sf2\" title=\"In Figure 5 &#8227; 5.4 Semantic Alignment of Synthetic Audio Pairs &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">5(b)</span></a>, lies precisely along the diagonal, confirming perfect alignment. These results demonstrate that the synthetic data input-output pairs are ideal training examples for voice conversion, enabling the model to learn effective one-to-one mapping.</p>\n\n",
                "matched_terms": [
                    "along",
                    "model",
                    "conversion",
                    "voice",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a robust voice conversion framework based on synthetic data and a two-phase training strategy. Our method enhances speaker similarity, speech quality, and content consistency, particularly in zero-shot scenarios with unseen target speakers. Experiments and ablation studies confirm the effectiveness of our approach and demonstrate its ability to prevent speaker information leakage from the source audio. Additionally, we showed that the model generalizes well to unseen languages without requiring labeled data, making it highly suitable for low-resource settings.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "model",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although our model improves speaker similarity and content intelligibility, it still depends on access to a high-quality, well-labeled speech corpus to train the TTS system. Furthermore, the effectiveness of synthetic data generation and its influence on the performance of voice conversion across different TTS systems remain insufficiently explored. Therefore, in future work, we plan to investigate alternative TTS models to gain a deeper and more comprehensive understanding of their impact on overall system performance.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "model",
                    "performance",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Voice conversion technology raises ethical concerns because it can be misused to generate deepfake audio and illegally spoof someone&#8217;s identity without consent. Such applications risk enabling fraud, misinformation, and reputational harm, while also undermining public trust in authentic communication. Since voices are unique biometric identifiers, misuse poses serious threats to privacy and security. Ethical use requires informed consent, transparency, and robust safeguards, including reliable spoofing verification methods, to prevent malicious exploitation.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate how well emotional information is preserved, we compare our proposed model with the baseline FreeVC, which shares the same backbone. For this experiment, we use the ESD dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Zhou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib45\" title=\"\">2021</a>)</cite>, which contains emotional speech. We randomly sample 10 audio clips from each of the 10 speakers across 4 emotions, resulting in a total of 400 source audio samples. For the target speakers, we randomly select a speaker from the LibriSpeech test set. After performing voice conversion, we extract emotion embeddings from the converted audio using the emotion2vec_plus_large model <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib26\" title=\"\">2024</a>)</cite> and visualize them using t-SNE. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#A1.F6\" title=\"Figure 6 &#8227; A.1 Preservation of Emotional Information &#8227; Appendix A Appendix &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, our proposed model produces more distinct emotion clusters, such as sad and surprise, while FreeVC exhibits little to no clustering, indicating that our model better preserves emotional characteristics.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "model",
                    "freevc",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess how &#8220;native-like&#8221; the converted speech is, we report CER/WER between the converted audio and ground-truth transcriptions of the original voice. The datasets used for Chinese and Italian are the same as those described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.SS3\" title=\"5.3 Adaptation to New Languages &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>. However, for Vietnamese, since the multi-speaker-vi dataset lacks text <cite class=\"ltx_cite ltx_citemacro_cite\">Tu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib42\" title=\"\">2025</a>)</cite>, we constructed a clean test set using utterances from the VIVOS dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Luong and Vu (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib25\" title=\"\">2016</a>)</cite> that do not overlap with the training data. Lower scores indicate higher intelligibility and a closer resemblance to native speech. Results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#A1.T4\" title=\"Table 4 &#8227; A.1 Preservation of Emotional Information &#8227; Appendix A Appendix &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "results",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results show clear gains in intelligibility after language adaptation, though still slightly below ground-truth levels. This indicates that the converted speech becomes substantially more native-like, while leaving room for further improvements to fully match natural speech.</p>\n\n",
                "matched_terms": [
                    "results",
                    "indicates"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Ablation study results.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SECS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">NISQA<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">O_O-VC (Ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">86.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">1.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">w/o F0 Encoder</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">87.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">2.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">w/o Finetuning</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">70.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.59</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">FreeVC</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">75.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">2.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">4.60</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ours",
            "wer↓downarrow",
            "cer↓downarrow",
            "finetuning",
            "nisqa↑uparrow",
            "study",
            "model",
            "freevc",
            "ablation",
            "encoder",
            "results",
            "secs↑uparrow",
            "oovc"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conduct an ablation study by modifying or removing key modules to evaluate their individual contributions, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Evaluation Metrics &#8227; 4 Experiment Setup &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. We observe that removing the use of synthetic data, the F0 encoder, or phase 2 fine-tuning each leads to a noticeable drop in intelligibility, highlighting the importance of all three components. Eliminating phase 2 fine-tuning also causes a significant reduction in speaker similarity, likely due to the limited speaker diversity in the phase 1 dataset. However, since the phase 1 data is of higher quality, the phase 2 adaptation may slightly reduce speech quality. These findings demonstrate that using a synthetic dataset in phase 1 can achieve speech quality comparable to real data (competitive NISQA with FreeVC), while our phase 2 adaptation enables the model to generalize effectively to new datasets and unseen speakers without transcript labels.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">After generating synthetic data consisting of source and target speech pairs for supervised training, these utterances are directly utilized as input-output pairs for the voice conversion model. As the backbone architecture, we adopt a VITS-base model. Following the design of FreeVC <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib22\" title=\"\">2023</a>)</cite>, our model structure retains its core components. However, we note that while the source and target speech pairs share the same underlying linguistic content, the target speech is conditioned on a different speaker identity, which primarily manifests in variations in pitch. To avoid mismatch between input and output during training, we incorporate the fundamental frequency (<math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math>) of the target speech as an additional conditioning feature when decoding the final audio. The general model pipeline is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Synthetic Data Strategy &#8227; 3 Methodology &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "freevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 2:</span> Fine-tune the model using real large multispeaker speech recordings corpus with a reconstruction-based objective, the input and output of the model is the same audio. During this phase, WavLM and content extractor is frozen to preserve speaker-independent representations learned in phase 1. This fine-tuning phase helps the model adapt to new speakers without the need for transcripts and improves the fidelity of linguistic content. Additionally, the model becomes more versatile and can easily adapt to different domains, such as various languages or accents.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the implementation and hyperparameter setup of FreeVC <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib22\" title=\"\">2023</a>)</cite>. Training occurs in two phases: up to 450k steps on synthetic data, followed by 150k steps of fine-tuning on real speech. All experiments are conducted on four NVIDIA Tesla A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "freevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our model in a zero-shot setting, where the target speaker is unseen during training. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S3.T1\" title=\"Table 1 &#8227; 3.2.4 Objective Function &#8227; 3.2 Model Overview &#8227; 3 Methodology &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> demonstrate that our model achieves the best performance in terms of content consistency, with the lowest WER and CER. Furthermore, our model achieves the second highest MOS, only slightly behind FreeVC. This can be attributed to the fact that FreeVC is trained on a high-quality speech dataset, whereas our model is fine-tuned and adapted on LibriSpeech, which is of comparatively lower quality, leading to decreased performance. Despite FreeVC&#8217;s strong MOS, it performs notably worse in terms of speaker similarity and content intelligibility compared to our model. Although DDDM-VC achieves the highest Similarity Mean Opinion Score (SMOS), its speech quality is comparatively poor. Overall, our model achieves the best intelligibility while maintaining a strong balance between naturalness (MOS) and speaker similarity (SMOS), outperforming recent systems in a zero-shot scenario.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "freevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to intelligibility and naturalness, we also assess pitch preservation by reporting the F0 Pearson Correlation Coefficient (F0-PCC) <cite class=\"ltx_cite ltx_citemacro_citep\">(Benesty et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib2\" title=\"\">2009</a>)</cite>, which is computed between the F0 contours of the source and converted audio. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Zero-Shot Voice Conversion &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our system achieves the highest F0-PCC, outperforming recent models that explicitly condition on F0 such as Diff-HierVC, as well as the same backbone model FreeVC without F0 conditioning. These results highlight the effectiveness of the proposed F0 encoder in maintaining accurate pitch contours and demonstrate strong pitch consistency across converted speech.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "encoder",
                    "freevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also quantitatively evaluate how effectively the prior encoder removes speaker information by comparing our model to FreeVC, which shares the same backbone architecture. Our goal is to demonstrate that training with synthetic data significantly improves the removal of speaker identity from source audio. To assess this, we use three clustering evaluation metrics: Adjusted Rand Index (ARI), Normalized Mutual Information (NMI) and Silhouette Score. The ARI measures the similarity between predicted clusters and true speaker labels, adjusted for chance. A lower ARI indicates that the clusters do not correspond well to speaker identities, suggesting better speaker information removal. NMI measures the amount of shared information between the predicted and true clusters; lower values indicate weaker correlation and thus stronger speaker anonymization. The Silhouette Score reflects how well each embedding fits within its cluster compared to the others. Lower scores imply that the model&#8217;s embeddings are not tightly grouped by speaker, further indicating that speaker identity has been suppressed. The quantitative results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Ablation Study &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Our model, trained with synthetic data, consistently achieves lower scores across all metrics, demonstrating its improved ability to remove speaker-specific information compared to FreeVC.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "encoder",
                    "freevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the adaptability of our approach to new languages by applying the model to speech data from previously unseen linguistic domains. In this experiment, we fine-tune the model in phase 2 using speech from three languages: Chinese (ZH), Italian (IT), and Vietnamese (VI). We use AISHELL-3 for Chinese <cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib38\" title=\"\">2021</a>)</cite>, the same dataset as <cite class=\"ltx_cite ltx_citemacro_citep\">(Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib42\" title=\"\">2025</a>)</cite> for Vietnamese, and the Multilingual LibriSpeech (MLS) training subset for Italian <cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib35\" title=\"\">2020</a>)</cite>. We reserve a portion of each training set as a test set and pair 400 utterances for evaluation. To measure content intelligibility, we use language-specific ASR tools: FunASR <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib9\" title=\"\">2023</a>)</cite> with paraformer-zh <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib10\" title=\"\">2022</a>)</cite> for Chinese, Chunkformer-large-vi <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib21\" title=\"\">2025</a>)</cite> for Vietnamese, and Whisper-large <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib36\" title=\"\">2023</a>)</cite> for Italian. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Adaptation to New Languages &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that Phase 2 fine-tuning boosts performance and enables language adaptation using only audio, without requiring labeled data. It also proves that our speaker-independent features still retain some accent or language information.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The heatmap displays a clear diagonal of high similarity values, indicating strong frame-level alignment between the source and target audio. Furthermore, the top-1 cosine similarity alignment path, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F5.sf2\" title=\"In Figure 5 &#8227; 5.4 Semantic Alignment of Synthetic Audio Pairs &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">5(b)</span></a>, lies precisely along the diagonal, confirming perfect alignment. These results demonstrate that the synthetic data input-output pairs are ideal training examples for voice conversion, enabling the model to learn effective one-to-one mapping.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a robust voice conversion framework based on synthetic data and a two-phase training strategy. Our method enhances speaker similarity, speech quality, and content consistency, particularly in zero-shot scenarios with unseen target speakers. Experiments and ablation studies confirm the effectiveness of our approach and demonstrate its ability to prevent speaker information leakage from the source audio. Additionally, we showed that the model generalizes well to unseen languages without requiring labeled data, making it highly suitable for low-resource settings.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ablation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate how well emotional information is preserved, we compare our proposed model with the baseline FreeVC, which shares the same backbone. For this experiment, we use the ESD dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Zhou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib45\" title=\"\">2021</a>)</cite>, which contains emotional speech. We randomly sample 10 audio clips from each of the 10 speakers across 4 emotions, resulting in a total of 400 source audio samples. For the target speakers, we randomly select a speaker from the LibriSpeech test set. After performing voice conversion, we extract emotion embeddings from the converted audio using the emotion2vec_plus_large model <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib26\" title=\"\">2024</a>)</cite> and visualize them using t-SNE. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#A1.F6\" title=\"Figure 6 &#8227; A.1 Preservation of Emotional Information &#8227; Appendix A Appendix &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, our proposed model produces more distinct emotion clusters, such as sad and surprise, while FreeVC exhibits little to no clustering, indicating that our model better preserves emotional characteristics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "freevc"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Evaluation of speaker information removal.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ARI<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">NMI<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Silhouette<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">O_O-VC (Ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">0.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">0.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0000FF;\">0.15</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">FreeVC</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.17</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ours",
            "removal",
            "silhouette↓downarrow",
            "model",
            "freevc",
            "speaker",
            "evaluation",
            "nmi↓downarrow",
            "ari↓downarrow",
            "oovc",
            "information"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We also quantitatively evaluate how effectively the prior encoder removes speaker information by comparing our model to FreeVC, which shares the same backbone architecture. Our goal is to demonstrate that training with synthetic data significantly improves the removal of speaker identity from source audio. To assess this, we use three clustering evaluation metrics: Adjusted Rand Index (ARI), Normalized Mutual Information (NMI) and Silhouette Score. The ARI measures the similarity between predicted clusters and true speaker labels, adjusted for chance. A lower ARI indicates that the clusters do not correspond well to speaker identities, suggesting better speaker information removal. NMI measures the amount of shared information between the predicted and true clusters; lower values indicate weaker correlation and thus stronger speaker anonymization. The Silhouette Score reflects how well each embedding fits within its cluster compared to the others. Lower scores imply that the model&#8217;s embeddings are not tightly grouped by speaker, further indicating that speaker identity has been suppressed. The quantitative results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Ablation Study &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Our model, trained with synthetic data, consistently achieves lower scores across all metrics, demonstrating its improved ability to remove speaker-specific information compared to FreeVC.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Traditional voice conversion (VC) methods typically attempt to separate speaker identity and linguistic information into distinct representations, which are then combined to reconstruct the audio. However, effectively disentangling these factors remains challenging, often leading to information loss during training. In this paper, we propose a new approach that leverages synthetic speech data generated by a high-quality, pretrained multispeaker text-to-speech (TTS) model. Specifically, synthetic data pairs that share the same linguistic content but differ in speaker identity are used as input-output pairs to train the voice conversion model. This enables the model to learn a direct mapping between source and target voices, effectively capturing speaker-specific characteristics while preserving linguistic content. Additionally, we introduce a flexible training strategy for any-to-any voice conversion that generalizes well to unseen speakers and new languages, enhancing adaptability and performance in zero-shot scenarios. Our experiments show that our proposed method achieves a 16.35% relative reduction in word error rate and a 5.91% improvement in speaker cosine similarity, outperforming several state-of-the-art methods. Voice conversion samples can be accessed at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://oovc-emnlp-2025.github.io/\" title=\"\">https://oovc-emnlp-2025.github.io/</a></p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several methods have been proposed for VC, with supervised training being a common approach. Content encoders are trained with text labels to extract linguistic features, and speaker encoders use speaker labels to capture identity-specific traits <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib12\" title=\"\">2023</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib24\" title=\"\">2021</a>)</cite>. Alternatively, phonetic posteriorgrams (PPGs) can be used directly as content representations <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib40\" title=\"\">2016</a>; Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib41\" title=\"\">2018</a>)</cite>. However, both approaches often struggle to capture speaker-independent prosody and accent information. Moreover, training content encoders as ASR models can introduce alignment errors or recognition errors, which can negatively impact conversion quality <cite class=\"ltx_cite ltx_citemacro_citep\">(Hussain et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib16\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, some methods avoid using text labels by leveraging self-supervised learning (SSL) to extract high-level phonetic representations <cite class=\"ltx_cite ltx_citemacro_citep\">(Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib32\" title=\"\">2021</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib23\" title=\"\">2021</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib14\" title=\"\">2022b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib15\" title=\"\">c</a>)</cite>. These approaches aim to remove speaker identity from source audio while preserving speaker-independent features such as accent and content. To achieve this separation, techniques such as vector quantization <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu and Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib43\" title=\"\">2020</a>)</cite>, instance normalization <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib6\" title=\"\">2021b</a>)</cite>, heuristic transformation <cite class=\"ltx_cite ltx_citemacro_citep\">(Neekhara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib29\" title=\"\">2024</a>)</cite>, bottleneck, and data augmentation <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib22\" title=\"\">2023</a>)</cite> are commonly applied. However, despite these efforts, such methods still struggle to completely eliminate speaker information from the source speech. This often leads to speaker leakage, where the converted audio retains unintended characteristics of the source speaker, resulting in mismatches between the synthesized voice and the intended target speaker <cite class=\"ltx_cite ltx_citemacro_citep\">(Baas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib1\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A common strategy is to leverage pretrained automatic speech recognition (ASR) models to extract phonetic features such as PPGs, which provide a speaker-independent representation of the input speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib40\" title=\"\">2016</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib24\" title=\"\">2021</a>; Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib41\" title=\"\">2018</a>)</cite>. Specifically, speaker information is obtained using a pretrained speaker verification (SV) model. The speaker embeddings are combined with the content features during decoding, allowing the system to generate speech in the target speaker&#8217;s voice. Some approaches leverage hidden text representations from pretrained multispeaker text-to-speech (TTS) models, using them either as semantic features or as target representations for learning a mapping from audio to text <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib31\" title=\"\">2020</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib44\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite their advantages, text-based methods suffer from several limitations. PPGs and other textual representations often fail to capture fine-grained attributes such as accent, prosody, and speaker-independent speaking style. As a result, these systems often produce speech that lacks expressiveness and sounds overly neutral <cite class=\"ltx_cite ltx_citemacro_citep\">(Hussain et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib16\" title=\"\">2023</a>)</cite>. Although ASR-based disentanglement methods have shown progress in separating speaker and content information, their reliance on textual supervision and limited prosodic modeling remain significant challenges for achieving natural and expressive voice conversion across diverse speakers and languages.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of text-based methods, text-free approaches have emerged, leveraging self-supervised learning models to extract content representations without requiring transcriptions <cite class=\"ltx_cite ltx_citemacro_citep\">(Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib32\" title=\"\">2021</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib23\" title=\"\">2021</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib14\" title=\"\">2022b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib15\" title=\"\">c</a>)</cite>. Although self-supervised learning (SSL) features capture high-level information related to linguistic content, they often retain residual speaker characteristics. To address this, methods such as bottleneck layers <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib22\" title=\"\">2023</a>)</cite>, vector quantization <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu and Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib43\" title=\"\">2020</a>)</cite>, and instance normalization <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib6\" title=\"\">2021b</a>)</cite> have been proposed to compress SSL features and extract speaker-independent content representations. However, effective disentanglement heavily depends on the choice of bottleneck configuration: if the bottleneck dimension is too large, speaker information may be retained; if too small, important content information can be lost. A similar trade-off exists in vector quantization: large codebooks may retain speaker traits, while overly small codebooks may lead to excessive loss of content information. Moreover, these compression techniques often degrade the quality of the generated audio, and full disentanglement of speaker and content information remains an open challenge.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In text-free VC systems, content and speaker identity are often not fully disentangled. As a result, speaker information can leak into the content representation, which undermines the system&#8217;s ability to perform clean speaker conversion. This leakage reduces the system&#8217;s generalization to unseen voices or speaking styles and often results in converted speech that retains characteristics of the source speaker.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As an alternative solution, synthetic data offers several advantages for voice conversion. When both source and target audio are generated from the same linguistic content, it provides a clean and direct supervisory signal. This shared content allows precise frame-level alignment between source and target audio, enabling more stable and fine-grained learning of the conversion function. Unlike traditional approaches that rely on symbolic representations (e.g., phonemes or characters), synthetic data eliminates the need for such intermediates and avoids issues like label noise commonly found in real-data training. Furthermore, with controlled or predefined durations, speaker-independent features are inherently aligned across domains, removing the need for forced alignment algorithms used in previous work <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib31\" title=\"\">2020</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib44\" title=\"\">2021</a>)</cite>. Unlike methods that map audio to hidden text representations from multispeaker TTS models, our approach directly maps source audio to target audio, simplifying the learning process. This enables one-to-one frame alignment, allowing the model to focus more effectively on speaker transformation while preserving linguistic content. Finally, synthetic data enables the creation of diverse speaker pairs with uniform content, supporting the learning of generalizable speaker conversion mappings. Motivated by these advantages, this work is the first to propose using synthetic data as a training paradigm for voice conversion models.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a multi-speaker TTS system that produces natural, intelligible speech across speakers from a shared linguistic latent space. Our selection criteria for the TTS system are: (1) the generated speech must be of high fidelity, exhibiting natural prosody and clarity, and (2) the model must synthesize source and target utterances from the same linguistic latent space, ensuring consistent phonetic and prosodic alignment across speakers. These criteria ensure that the synthetic speech pairs are perfectly aligned in linguistic structure while differing only in speaker identity.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After generating synthetic data consisting of source and target speech pairs for supervised training, these utterances are directly utilized as input-output pairs for the voice conversion model. As the backbone architecture, we adopt a VITS-base model. Following the design of FreeVC <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib22\" title=\"\">2023</a>)</cite>, our model structure retains its core components. However, we note that while the source and target speech pairs share the same underlying linguistic content, the target speech is conditioned on a different speaker identity, which primarily manifests in variations in pitch. To avoid mismatch between input and output during training, we incorporate the fundamental frequency (<math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math>) of the target speech as an additional conditioning feature when decoding the final audio. The general model pipeline is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Synthetic Data Strategy &#8227; 3 Methodology &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model",
                    "freevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 1:</span> Train the model with synthetic data, where the WavLM and content extractor components are responsible for learning independent speaker representations. The WavLM is frozen during phase 1.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During inference, the source audio is processed by WavLM and the content extractor to obtain the content distribution <math alttext=\"\\mathcal{N}(\\mu_{\\theta},\\sigma^{2}_{\\theta})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#956;</mi><mi>&#952;</mi></msub><mo>,</mo><msubsup><mi>&#963;</mi><mi>&#952;</mi><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(\\mu_{\\theta},\\sigma^{2}_{\\theta})</annotation></semantics></math>. A latent sample <math alttext=\"z_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">z_{p}</annotation></semantics></math> is drawn and combined with the speaker embedding from the reference audio via the inverse flow model, producing a feature that captures both content and speaker information.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For phase 1 of training, we use synthetic speech generated by a publicly available pretrained VITS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jaywalnut310/vits\" title=\"\">https://github.com/jaywalnut310/vits</a></span></span></span>. Specifically, we adopt the model released in the official repository. The amount of synthetic data corresponds to the VCTK training set used in the original VITS implementation. For each sample, the target audio is synthesized using the ground-truth text and speaker ID, while the source audio is generated by sampling a different random speaker ID. In phase 2, we fine-tune the model on the LibriSpeech dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib30\" title=\"\">2015</a>)</cite>, using the train-clean-360 and train-clean-100 subsets, totaling approximately 460 hours of speech from 1,172 speakers. Evaluation is conducted on the test-clean subset under any-to-any voice conversion scenarios.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective Evaluation</span>: We evaluate system performance using four objective metrics: Character Error Rate (CER), Word Error Rate (WER), Speaker Encoder Cosine Similarity (SECS), and Objective Naturalness. CER and WER assess intelligibility between the source and converted speech, using the HuBERT model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-large-ls960-ft\" title=\"\">https://huggingface.co/facebook/hubert-large-ls960-ft</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib11\" title=\"\">2021</a>)</cite>. SECS measures speaker similarity using the cosine similarity between embeddings extracted by Resemblyzer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/resemble-ai/Resemblyzer\" title=\"\">https://github.com/resemble-ai/Resemblyzer</a></span></span></span>. Naturalness is assessed using NISQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Mittag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib28\" title=\"\">2021</a>)</cite>, which estimates perceptual speech quality without reference audio. We compute these metrics on 1,000 randomly sampled audio pairs from LibriSpeech test-clean.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Evaluation</span>: For human evaluation, we use Mean Opinion Score (MOS) and Speaker Similarity Mean Opinion Score (SMOS). MOS rates naturalness, while SMOS rates speaker similarity, both on a 1-5 scale. We randomly select 30 audio pairs from the objective set, each evaluated by three different annotators, resulting in a total of 540 labeled audio samples. A total of 12 volunteer listeners participate in the evaluation. Final scores are calculated by averaging the ratings across annotators for each pair to ensure reliability. In voice conversion systems, both MOS and SMOS help evaluate model quality. To provide an overall comparison, we introduce a new metric called balance-MOS (B-MOS), defined as the average of these two scores.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our model in a zero-shot setting, where the target speaker is unseen during training. The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S3.T1\" title=\"Table 1 &#8227; 3.2.4 Objective Function &#8227; 3.2 Model Overview &#8227; 3 Methodology &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> demonstrate that our model achieves the best performance in terms of content consistency, with the lowest WER and CER. Furthermore, our model achieves the second highest MOS, only slightly behind FreeVC. This can be attributed to the fact that FreeVC is trained on a high-quality speech dataset, whereas our model is fine-tuned and adapted on LibriSpeech, which is of comparatively lower quality, leading to decreased performance. Despite FreeVC&#8217;s strong MOS, it performs notably worse in terms of speaker similarity and content intelligibility compared to our model. Although DDDM-VC achieves the highest Similarity Mean Opinion Score (SMOS), its speech quality is comparatively poor. Overall, our model achieves the best intelligibility while maintaining a strong balance between naturalness (MOS) and speaker similarity (SMOS), outperforming recent systems in a zero-shot scenario.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model",
                    "freevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct an ablation study by modifying or removing key modules to evaluate their individual contributions, summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Evaluation Metrics &#8227; 4 Experiment Setup &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. We observe that removing the use of synthetic data, the F0 encoder, or phase 2 fine-tuning each leads to a noticeable drop in intelligibility, highlighting the importance of all three components. Eliminating phase 2 fine-tuning also causes a significant reduction in speaker similarity, likely due to the limited speaker diversity in the phase 1 dataset. However, since the phase 1 data is of higher quality, the phase 2 adaptation may slightly reduce speech quality. These findings demonstrate that using a synthetic dataset in phase 1 can achieve speech quality comparable to real data (competitive NISQA with FreeVC), while our phase 2 adaptation enables the model to generalize effectively to new datasets and unseen speakers without transcript labels.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model",
                    "freevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to intelligibility and naturalness, we also assess pitch preservation by reporting the F0 Pearson Correlation Coefficient (F0-PCC) <cite class=\"ltx_cite ltx_citemacro_citep\">(Benesty et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib2\" title=\"\">2009</a>)</cite>, which is computed between the F0 contours of the source and converted audio. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Zero-Shot Voice Conversion &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our system achieves the highest F0-PCC, outperforming recent models that explicitly condition on F0 such as Diff-HierVC, as well as the same backbone model FreeVC without F0 conditioning. These results highlight the effectiveness of the proposed F0 encoder in maintaining accurate pitch contours and demonstrate strong pitch consistency across converted speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "freevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For intuitive visualization, we use t-SNE to plot the speaker-independent features in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F2\" title=\"Figure 2 &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Our model&#8217;s embeddings are more evenly dispersed across speakers (different colors), indicating greater speaker independence. In contrast, FreeVC shows noticeable clustering, such as for speakers <math alttext=\"6563\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p4.m1\" intent=\":literal\"><semantics><mn>6563</mn><annotation encoding=\"application/x-tex\">6563</annotation></semantics></math> and <math alttext=\"5192\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p4.m2\" intent=\":literal\"><semantics><mn>5192</mn><annotation encoding=\"application/x-tex\">5192</annotation></semantics></math>, which indicates that its features preserve more speaker-specific information.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "freevc",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the adaptability of our approach to new languages by applying the model to speech data from previously unseen linguistic domains. In this experiment, we fine-tune the model in phase 2 using speech from three languages: Chinese (ZH), Italian (IT), and Vietnamese (VI). We use AISHELL-3 for Chinese <cite class=\"ltx_cite ltx_citemacro_citep\">(Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib38\" title=\"\">2021</a>)</cite>, the same dataset as <cite class=\"ltx_cite ltx_citemacro_citep\">(Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib42\" title=\"\">2025</a>)</cite> for Vietnamese, and the Multilingual LibriSpeech (MLS) training subset for Italian <cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib35\" title=\"\">2020</a>)</cite>. We reserve a portion of each training set as a test set and pair 400 utterances for evaluation. To measure content intelligibility, we use language-specific ASR tools: FunASR <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib9\" title=\"\">2023</a>)</cite> with paraformer-zh <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib10\" title=\"\">2022</a>)</cite> for Chinese, Chunkformer-large-vi <cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib21\" title=\"\">2025</a>)</cite> for Vietnamese, and Whisper-large <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib36\" title=\"\">2023</a>)</cite> for Italian. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Adaptation to New Languages &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that Phase 2 fine-tuning boosts performance and enables language adaptation using only audio, without requiring labeled data. It also proves that our speaker-independent features still retain some accent or language information.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a robust voice conversion framework based on synthetic data and a two-phase training strategy. Our method enhances speaker similarity, speech quality, and content consistency, particularly in zero-shot scenarios with unseen target speakers. Experiments and ablation studies confirm the effectiveness of our approach and demonstrate its ability to prevent speaker information leakage from the source audio. Additionally, we showed that the model generalizes well to unseen languages without requiring labeled data, making it highly suitable for low-resource settings.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although our model improves speaker similarity and content intelligibility, it still depends on access to a high-quality, well-labeled speech corpus to train the TTS system. Furthermore, the effectiveness of synthetic data generation and its influence on the performance of voice conversion across different TTS systems remain insufficiently explored. Therefore, in future work, we plan to investigate alternative TTS models to gain a deeper and more comprehensive understanding of their impact on overall system performance.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate how well emotional information is preserved, we compare our proposed model with the baseline FreeVC, which shares the same backbone. For this experiment, we use the ESD dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Zhou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib45\" title=\"\">2021</a>)</cite>, which contains emotional speech. We randomly sample 10 audio clips from each of the 10 speakers across 4 emotions, resulting in a total of 400 source audio samples. For the target speakers, we randomly select a speaker from the LibriSpeech test set. After performing voice conversion, we extract emotion embeddings from the converted audio using the emotion2vec_plus_large model <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib26\" title=\"\">2024</a>)</cite> and visualize them using t-SNE. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#A1.F6\" title=\"Figure 6 &#8227; A.1 Preservation of Emotional Information &#8227; Appendix A Appendix &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, our proposed model produces more distinct emotion clusters, such as sad and surprise, while FreeVC exhibits little to no clustering, indicating that our model better preserves emotional characteristics.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model",
                    "freevc",
                    "information"
                ]
            }
        ]
    },
    "A1.T4": {
        "caption": "Table 4: CER/WER between converted audio and ground-truth transcriptions.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Lang</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Stage</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CER/WER</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">ZH</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16.59</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">ZH</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">7.76</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">ZH</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">GT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.84</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">IT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">28.93</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">IT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">22.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">IT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">GT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">11.96</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">VI</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">11.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">VI</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8.83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">VI</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">GT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">2.53</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "transcriptions",
            "audio",
            "groundtruth",
            "stage",
            "converted",
            "lang",
            "between",
            "cerwer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To assess how &#8220;native-like&#8221; the converted speech is, we report CER/WER between the converted audio and ground-truth transcriptions of the original voice. The datasets used for Chinese and Italian are the same as those described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.SS3\" title=\"5.3 Adaptation to New Languages &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>. However, for Vietnamese, since the multi-speaker-vi dataset lacks text <cite class=\"ltx_cite ltx_citemacro_cite\">Tu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib42\" title=\"\">2025</a>)</cite>, we constructed a clean test set using utterances from the VIVOS dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Luong and Vu (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib25\" title=\"\">2016</a>)</cite> that do not overlap with the training data. Lower scores indicate higher intelligibility and a closer resemblance to native speech. Results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#A1.T4\" title=\"Table 4 &#8227; A.1 Preservation of Emotional Information &#8227; Appendix A Appendix &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Traditional voice conversion (VC) methods typically attempt to separate speaker identity and linguistic information into distinct representations, which are then combined to reconstruct the audio. However, effectively disentangling these factors remains challenging, often leading to information loss during training. In this paper, we propose a new approach that leverages synthetic speech data generated by a high-quality, pretrained multispeaker text-to-speech (TTS) model. Specifically, synthetic data pairs that share the same linguistic content but differ in speaker identity are used as input-output pairs to train the voice conversion model. This enables the model to learn a direct mapping between source and target voices, effectively capturing speaker-specific characteristics while preserving linguistic content. Additionally, we introduce a flexible training strategy for any-to-any voice conversion that generalizes well to unseen speakers and new languages, enhancing adaptability and performance in zero-shot scenarios. Our experiments show that our proposed method achieves a 16.35% relative reduction in word error rate and a 5.91% improvement in speaker cosine similarity, outperforming several state-of-the-art methods. Voice conversion samples can be accessed at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://oovc-emnlp-2025.github.io/\" title=\"\">https://oovc-emnlp-2025.github.io/</a></p>\n\n",
                "matched_terms": [
                    "audio",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Voice conversion specifically aims to transform a source speaker&#8217;s voice to match a target speaker while preserving the original linguistic content. This is typically done by disentangling speech into content and speaker identity representations, which are combined during training to reconstruct the audio. At inference time, the source content is paired with a target speaker embedding to generate the converted speech.\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "converted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the other hand, some methods avoid using text labels by leveraging self-supervised learning (SSL) to extract high-level phonetic representations <cite class=\"ltx_cite ltx_citemacro_citep\">(Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib32\" title=\"\">2021</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib23\" title=\"\">2021</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib14\" title=\"\">2022b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib15\" title=\"\">c</a>)</cite>. These approaches aim to remove speaker identity from source audio while preserving speaker-independent features such as accent and content. To achieve this separation, techniques such as vector quantization <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu and Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib43\" title=\"\">2020</a>)</cite>, instance normalization <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib6\" title=\"\">2021b</a>)</cite>, heuristic transformation <cite class=\"ltx_cite ltx_citemacro_citep\">(Neekhara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib29\" title=\"\">2024</a>)</cite>, bottleneck, and data augmentation <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib22\" title=\"\">2023</a>)</cite> are commonly applied. However, despite these efforts, such methods still struggle to completely eliminate speaker information from the source speech. This often leads to speaker leakage, where the converted audio retains unintended characteristics of the source speaker, resulting in mismatches between the synthesized voice and the intended target speaker <cite class=\"ltx_cite ltx_citemacro_citep\">(Baas et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib1\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "converted",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the limitations of text-based methods, text-free approaches have emerged, leveraging self-supervised learning models to extract content representations without requiring transcriptions <cite class=\"ltx_cite ltx_citemacro_citep\">(Polyak et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib32\" title=\"\">2021</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib23\" title=\"\">2021</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib14\" title=\"\">2022b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib15\" title=\"\">c</a>)</cite>. Although self-supervised learning (SSL) features capture high-level information related to linguistic content, they often retain residual speaker characteristics. To address this, methods such as bottleneck layers <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib22\" title=\"\">2023</a>)</cite>, vector quantization <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu and Lee, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib43\" title=\"\">2020</a>)</cite>, and instance normalization <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib6\" title=\"\">2021b</a>)</cite> have been proposed to compress SSL features and extract speaker-independent content representations. However, effective disentanglement heavily depends on the choice of bottleneck configuration: if the bottleneck dimension is too large, speaker information may be retained; if too small, important content information can be lost. A similar trade-off exists in vector quantization: large codebooks may retain speaker traits, while overly small codebooks may lead to excessive loss of content information. Moreover, these compression techniques often degrade the quality of the generated audio, and full disentanglement of speaker and content information remains an open challenge.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "transcriptions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In text-based VC, ASR-derived content representations are highly sensitive to transcription errors, mispronunciations, and noisy labels, which can compromise their reliability and degrade the quality of converted speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib40\" title=\"\">2016</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib24\" title=\"\">2021</a>; Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib41\" title=\"\">2018</a>)</cite>. Some approaches attempt to leverage knowledge transfer from hidden representations of text encoders in multispeaker TTS models <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib31\" title=\"\">2020</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib44\" title=\"\">2021</a>)</cite>. However, mapping audio representations directly to these text-based features is a difficult task. In addition, this process typically requires an explicit alignment mechanism between speech and text, which introduces further complexity.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "converted",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As an alternative solution, synthetic data offers several advantages for voice conversion. When both source and target audio are generated from the same linguistic content, it provides a clean and direct supervisory signal. This shared content allows precise frame-level alignment between source and target audio, enabling more stable and fine-grained learning of the conversion function. Unlike traditional approaches that rely on symbolic representations (e.g., phonemes or characters), synthetic data eliminates the need for such intermediates and avoids issues like label noise commonly found in real-data training. Furthermore, with controlled or predefined durations, speaker-independent features are inherently aligned across domains, removing the need for forced alignment algorithms used in previous work <cite class=\"ltx_cite ltx_citemacro_citep\">(Park et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib31\" title=\"\">2020</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib44\" title=\"\">2021</a>)</cite>. Unlike methods that map audio to hidden text representations from multispeaker TTS models, our approach directly maps source audio to target audio, simplifying the learning process. This enables one-to-one frame alignment, allowing the model to focus more effectively on speaker transformation while preserving linguistic content. Finally, synthetic data enables the creation of diverse speaker pairs with uniform content, supporting the learning of generalizable speaker conversion mappings. Motivated by these advantages, this work is the first to propose using synthetic data as a training paradigm for voice conversion models.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After generating synthetic data consisting of source and target speech pairs for supervised training, these utterances are directly utilized as input-output pairs for the voice conversion model. As the backbone architecture, we adopt a VITS-base model. Following the design of FreeVC <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib22\" title=\"\">2023</a>)</cite>, our model structure retains its core components. However, we note that while the source and target speech pairs share the same underlying linguistic content, the target speech is conditioned on a different speaker identity, which primarily manifests in variations in pitch. To avoid mismatch between input and output during training, we incorporate the fundamental frequency (<math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math>) of the target speech as an additional conditioning feature when decoding the final audio. The general model pipeline is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Synthetic Data Strategy &#8227; 3 Methodology &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Target Audio Processing:</span> The target audio is passed through a speaker encoder and a posterior encoder to extract the posterior latent distribution <math alttext=\"\\mathcal{N}(\\mu_{\\phi},\\sigma^{2}_{\\phi})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#956;</mi><mi>&#981;</mi></msub><mo>,</mo><msubsup><mi>&#963;</mi><mi>&#981;</mi><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(\\mu_{\\phi},\\sigma^{2}_{\\phi})</annotation></semantics></math>. Then, a latent variable <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m2\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> is sampled from this distribution\n<math alttext=\"z\\sim\\mathcal{N}(\\mu_{\\phi},\\sigma^{2}_{\\phi})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><mi>z</mi><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#956;</mi><mi>&#981;</mi></msub><mo>,</mo><msubsup><mi>&#963;</mi><mi>&#981;</mi><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z\\sim\\mathcal{N}(\\mu_{\\phi},\\sigma^{2}_{\\phi})</annotation></semantics></math>.\nThe sampled latent vector <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m4\" intent=\":literal\"><semantics><mi>z</mi><annotation encoding=\"application/x-tex\">z</annotation></semantics></math> is passed through a flow-based module to obtain <math alttext=\"z_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p3.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">z_{p}</annotation></semantics></math>, transforming the posterior distribution to match the prior distribution. A Kullback-Leibler (KL) divergence loss is calculated to minimize the discrepancy between the posterior and prior distributions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fundamental Frequency (<span class=\"ltx_text ltx_markedasmath\">F0</span>) Adjustment:</span> To address the mismatch in <math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math> between the source and target audio, we extract the <math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p4.m3\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math> of the target audio and pass it through an <math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p4.m4\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math> encoder to obtain pitch-related features. The decoder then takes the transformed latent representation along with the <math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p4.m5\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math> features to generate the target audio. In this work, we extract <math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p4.m6\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math> using Parselmouth<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/YannickJadoul/Parselmouth\" title=\"\">https://github.com/YannickJadoul/Parselmouth</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the mismatch in <math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math> between the source and reference audio, we shift <math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math> source to <math alttext=\"F0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">F0</annotation></semantics></math> target with same median level, the following steps are performed:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For phase 1 of training, we use synthetic speech generated by a publicly available pretrained VITS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/jaywalnut310/vits\" title=\"\">https://github.com/jaywalnut310/vits</a></span></span></span>. Specifically, we adopt the model released in the official repository. The amount of synthetic data corresponds to the VCTK training set used in the original VITS implementation. For each sample, the target audio is synthesized using the ground-truth text and speaker ID, while the source audio is generated by sampling a different random speaker ID. In phase 2, we fine-tune the model on the LibriSpeech dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib30\" title=\"\">2015</a>)</cite>, using the train-clean-360 and train-clean-100 subsets, totaling approximately 460 hours of speech from 1,172 speakers. Evaluation is conducted on the test-clean subset under any-to-any voice conversion scenarios.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "groundtruth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective Evaluation</span>: We evaluate system performance using four objective metrics: Character Error Rate (CER), Word Error Rate (WER), Speaker Encoder Cosine Similarity (SECS), and Objective Naturalness. CER and WER assess intelligibility between the source and converted speech, using the HuBERT model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-large-ls960-ft\" title=\"\">https://huggingface.co/facebook/hubert-large-ls960-ft</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib11\" title=\"\">2021</a>)</cite>. SECS measures speaker similarity using the cosine similarity between embeddings extracted by Resemblyzer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/resemble-ai/Resemblyzer\" title=\"\">https://github.com/resemble-ai/Resemblyzer</a></span></span></span>. Naturalness is assessed using NISQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Mittag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib28\" title=\"\">2021</a>)</cite>, which estimates perceptual speech quality without reference audio. We compute these metrics on 1,000 randomly sampled audio pairs from LibriSpeech test-clean.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "converted",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to intelligibility and naturalness, we also assess pitch preservation by reporting the F0 Pearson Correlation Coefficient (F0-PCC) <cite class=\"ltx_cite ltx_citemacro_citep\">(Benesty et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib2\" title=\"\">2009</a>)</cite>, which is computed between the F0 contours of the source and converted audio. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Zero-Shot Voice Conversion &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our system achieves the highest F0-PCC, outperforming recent models that explicitly condition on F0 such as Diff-HierVC, as well as the same backbone model FreeVC without F0 conditioning. These results highlight the effectiveness of the proposed F0 encoder in maintaining accurate pitch contours and demonstrate strong pitch consistency across converted speech.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "converted",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also quantitatively evaluate how effectively the prior encoder removes speaker information by comparing our model to FreeVC, which shares the same backbone architecture. Our goal is to demonstrate that training with synthetic data significantly improves the removal of speaker identity from source audio. To assess this, we use three clustering evaluation metrics: Adjusted Rand Index (ARI), Normalized Mutual Information (NMI) and Silhouette Score. The ARI measures the similarity between predicted clusters and true speaker labels, adjusted for chance. A lower ARI indicates that the clusters do not correspond well to speaker identities, suggesting better speaker information removal. NMI measures the amount of shared information between the predicted and true clusters; lower values indicate weaker correlation and thus stronger speaker anonymization. The Silhouette Score reflects how well each embedding fits within its cluster compared to the others. Lower scores imply that the model&#8217;s embeddings are not tightly grouped by speaker, further indicating that speaker identity has been suppressed. The quantitative results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.T3\" title=\"Table 3 &#8227; 5.2 Ablation Study &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Our model, trained with synthetic data, consistently achieves lower scores across all metrics, demonstrating its improved ability to remove speaker-specific information compared to FreeVC.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the effectiveness of synthetic speech for input-output training, we examine the semantic alignment between the source and target audio generated. To assess semantic alignment quality, we extract semantic features with a pretrained HuBERT ASR model, as described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S4.SS3\" title=\"4.3 Evaluation Metrics &#8227; 4 Experiment Setup &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>. We then compute the cosine similarity between all pairs of frames from the source and target audio, resulting in a pairwise similarity matrix. This matrix is visualized as a cosine similarity heatmap in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F5.sf1\" title=\"In Figure 5 &#8227; 5.4 Semantic Alignment of Synthetic Audio Pairs &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">5(a)</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The heatmap displays a clear diagonal of high similarity values, indicating strong frame-level alignment between the source and target audio. Furthermore, the top-1 cosine similarity alignment path, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#S5.F5.sf2\" title=\"In Figure 5 &#8227; 5.4 Semantic Alignment of Synthetic Audio Pairs &#8227; 5 Results and Analysis &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">5(b)</span></a>, lies precisely along the diagonal, confirming perfect alignment. These results demonstrate that the synthetic data input-output pairs are ideal training examples for voice conversion, enabling the model to learn effective one-to-one mapping.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate how well emotional information is preserved, we compare our proposed model with the baseline FreeVC, which shares the same backbone. For this experiment, we use the ESD dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Zhou et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib45\" title=\"\">2021</a>)</cite>, which contains emotional speech. We randomly sample 10 audio clips from each of the 10 speakers across 4 emotions, resulting in a total of 400 source audio samples. For the target speakers, we randomly select a speaker from the LibriSpeech test set. After performing voice conversion, we extract emotion embeddings from the converted audio using the emotion2vec_plus_large model <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#bib.bib26\" title=\"\">2024</a>)</cite> and visualize them using t-SNE. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09061v1#A1.F6\" title=\"Figure 6 &#8227; A.1 Preservation of Emotional Information &#8227; Appendix A Appendix &#8227; O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, our proposed model produces more distinct emotion clusters, such as sad and surprise, while FreeVC exhibits little to no clustering, indicating that our model better preserves emotional characteristics.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "converted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results show clear gains in intelligibility after language adaptation, though still slightly below ground-truth levels. This indicates that the converted speech becomes substantially more native-like, while leaving room for further improvements to fully match natural speech.</p>\n\n",
                "matched_terms": [
                    "groundtruth",
                    "converted"
                ]
            }
        ]
    }
}