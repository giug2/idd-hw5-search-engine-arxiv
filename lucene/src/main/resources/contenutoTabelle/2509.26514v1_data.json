{
    "S3.T1": {
        "source_file": "BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs",
        "caption": "Table 1: Performance on the English TTS Benchmark. BatonVoice demonstrates superior emotion ability (Acc.) while maintaining high intelligibility (WER).",
        "body": "Model\nSize\nPre-Train\nInstruction\nSeed-TTS\nEmotion\n\n\nData (Hours)\nData (Hours)\nWER (↓\\downarrow)\nAcc. (↑\\uparrow)\n\n\nClose-Source\n\n\n\n     Minimax-2.5-HD\n-\n-\n-\n1.5\n48.6\n\n\n     Minimax-2.5-Turbo\n-\n-\n-\n1.5\n46.4\n\n\n     Minimax-2.0-HD\n-\n-\n-\n1.5\n39.2\n\n\nOpen-Source\n\n\n\n     Spark-TTS\n0.5B\n103K\n0\n1.9\n27.4\n\n\n     CosyVoice\n0.3B\n172K\n556\n3.4\n43.8\n\n\n     CosyVoice2\n0.5B\n167K\n1,500\n2.1\n37.8\n\n\n     Higgs speech V2\n3.0B\n\n>>10,000K\n-\n1.8\n23.5\n\n\n\nBatonVoice (Ours)\n0.5B\n103K\n0\n2.9\n52.8\n\n\n1.7B\n2.5\n57.6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Size</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Pre-Train</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Instruction</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Seed-TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Emotion</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">Data (Hours)</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">Data (Hours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Acc. (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">Close-Source</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#160;&#160;&#160;Minimax-2.5-HD</td>\n<td class=\"ltx_td ltx_align_right\">-</td>\n<td class=\"ltx_td ltx_align_right\">-</td>\n<td class=\"ltx_td ltx_align_right\">-</td>\n<td class=\"ltx_td ltx_align_center\">1.5</td>\n<td class=\"ltx_td ltx_align_center\">48.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#160;&#160;&#160;Minimax-2.5-Turbo</td>\n<td class=\"ltx_td ltx_align_right\">-</td>\n<td class=\"ltx_td ltx_align_right\">-</td>\n<td class=\"ltx_td ltx_align_right\">-</td>\n<td class=\"ltx_td ltx_align_center\">1.5</td>\n<td class=\"ltx_td ltx_align_center\">46.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#160;&#160;&#160;Minimax-2.0-HD</td>\n<td class=\"ltx_td ltx_align_right\">-</td>\n<td class=\"ltx_td ltx_align_right\">-</td>\n<td class=\"ltx_td ltx_align_right\">-</td>\n<td class=\"ltx_td ltx_align_center\">1.5</td>\n<td class=\"ltx_td ltx_align_center\">39.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">Open-Source</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#160;&#160;&#160;Spark-TTS</td>\n<td class=\"ltx_td ltx_align_right\">0.5B</td>\n<td class=\"ltx_td ltx_align_right\">103K</td>\n<td class=\"ltx_td ltx_align_right\">0</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">27.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#160;&#160;&#160;CosyVoice</td>\n<td class=\"ltx_td ltx_align_right\">0.3B</td>\n<td class=\"ltx_td ltx_align_right\">172K</td>\n<td class=\"ltx_td ltx_align_right\">556</td>\n<td class=\"ltx_td ltx_align_center\">3.4</td>\n<td class=\"ltx_td ltx_align_center\">43.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#160;&#160;&#160;CosyVoice2</td>\n<td class=\"ltx_td ltx_align_right\">0.5B</td>\n<td class=\"ltx_td ltx_align_right\">167K</td>\n<td class=\"ltx_td ltx_align_right\">1,500</td>\n<td class=\"ltx_td ltx_align_center\">2.1</td>\n<td class=\"ltx_td ltx_align_center\">37.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#160;&#160;&#160;Higgs speech V2</td>\n<td class=\"ltx_td ltx_align_right\">3.0B</td>\n<td class=\"ltx_td ltx_align_right\">\n<math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>10,000K</td>\n<td class=\"ltx_td ltx_align_right\">-</td>\n<td class=\"ltx_td ltx_align_center\">1.8</td>\n<td class=\"ltx_td ltx_align_center\">23.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\">\n<span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> (Ours)</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.5B</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" rowspan=\"2\">103K</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" rowspan=\"2\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">52.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">1.7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">57.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "17b",
            "cosyvoice2",
            "ability",
            "minimax20hd",
            "superior",
            "wer",
            "speech",
            "data",
            "high",
            "demonstrates",
            "instruction",
            "emotion",
            "benchmark",
            "cosyvoice",
            "↑uparrow",
            "167k",
            "minimax25turbo",
            "batonvoice",
            "30b",
            "while",
            "performance",
            "172k",
            "↓downarrow",
            "maintaining",
            "103k",
            "acc",
            "sparktts",
            "closesource",
            "opensource",
            "03b",
            "hours",
            "minimax25hd",
            "higgs",
            "pretrain",
            "english",
            "ours",
            "size",
            "10000k",
            "tts",
            "05b",
            "seedtts",
            "intelligibility",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Main Results &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>-1.7B achieves 57.6% accuracy on the Emotion benchmark, surpassing the strongest closed-source baseline, Minimax-2.5-HD (48.6%), by 9.0 absolute points. It also outperforms all open-source systems by a wide margin, e.g., +13.8 points over CosyVoice (43.8%). On Seed-TTS, our 1.7B model attains a WER of 2.5 &#8211; competitive with high-quality open models (better than CosyVoice at 3.4, slightly above CosyVoice2 at 2.1 and Spark-TTS at 1.9) and within a small gap of the closed-source Minimax series (1.5). These results validate that our decoupled &#8220;conductor&#8211;orchestra&#8221; design substantially enhances emotional expressiveness without sacrificing intelligibility.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The rise of Large Language Models (LLMs) is reshaping multimodel models, with speech synthesis being a prominent application. However, existing approaches often underutilize the linguistic intelligence of these models, typically failing to leverage their powerful instruction-following capabilities. This limitation hinders the model&#8217;s ability to follow text instructions for controllable Text-to-Speech&#160;(TTS). To address this, we propose a new paradigm inspired by &#8220;operationalism&#8221; that decouples instruction understanding from speech generation. We introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, a framework where an LLM acts as a &#8220;conductor&#8221;, understanding user instructions and generating a textual &#8220;plan&#8221; &#8211; explicit vocal features (e.g., pitch, energy). A separate TTS model, the &#8220;orchestra&#8221;, then generates the speech from these features. To realize this component, we develop <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a TTS model trained specifically for this task. Our experiments demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in controllable and\nemotional speech synthesis, outperforming strong open- and closed-source baselines. Notably, our approach enables remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to languages unseen during post-training. This demonstrates that objectifying speech into textual vocal features can more effectively unlock the linguistic intelligence of LLMs.</p>\n\n",
                "matched_terms": [
                    "ability",
                    "performance",
                    "speech",
                    "tts",
                    "demonstrates",
                    "batonvoice",
                    "instruction",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid advancement of Large Language Models (LLMs) has catalyzed a paradigm shift in Multimodal Large Language Models&#160;(MLLMs), with frameworks now unifying text, images, and speech within a single model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib32\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib30\" title=\"\">2025</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib3\" title=\"\">2025</a>)</cite>. In Text-to-Speech (TTS), this has led to a new generation of systems that fine-tune a pre-trained LLM as a backbone to generate speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib27\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib7\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib33\" title=\"\">2025a</a>)</cite>. However, a critical yet underexplored question remains: <span class=\"ltx_text ltx_font_italic\">Are we fully leveraging the linguistic intelligence of LLMs in these TTS models?</span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing LLM-based TTS models primarily treat the LLM as a backbone. This approach typically involves designing a tokenizer to convert speech into discrete tokens and then training the model on large-scale datasets tailored to specific objectives. For instance, training a controllable TTS model necessitates extensive manual annotation of existing speech data to acquire the corresponding control labels and instructions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib6\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib5\" title=\"\">a</a>)</cite>, a process that is not only prohibitively expensive but also suffers from low inter-annotator agreement. We contend that this methodology largely bypasses the LLM&#8217;s inherent linguistic intelligence, such as its strong capabilities for complex context understanding and instruction following.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "data",
                    "instruction",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, we draw inspiration from the principle of &#8220;operationalism&#8221;, where complex concepts are understood through quantifiable, interpretable operations. For instance, to analyze imperceptible ultrasound, we use sensors to extract quantifiable features like frequency and amplitude. We posit that controllable TTS can be transformed by operationalizing user instructions into the desired vocal features. This reframes the problem: the LLM first leverages its linguistic intelligence to understand instructions and generate explicit vocal features, which then serves as input for a subsequent TTS model. This approach allows us to circumvent the need for manually annotating speech with controllable labels.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To realize this vision, we introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, a novel TTS framework that decouples instruction understanding from speech generation, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S0.F1\" title=\"Figure 1 &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> employs an LLM as a &#8220;conductor&#8221;, which interprets the user&#8217;s instructions to explicit vocal features, like pitch and energy. This plan is then fed into a separate TTS model, the &#8220;orchestra&#8221;, which generates the final speech. The &#8220;orchestra&#8221; in our framework is <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a TTS model we trained specifically to synthesize high-quality speech conditioned on these textual vocal features.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "batonvoice",
                    "instruction",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments validate the power of this decoupled approach. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in emotional speech synthesis, outperforming strong open- and closed-source models. For example, our 1.7B parameter model achieves an emotion accuracy of 57.6%, significantly surpassing all baselines. To verify our hypothesis, we show that stronger linguistic intelligence directly translates to superior synthesis: upgrading the &#8220;conductor&#8221; LLM from our 1.7B model to the more capable <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span> boosts the final model&#8217;s emotion accuracy from 29.8% to 57.6%. Furthermore, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> exhibits remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to Chinese, which is an unseen language during feature control training stage. This work not only advances controllable speech synthesis but also presents a promising new paradigm for MLLM development, demonstrating how objectifying modalities into text can more fully unlock the linguistic intelligence of LLMs.</p>\n\n",
                "matched_terms": [
                    "17b",
                    "performance",
                    "superior",
                    "speech",
                    "batonvoice",
                    "emotion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a methodology for realizing this paradigm, including a novel data pipeline that automatically generates instruction-feature pairs, and we introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a specialized TTS model trained on this data to generate speech from the vocal features.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through extensive experiments, we demonstrate that our framework, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, achieves strong performance in controllable, expressive speech synthesis. It exhibits superior emotional control and remarkable zero-shot cross-lingual generalization performance, validating the effectiveness of our operationalism-inspired approach.</p>\n\n",
                "matched_terms": [
                    "superior",
                    "batonvoice",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, a controllable TTS framework capable of synthesizing speech that adheres to arbitrary text-based instructions. Adopting an operationalist stance, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> leverages LLMs to interpret users&#8217; instructions into a JSON list of fine-grained vocal features. The core of this framework is <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a TTS model trained specifically developed to synthesize speech from these features. We first describe the overall framework and its inference process, followed by a detailed introduction of the <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> architecture and its three-stage training pipeline.</p>\n\n",
                "matched_terms": [
                    "batonvoice",
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The inference process of the <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> framework is structured in two stages. In the first stage, for a given input text <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and a corresponding instruction <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>, an external LLM&#160;(specifically, <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span>) is employed to interpret the instruction. This interpretation yields a set of fine-grained vocal features, denoted as <math alttext=\"F_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">F_{v}</annotation></semantics></math>. These features constitute a quantitative vocal plan and encompass the following attributes:</p>\n\n",
                "matched_terms": [
                    "batonvoice",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We now detail the architecture of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, the model responsible for generating speech from the specified feature list. Inspired by recent advancements such as CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib6\" title=\"\">2024b</a>)</cite>, the architecture of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> comprises two primary components: an LLM backbone and a pre-trained speech decoder.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the LLM backbone, we employ representative open-source models, specifically Qwen3-1.7B and Qwen2.5-0.5B. As will be demonstrated in our experimental section, our proposed method is effective across LLM backbones of varying capacities. The LLM is tasked with autoregressively generating a sequence that includes the input text to be synthesized, the corresponding speech features (i.e., the vocal plan), and the discrete speech tokens that realize this plan. The structure of this input sequence during the Supervised Fine-Tuning (SFT) stage is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Overall Framework and Inference Process &#8227; 2 BatonVoice: A Framework for Controllable TTS &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. It is important to note that while the features are part of the training sequence, during inference, they are generated by an external LLM as previously described.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "speech",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the final synthesis step, we leverage the speech decoder from the publicly available CosyVoice2 model. This decoder converts the discrete speech tokens produced by the LLM into a high-quality speech. It consists of a speech token encoder, a conditional flow matching model, and a HiFi-GAN vocoder. The flow matching model generates Mel spectrograms conditioned on the discrete speech tokens, and the HiFi-GAN vocoder then converts these spectrograms into the final speech. By utilizing a pre-trained speech decoder, we can focus our training efforts exclusively on teaching the LLM to control speech features through language. Consequently, the speech decoder remains frozen throughout our training process.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Pre-Training.</span> Establishes a foundational TTS capability by training the LLM to generate speech tokens from text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective of this stage is to equip the LLM with a fundamental text-to-speech capability, providing a robust weight initialization for subsequent stages. We use a large-scale corpus of speech-text pairs, <math alttext=\"\\mathcal{D}_{\\text{pretrain}}=\\{(x_{i},S_{i})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>pretrain</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{pretrain}}=\\{(x_{i},S_{i})\\}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is the transcript and <math alttext=\"S_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">S_{i}</annotation></semantics></math> represents the corresponding discrete speech tokens. The model, denoted as policy <math alttext=\"\\pi_{\\text{pre}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>pre</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{pre}}</annotation></semantics></math>, is trained using a standard causal language modeling objective to predict the next token autoregressively over the concatenated sequence of text and speech tokens. The training objective is:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SFT stage aims to instill fine-grained controllability by training the model to generate speech conditioned on both the transcript and a set of explicit, verbalized vocal features. This process, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Overall Framework and Inference Process &#8227; 2 BatonVoice: A Framework for Controllable TTS &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, trains the model to associate textual vocal features with corresponding discrete speech tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During this stage, we fine-tune the policy <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math>. The input sequence is formed by concatenating the transcript <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>, the verbalized features <math alttext=\"F_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">F_{v}</annotation></semantics></math>, and the speech tokens <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m4\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>. The model is trained to predict the next token autoregressively by minimizing the cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"Y=[x;F_{v};S]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m5\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mi>x</mi><mo>;</mo><msub><mi>F</mi><mi>v</mi></msub><mo>;</mo><mi>S</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=[x;F_{v};S]</annotation></semantics></math> is the concatenated sequence. This objective teaches the model to generate speech tokens <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m6\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> that adhere to the vocal plan specified by <math alttext=\"F_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">F_{v}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although SFT offers a direct mechanism for control, the resulting model, <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math>, is still prone to certain failure modes. These include a high Word Error Rate (WER), an unnaturally slow speaking rate and insufficient expressiveness. To overcome these limitations, we employ a subsequent preference optimazation stage. The central principle is to construct a preference dataset, <math alttext=\"\\mathcal{D}_{\\text{pref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>pref</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{pref}}</annotation></semantics></math>, designed to align the model&#8217;s outputs with more desirable vocal features, crucially without the need for manually annotated expressive data.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model",
                    "data",
                    "high"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Initial Generation and Rejection Sampling</span>: For each text prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> in a corpus <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math>, we use the pre-trained model <math alttext=\"\\pi_{\\text{pre}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>pre</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{pre}}</annotation></semantics></math> from Stage 1 to synthesize an speech sample <math alttext=\"s_{\\text{base}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m4\" intent=\":literal\"><semantics><msub><mi>s</mi><mtext>base</mtext></msub><annotation encoding=\"application/x-tex\">s_{\\text{base}}</annotation></semantics></math>. Samples are designated as <em class=\"ltx_emph ltx_font_italic\">rejected</em> (<math alttext=\"s_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m5\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">s_{l}</annotation></semantics></math>) if they exhibit a high WER or a slow speech rate (SR). The corresponding speech tokens <math alttext=\"S_{\\text{base}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mtext>base</mtext></msub><annotation encoding=\"application/x-tex\">S_{\\text{base}}</annotation></semantics></math> are stored as the rejected sequence <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m7\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "model",
                    "high"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preferred Data Construction</span>: For each text <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> corresponding to a rejected sample, we use our SFT model <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math> to generate a new candidate speech <math alttext=\"\\hat{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{s}</annotation></semantics></math>. These candidates are accepted as <em class=\"ltx_emph ltx_font_italic\">chosen</em> samples (<math alttext=\"s_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m4\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">s_{w}</annotation></semantics></math>) if they meet the quality criteria (low WER and adequate SR). The corresponding features and tokens <math alttext=\"(F_{v,w},S_{w})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(F_{v,w},S_{w})</annotation></semantics></math> are stored.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "model",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference Dataset Construction</span>: This filtering process yields pairs of chosen sequences <math alttext=\"(F_{v,w},S_{w})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(F_{v,w},S_{w})</annotation></semantics></math> and rejected sequences <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math>. To create a controlled comparison, we form preference tuples where the model learns to prefer <math alttext=\"S_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">S_{w}</annotation></semantics></math> over <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math> under the same vocal plan, <math alttext=\"F_{v,w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">F_{v,w}</annotation></semantics></math>. This setup creates a powerful learning signal: because <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math> was generated without knowledge of <math alttext=\"F_{v,w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">F_{v,w}</annotation></semantics></math>, while <math alttext=\"S_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m8\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">S_{w}</annotation></semantics></math> was explicitly conditioned on it, teaching the model to prefer <math alttext=\"S_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m9\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">S_{w}</annotation></semantics></math> over <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m10\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math> not only improves general quality but also implicitly reinforces the model&#8217;s ability to follow the specified vocal features. The final dataset consists of tuples: <math alttext=\"\\mathcal{D}_{\\text{pref}}=\\{(x_{i},F_{v,w,i},S_{w,i},S_{l,i})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>pref</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mi>w</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mi>l</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{pref}}=\\{(x_{i},F_{v,w,i},S_{w,i},S_{l,i})\\}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "ability",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we fine-tune the model using Anchored Preference Optimization (APO-down)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#8217;Oosterlinck et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib4\" title=\"\">2025</a>)</cite>, with the SFT model serving as the reference policy (<math alttext=\"\\pi_{\\text{ref}}=\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p4.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#960;</mi><mtext>ref</mtext></msub><mo>=</mo><msub><mi>&#960;</mi><mtext>sft</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\text{ref}}=\\pi_{\\text{sft}}</annotation></semantics></math>). The APO-down objective penalizes deviations from the reference for the chosen sequence <math alttext=\"S_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p4.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">S_{w}</annotation></semantics></math> while maximizing the reward margin between the chosen (<math alttext=\"S_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p4.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">S_{w}</annotation></semantics></math>) and rejected (<math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p4.m4\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math>) sequences, given the shared prefix <math alttext=\"(x,F_{v,w})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p4.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x,F_{v,w})</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"r_{\\theta}(x,F_{v},S)=\\beta\\log\\big(\\pi_{\\theta}(S\\mid x,F_{v})/\\pi_{\\text{ref}}(S\\mid x,F_{v})\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p4.m6\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>r</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi>F</mi><mi>v</mi></msub><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>&#946;</mi><mo lspace=\"0.167em\" rspace=\"0em\">&#8203;</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><mrow><mrow><msub><mi>&#960;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>S</mi><mo>&#8739;</mo><mrow><mi>x</mi><mo>,</mo><msub><mi>F</mi><mi>v</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><msub><mi>&#960;</mi><mtext>ref</mtext></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>S</mi><mo>&#8739;</mo><mrow><mi>x</mi><mo>,</mo><msub><mi>F</mi><mi>v</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">r_{\\theta}(x,F_{v},S)=\\beta\\log\\big(\\pi_{\\theta}(S\\mid x,F_{v})/\\pi_{\\text{ref}}(S\\mid x,F_{v})\\big)</annotation></semantics></math> is the implicit reward. Term 1 anchors the policy to the SFT model for chosen samples, while Term 2 maximizes the preference margin. This dual objective allows the model to mitigate common failure modes without requiring any explicitly labeled expressive data.</p>\n\n",
                "matched_terms": [
                    "while",
                    "model",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training stage equips the LLM with the fundamental capability of converting text into a corresponding sequence of speech tokens, establishing a strong foundation for standard TTS before introducing complex instruction-following behavior. We use the VoxBox dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib29\" title=\"\">2025b</a>)</cite>, a large-scale, multi-speaker English speech corpus of approximately 103K hours. The speech is tokenized into discrete vocal units using the official CosyVoice2 tokenizer. To maximize throughput, we pack tokenized sequences into 4096-token chunks, reducing padding overhead. Pre-training is conducted on 80 GPUs for 3 epochs (approximately one day), using AdamW with a learning rate of 1e-4, 500 warmup steps, a global batch size of 640, and DeepSpeed ZeRO-2 for memory optimization.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "english",
                    "speech",
                    "103k",
                    "size",
                    "tts",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our post-training process consists of SFT and PO. A key challenge in preparing the SFT data is that our speech decoder cannot perfectly reconstruct original speech from its quantized tokens. To ensure the vocal features are faithfully synthesizable, we derive them from speech that has been reconstructed by the decoder itself.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our SFT dataset is compiled from two primary sources. First, we take a diverse collection of expressive speech corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib26\" title=\"\">2017</a>; Nagraniy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib19\" title=\"\">2017</a>; Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib2\" title=\"\">2018</a>; Richter et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib21\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib20\" title=\"\">2023</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib31\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib28\" title=\"\">2025a</a>)</cite>, pass the speech through our decoder for reconstruction, and then extract features from the synthesized output. Second, we collect colloquial sentences from the Synthetic-Persona-Chat dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jandaghi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib11\" title=\"\">2024</a>)</cite> and synthesize them. We then apply a filtering process to the combined data, removing samples with a high Word Error Rate (WER), which indicates potential misalignments, or an abnormally slow speaking rate. <math alttext=\"\\tau_{\\text{wer\\_high}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mtext>wer_high</mtext></msub><annotation encoding=\"application/x-tex\">\\tau_{\\text{wer\\_high}}</annotation></semantics></math> is 0.1 and <math alttext=\"\\tau_{\\text{sr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mtext>sr</mtext></msub><annotation encoding=\"application/x-tex\">\\tau_{\\text{sr}}</annotation></semantics></math> is 1.5 words per seconds.This results in a final SFT dataset of 377,619 utterances, totaling over 500 hours (see Appendix for a detailed distribution). For the PO stage, we collected a dataset of 9,823 preference samples.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "data",
                    "high",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The feature extraction pipeline for this data begins with grounding features in semantically meaningful units. We first obtain word-level timestamps for each speech sample using a pre-trained model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self\" title=\"\">https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self</a></span></span></span>. Since individual words are often too short to carry significant prosodic information, we merge adjacent words into segments until each segment&#8217;s duration exceeds a one-second threshold, ensuring a stable and analyzable prosodic contour. Finally, we use the Parselmouth library&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/YannickJadoul/Parselmouth\" title=\"\">https://github.com/YannickJadoul/Parselmouth</a></span></span></span> to extract a set of vocal features from these segments. The model is trained with SFT for 3 epochs, followed by 1 epoch of APO-down.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TTS Intelligibility</span>: We use the test set from the <span class=\"ltx_text ltx_font_bold\">Seed-TTS</span> benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib1\" title=\"\">2024</a>)</cite>, which is designed for assessing speech synthesis from short speech prompts. Performance is measured by <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, calculated with pre-trained ASR models&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span>. A lower WER score signifies higher intelligibility.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "intelligibility",
                    "wer",
                    "speech",
                    "tts",
                    "seedtts",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Control</span>: This is assessed on a curated test set from the <span class=\"ltx_text ltx_font_bold\">Emotion</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saravia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib23\" title=\"\">2018</a>)</cite>. We use includes 100 samples for each of five emotions (joy, sadness, anger, surprise, and fear). We measure performance using <span class=\"ltx_text ltx_font_bold\">Emotion Classification Accuracy</span>. This metric is derived by employing Google&#8217;s Gemini-2.5-Pro to classify the emotion of the synthesized speech. A higher accuracy indicates a greater success rate in generating perceptually accurate emotional speech. The prompt template for this evaluation is provided in the Appendix.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotion",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> framework achieves these results with 0 hours of manually annotated instruction data, in contrast to CosyVoice and CosyVoice2, which use 556 and 1,500 hours respectively yet underperform on emotion accuracy (43.8% and 37.8%). Preference optimization over textual vocal plans yields consistent improvements over SFT alone: for the 1.7B model, emotion accuracy increases from 52.2% (SFT) to 57.6% (Instruct, +5.4 points) while WER improves from 2.9 to 2.5. Even at 0.5B, instruction tuning further boosts accuracy from 51.6% to 52.8% (+1.2). These gains directly confirm the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>.</p>\n\n",
                "matched_terms": [
                    "17b",
                    "cosyvoice2",
                    "wer",
                    "model",
                    "data",
                    "05b",
                    "batonvoice",
                    "instruction",
                    "emotion",
                    "cosyvoice",
                    "hours",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving from 0.5B to 1.7B parameters improves emotion accuracy from 52.8% to 57.6% (+4.8) and reduces WER from 2.9 to 2.5 for the instruction-tuned models. This trend demonstrates the scalability of our method, and showcasing its consistent performance benefits across different model sizes..</p>\n\n",
                "matched_terms": [
                    "17b",
                    "performance",
                    "wer",
                    "05b",
                    "demonstrates",
                    "emotion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess our model&#8217;s performance on controllable TTS with free-form instructions, we create a specialized test set. We begin by sourcing 50 diverse social situations from the Social IQa benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib22\" title=\"\">2019</a>)</cite>, chosen for its rich contextual and emotional nuance. For each situation, we utilize <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span> to generate a challenging test case. The model is prompted to produce two outputs: first, a detailed, role-playing style instruction framed in a second-person narrative, which specifies the desired persona and delivery style. Second, it generates a corresponding target utterance to be synthesized. This pipeline yield a high-quality benchmark of 50 pairs, specifically designed to test the model&#8217;s ability to follow complex, descriptive instructions beyond simple labels.</p>\n\n",
                "matched_terms": [
                    "ability",
                    "performance",
                    "tts",
                    "instruction",
                    "benchmark",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that long instructions were incorrectly synthesized by CosyVoice. To mitigate this, we use Gemini 2.5 Pro to map each detailed instruction to a discrete emotion label (Neutral + 6 Ekman emotions), which was then fed to CosyVoice. This label-based approach was also necessary for Minimax 2.5, which only accepts emotion labels as input.\nWe all the prompt template in the appendix.\nWe performed a human evaluation comparing <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> with the top-performing open-source (CosyVoice) and closed-source (Minimax-2.5-HD) models. The evaluation involved three trained annotators with a Cohen&#8217;s Kappa of 0.61. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Human Evaluation of Instruction-Following &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves performance comparable to CosyVoice but is outperformed by the commercial system Minimax-2.5-HD, falling short in aspects of fluency and naturalness.</p>\n\n",
                "matched_terms": [
                    "minimax25hd",
                    "performance",
                    "batonvoice",
                    "instruction",
                    "emotion",
                    "cosyvoice",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A significant and surprising finding is the model&#8217;s ability to generalize to languages not seen during the <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> post-training stage. We evaluated this by testing on a Chinese emotion benchmark, employing the same methodology as the English evaluation, with the text and instructions translated into Chinese by <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span>. Notably, this cross-lingual generalization occurs despite the post-training stage being conducted exclusively on English data, demonstrating a strong zero-shot transfer capability.</p>\n\n",
                "matched_terms": [
                    "ability",
                    "english",
                    "data",
                    "emotion",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.T3\" title=\"Table 3 &#8227; 3.4 Cross-Lingual Generalization &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>-1.7B achieves a 56.2% accuracy on the Chinese emotion benchmark. This result is not only strong in absolute terms but also surpasses leading models that are either native to or heavily optimized for Chinese, such as CosyVoice (52.0%) and the closed-source Minimax-2.5-Turbo (50.6%). This performance is achieved without any Chinese instruction data, highlighting a key advantage of our &#8220;operationalism&#8221; paradigm.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "data",
                    "instruction",
                    "emotion",
                    "benchmark",
                    "cosyvoice",
                    "minimax25turbo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a series of in-depth analyses to better understand the capabilities of <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>. Otherwise stated, we report the results of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>-1.7B on the English Emotion benchmark.</p>\n\n",
                "matched_terms": [
                    "batonvoice",
                    "english",
                    "emotion",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform a step-by-step ablation to examine the effectiveness of each stage in our proposed <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> framework. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 3.5 Component Analysis &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, the base model, trained only on foundational TTS without any instruction tuning, yields poor performance on the English Emotion benchmark &#8211; achieving just 23.2% accuracy for the 1.7B model. Incorporating the SFT stage causes a dramatic improvement, boosting the accuracy to 52.2% (+29.0 points), showing that teaching the model to generate and condition on verbalized vocal plans is key to enabling stylistic control. Adding the APO-based preference optimization further improves performance to 57.6% (+5.4 over SFT), illustrating the importance of our post-training strategy. Consistent gains are observed for the smaller 0.5B model (25.8 <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 51.6 <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 52.8), demonstrating that the framework is effective across model scales. These results validate the design of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> in sequentially teaching foundational TTS capability and improving control quality.</p>\n\n",
                "matched_terms": [
                    "17b",
                    "english",
                    "performance",
                    "tts",
                    "05b",
                    "instruction",
                    "emotion",
                    "benchmark",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate how our framework leverages the linguistic intelligence of Large Language Models (LLMs), we performed an experiment to measure the impact of the vocal feature generator on final synthesis quality. We used a fixed <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> model and generated vocal plans at inference time using a range of LLMs with varying capabilities. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.5 Component Analysis &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, the results show a clear, positive correlation between the performance of the LLM and the emotion accuracy of the synthesized speech. The accuracy climbs steadily from 29.8% with Qwen3-1.7B to 57.6% with Gemini-2.5-Pro, with intermediate models like Qwen3-80B (39.8%) and Qwen3-Max (47.8%) falling along this expected trajectory.\nThese findings strongly support our core claim: representing speech as vocal features allows the synthesis model to directly benefit from advances in LLMs. This highlights a key advantage of our decoupled &#8220;conductor&#8211;orchestra&#8221; design: its modularity. Even our compact 1.7B model can tap into the power of a much larger model like Gemini-2.5-Pro at inference time, effectively upgrading its expressive capability without any modification to the model.</p>\n\n",
                "matched_terms": [
                    "17b",
                    "performance",
                    "speech",
                    "batonvoice",
                    "emotion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable speech synthesis is typically classified into three primary paradigms. The first, style tagging, employs discrete labels (e.g., emotion, gender) to guide the synthesis process&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib7\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib29\" title=\"\">2025b</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib33\" title=\"\">2025a</a>)</cite>. While conceptually simple, this approach is restricted to a predefined set of styles, which fundamentally limits its expressive range.\nThe second paradigm leverages reference speech to enable few-shot or zero-shot speaker adaptation. This is accomplished by extracting speaker embeddings from short speech samples and conditioning the TTS decoder on them &#8211; a technique proven effective for voice cloning and style transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib14\" title=\"\">2024</a>; Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib12\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib15\" title=\"\">2024</a>)</cite>.\nThe third and most flexible paradigm, instruction-guided control, conceptualizes TTS as a task of interpreting natural language instructions. Frameworks such as VoxInstruct exemplify this approach, guiding synthesis\nwith free-form instructions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib6\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib5\" title=\"\">a</a>)</cite>.\nHowever, these instruction-following methods are constrained by the high cost and difficulty of creating large-scale, annotated instruction-speech datasets, which limits their generalization and performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "tts",
                    "high",
                    "emotion",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, our approach circumvents the need for manually annotated data by leveraging a powerful LLM. This enables robust, zero-shot generalization to unseen instructions, generating vocal features that exhibit high fidelity to the prompts while affording a high degree of control. Our method thus addresses the key limitations of data scarcity and annotation cost in instruction-guided TTS, showing significant promise for future research in expressive and controllable speech synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "data",
                    "high",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remarkable reasoning capabilities of LLMs have catalyzed extensive research into extending these faculties to multimodal domains. Early efforts sought to enhance multimodal understanding by employing techniques such as reinforcement learning to better align visual and textual representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib8\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib10\" title=\"\">2025b</a>; Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib18\" title=\"\">2025</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib24\" title=\"\">2025</a>)</cite>. More recent and prominent approaches aim for a deeper integration of reasoning. One prominent direction integrates multimodal information as intermediate steps within a reasoning chain, analogous to a &#8220;chain of thought&#8221;, to derive conclusions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib25\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib35\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib34\" title=\"\">2025b</a>)</cite>. Another emerging strategy involves performing explicit, text-based reasoning prior to the final multimodal generation, thereby ensuring the output is logically grounded and coherent with the input prompt&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib16\" title=\"\">2025</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib13\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib9\" title=\"\">2025a</a>)</cite>. While powerful, these methods typically rely on training large-scale, end-to-end multimodal models &#8211; a process that is computationally intensive and demands vast quantities of aligned data.</p>\n\n",
                "matched_terms": [
                    "while",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we address a key limitation in current speech synthesis systems: the underutilization of the linguistic intelligence of LLMs. We introduce a new paradigm inspired by &#8220;operationalism&#8221;, which decouples instruction understanding from speech generation by first translating instructions into quantifiable, interpretable vocal features. Our framework, <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">BatonVoice</span>, embodies this principle by using LLMs to generate a vocal &#8220;plan&#8221;, which is then fed into a TTS model. We train this model using a three-stage training pipeline that requires no manual instruction data. Our empirical results demonstrate the effectiveness of this approach. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in emotional speech synthesis and shows that its capabilities scale positively with the linguistic intelligence of LLMs. Furthermore, it exhibits powerful zero-shot cross-lingual generalization.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "tts",
                    "data",
                    "batonvoice",
                    "instruction",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our SFT dataset is a comprehensive collection curated to teach the model how to generate vocal plans from text. It comprises 377,619 utterances, totaling over 500 hours of speech, and is compiled from two primary sources as detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#A2.T4\" title=\"Table 4 &#8227; B.1 Data Source &#8227; Appendix B Experimental Details &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each sample from these corpora, we first passed the original speech through our pre-trained vocal decoder to obtain a reconstructed waveform. We then extracted the textual vocal features (i.e., the vocal plan) from this synthesized output. This reconstruction step ensures that the vocal features are derived from a distribution that our TTS &#8221;orchestra&#8221; model can faithfully render.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance the linguistic diversity and colloquial nature of our training data, we incorporated sentences from the Synthetic-Persona-Chat dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jandaghi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib11\" title=\"\">2024</a>)</cite>. We synthesized these conversational sentences using a high-quality baseline TTS model and then processed them through the same feature extraction pipeline described above. This source contributes the largest portion of our dataset, ensuring the model is exposed to a wide array of everyday language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our analysis, we first compared different formats for representing the vocal plan. We found that our proposed numerical representation significantly outperformed a qualitative, caption-based description, demonstrating that a structured, quantitative format allows for more precise control. Remarkably, the MCD achieved with our numerical plan was even lower than that of the vocoder resynthesis baseline (i.e., the MCD between the reconstructed and the original speech). This suggests that <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> not only faithfully renders the vocal plan but can also compensate for some information loss introduced during the initial decoding stage. Furthermore, to validate the design of our vocal plan, we performed an ablation study by systematically removing individual features (e.g., pitch, energy) from the plan before feeding it to <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>. We observed a consistent performance degradation (i.e., an increase in MCD) upon the removal of any feature. This result confirms that all components of our proposed vocal representation are necessary and contribute meaningfully to the final synthesis quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs",
        "caption": "Table 2: Human preference evaluation for instruction following TTS.",
        "body": "Compare with\nWin Rate\n\n\n\n\nCosyVoice\n56%\n\n\nMinimax-2.5-HD\n30%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Compare with</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Win Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">CosyVoice</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">56%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Minimax-2.5-HD</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">30%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "win",
            "evaluation",
            "rate",
            "human",
            "following",
            "tts",
            "compare",
            "preference",
            "instruction",
            "cosyvoice",
            "minimax25hd"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We found that long instructions were incorrectly synthesized by CosyVoice. To mitigate this, we use Gemini 2.5 Pro to map each detailed instruction to a discrete emotion label (Neutral + 6 Ekman emotions), which was then fed to CosyVoice. This label-based approach was also necessary for Minimax 2.5, which only accepts emotion labels as input.\nWe all the prompt template in the appendix.\nWe performed a human evaluation comparing <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> with the top-performing open-source (CosyVoice) and closed-source (Minimax-2.5-HD) models. The evaluation involved three trained annotators with a Cohen&#8217;s Kappa of 0.61. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Human Evaluation of Instruction-Following &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves performance comparable to CosyVoice but is outperformed by the commercial system Minimax-2.5-HD, falling short in aspects of fluency and naturalness.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The rise of Large Language Models (LLMs) is reshaping multimodel models, with speech synthesis being a prominent application. However, existing approaches often underutilize the linguistic intelligence of these models, typically failing to leverage their powerful instruction-following capabilities. This limitation hinders the model&#8217;s ability to follow text instructions for controllable Text-to-Speech&#160;(TTS). To address this, we propose a new paradigm inspired by &#8220;operationalism&#8221; that decouples instruction understanding from speech generation. We introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, a framework where an LLM acts as a &#8220;conductor&#8221;, understanding user instructions and generating a textual &#8220;plan&#8221; &#8211; explicit vocal features (e.g., pitch, energy). A separate TTS model, the &#8220;orchestra&#8221;, then generates the speech from these features. To realize this component, we develop <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a TTS model trained specifically for this task. Our experiments demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in controllable and\nemotional speech synthesis, outperforming strong open- and closed-source baselines. Notably, our approach enables remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to languages unseen during post-training. This demonstrates that objectifying speech into textual vocal features can more effectively unlock the linguistic intelligence of LLMs.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing LLM-based TTS models primarily treat the LLM as a backbone. This approach typically involves designing a tokenizer to convert speech into discrete tokens and then training the model on large-scale datasets tailored to specific objectives. For instance, training a controllable TTS model necessitates extensive manual annotation of existing speech data to acquire the corresponding control labels and instructions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib6\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib5\" title=\"\">a</a>)</cite>, a process that is not only prohibitively expensive but also suffers from low inter-annotator agreement. We contend that this methodology largely bypasses the LLM&#8217;s inherent linguistic intelligence, such as its strong capabilities for complex context understanding and instruction following.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "tts",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To realize this vision, we introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, a novel TTS framework that decouples instruction understanding from speech generation, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S0.F1\" title=\"Figure 1 &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> employs an LLM as a &#8220;conductor&#8221;, which interprets the user&#8217;s instructions to explicit vocal features, like pitch and energy. This plan is then fed into a separate TTS model, the &#8220;orchestra&#8221;, which generates the final speech. The &#8220;orchestra&#8221; in our framework is <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a TTS model we trained specifically to synthesize high-quality speech conditioned on these textual vocal features.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The inference process of the <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> framework is structured in two stages. In the first stage, for a given input text <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> and a corresponding instruction <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>, an external LLM&#160;(specifically, <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span>) is employed to interpret the instruction. This interpretation yields a set of fine-grained vocal features, denoted as <math alttext=\"F_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">F_{v}</annotation></semantics></math>. These features constitute a quantitative vocal plan and encompass the following attributes:</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although SFT offers a direct mechanism for control, the resulting model, <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math>, is still prone to certain failure modes. These include a high Word Error Rate (WER), an unnaturally slow speaking rate and insufficient expressiveness. To overcome these limitations, we employ a subsequent preference optimazation stage. The central principle is to construct a preference dataset, <math alttext=\"\\mathcal{D}_{\\text{pref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>pref</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{pref}}</annotation></semantics></math>, designed to align the model&#8217;s outputs with more desirable vocal features, crucially without the need for manually annotated expressive data.</p>\n\n",
                "matched_terms": [
                    "preference",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training stage equips the LLM with the fundamental capability of converting text into a corresponding sequence of speech tokens, establishing a strong foundation for standard TTS before introducing complex instruction-following behavior. We use the VoxBox dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib29\" title=\"\">2025b</a>)</cite>, a large-scale, multi-speaker English speech corpus of approximately 103K hours. The speech is tokenized into discrete vocal units using the official CosyVoice2 tokenizer. To maximize throughput, we pack tokenized sequences into 4096-token chunks, reducing padding overhead. Pre-training is conducted on 80 GPUs for 3 epochs (approximately one day), using AdamW with a learning rate of 1e-4, 500 warmup steps, a global batch size of 640, and DeepSpeed ZeRO-2 for memory optimization.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our SFT dataset is compiled from two primary sources. First, we take a diverse collection of expressive speech corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib26\" title=\"\">2017</a>; Nagraniy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib19\" title=\"\">2017</a>; Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib2\" title=\"\">2018</a>; Richter et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib21\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib20\" title=\"\">2023</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib31\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib28\" title=\"\">2025a</a>)</cite>, pass the speech through our decoder for reconstruction, and then extract features from the synthesized output. Second, we collect colloquial sentences from the Synthetic-Persona-Chat dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jandaghi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib11\" title=\"\">2024</a>)</cite> and synthesize them. We then apply a filtering process to the combined data, removing samples with a high Word Error Rate (WER), which indicates potential misalignments, or an abnormally slow speaking rate. <math alttext=\"\\tau_{\\text{wer\\_high}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mtext>wer_high</mtext></msub><annotation encoding=\"application/x-tex\">\\tau_{\\text{wer\\_high}}</annotation></semantics></math> is 0.1 and <math alttext=\"\\tau_{\\text{sr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mtext>sr</mtext></msub><annotation encoding=\"application/x-tex\">\\tau_{\\text{sr}}</annotation></semantics></math> is 1.5 words per seconds.This results in a final SFT dataset of 377,619 utterances, totaling over 500 hours (see Appendix for a detailed distribution). For the PO stage, we collected a dataset of 9,823 preference samples.</p>\n\n",
                "matched_terms": [
                    "preference",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TTS Intelligibility</span>: We use the test set from the <span class=\"ltx_text ltx_font_bold\">Seed-TTS</span> benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib1\" title=\"\">2024</a>)</cite>, which is designed for assessing speech synthesis from short speech prompts. Performance is measured by <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, calculated with pre-trained ASR models&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span>. A lower WER score signifies higher intelligibility.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Control</span>: This is assessed on a curated test set from the <span class=\"ltx_text ltx_font_bold\">Emotion</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saravia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib23\" title=\"\">2018</a>)</cite>. We use includes 100 samples for each of five emotions (joy, sadness, anger, surprise, and fear). We measure performance using <span class=\"ltx_text ltx_font_bold\">Emotion Classification Accuracy</span>. This metric is derived by employing Google&#8217;s Gemini-2.5-Pro to classify the emotion of the synthesized speech. A higher accuracy indicates a greater success rate in generating perceptually accurate emotional speech. The prompt template for this evaluation is provided in the Appendix.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Main Results &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>-1.7B achieves 57.6% accuracy on the Emotion benchmark, surpassing the strongest closed-source baseline, Minimax-2.5-HD (48.6%), by 9.0 absolute points. It also outperforms all open-source systems by a wide margin, e.g., +13.8 points over CosyVoice (43.8%). On Seed-TTS, our 1.7B model attains a WER of 2.5 &#8211; competitive with high-quality open models (better than CosyVoice at 3.4, slightly above CosyVoice2 at 2.1 and Spark-TTS at 1.9) and within a small gap of the closed-source Minimax series (1.5). These results validate that our decoupled &#8220;conductor&#8211;orchestra&#8221; design substantially enhances emotional expressiveness without sacrificing intelligibility.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "minimax25hd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> framework achieves these results with 0 hours of manually annotated instruction data, in contrast to CosyVoice and CosyVoice2, which use 556 and 1,500 hours respectively yet underperform on emotion accuracy (43.8% and 37.8%). Preference optimization over textual vocal plans yields consistent improvements over SFT alone: for the 1.7B model, emotion accuracy increases from 52.2% (SFT) to 57.6% (Instruct, +5.4 points) while WER improves from 2.9 to 2.5. Even at 0.5B, instruction tuning further boosts accuracy from 51.6% to 52.8% (+1.2). These gains directly confirm the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "preference",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess our model&#8217;s performance on controllable TTS with free-form instructions, we create a specialized test set. We begin by sourcing 50 diverse social situations from the Social IQa benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib22\" title=\"\">2019</a>)</cite>, chosen for its rich contextual and emotional nuance. For each situation, we utilize <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span> to generate a challenging test case. The model is prompted to produce two outputs: first, a detailed, role-playing style instruction framed in a second-person narrative, which specifies the desired persona and delivery style. Second, it generates a corresponding target utterance to be synthesized. This pipeline yield a high-quality benchmark of 50 pairs, specifically designed to test the model&#8217;s ability to follow complex, descriptive instructions beyond simple labels.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.T3\" title=\"Table 3 &#8227; 3.4 Cross-Lingual Generalization &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>-1.7B achieves a 56.2% accuracy on the Chinese emotion benchmark. This result is not only strong in absolute terms but also surpasses leading models that are either native to or heavily optimized for Chinese, such as CosyVoice (52.0%) and the closed-source Minimax-2.5-Turbo (50.6%). This performance is achieved without any Chinese instruction data, highlighting a key advantage of our &#8220;operationalism&#8221; paradigm.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform a step-by-step ablation to examine the effectiveness of each stage in our proposed <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> framework. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 3.5 Component Analysis &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, the base model, trained only on foundational TTS without any instruction tuning, yields poor performance on the English Emotion benchmark &#8211; achieving just 23.2% accuracy for the 1.7B model. Incorporating the SFT stage causes a dramatic improvement, boosting the accuracy to 52.2% (+29.0 points), showing that teaching the model to generate and condition on verbalized vocal plans is key to enabling stylistic control. Adding the APO-based preference optimization further improves performance to 57.6% (+5.4 over SFT), illustrating the importance of our post-training strategy. Consistent gains are observed for the smaller 0.5B model (25.8 <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 51.6 <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 52.8), demonstrating that the framework is effective across model scales. These results validate the design of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> in sequentially teaching foundational TTS capability and improving control quality.</p>\n\n",
                "matched_terms": [
                    "preference",
                    "instruction",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we address a key limitation in current speech synthesis systems: the underutilization of the linguistic intelligence of LLMs. We introduce a new paradigm inspired by &#8220;operationalism&#8221;, which decouples instruction understanding from speech generation by first translating instructions into quantifiable, interpretable vocal features. Our framework, <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">BatonVoice</span>, embodies this principle by using LLMs to generate a vocal &#8220;plan&#8221;, which is then fed into a TTS model. We train this model using a three-stage training pipeline that requires no manual instruction data. Our empirical results demonstrate the effectiveness of this approach. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in emotional speech synthesis and shows that its capabilities scale positively with the linguistic intelligence of LLMs. Furthermore, it exhibits powerful zero-shot cross-lingual generalization.</p>\n\n",
                "matched_terms": [
                    "instruction",
                    "tts"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs",
        "caption": "Table 3: Performance on the Chinese TTS Benchmark. BatonVoice only uses English data for feature control training, yet demonstrates strong zero-shot generalization.",
        "body": "Model\nSeed-TTS\nEmotion\n\n\nWER (↓\\downarrow)\nAcc. (↑\\uparrow)\n\n\nClose-Source\n\n\nMinimax-2.5-HD\n0.9\n49.0\n\n\nMinimax-2.5-Turbo\n1.0\n50.6\n\n\nMinimax-2.0-HD\n0.9\n48.8\n\n\nOpen-Source\n\n\nSpark-TTS\n1.5\n29.2\n\n\nCosyVoice\n2.1\n52.0\n\n\nCosyVoice2\n2.0\n42.0\n\n\nHiggs speech V2\n1.2\n28.8\n\n\n\nBatonVoice-1.7B\n2.1\n56.2",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Seed-TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Emotion</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">WER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Acc. (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Close-Source</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Minimax-2.5-HD</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">49.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Minimax-2.5-Turbo</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">50.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Minimax-2.0-HD</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">48.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Open-Source</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Spark-TTS</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">29.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">CosyVoice</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">52.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">CosyVoice2</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">42.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Higgs speech V2</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">28.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>-1.7B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">56.2</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "yet",
            "cosyvoice2",
            "minimax20hd",
            "control",
            "wer",
            "speech",
            "data",
            "demonstrates",
            "strong",
            "emotion",
            "chinese",
            "benchmark",
            "cosyvoice",
            "↑uparrow",
            "batonvoice17b",
            "only",
            "feature",
            "generalization",
            "batonvoice",
            "performance",
            "↓downarrow",
            "acc",
            "sparktts",
            "training",
            "closesource",
            "zeroshot",
            "opensource",
            "minimax25hd",
            "higgs",
            "english",
            "tts",
            "uses",
            "seedtts",
            "minimax25turbo",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.T3\" title=\"Table 3 &#8227; 3.4 Cross-Lingual Generalization &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>-1.7B achieves a 56.2% accuracy on the Chinese emotion benchmark. This result is not only strong in absolute terms but also surpasses leading models that are either native to or heavily optimized for Chinese, such as CosyVoice (52.0%) and the closed-source Minimax-2.5-Turbo (50.6%). This performance is achieved without any Chinese instruction data, highlighting a key advantage of our &#8220;operationalism&#8221; paradigm.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The rise of Large Language Models (LLMs) is reshaping multimodel models, with speech synthesis being a prominent application. However, existing approaches often underutilize the linguistic intelligence of these models, typically failing to leverage their powerful instruction-following capabilities. This limitation hinders the model&#8217;s ability to follow text instructions for controllable Text-to-Speech&#160;(TTS). To address this, we propose a new paradigm inspired by &#8220;operationalism&#8221; that decouples instruction understanding from speech generation. We introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, a framework where an LLM acts as a &#8220;conductor&#8221;, understanding user instructions and generating a textual &#8220;plan&#8221; &#8211; explicit vocal features (e.g., pitch, energy). A separate TTS model, the &#8220;orchestra&#8221;, then generates the speech from these features. To realize this component, we develop <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a TTS model trained specifically for this task. Our experiments demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in controllable and\nemotional speech synthesis, outperforming strong open- and closed-source baselines. Notably, our approach enables remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to languages unseen during post-training. This demonstrates that objectifying speech into textual vocal features can more effectively unlock the linguistic intelligence of LLMs.</p>\n\n",
                "matched_terms": [
                    "control",
                    "performance",
                    "speech",
                    "model",
                    "feature",
                    "zeroshot",
                    "tts",
                    "demonstrates",
                    "strong",
                    "batonvoice",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid advancement of Large Language Models (LLMs) has catalyzed a paradigm shift in Multimodal Large Language Models&#160;(MLLMs), with frameworks now unifying text, images, and speech within a single model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib32\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib30\" title=\"\">2025</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib3\" title=\"\">2025</a>)</cite>. In Text-to-Speech (TTS), this has led to a new generation of systems that fine-tune a pre-trained LLM as a backbone to generate speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib27\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib7\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib33\" title=\"\">2025a</a>)</cite>. However, a critical yet underexplored question remains: <span class=\"ltx_text ltx_font_italic\">Are we fully leveraging the linguistic intelligence of LLMs in these TTS models?</span></p>\n\n",
                "matched_terms": [
                    "yet",
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing LLM-based TTS models primarily treat the LLM as a backbone. This approach typically involves designing a tokenizer to convert speech into discrete tokens and then training the model on large-scale datasets tailored to specific objectives. For instance, training a controllable TTS model necessitates extensive manual annotation of existing speech data to acquire the corresponding control labels and instructions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib6\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib5\" title=\"\">a</a>)</cite>, a process that is not only prohibitively expensive but also suffers from low inter-annotator agreement. We contend that this methodology largely bypasses the LLM&#8217;s inherent linguistic intelligence, such as its strong capabilities for complex context understanding and instruction following.</p>\n\n",
                "matched_terms": [
                    "control",
                    "only",
                    "speech",
                    "tts",
                    "data",
                    "strong",
                    "training",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, we draw inspiration from the principle of &#8220;operationalism&#8221;, where complex concepts are understood through quantifiable, interpretable operations. For instance, to analyze imperceptible ultrasound, we use sensors to extract quantifiable features like frequency and amplitude. We posit that controllable TTS can be transformed by operationalizing user instructions into the desired vocal features. This reframes the problem: the LLM first leverages its linguistic intelligence to understand instructions and generate explicit vocal features, which then serves as input for a subsequent TTS model. This approach allows us to circumvent the need for manually annotating speech with controllable labels.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To realize this vision, we introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, a novel TTS framework that decouples instruction understanding from speech generation, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S0.F1\" title=\"Figure 1 &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> employs an LLM as a &#8220;conductor&#8221;, which interprets the user&#8217;s instructions to explicit vocal features, like pitch and energy. This plan is then fed into a separate TTS model, the &#8220;orchestra&#8221;, which generates the final speech. The &#8220;orchestra&#8221; in our framework is <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a TTS model we trained specifically to synthesize high-quality speech conditioned on these textual vocal features.</p>\n\n",
                "matched_terms": [
                    "batonvoice",
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments validate the power of this decoupled approach. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in emotional speech synthesis, outperforming strong open- and closed-source models. For example, our 1.7B parameter model achieves an emotion accuracy of 57.6%, significantly surpassing all baselines. To verify our hypothesis, we show that stronger linguistic intelligence directly translates to superior synthesis: upgrading the &#8220;conductor&#8221; LLM from our 1.7B model to the more capable <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span> boosts the final model&#8217;s emotion accuracy from 29.8% to 57.6%. Furthermore, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> exhibits remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to Chinese, which is an unseen language during feature control training stage. This work not only advances controllable speech synthesis but also presents a promising new paradigm for MLLM development, demonstrating how objectifying modalities into text can more fully unlock the linguistic intelligence of LLMs.</p>\n\n",
                "matched_terms": [
                    "control",
                    "performance",
                    "only",
                    "speech",
                    "model",
                    "feature",
                    "zeroshot",
                    "strong",
                    "batonvoice",
                    "emotion",
                    "training",
                    "chinese",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a methodology for realizing this paradigm, including a novel data pipeline that automatically generates instruction-feature pairs, and we introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a specialized TTS model trained on this data to generate speech from the vocal features.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through extensive experiments, we demonstrate that our framework, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, achieves strong performance in controllable, expressive speech synthesis. It exhibits superior emotional control and remarkable zero-shot cross-lingual generalization performance, validating the effectiveness of our operationalism-inspired approach.</p>\n\n",
                "matched_terms": [
                    "control",
                    "performance",
                    "speech",
                    "zeroshot",
                    "batonvoice",
                    "strong",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, a controllable TTS framework capable of synthesizing speech that adheres to arbitrary text-based instructions. Adopting an operationalist stance, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> leverages LLMs to interpret users&#8217; instructions into a JSON list of fine-grained vocal features. The core of this framework is <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a TTS model trained specifically developed to synthesize speech from these features. We first describe the overall framework and its inference process, followed by a detailed introduction of the <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> architecture and its three-stage training pipeline.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "batonvoice",
                    "training",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide the prompt template utilized for this feature prediction in the Appendix. Subsequently, in the second stage, this feature list <math alttext=\"F_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">F_{v}</annotation></semantics></math>, along with the original text <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>, is fed into <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> to synthesize the final speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We now detail the architecture of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, the model responsible for generating speech from the specified feature list. Inspired by recent advancements such as CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib6\" title=\"\">2024b</a>)</cite>, the architecture of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> comprises two primary components: an LLM backbone and a pre-trained speech decoder.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "speech",
                    "model",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the LLM backbone, we employ representative open-source models, specifically Qwen3-1.7B and Qwen2.5-0.5B. As will be demonstrated in our experimental section, our proposed method is effective across LLM backbones of varying capacities. The LLM is tasked with autoregressively generating a sequence that includes the input text to be synthesized, the corresponding speech features (i.e., the vocal plan), and the discrete speech tokens that realize this plan. The structure of this input sequence during the Supervised Fine-Tuning (SFT) stage is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Overall Framework and Inference Process &#8227; 2 BatonVoice: A Framework for Controllable TTS &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. It is important to note that while the features are part of the training sequence, during inference, they are generated by an external LLM as previously described.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the final synthesis step, we leverage the speech decoder from the publicly available CosyVoice2 model. This decoder converts the discrete speech tokens produced by the LLM into a high-quality speech. It consists of a speech token encoder, a conditional flow matching model, and a HiFi-GAN vocoder. The flow matching model generates Mel spectrograms conditioned on the discrete speech tokens, and the HiFi-GAN vocoder then converts these spectrograms into the final speech. By utilizing a pre-trained speech decoder, we can focus our training efforts exclusively on teaching the LLM to control speech features through language. Consequently, the speech decoder remains frozen throughout our training process.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "control",
                    "speech",
                    "training",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a three-stage training pipeline designed to incrementally build the TTS model&#8217;s feature control capability:</p>\n\n",
                "matched_terms": [
                    "tts",
                    "control",
                    "training",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Pre-Training.</span> Establishes a foundational TTS capability by training the LLM to generate speech tokens from text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Supervised Fine-Tuning&#160;(SFT).</span> Teaches the LLM to generate speech conditioned on specific vocal features (<math alttext=\"F_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">F_{v}</annotation></semantics></math>), enabling fine-grained control.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Preference Optimization&#160;(PO).</span> Refines the model by preference optimizing, mitigating failure modes and enhancing the precision of feature control.</p>\n\n",
                "matched_terms": [
                    "model",
                    "control",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective of this stage is to equip the LLM with a fundamental text-to-speech capability, providing a robust weight initialization for subsequent stages. We use a large-scale corpus of speech-text pairs, <math alttext=\"\\mathcal{D}_{\\text{pretrain}}=\\{(x_{i},S_{i})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>pretrain</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{pretrain}}=\\{(x_{i},S_{i})\\}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is the transcript and <math alttext=\"S_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">S_{i}</annotation></semantics></math> represents the corresponding discrete speech tokens. The model, denoted as policy <math alttext=\"\\pi_{\\text{pre}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>pre</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{pre}}</annotation></semantics></math>, is trained using a standard causal language modeling objective to predict the next token autoregressively over the concatenated sequence of text and speech tokens. The training objective is:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"Y=[x;S]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mi>x</mi><mo>;</mo><mi>S</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=[x;S]</annotation></semantics></math> is the concatenated sequence. This process trains the model on both text-to-text and text-to-speech-token generation, establishing a strong baseline.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SFT stage aims to instill fine-grained controllability by training the model to generate speech conditioned on both the transcript and a set of explicit, verbalized vocal features. This process, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Overall Framework and Inference Process &#8227; 2 BatonVoice: A Framework for Controllable TTS &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, trains the model to associate textual vocal features with corresponding discrete speech tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During this stage, we fine-tune the policy <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math>. The input sequence is formed by concatenating the transcript <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>, the verbalized features <math alttext=\"F_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">F_{v}</annotation></semantics></math>, and the speech tokens <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m4\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>. The model is trained to predict the next token autoregressively by minimizing the cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"Y=[x;F_{v};S]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m5\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mi>x</mi><mo>;</mo><msub><mi>F</mi><mi>v</mi></msub><mo>;</mo><mi>S</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=[x;F_{v};S]</annotation></semantics></math> is the concatenated sequence. This objective teaches the model to generate speech tokens <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m6\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> that adhere to the vocal plan specified by <math alttext=\"F_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">F_{v}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although SFT offers a direct mechanism for control, the resulting model, <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math>, is still prone to certain failure modes. These include a high Word Error Rate (WER), an unnaturally slow speaking rate and insufficient expressiveness. To overcome these limitations, we employ a subsequent preference optimazation stage. The central principle is to construct a preference dataset, <math alttext=\"\\mathcal{D}_{\\text{pref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>pref</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{pref}}</annotation></semantics></math>, designed to align the model&#8217;s outputs with more desirable vocal features, crucially without the need for manually annotated expressive data.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model",
                    "control",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Initial Generation and Rejection Sampling</span>: For each text prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> in a corpus <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math>, we use the pre-trained model <math alttext=\"\\pi_{\\text{pre}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>pre</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{pre}}</annotation></semantics></math> from Stage 1 to synthesize an speech sample <math alttext=\"s_{\\text{base}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m4\" intent=\":literal\"><semantics><msub><mi>s</mi><mtext>base</mtext></msub><annotation encoding=\"application/x-tex\">s_{\\text{base}}</annotation></semantics></math>. Samples are designated as <em class=\"ltx_emph ltx_font_italic\">rejected</em> (<math alttext=\"s_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m5\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">s_{l}</annotation></semantics></math>) if they exhibit a high WER or a slow speech rate (SR). The corresponding speech tokens <math alttext=\"S_{\\text{base}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mtext>base</mtext></msub><annotation encoding=\"application/x-tex\">S_{\\text{base}}</annotation></semantics></math> are stored as the rejected sequence <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m7\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preferred Data Construction</span>: For each text <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> corresponding to a rejected sample, we use our SFT model <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math> to generate a new candidate speech <math alttext=\"\\hat{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{s}</annotation></semantics></math>. These candidates are accepted as <em class=\"ltx_emph ltx_font_italic\">chosen</em> samples (<math alttext=\"s_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m4\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">s_{w}</annotation></semantics></math>) if they meet the quality criteria (low WER and adequate SR). The corresponding features and tokens <math alttext=\"(F_{v,w},S_{w})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(F_{v,w},S_{w})</annotation></semantics></math> are stored.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "model",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference Dataset Construction</span>: This filtering process yields pairs of chosen sequences <math alttext=\"(F_{v,w},S_{w})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(F_{v,w},S_{w})</annotation></semantics></math> and rejected sequences <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math>. To create a controlled comparison, we form preference tuples where the model learns to prefer <math alttext=\"S_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">S_{w}</annotation></semantics></math> over <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math> under the same vocal plan, <math alttext=\"F_{v,w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">F_{v,w}</annotation></semantics></math>. This setup creates a powerful learning signal: because <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math> was generated without knowledge of <math alttext=\"F_{v,w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">F_{v,w}</annotation></semantics></math>, while <math alttext=\"S_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m8\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">S_{w}</annotation></semantics></math> was explicitly conditioned on it, teaching the model to prefer <math alttext=\"S_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m9\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">S_{w}</annotation></semantics></math> over <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m10\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math> not only improves general quality but also implicitly reinforces the model&#8217;s ability to follow the specified vocal features. The final dataset consists of tuples: <math alttext=\"\\mathcal{D}_{\\text{pref}}=\\{(x_{i},F_{v,w,i},S_{w,i},S_{l,i})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>pref</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mi>w</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mi>l</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{pref}}=\\{(x_{i},F_{v,w,i},S_{w,i},S_{l,i})\\}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "only",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"r_{\\theta}(x,F_{v},S)=\\beta\\log\\big(\\pi_{\\theta}(S\\mid x,F_{v})/\\pi_{\\text{ref}}(S\\mid x,F_{v})\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p4.m6\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>r</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi>F</mi><mi>v</mi></msub><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>&#946;</mi><mo lspace=\"0.167em\" rspace=\"0em\">&#8203;</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><mrow><mrow><msub><mi>&#960;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>S</mi><mo>&#8739;</mo><mrow><mi>x</mi><mo>,</mo><msub><mi>F</mi><mi>v</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><msub><mi>&#960;</mi><mtext>ref</mtext></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>S</mi><mo>&#8739;</mo><mrow><mi>x</mi><mo>,</mo><msub><mi>F</mi><mi>v</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">r_{\\theta}(x,F_{v},S)=\\beta\\log\\big(\\pi_{\\theta}(S\\mid x,F_{v})/\\pi_{\\text{ref}}(S\\mid x,F_{v})\\big)</annotation></semantics></math> is the implicit reward. Term 1 anchors the policy to the SFT model for chosen samples, while Term 2 maximizes the preference margin. This dual objective allows the model to mitigate common failure modes without requiring any explicitly labeled expressive data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pre-training stage equips the LLM with the fundamental capability of converting text into a corresponding sequence of speech tokens, establishing a strong foundation for standard TTS before introducing complex instruction-following behavior. We use the VoxBox dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib29\" title=\"\">2025b</a>)</cite>, a large-scale, multi-speaker English speech corpus of approximately 103K hours. The speech is tokenized into discrete vocal units using the official CosyVoice2 tokenizer. To maximize throughput, we pack tokenized sequences into 4096-token chunks, reducing padding overhead. Pre-training is conducted on 80 GPUs for 3 epochs (approximately one day), using AdamW with a learning rate of 1e-4, 500 warmup steps, a global batch size of 640, and DeepSpeed ZeRO-2 for memory optimization.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "english",
                    "speech",
                    "tts",
                    "strong"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our post-training process consists of SFT and PO. A key challenge in preparing the SFT data is that our speech decoder cannot perfectly reconstruct original speech from its quantized tokens. To ensure the vocal features are faithfully synthesizable, we derive them from speech that has been reconstructed by the decoder itself.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our SFT dataset is compiled from two primary sources. First, we take a diverse collection of expressive speech corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib26\" title=\"\">2017</a>; Nagraniy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib19\" title=\"\">2017</a>; Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib2\" title=\"\">2018</a>; Richter et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib21\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib20\" title=\"\">2023</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib31\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib28\" title=\"\">2025a</a>)</cite>, pass the speech through our decoder for reconstruction, and then extract features from the synthesized output. Second, we collect colloquial sentences from the Synthetic-Persona-Chat dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jandaghi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib11\" title=\"\">2024</a>)</cite> and synthesize them. We then apply a filtering process to the combined data, removing samples with a high Word Error Rate (WER), which indicates potential misalignments, or an abnormally slow speaking rate. <math alttext=\"\\tau_{\\text{wer\\_high}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mtext>wer_high</mtext></msub><annotation encoding=\"application/x-tex\">\\tau_{\\text{wer\\_high}}</annotation></semantics></math> is 0.1 and <math alttext=\"\\tau_{\\text{sr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mtext>sr</mtext></msub><annotation encoding=\"application/x-tex\">\\tau_{\\text{sr}}</annotation></semantics></math> is 1.5 words per seconds.This results in a final SFT dataset of 377,619 utterances, totaling over 500 hours (see Appendix for a detailed distribution). For the PO stage, we collected a dataset of 9,823 preference samples.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The feature extraction pipeline for this data begins with grounding features in semantically meaningful units. We first obtain word-level timestamps for each speech sample using a pre-trained model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self\" title=\"\">https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self</a></span></span></span>. Since individual words are often too short to carry significant prosodic information, we merge adjacent words into segments until each segment&#8217;s duration exceeds a one-second threshold, ensuring a stable and analyzable prosodic contour. Finally, we use the Parselmouth library&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/YannickJadoul/Parselmouth\" title=\"\">https://github.com/YannickJadoul/Parselmouth</a></span></span></span> to extract a set of vocal features from these segments. The model is trained with SFT for 3 epochs, followed by 1 epoch of APO-down.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "feature",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TTS Intelligibility</span>: We use the test set from the <span class=\"ltx_text ltx_font_bold\">Seed-TTS</span> benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib1\" title=\"\">2024</a>)</cite>, which is designed for assessing speech synthesis from short speech prompts. Performance is measured by <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, calculated with pre-trained ASR models&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span>. A lower WER score signifies higher intelligibility.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "wer",
                    "speech",
                    "tts",
                    "seedtts",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Control</span>: This is assessed on a curated test set from the <span class=\"ltx_text ltx_font_bold\">Emotion</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saravia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib23\" title=\"\">2018</a>)</cite>. We use includes 100 samples for each of five emotions (joy, sadness, anger, surprise, and fear). We measure performance using <span class=\"ltx_text ltx_font_bold\">Emotion Classification Accuracy</span>. This metric is derived by employing Google&#8217;s Gemini-2.5-Pro to classify the emotion of the synthesized speech. A higher accuracy indicates a greater success rate in generating perceptually accurate emotional speech. The prompt template for this evaluation is provided in the Appendix.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "speech",
                    "control",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Main Results &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>-1.7B achieves 57.6% accuracy on the Emotion benchmark, surpassing the strongest closed-source baseline, Minimax-2.5-HD (48.6%), by 9.0 absolute points. It also outperforms all open-source systems by a wide margin, e.g., +13.8 points over CosyVoice (43.8%). On Seed-TTS, our 1.7B model attains a WER of 2.5 &#8211; competitive with high-quality open models (better than CosyVoice at 3.4, slightly above CosyVoice2 at 2.1 and Spark-TTS at 1.9) and within a small gap of the closed-source Minimax series (1.5). These results validate that our decoupled &#8220;conductor&#8211;orchestra&#8221; design substantially enhances emotional expressiveness without sacrificing intelligibility.</p>\n\n",
                "matched_terms": [
                    "cosyvoice2",
                    "minimax25hd",
                    "wer",
                    "model",
                    "sparktts",
                    "seedtts",
                    "emotion",
                    "benchmark",
                    "cosyvoice",
                    "batonvoice17b",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> framework achieves these results with 0 hours of manually annotated instruction data, in contrast to CosyVoice and CosyVoice2, which use 556 and 1,500 hours respectively yet underperform on emotion accuracy (43.8% and 37.8%). Preference optimization over textual vocal plans yields consistent improvements over SFT alone: for the 1.7B model, emotion accuracy increases from 52.2% (SFT) to 57.6% (Instruct, +5.4 points) while WER improves from 2.9 to 2.5. Even at 0.5B, instruction tuning further boosts accuracy from 51.6% to 52.8% (+1.2). These gains directly confirm the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>.</p>\n\n",
                "matched_terms": [
                    "yet",
                    "cosyvoice2",
                    "wer",
                    "model",
                    "data",
                    "batonvoice",
                    "emotion",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving from 0.5B to 1.7B parameters improves emotion accuracy from 52.8% to 57.6% (+4.8) and reduces WER from 2.9 to 2.5 for the instruction-tuned models. This trend demonstrates the scalability of our method, and showcasing its consistent performance benefits across different model sizes..</p>\n\n",
                "matched_terms": [
                    "performance",
                    "wer",
                    "demonstrates",
                    "emotion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess our model&#8217;s performance on controllable TTS with free-form instructions, we create a specialized test set. We begin by sourcing 50 diverse social situations from the Social IQa benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib22\" title=\"\">2019</a>)</cite>, chosen for its rich contextual and emotional nuance. For each situation, we utilize <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span> to generate a challenging test case. The model is prompted to produce two outputs: first, a detailed, role-playing style instruction framed in a second-person narrative, which specifies the desired persona and delivery style. Second, it generates a corresponding target utterance to be synthesized. This pipeline yield a high-quality benchmark of 50 pairs, specifically designed to test the model&#8217;s ability to follow complex, descriptive instructions beyond simple labels.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts",
                    "performance",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We found that long instructions were incorrectly synthesized by CosyVoice. To mitigate this, we use Gemini 2.5 Pro to map each detailed instruction to a discrete emotion label (Neutral + 6 Ekman emotions), which was then fed to CosyVoice. This label-based approach was also necessary for Minimax 2.5, which only accepts emotion labels as input.\nWe all the prompt template in the appendix.\nWe performed a human evaluation comparing <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> with the top-performing open-source (CosyVoice) and closed-source (Minimax-2.5-HD) models. The evaluation involved three trained annotators with a Cohen&#8217;s Kappa of 0.61. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Human Evaluation of Instruction-Following &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves performance comparable to CosyVoice but is outperformed by the commercial system Minimax-2.5-HD, falling short in aspects of fluency and naturalness.</p>\n\n",
                "matched_terms": [
                    "minimax25hd",
                    "performance",
                    "only",
                    "batonvoice",
                    "emotion",
                    "cosyvoice",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A significant and surprising finding is the model&#8217;s ability to generalize to languages not seen during the <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> post-training stage. We evaluated this by testing on a Chinese emotion benchmark, employing the same methodology as the English evaluation, with the text and instructions translated into Chinese by <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span>. Notably, this cross-lingual generalization occurs despite the post-training stage being conducted exclusively on English data, demonstrating a strong zero-shot transfer capability.</p>\n\n",
                "matched_terms": [
                    "english",
                    "chinese",
                    "data",
                    "zeroshot",
                    "strong",
                    "emotion",
                    "benchmark",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a series of in-depth analyses to better understand the capabilities of <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>. Otherwise stated, we report the results of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>-1.7B on the English Emotion benchmark.</p>\n\n",
                "matched_terms": [
                    "batonvoice",
                    "english",
                    "emotion",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform a step-by-step ablation to examine the effectiveness of each stage in our proposed <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> framework. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 3.5 Component Analysis &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, the base model, trained only on foundational TTS without any instruction tuning, yields poor performance on the English Emotion benchmark &#8211; achieving just 23.2% accuracy for the 1.7B model. Incorporating the SFT stage causes a dramatic improvement, boosting the accuracy to 52.2% (+29.0 points), showing that teaching the model to generate and condition on verbalized vocal plans is key to enabling stylistic control. Adding the APO-based preference optimization further improves performance to 57.6% (+5.4 over SFT), illustrating the importance of our post-training strategy. Consistent gains are observed for the smaller 0.5B model (25.8 <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 51.6 <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 52.8), demonstrating that the framework is effective across model scales. These results validate the design of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> in sequentially teaching foundational TTS capability and improving control quality.</p>\n\n",
                "matched_terms": [
                    "english",
                    "control",
                    "performance",
                    "only",
                    "tts",
                    "emotion",
                    "benchmark",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate how our framework leverages the linguistic intelligence of Large Language Models (LLMs), we performed an experiment to measure the impact of the vocal feature generator on final synthesis quality. We used a fixed <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> model and generated vocal plans at inference time using a range of LLMs with varying capabilities. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.5 Component Analysis &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, the results show a clear, positive correlation between the performance of the LLM and the emotion accuracy of the synthesized speech. The accuracy climbs steadily from 29.8% with Qwen3-1.7B to 57.6% with Gemini-2.5-Pro, with intermediate models like Qwen3-80B (39.8%) and Qwen3-Max (47.8%) falling along this expected trajectory.\nThese findings strongly support our core claim: representing speech as vocal features allows the synthesis model to directly benefit from advances in LLMs. This highlights a key advantage of our decoupled &#8220;conductor&#8211;orchestra&#8221; design: its modularity. Even our compact 1.7B model can tap into the power of a much larger model like Gemini-2.5-Pro at inference time, effectively upgrading its expressive capability without any modification to the model.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "feature",
                    "batonvoice",
                    "emotion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable speech synthesis is typically classified into three primary paradigms. The first, style tagging, employs discrete labels (e.g., emotion, gender) to guide the synthesis process&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib7\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib29\" title=\"\">2025b</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib33\" title=\"\">2025a</a>)</cite>. While conceptually simple, this approach is restricted to a predefined set of styles, which fundamentally limits its expressive range.\nThe second paradigm leverages reference speech to enable few-shot or zero-shot speaker adaptation. This is accomplished by extracting speaker embeddings from short speech samples and conditioning the TTS decoder on them &#8211; a technique proven effective for voice cloning and style transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib14\" title=\"\">2024</a>; Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib12\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib15\" title=\"\">2024</a>)</cite>.\nThe third and most flexible paradigm, instruction-guided control, conceptualizes TTS as a task of interpreting natural language instructions. Frameworks such as VoxInstruct exemplify this approach, guiding synthesis\nwith free-form instructions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib6\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib5\" title=\"\">a</a>)</cite>.\nHowever, these instruction-following methods are constrained by the high cost and difficulty of creating large-scale, annotated instruction-speech datasets, which limits their generalization and performance.</p>\n\n",
                "matched_terms": [
                    "control",
                    "performance",
                    "speech",
                    "tts",
                    "zeroshot",
                    "emotion",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, our approach circumvents the need for manually annotated data by leveraging a powerful LLM. This enables robust, zero-shot generalization to unseen instructions, generating vocal features that exhibit high fidelity to the prompts while affording a high degree of control. Our method thus addresses the key limitations of data scarcity and annotation cost in instruction-guided TTS, showing significant promise for future research in expressive and controllable speech synthesis.</p>\n\n",
                "matched_terms": [
                    "control",
                    "speech",
                    "tts",
                    "data",
                    "generalization",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remarkable reasoning capabilities of LLMs have catalyzed extensive research into extending these faculties to multimodal domains. Early efforts sought to enhance multimodal understanding by employing techniques such as reinforcement learning to better align visual and textual representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib8\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib10\" title=\"\">2025b</a>; Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib18\" title=\"\">2025</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib24\" title=\"\">2025</a>)</cite>. More recent and prominent approaches aim for a deeper integration of reasoning. One prominent direction integrates multimodal information as intermediate steps within a reasoning chain, analogous to a &#8220;chain of thought&#8221;, to derive conclusions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib25\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib35\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib34\" title=\"\">2025b</a>)</cite>. Another emerging strategy involves performing explicit, text-based reasoning prior to the final multimodal generation, thereby ensuring the output is logically grounded and coherent with the input prompt&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib16\" title=\"\">2025</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib13\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib9\" title=\"\">2025a</a>)</cite>. While powerful, these methods typically rely on training large-scale, end-to-end multimodal models &#8211; a process that is computationally intensive and demands vast quantities of aligned data.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we address a key limitation in current speech synthesis systems: the underutilization of the linguistic intelligence of LLMs. We introduce a new paradigm inspired by &#8220;operationalism&#8221;, which decouples instruction understanding from speech generation by first translating instructions into quantifiable, interpretable vocal features. Our framework, <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">BatonVoice</span>, embodies this principle by using LLMs to generate a vocal &#8220;plan&#8221;, which is then fed into a TTS model. We train this model using a three-stage training pipeline that requires no manual instruction data. Our empirical results demonstrate the effectiveness of this approach. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in emotional speech synthesis and shows that its capabilities scale positively with the linguistic intelligence of LLMs. Furthermore, it exhibits powerful zero-shot cross-lingual generalization.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "model",
                    "tts",
                    "data",
                    "zeroshot",
                    "strong",
                    "batonvoice",
                    "training",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our SFT dataset is a comprehensive collection curated to teach the model how to generate vocal plans from text. It comprises 377,619 utterances, totaling over 500 hours of speech, and is compiled from two primary sources as detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#A2.T4\" title=\"Table 4 &#8227; B.1 Data Source &#8227; Appendix B Experimental Details &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each sample from these corpora, we first passed the original speech through our pre-trained vocal decoder to obtain a reconstructed waveform. We then extracted the textual vocal features (i.e., the vocal plan) from this synthesized output. This reconstruction step ensures that the vocal features are derived from a distribution that our TTS &#8221;orchestra&#8221; model can faithfully render.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance the linguistic diversity and colloquial nature of our training data, we incorporated sentences from the Synthetic-Persona-Chat dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jandaghi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib11\" title=\"\">2024</a>)</cite>. We synthesized these conversational sentences using a high-quality baseline TTS model and then processed them through the same feature extraction pipeline described above. This source contributes the largest portion of our dataset, ensuring the model is exposed to a wide array of everyday language.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "feature",
                    "data",
                    "training",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantitatively assess the feature control capability of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, we conducted a reconstruction experiment using 384 samples from the RAVDESS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Livingstone &amp; Russo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib17\" title=\"\">2018</a>)</cite>. The evaluation protocol was as follows: for each original speech sample, we first generated a &#8221;reconstructed&#8221; version by passing it through our pre-trained vocal decoder. We then extracted the textual vocal plan (i.e., the vocal features) from this reconstructed speech. This plan was subsequently fed into <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> to synthesize the final speech output. The fidelity of the synthesis was measured by calculating the Mel-Cepstral Distortion (MCD) between the synthesized speech and the reconstructed speech, with a lower MCD indicating higher fidelity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control",
                    "feature"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our analysis, we first compared different formats for representing the vocal plan. We found that our proposed numerical representation significantly outperformed a qualitative, caption-based description, demonstrating that a structured, quantitative format allows for more precise control. Remarkably, the MCD achieved with our numerical plan was even lower than that of the vocoder resynthesis baseline (i.e., the MCD between the reconstructed and the original speech). This suggests that <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> not only faithfully renders the vocal plan but can also compensate for some information loss introduced during the initial decoding stage. Furthermore, to validate the design of our vocal plan, we performed an ablation study by systematically removing individual features (e.g., pitch, energy) from the plan before feeding it to <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>. We observed a consistent performance degradation (i.e., an increase in MCD) upon the removal of any feature. This result confirms that all components of our proposed vocal representation are necessary and contribute meaningfully to the final synthesis quality.</p>\n\n",
                "matched_terms": [
                    "control",
                    "performance",
                    "only",
                    "speech",
                    "feature"
                ]
            }
        ]
    },
    "A2.T4": {
        "source_file": "BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs",
        "caption": "Table 4: Details of SFT training data.",
        "body": "Dataset\n# Samples\n\n\n\n\nVCTK\n23,677\n\n\nVoxCeleb1&2\n89,520\n\n\nEARS\n14,159\n\n\nExpresso\n12,269\n\n\nEmoVoice-DB\n21,050\n\n\nCapSpeech-AgentDB\n9,625\n\n\nSynthetic-Persona-Chat\n20,7319\n\n\nAll\n377,619",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\"># Samples</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">VCTK</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">23,677</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">VoxCeleb1&amp;2</th>\n<td class=\"ltx_td ltx_align_right\">89,520</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">EARS</th>\n<td class=\"ltx_td ltx_align_right\">14,159</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Expresso</th>\n<td class=\"ltx_td ltx_align_right\">12,269</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">EmoVoice-DB</th>\n<td class=\"ltx_td ltx_align_right\">21,050</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CapSpeech-AgentDB</th>\n<td class=\"ltx_td ltx_align_right\">9,625</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Synthetic-Persona-Chat</th>\n<td class=\"ltx_td ltx_align_right\">20,7319</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">All</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">377,619</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "samples",
            "ears",
            "emovoicedb",
            "all",
            "vctk",
            "data",
            "sft",
            "details",
            "voxceleb12",
            "training",
            "syntheticpersonachat",
            "expresso",
            "capspeechagentdb",
            "dataset"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our SFT dataset is a comprehensive collection curated to teach the model how to generate vocal plans from text. It comprises 377,619 utterances, totaling over 500 hours of speech, and is compiled from two primary sources as detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#A2.T4\" title=\"Table 4 &#8227; B.1 Data Source &#8227; Appendix B Experimental Details &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Existing LLM-based TTS models primarily treat the LLM as a backbone. This approach typically involves designing a tokenizer to convert speech into discrete tokens and then training the model on large-scale datasets tailored to specific objectives. For instance, training a controllable TTS model necessitates extensive manual annotation of existing speech data to acquire the corresponding control labels and instructions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib6\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib5\" title=\"\">a</a>)</cite>, a process that is not only prohibitively expensive but also suffers from low inter-annotator agreement. We contend that this methodology largely bypasses the LLM&#8217;s inherent linguistic intelligence, such as its strong capabilities for complex context understanding and instruction following.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments validate the power of this decoupled approach. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in emotional speech synthesis, outperforming strong open- and closed-source models. For example, our 1.7B parameter model achieves an emotion accuracy of 57.6%, significantly surpassing all baselines. To verify our hypothesis, we show that stronger linguistic intelligence directly translates to superior synthesis: upgrading the &#8220;conductor&#8221; LLM from our 1.7B model to the more capable <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span> boosts the final model&#8217;s emotion accuracy from 29.8% to 57.6%. Furthermore, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> exhibits remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to Chinese, which is an unseen language during feature control training stage. This work not only advances controllable speech synthesis but also presents a promising new paradigm for MLLM development, demonstrating how objectifying modalities into text can more fully unlock the linguistic intelligence of LLMs.</p>\n\n",
                "matched_terms": [
                    "training",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the LLM backbone, we employ representative open-source models, specifically Qwen3-1.7B and Qwen2.5-0.5B. As will be demonstrated in our experimental section, our proposed method is effective across LLM backbones of varying capacities. The LLM is tasked with autoregressively generating a sequence that includes the input text to be synthesized, the corresponding speech features (i.e., the vocal plan), and the discrete speech tokens that realize this plan. The structure of this input sequence during the Supervised Fine-Tuning (SFT) stage is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Overall Framework and Inference Process &#8227; 2 BatonVoice: A Framework for Controllable TTS &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. It is important to note that while the features are part of the training sequence, during inference, they are generated by an external LLM as previously described.</p>\n\n",
                "matched_terms": [
                    "training",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SFT stage aims to instill fine-grained controllability by training the model to generate speech conditioned on both the transcript and a set of explicit, verbalized vocal features. This process, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Overall Framework and Inference Process &#8227; 2 BatonVoice: A Framework for Controllable TTS &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, trains the model to associate textual vocal features with corresponding discrete speech tokens.</p>\n\n",
                "matched_terms": [
                    "training",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although SFT offers a direct mechanism for control, the resulting model, <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math>, is still prone to certain failure modes. These include a high Word Error Rate (WER), an unnaturally slow speaking rate and insufficient expressiveness. To overcome these limitations, we employ a subsequent preference optimazation stage. The central principle is to construct a preference dataset, <math alttext=\"\\mathcal{D}_{\\text{pref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>pref</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{pref}}</annotation></semantics></math>, designed to align the model&#8217;s outputs with more desirable vocal features, crucially without the need for manually annotated expressive data.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preferred Data Construction</span>: For each text <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> corresponding to a rejected sample, we use our SFT model <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math> to generate a new candidate speech <math alttext=\"\\hat{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{s}</annotation></semantics></math>. These candidates are accepted as <em class=\"ltx_emph ltx_font_italic\">chosen</em> samples (<math alttext=\"s_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m4\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">s_{w}</annotation></semantics></math>) if they meet the quality criteria (low WER and adequate SR). The corresponding features and tokens <math alttext=\"(F_{v,w},S_{w})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(F_{v,w},S_{w})</annotation></semantics></math> are stored.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "data",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"r_{\\theta}(x,F_{v},S)=\\beta\\log\\big(\\pi_{\\theta}(S\\mid x,F_{v})/\\pi_{\\text{ref}}(S\\mid x,F_{v})\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p4.m6\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>r</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi>F</mi><mi>v</mi></msub><mo>,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>&#946;</mi><mo lspace=\"0.167em\" rspace=\"0em\">&#8203;</mo><mrow><mi>log</mi><mo>&#8289;</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><mrow><mrow><mrow><msub><mi>&#960;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>S</mi><mo>&#8739;</mo><mrow><mi>x</mi><mo>,</mo><msub><mi>F</mi><mi>v</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><msub><mi>&#960;</mi><mtext>ref</mtext></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>S</mi><mo>&#8739;</mo><mrow><mi>x</mi><mo>,</mo><msub><mi>F</mi><mi>v</mi></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">r_{\\theta}(x,F_{v},S)=\\beta\\log\\big(\\pi_{\\theta}(S\\mid x,F_{v})/\\pi_{\\text{ref}}(S\\mid x,F_{v})\\big)</annotation></semantics></math> is the implicit reward. Term 1 anchors the policy to the SFT model for chosen samples, while Term 2 maximizes the preference margin. This dual objective allows the model to mitigate common failure modes without requiring any explicitly labeled expressive data.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "data",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our post-training process consists of SFT and PO. A key challenge in preparing the SFT data is that our speech decoder cannot perfectly reconstruct original speech from its quantized tokens. To ensure the vocal features are faithfully synthesizable, we derive them from speech that has been reconstructed by the decoder itself.</p>\n\n",
                "matched_terms": [
                    "data",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our SFT dataset is compiled from two primary sources. First, we take a diverse collection of expressive speech corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib26\" title=\"\">2017</a>; Nagraniy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib19\" title=\"\">2017</a>; Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib2\" title=\"\">2018</a>; Richter et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib21\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib20\" title=\"\">2023</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib31\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib28\" title=\"\">2025a</a>)</cite>, pass the speech through our decoder for reconstruction, and then extract features from the synthesized output. Second, we collect colloquial sentences from the Synthetic-Persona-Chat dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jandaghi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib11\" title=\"\">2024</a>)</cite> and synthesize them. We then apply a filtering process to the combined data, removing samples with a high Word Error Rate (WER), which indicates potential misalignments, or an abnormally slow speaking rate. <math alttext=\"\\tau_{\\text{wer\\_high}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mtext>wer_high</mtext></msub><annotation encoding=\"application/x-tex\">\\tau_{\\text{wer\\_high}}</annotation></semantics></math> is 0.1 and <math alttext=\"\\tau_{\\text{sr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mtext>sr</mtext></msub><annotation encoding=\"application/x-tex\">\\tau_{\\text{sr}}</annotation></semantics></math> is 1.5 words per seconds.This results in a final SFT dataset of 377,619 utterances, totaling over 500 hours (see Appendix for a detailed distribution). For the PO stage, we collected a dataset of 9,823 preference samples.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "data",
                    "sft",
                    "syntheticpersonachat",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The feature extraction pipeline for this data begins with grounding features in semantically meaningful units. We first obtain word-level timestamps for each speech sample using a pre-trained model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self\" title=\"\">https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self</a></span></span></span>. Since individual words are often too short to carry significant prosodic information, we merge adjacent words into segments until each segment&#8217;s duration exceeds a one-second threshold, ensuring a stable and analyzable prosodic contour. Finally, we use the Parselmouth library&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/YannickJadoul/Parselmouth\" title=\"\">https://github.com/YannickJadoul/Parselmouth</a></span></span></span> to extract a set of vocal features from these segments. The model is trained with SFT for 3 epochs, followed by 1 epoch of APO-down.</p>\n\n",
                "matched_terms": [
                    "data",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Control</span>: This is assessed on a curated test set from the <span class=\"ltx_text ltx_font_bold\">Emotion</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saravia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib23\" title=\"\">2018</a>)</cite>. We use includes 100 samples for each of five emotions (joy, sadness, anger, surprise, and fear). We measure performance using <span class=\"ltx_text ltx_font_bold\">Emotion Classification Accuracy</span>. This metric is derived by employing Google&#8217;s Gemini-2.5-Pro to classify the emotion of the synthesized speech. A higher accuracy indicates a greater success rate in generating perceptually accurate emotional speech. The prompt template for this evaluation is provided in the Appendix.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> framework achieves these results with 0 hours of manually annotated instruction data, in contrast to CosyVoice and CosyVoice2, which use 556 and 1,500 hours respectively yet underperform on emotion accuracy (43.8% and 37.8%). Preference optimization over textual vocal plans yields consistent improvements over SFT alone: for the 1.7B model, emotion accuracy increases from 52.2% (SFT) to 57.6% (Instruct, +5.4 points) while WER improves from 2.9 to 2.5. Even at 0.5B, instruction tuning further boosts accuracy from 51.6% to 52.8% (+1.2). These gains directly confirm the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remarkable reasoning capabilities of LLMs have catalyzed extensive research into extending these faculties to multimodal domains. Early efforts sought to enhance multimodal understanding by employing techniques such as reinforcement learning to better align visual and textual representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib8\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib10\" title=\"\">2025b</a>; Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib18\" title=\"\">2025</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib24\" title=\"\">2025</a>)</cite>. More recent and prominent approaches aim for a deeper integration of reasoning. One prominent direction integrates multimodal information as intermediate steps within a reasoning chain, analogous to a &#8220;chain of thought&#8221;, to derive conclusions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Su et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib25\" title=\"\">2025</a>; Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib35\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib34\" title=\"\">2025b</a>)</cite>. Another emerging strategy involves performing explicit, text-based reasoning prior to the final multimodal generation, thereby ensuring the output is logically grounded and coherent with the input prompt&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib16\" title=\"\">2025</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib13\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib9\" title=\"\">2025a</a>)</cite>. While powerful, these methods typically rely on training large-scale, end-to-end multimodal models &#8211; a process that is computationally intensive and demands vast quantities of aligned data.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we address a key limitation in current speech synthesis systems: the underutilization of the linguistic intelligence of LLMs. We introduce a new paradigm inspired by &#8220;operationalism&#8221;, which decouples instruction understanding from speech generation by first translating instructions into quantifiable, interpretable vocal features. Our framework, <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">BatonVoice</span>, embodies this principle by using LLMs to generate a vocal &#8220;plan&#8221;, which is then fed into a TTS model. We train this model using a three-stage training pipeline that requires no manual instruction data. Our empirical results demonstrate the effectiveness of this approach. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in emotional speech synthesis and shows that its capabilities scale positively with the linguistic intelligence of LLMs. Furthermore, it exhibits powerful zero-shot cross-lingual generalization.</p>\n\n",
                "matched_terms": [
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">EARS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Richter et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib21\" title=\"\">2024</a>)</cite>, Expresso&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib20\" title=\"\">2023</a>)</cite>, and EmoVoice-DB&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib31\" title=\"\">2025</a>)</cite>: Datasets specifically designed for expressive and emotional speech synthesis, containing professionally recorded utterances with clear stylistic annotations.</p>\n\n",
                "matched_terms": [
                    "emovoicedb",
                    "expresso",
                    "ears"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance the linguistic diversity and colloquial nature of our training data, we incorporated sentences from the Synthetic-Persona-Chat dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jandaghi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib11\" title=\"\">2024</a>)</cite>. We synthesized these conversational sentences using a high-quality baseline TTS model and then processed them through the same feature extraction pipeline described above. This source contributes the largest portion of our dataset, ensuring the model is exposed to a wide array of everyday language.</p>\n\n",
                "matched_terms": [
                    "syntheticpersonachat",
                    "dataset",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantitatively assess the feature control capability of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, we conducted a reconstruction experiment using 384 samples from the RAVDESS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Livingstone &amp; Russo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib17\" title=\"\">2018</a>)</cite>. The evaluation protocol was as follows: for each original speech sample, we first generated a &#8221;reconstructed&#8221; version by passing it through our pre-trained vocal decoder. We then extracted the textual vocal plan (i.e., the vocal features) from this reconstructed speech. This plan was subsequently fed into <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> to synthesize the final speech output. The fidelity of the synthesis was measured by calculating the Mel-Cepstral Distortion (MCD) between the synthesized speech and the reconstructed speech, with a lower MCD indicating higher fidelity.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "dataset"
                ]
            }
        ]
    },
    "A2.T5": {
        "source_file": "BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs",
        "caption": "Table 5: Emotion control results on the RAVDESS benchmark. Our model excels in generating speech with the specified emotion. Lower scores are better (↓) for MCD.",
        "body": "Method\nMCD (↓\\downarrow)\n\n\n\n\nVocoder Resyn.\n2.46\n\n\nCaption\n2.62\n\n\nNumerical\n1.54\n\n\n- pitch\n1.63\n\n\n- energy\n2.13\n\n\n- spectral centroid\n1.57",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MCD&#160;(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Vocoder Resyn.</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Caption</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Numerical</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.54</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">- pitch</th>\n<td class=\"ltx_td ltx_align_center\">1.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">- energy</th>\n<td class=\"ltx_td ltx_align_center\">2.13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">- spectral centroid</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.57</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ravdess",
            "control",
            "caption",
            "speech",
            "spectral",
            "centroid",
            "energy",
            "lower",
            "emotion",
            "benchmark",
            "resyn",
            "numerical",
            "pitch",
            "mcd",
            "vocoder",
            "results",
            "scores",
            "method",
            "↓downarrow",
            "specified",
            "generating",
            "better",
            "excels",
            "our",
            "model"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The rise of Large Language Models (LLMs) is reshaping multimodel models, with speech synthesis being a prominent application. However, existing approaches often underutilize the linguistic intelligence of these models, typically failing to leverage their powerful instruction-following capabilities. This limitation hinders the model&#8217;s ability to follow text instructions for controllable Text-to-Speech&#160;(TTS). To address this, we propose a new paradigm inspired by &#8220;operationalism&#8221; that decouples instruction understanding from speech generation. We introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, a framework where an LLM acts as a &#8220;conductor&#8221;, understanding user instructions and generating a textual &#8220;plan&#8221; &#8211; explicit vocal features (e.g., pitch, energy). A separate TTS model, the &#8220;orchestra&#8221;, then generates the speech from these features. To realize this component, we develop <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a TTS model trained specifically for this task. Our experiments demonstrate that <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in controllable and\nemotional speech synthesis, outperforming strong open- and closed-source baselines. Notably, our approach enables remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to languages unseen during post-training. This demonstrates that objectifying speech into textual vocal features can more effectively unlock the linguistic intelligence of LLMs.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "generating",
                    "control",
                    "speech",
                    "energy",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rapid advancement of Large Language Models (LLMs) has catalyzed a paradigm shift in Multimodal Large Language Models&#160;(MLLMs), with frameworks now unifying text, images, and speech within a single model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib32\" title=\"\">2024</a>; Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib30\" title=\"\">2025</a>; Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib3\" title=\"\">2025</a>)</cite>. In Text-to-Speech (TTS), this has led to a new generation of systems that fine-tune a pre-trained LLM as a backbone to generate speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib27\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib7\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib33\" title=\"\">2025a</a>)</cite>. However, a critical yet underexplored question remains: <span class=\"ltx_text ltx_font_italic\">Are we fully leveraging the linguistic intelligence of LLMs in these TTS models?</span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing LLM-based TTS models primarily treat the LLM as a backbone. This approach typically involves designing a tokenizer to convert speech into discrete tokens and then training the model on large-scale datasets tailored to specific objectives. For instance, training a controllable TTS model necessitates extensive manual annotation of existing speech data to acquire the corresponding control labels and instructions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib6\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib5\" title=\"\">a</a>)</cite>, a process that is not only prohibitively expensive but also suffers from low inter-annotator agreement. We contend that this methodology largely bypasses the LLM&#8217;s inherent linguistic intelligence, such as its strong capabilities for complex context understanding and instruction following.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, we draw inspiration from the principle of &#8220;operationalism&#8221;, where complex concepts are understood through quantifiable, interpretable operations. For instance, to analyze imperceptible ultrasound, we use sensors to extract quantifiable features like frequency and amplitude. We posit that controllable TTS can be transformed by operationalizing user instructions into the desired vocal features. This reframes the problem: the LLM first leverages its linguistic intelligence to understand instructions and generate explicit vocal features, which then serves as input for a subsequent TTS model. This approach allows us to circumvent the need for manually annotating speech with controllable labels.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To realize this vision, we introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, a novel TTS framework that decouples instruction understanding from speech generation, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S0.F1\" title=\"Figure 1 &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> employs an LLM as a &#8220;conductor&#8221;, which interprets the user&#8217;s instructions to explicit vocal features, like pitch and energy. This plan is then fed into a separate TTS model, the &#8220;orchestra&#8221;, which generates the final speech. The &#8220;orchestra&#8221; in our framework is <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a TTS model we trained specifically to synthesize high-quality speech conditioned on these textual vocal features.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "speech",
                    "energy",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments validate the power of this decoupled approach. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in emotional speech synthesis, outperforming strong open- and closed-source models. For example, our 1.7B parameter model achieves an emotion accuracy of 57.6%, significantly surpassing all baselines. To verify our hypothesis, we show that stronger linguistic intelligence directly translates to superior synthesis: upgrading the &#8220;conductor&#8221; LLM from our 1.7B model to the more capable <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span> boosts the final model&#8217;s emotion accuracy from 29.8% to 57.6%. Furthermore, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> exhibits remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to Chinese, which is an unseen language during feature control training stage. This work not only advances controllable speech synthesis but also presents a promising new paradigm for MLLM development, demonstrating how objectifying modalities into text can more fully unlock the linguistic intelligence of LLMs.</p>\n\n",
                "matched_terms": [
                    "control",
                    "speech",
                    "emotion",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a methodology for realizing this paradigm, including a novel data pipeline that automatically generates instruction-feature pairs, and we introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a specialized TTS model trained on this data to generate speech from the vocal features.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through extensive experiments, we demonstrate that our framework, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, achieves strong performance in controllable, expressive speech synthesis. It exhibits superior emotional control and remarkable zero-shot cross-lingual generalization performance, validating the effectiveness of our operationalism-inspired approach.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>, a controllable TTS framework capable of synthesizing speech that adheres to arbitrary text-based instructions. Adopting an operationalist stance, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> leverages LLMs to interpret users&#8217; instructions into a JSON list of fine-grained vocal features. The core of this framework is <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, a TTS model trained specifically developed to synthesize speech from these features. We first describe the overall framework and its inference process, followed by a detailed introduction of the <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> architecture and its three-stage training pipeline.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Timbre</span> (<span class=\"ltx_text ltx_font_italic\">spectral centroid</span>): The perceived brightness of the speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spectral",
                    "centroid"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We now detail the architecture of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, the model responsible for generating speech from the specified feature list. Inspired by recent advancements such as CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib6\" title=\"\">2024b</a>)</cite>, the architecture of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> comprises two primary components: an LLM backbone and a pre-trained speech decoder.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "generating",
                    "specified"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the LLM backbone, we employ representative open-source models, specifically Qwen3-1.7B and Qwen2.5-0.5B. As will be demonstrated in our experimental section, our proposed method is effective across LLM backbones of varying capacities. The LLM is tasked with autoregressively generating a sequence that includes the input text to be synthesized, the corresponding speech features (i.e., the vocal plan), and the discrete speech tokens that realize this plan. The structure of this input sequence during the Supervised Fine-Tuning (SFT) stage is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Overall Framework and Inference Process &#8227; 2 BatonVoice: A Framework for Controllable TTS &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. It is important to note that while the features are part of the training sequence, during inference, they are generated by an external LLM as previously described.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "method",
                    "generating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the final synthesis step, we leverage the speech decoder from the publicly available CosyVoice2 model. This decoder converts the discrete speech tokens produced by the LLM into a high-quality speech. It consists of a speech token encoder, a conditional flow matching model, and a HiFi-GAN vocoder. The flow matching model generates Mel spectrograms conditioned on the discrete speech tokens, and the HiFi-GAN vocoder then converts these spectrograms into the final speech. By utilizing a pre-trained speech decoder, we can focus our training efforts exclusively on teaching the LLM to control speech features through language. Consequently, the speech decoder remains frozen throughout our training process.</p>\n\n",
                "matched_terms": [
                    "control",
                    "speech",
                    "vocoder",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 2: Supervised Fine-Tuning&#160;(SFT).</span> Teaches the LLM to generate speech conditioned on specific vocal features (<math alttext=\"F_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I2.i2.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">F_{v}</annotation></semantics></math>), enabling fine-grained control.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 3: Preference Optimization&#160;(PO).</span> Refines the model by preference optimizing, mitigating failure modes and enhancing the precision of feature control.</p>\n\n",
                "matched_terms": [
                    "model",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective of this stage is to equip the LLM with a fundamental text-to-speech capability, providing a robust weight initialization for subsequent stages. We use a large-scale corpus of speech-text pairs, <math alttext=\"\\mathcal{D}_{\\text{pretrain}}=\\{(x_{i},S_{i})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>pretrain</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>S</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{pretrain}}=\\{(x_{i},S_{i})\\}</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is the transcript and <math alttext=\"S_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">S_{i}</annotation></semantics></math> represents the corresponding discrete speech tokens. The model, denoted as policy <math alttext=\"\\pi_{\\text{pre}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>pre</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{pre}}</annotation></semantics></math>, is trained using a standard causal language modeling objective to predict the next token autoregressively over the concatenated sequence of text and speech tokens. The training objective is:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SFT stage aims to instill fine-grained controllability by training the model to generate speech conditioned on both the transcript and a set of explicit, verbalized vocal features. This process, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S2.F2\" title=\"Figure 2 &#8227; 2.1 Overall Framework and Inference Process &#8227; 2 BatonVoice: A Framework for Controllable TTS &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, trains the model to associate textual vocal features with corresponding discrete speech tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During this stage, we fine-tune the policy <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math>. The input sequence is formed by concatenating the transcript <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>, the verbalized features <math alttext=\"F_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">F_{v}</annotation></semantics></math>, and the speech tokens <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m4\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>. The model is trained to predict the next token autoregressively by minimizing the cross-entropy loss:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"Y=[x;F_{v};S]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m5\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><mi>x</mi><mo>;</mo><msub><mi>F</mi><mi>v</mi></msub><mo>;</mo><mi>S</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=[x;F_{v};S]</annotation></semantics></math> is the concatenated sequence. This objective teaches the model to generate speech tokens <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m6\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> that adhere to the vocal plan specified by <math alttext=\"F_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px2.p3.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">F_{v}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "specified"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although SFT offers a direct mechanism for control, the resulting model, <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math>, is still prone to certain failure modes. These include a high Word Error Rate (WER), an unnaturally slow speaking rate and insufficient expressiveness. To overcome these limitations, we employ a subsequent preference optimazation stage. The central principle is to construct a preference dataset, <math alttext=\"\\mathcal{D}_{\\text{pref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>pref</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{pref}}</annotation></semantics></math>, designed to align the model&#8217;s outputs with more desirable vocal features, crucially without the need for manually annotated expressive data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Initial Generation and Rejection Sampling</span>: For each text prompt <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> in a corpus <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math>, we use the pre-trained model <math alttext=\"\\pi_{\\text{pre}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>pre</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{pre}}</annotation></semantics></math> from Stage 1 to synthesize an speech sample <math alttext=\"s_{\\text{base}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m4\" intent=\":literal\"><semantics><msub><mi>s</mi><mtext>base</mtext></msub><annotation encoding=\"application/x-tex\">s_{\\text{base}}</annotation></semantics></math>. Samples are designated as <em class=\"ltx_emph ltx_font_italic\">rejected</em> (<math alttext=\"s_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m5\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">s_{l}</annotation></semantics></math>) if they exhibit a high WER or a slow speech rate (SR). The corresponding speech tokens <math alttext=\"S_{\\text{base}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mtext>base</mtext></msub><annotation encoding=\"application/x-tex\">S_{\\text{base}}</annotation></semantics></math> are stored as the rejected sequence <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i1.p1.m7\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preferred Data Construction</span>: For each text <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> corresponding to a rejected sample, we use our SFT model <math alttext=\"\\pi_{\\text{sft}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mtext>sft</mtext></msub><annotation encoding=\"application/x-tex\">\\pi_{\\text{sft}}</annotation></semantics></math> to generate a new candidate speech <math alttext=\"\\hat{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{s}</annotation></semantics></math>. These candidates are accepted as <em class=\"ltx_emph ltx_font_italic\">chosen</em> samples (<math alttext=\"s_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m4\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">s_{w}</annotation></semantics></math>) if they meet the quality criteria (low WER and adequate SR). The corresponding features and tokens <math alttext=\"(F_{v,w},S_{w})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i2.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(F_{v,w},S_{w})</annotation></semantics></math> are stored.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference Dataset Construction</span>: This filtering process yields pairs of chosen sequences <math alttext=\"(F_{v,w},S_{w})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mi>w</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(F_{v,w},S_{w})</annotation></semantics></math> and rejected sequences <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math>. To create a controlled comparison, we form preference tuples where the model learns to prefer <math alttext=\"S_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">S_{w}</annotation></semantics></math> over <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m4\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math> under the same vocal plan, <math alttext=\"F_{v,w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">F_{v,w}</annotation></semantics></math>. This setup creates a powerful learning signal: because <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m6\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math> was generated without knowledge of <math alttext=\"F_{v,w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m7\" intent=\":literal\"><semantics><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi></mrow></msub><annotation encoding=\"application/x-tex\">F_{v,w}</annotation></semantics></math>, while <math alttext=\"S_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m8\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">S_{w}</annotation></semantics></math> was explicitly conditioned on it, teaching the model to prefer <math alttext=\"S_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m9\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">S_{w}</annotation></semantics></math> over <math alttext=\"S_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m10\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">S_{l}</annotation></semantics></math> not only improves general quality but also implicitly reinforces the model&#8217;s ability to follow the specified vocal features. The final dataset consists of tuples: <math alttext=\"\\mathcal{D}_{\\text{pref}}=\\{(x_{i},F_{v,w,i},S_{w,i},S_{l,i})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.I3.i3.p1.m11\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>pref</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>F</mi><mrow><mi>v</mi><mo>,</mo><mi>w</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mi>w</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>S</mi><mrow><mi>l</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{pref}}=\\{(x_{i},F_{v,w,i},S_{w,i},S_{l,i})\\}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "specified"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our post-training process consists of SFT and PO. A key challenge in preparing the SFT data is that our speech decoder cannot perfectly reconstruct original speech from its quantized tokens. To ensure the vocal features are faithfully synthesizable, we derive them from speech that has been reconstructed by the decoder itself.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our SFT dataset is compiled from two primary sources. First, we take a diverse collection of expressive speech corpora&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Veaux et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib26\" title=\"\">2017</a>; Nagraniy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib19\" title=\"\">2017</a>; Chung et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib2\" title=\"\">2018</a>; Richter et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib21\" title=\"\">2024</a>; Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib20\" title=\"\">2023</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib31\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib28\" title=\"\">2025a</a>)</cite>, pass the speech through our decoder for reconstruction, and then extract features from the synthesized output. Second, we collect colloquial sentences from the Synthetic-Persona-Chat dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jandaghi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib11\" title=\"\">2024</a>)</cite> and synthesize them. We then apply a filtering process to the combined data, removing samples with a high Word Error Rate (WER), which indicates potential misalignments, or an abnormally slow speaking rate. <math alttext=\"\\tau_{\\text{wer\\_high}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mtext>wer_high</mtext></msub><annotation encoding=\"application/x-tex\">\\tau_{\\text{wer\\_high}}</annotation></semantics></math> is 0.1 and <math alttext=\"\\tau_{\\text{sr}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mtext>sr</mtext></msub><annotation encoding=\"application/x-tex\">\\tau_{\\text{sr}}</annotation></semantics></math> is 1.5 words per seconds.This results in a final SFT dataset of 377,619 utterances, totaling over 500 hours (see Appendix for a detailed distribution). For the PO stage, we collected a dataset of 9,823 preference samples.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The feature extraction pipeline for this data begins with grounding features in semantically meaningful units. We first obtain word-level timestamps for each speech sample using a pre-trained model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self\" title=\"\">https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self</a></span></span></span>. Since individual words are often too short to carry significant prosodic information, we merge adjacent words into segments until each segment&#8217;s duration exceeds a one-second threshold, ensuring a stable and analyzable prosodic contour. Finally, we use the Parselmouth library&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/YannickJadoul/Parselmouth\" title=\"\">https://github.com/YannickJadoul/Parselmouth</a></span></span></span> to extract a set of vocal features from these segments. The model is trained with SFT for 3 epochs, followed by 1 epoch of APO-down.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TTS Intelligibility</span>: We use the test set from the <span class=\"ltx_text ltx_font_bold\">Seed-TTS</span> benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib1\" title=\"\">2024</a>)</cite>, which is designed for assessing speech synthesis from short speech prompts. Performance is measured by <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span>, calculated with pre-trained ASR models&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span>. A lower WER score signifies higher intelligibility.</p>\n\n",
                "matched_terms": [
                    "lower",
                    "speech",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Control</span>: This is assessed on a curated test set from the <span class=\"ltx_text ltx_font_bold\">Emotion</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saravia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib23\" title=\"\">2018</a>)</cite>. We use includes 100 samples for each of five emotions (joy, sadness, anger, surprise, and fear). We measure performance using <span class=\"ltx_text ltx_font_bold\">Emotion Classification Accuracy</span>. This metric is derived by employing Google&#8217;s Gemini-2.5-Pro to classify the emotion of the synthesized speech. A higher accuracy indicates a greater success rate in generating perceptually accurate emotional speech. The prompt template for this evaluation is provided in the Appendix.</p>\n\n",
                "matched_terms": [
                    "generating",
                    "emotion",
                    "speech",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Main Results &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>-1.7B achieves 57.6% accuracy on the Emotion benchmark, surpassing the strongest closed-source baseline, Minimax-2.5-HD (48.6%), by 9.0 absolute points. It also outperforms all open-source systems by a wide margin, e.g., +13.8 points over CosyVoice (43.8%). On Seed-TTS, our 1.7B model attains a WER of 2.5 &#8211; competitive with high-quality open models (better than CosyVoice at 3.4, slightly above CosyVoice2 at 2.1 and Spark-TTS at 1.9) and within a small gap of the closed-source Minimax series (1.5). These results validate that our decoupled &#8220;conductor&#8211;orchestra&#8221; design substantially enhances emotional expressiveness without sacrificing intelligibility.</p>\n\n",
                "matched_terms": [
                    "better",
                    "results",
                    "emotion",
                    "benchmark",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> framework achieves these results with 0 hours of manually annotated instruction data, in contrast to CosyVoice and CosyVoice2, which use 556 and 1,500 hours respectively yet underperform on emotion accuracy (43.8% and 37.8%). Preference optimization over textual vocal plans yields consistent improvements over SFT alone: for the 1.7B model, emotion accuracy increases from 52.2% (SFT) to 57.6% (Instruct, +5.4 points) while WER improves from 2.9 to 2.5. Even at 0.5B, instruction tuning further boosts accuracy from 51.6% to 52.8% (+1.2). These gains directly confirm the effectiveness of <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "emotion",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving from 0.5B to 1.7B parameters improves emotion accuracy from 52.8% to 57.6% (+4.8) and reduces WER from 2.9 to 2.5 for the instruction-tuned models. This trend demonstrates the scalability of our method, and showcasing its consistent performance benefits across different model sizes..</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "method",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess our model&#8217;s performance on controllable TTS with free-form instructions, we create a specialized test set. We begin by sourcing 50 diverse social situations from the Social IQa benchmark&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Sap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib22\" title=\"\">2019</a>)</cite>, chosen for its rich contextual and emotional nuance. For each situation, we utilize <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span> to generate a challenging test case. The model is prompted to produce two outputs: first, a detailed, role-playing style instruction framed in a second-person narrative, which specifies the desired persona and delivery style. Second, it generates a corresponding target utterance to be synthesized. This pipeline yield a high-quality benchmark of 50 pairs, specifically designed to test the model&#8217;s ability to follow complex, descriptive instructions beyond simple labels.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A significant and surprising finding is the model&#8217;s ability to generalize to languages not seen during the <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> post-training stage. We evaluated this by testing on a Chinese emotion benchmark, employing the same methodology as the English evaluation, with the text and instructions translated into Chinese by <span class=\"ltx_text ltx_font_italic\">Gemini 2.5 Pro</span>. Notably, this cross-lingual generalization occurs despite the post-training stage being conducted exclusively on English data, demonstrating a strong zero-shot transfer capability.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.T3\" title=\"Table 3 &#8227; 3.4 Cross-Lingual Generalization &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>-1.7B achieves a 56.2% accuracy on the Chinese emotion benchmark. This result is not only strong in absolute terms but also surpasses leading models that are either native to or heavily optimized for Chinese, such as CosyVoice (52.0%) and the closed-source Minimax-2.5-Turbo (50.6%). This performance is achieved without any Chinese instruction data, highlighting a key advantage of our &#8220;operationalism&#8221; paradigm.</p>\n\n",
                "matched_terms": [
                    "our",
                    "emotion",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a series of in-depth analyses to better understand the capabilities of <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span>. Otherwise stated, we report the results of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>-1.7B on the English Emotion benchmark.</p>\n\n",
                "matched_terms": [
                    "results",
                    "emotion",
                    "better",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform a step-by-step ablation to examine the effectiveness of each stage in our proposed <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> framework. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.F3.sf1\" title=\"Figure 3(a) &#8227; Figure 3 &#8227; 3.5 Component Analysis &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a>, the base model, trained only on foundational TTS without any instruction tuning, yields poor performance on the English Emotion benchmark &#8211; achieving just 23.2% accuracy for the 1.7B model. Incorporating the SFT stage causes a dramatic improvement, boosting the accuracy to 52.2% (+29.0 points), showing that teaching the model to generate and condition on verbalized vocal plans is key to enabling stylistic control. Adding the APO-based preference optimization further improves performance to 57.6% (+5.4 over SFT), illustrating the importance of our post-training strategy. Consistent gains are observed for the smaller 0.5B model (25.8 <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 51.6 <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 52.8), demonstrating that the framework is effective across model scales. These results validate the design of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> in sequentially teaching foundational TTS capability and improving control quality.</p>\n\n",
                "matched_terms": [
                    "control",
                    "results",
                    "emotion",
                    "benchmark",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate how our framework leverages the linguistic intelligence of Large Language Models (LLMs), we performed an experiment to measure the impact of the vocal feature generator on final synthesis quality. We used a fixed <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> model and generated vocal plans at inference time using a range of LLMs with varying capabilities. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#S3.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; 3.5 Component Analysis &#8227; 3 Experiment &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, the results show a clear, positive correlation between the performance of the LLM and the emotion accuracy of the synthesized speech. The accuracy climbs steadily from 29.8% with Qwen3-1.7B to 57.6% with Gemini-2.5-Pro, with intermediate models like Qwen3-80B (39.8%) and Qwen3-Max (47.8%) falling along this expected trajectory.\nThese findings strongly support our core claim: representing speech as vocal features allows the synthesis model to directly benefit from advances in LLMs. This highlights a key advantage of our decoupled &#8220;conductor&#8211;orchestra&#8221; design: its modularity. Even our compact 1.7B model can tap into the power of a much larger model like Gemini-2.5-Pro at inference time, effectively upgrading its expressive capability without any modification to the model.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "results",
                    "emotion",
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Controllable speech synthesis is typically classified into three primary paradigms. The first, style tagging, employs discrete labels (e.g., emotion, gender) to guide the synthesis process&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib7\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib29\" title=\"\">2025b</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib33\" title=\"\">2025a</a>)</cite>. While conceptually simple, this approach is restricted to a predefined set of styles, which fundamentally limits its expressive range.\nThe second paradigm leverages reference speech to enable few-shot or zero-shot speaker adaptation. This is accomplished by extracting speaker embeddings from short speech samples and conditioning the TTS decoder on them &#8211; a technique proven effective for voice cloning and style transfer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib14\" title=\"\">2024</a>; Ji et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib12\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib15\" title=\"\">2024</a>)</cite>.\nThe third and most flexible paradigm, instruction-guided control, conceptualizes TTS as a task of interpreting natural language instructions. Frameworks such as VoxInstruct exemplify this approach, guiding synthesis\nwith free-form instructions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib6\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib5\" title=\"\">a</a>)</cite>.\nHowever, these instruction-following methods are constrained by the high cost and difficulty of creating large-scale, annotated instruction-speech datasets, which limits their generalization and performance.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "speech",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, our approach circumvents the need for manually annotated data by leveraging a powerful LLM. This enables robust, zero-shot generalization to unseen instructions, generating vocal features that exhibit high fidelity to the prompts while affording a high degree of control. Our method thus addresses the key limitations of data scarcity and annotation cost in instruction-guided TTS, showing significant promise for future research in expressive and controllable speech synthesis.</p>\n\n",
                "matched_terms": [
                    "method",
                    "generating",
                    "control",
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we address a key limitation in current speech synthesis systems: the underutilization of the linguistic intelligence of LLMs. We introduce a new paradigm inspired by &#8220;operationalism&#8221;, which decouples instruction understanding from speech generation by first translating instructions into quantifiable, interpretable vocal features. Our framework, <span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">BatonVoice</span>, embodies this principle by using LLMs to generate a vocal &#8220;plan&#8221;, which is then fed into a TTS model. We train this model using a three-stage training pipeline that requires no manual instruction data. Our empirical results demonstrate the effectiveness of this approach. <span class=\"ltx_text ltx_font_smallcaps\">BatonVoice</span> achieves strong performance in emotional speech synthesis and shows that its capabilities scale positively with the linguistic intelligence of LLMs. Furthermore, it exhibits powerful zero-shot cross-lingual generalization.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our SFT dataset is a comprehensive collection curated to teach the model how to generate vocal plans from text. It comprises 377,619 utterances, totaling over 500 hours of speech, and is compiled from two primary sources as detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#A2.T4\" title=\"Table 4 &#8227; B.1 Data Source &#8227; Appendix B Experimental Details &#8227; BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each sample from these corpora, we first passed the original speech through our pre-trained vocal decoder to obtain a reconstructed waveform. We then extracted the textual vocal features (i.e., the vocal plan) from this synthesized output. This reconstruction step ensures that the vocal features are derived from a distribution that our TTS &#8221;orchestra&#8221; model can faithfully render.</p>\n\n",
                "matched_terms": [
                    "our",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance the linguistic diversity and colloquial nature of our training data, we incorporated sentences from the Synthetic-Persona-Chat dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jandaghi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib11\" title=\"\">2024</a>)</cite>. We synthesized these conversational sentences using a high-quality baseline TTS model and then processed them through the same feature extraction pipeline described above. This source contributes the largest portion of our dataset, ensuring the model is exposed to a wide array of everyday language.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantitatively assess the feature control capability of <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>, we conducted a reconstruction experiment using 384 samples from the RAVDESS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Livingstone &amp; Russo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26514v1#bib.bib17\" title=\"\">2018</a>)</cite>. The evaluation protocol was as follows: for each original speech sample, we first generated a &#8221;reconstructed&#8221; version by passing it through our pre-trained vocal decoder. We then extracted the textual vocal plan (i.e., the vocal features) from this reconstructed speech. This plan was subsequently fed into <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> to synthesize the final speech output. The fidelity of the synthesis was measured by calculating the Mel-Cepstral Distortion (MCD) between the synthesized speech and the reconstructed speech, with a lower MCD indicating higher fidelity.</p>\n\n",
                "matched_terms": [
                    "ravdess",
                    "control",
                    "speech",
                    "mcd",
                    "lower",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our analysis, we first compared different formats for representing the vocal plan. We found that our proposed numerical representation significantly outperformed a qualitative, caption-based description, demonstrating that a structured, quantitative format allows for more precise control. Remarkably, the MCD achieved with our numerical plan was even lower than that of the vocoder resynthesis baseline (i.e., the MCD between the reconstructed and the original speech). This suggests that <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span> not only faithfully renders the vocal plan but can also compensate for some information loss introduced during the initial decoding stage. Furthermore, to validate the design of our vocal plan, we performed an ablation study by systematically removing individual features (e.g., pitch, energy) from the plan before feeding it to <span class=\"ltx_text ltx_font_smallcaps\">BatonTTS</span>. We observed a consistent performance degradation (i.e., an increase in MCD) upon the removal of any feature. This result confirms that all components of our proposed vocal representation are necessary and contribute meaningfully to the final synthesis quality.</p>\n\n",
                "matched_terms": [
                    "numerical",
                    "pitch",
                    "control",
                    "speech",
                    "mcd",
                    "vocoder",
                    "energy",
                    "lower",
                    "our"
                ]
            }
        ]
    }
}