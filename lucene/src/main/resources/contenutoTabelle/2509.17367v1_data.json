{
    "S5.T1": {
        "source_file": "Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs",
        "caption": "Table 1: Basic statistics for different text categories.",
        "body": "Vocabulary\n\n\n(VV)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Vocabulary</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">(<math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math>)</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "text",
            "vocabulary",
            "basic",
            "different",
            "statistics",
            "categories"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S5.T1\" title=\"Table 1 &#8227; 5 Data &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the sizes and vocabularies of each dataset. We compiled publicly available corpora in three domains:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present a comparative analysis of text complexity across domains using scale-free metrics. We quantify linguistic complexity via Heaps&#8217; exponent <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> (vocabulary growth), Taylor&#8217;s exponent <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> (word-frequency fluctuation scaling), compression rate <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> (redundancy), and entropy. Our corpora span three domains: legal documents (statutes, cases, deeds) as a specialized domain, general natural language texts (literature, Wikipedia), and AI-generated (GPT) text. We find that legal texts exhibit slower vocabulary growth (lower <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"m4\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>) and higher term consistency (higher <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"m5\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>) than general texts. Within legal domain, statutory codes have the lowest <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"m6\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> and highest <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"m7\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, reflecting strict drafting conventions, while cases and deeds show higher <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"m8\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> and lower <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"m9\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>. In contrast, GPT-generated text shows the statistics more aligning with general language patterns. These results demonstrate that legal texts exhibit domain-specific structures and complexities, which current generative models do not fully replicate.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vocabulary",
                    "statistics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we consider how such characteristics of legal text can be quantified by complexity measures. Our analysis reveals that legal texts exhibit distinctive statistical patterns: specifically, they have slower vocabulary growth (smaller <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>) and greater term consistency (larger <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>) than general-language texts. In contrast, GPT-generated texts show higher <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m3\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> and lower <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m4\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, aligning more closely with general language patterns.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional approaches to textual analysis frequently utilize machine learning techniques, such as support vector machines (SVM), k-means clustering, and transformer-based embeddings, to categorize or cluster documents by topic, style, or content type <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib21\" title=\"\">21</a>]</cite>. Although these methods achieve high classification accuracy, they operate as &#8216;black boxes&#8217; and do not reveal the underlying linguistic principles, such as vocabulary growth dynamics or term clustering, that distinguish domains. In contrast, our scale-free framework directly measures statistical exponents on texts, thus quantifying underlying structural characteristics and linguistic complexities across text domains.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Machine learning, growing from early rule-based systems to current deep neural architectures, has revolutionized text processing. Transformer-based language models like GPT-4 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib20\" title=\"\">20</a>]</cite> and Llama <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib32\" title=\"\">32</a>]</cite> demonstrate state-of-the-art performance in generation and understanding tasks. However, capturing the specialized structure of domain-specific texts remains challenging <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib35\" title=\"\">35</a>]</cite>. Here, our approach using scaling-law metrics shows that, although modern models can produce fluent text, they often mirror general language statistics rather than the precise patterns of specialized corpora.</p>\n\n",
                "matched_terms": [
                    "text",
                    "statistics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section describes the methodology used to assess the differences in complexity between the text categories. We propose two types of metrics: scale-free properties (our original focus) and general metrics (compression rate), which have frequently been used in previous work on text complexity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib12\" title=\"\">12</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "categories"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Texts often exhibit universal statistical behaviors in the form of scale-free properties, meaning these properties hold regardless of text size. This scale-free quality is crucial because many statistical metrics, including entropy, typically depend on the corpus size&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib30\" title=\"\">30</a>]</cite>. A comprehensive summary of scale-free properties is given in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib29\" title=\"\">29</a>]</cite>, which categorizes them into frequency distribution and long-memory aspects. We will demonstrate that different text domains possess distinct characteristics in both of these aspects.</p>\n\n",
                "matched_terms": [
                    "text",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding the frequency distribution, Zipf&#8217;s law is the best-known power-law property of language, describing the relationship between word frequency rank and frequency. However, because the Zipf exponent is close to <math alttext=\"-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math> for most natural texts, Zipf&#8217;s law alone does not distinguish between corpora. We therefore focus on the type-token relationship (Heaps&#8217; law). Given a text <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> with vocabulary size <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> and total words <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math>, we have:</p>\n\n",
                "matched_terms": [
                    "text",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Information&#8208;theoretic measures have long been used to investigate legal language complexity. Early work used Shannon entropy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib24\" title=\"\">24</a>]</cite> to distinguish legal from general&#8208;language corpora <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib15\" title=\"\">15</a>]</cite>. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib12\" title=\"\">12</a>]</cite> later proposed pairing a &#8220;normalized vocabulary entropy&#8221; with a gzip&#8208;based compression rate to separate text types.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before presenting our large-scale results, we illustrate the metrics in two examples. We selected one literature text (&#8220;Terminal Compromise&#8221; by Winn Schwartau; 265,780 words) and one legal code text (the &#8220;Abandoned Property Article&#8221; from the U.S. Code; 300,770 words). Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 Entropy and Compression Rate Analysis &#8227; 3 Methodology &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a) and Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 Entropy and Compression Rate Analysis &#8227; 3 Methodology &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(c) show Heaps&#8217; analysis for these texts. The U.S. Code example has a much smaller <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> than the literature example, indicating slower vocabulary growth for that text. Similarly, Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 Entropy and Compression Rate Analysis &#8227; 3 Methodology &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b) and Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 Entropy and Compression Rate Analysis &#8227; 3 Methodology &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(d) show Taylor&#8217;s analysis: the literature example has a smaller <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, whereas the U.S. Code example&#8217;s larger <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> indicates more clustered word usage. We also computed the compression rate for these two examples: U.S. Code (<math alttext=\"r=4.383\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>4.383</mn></mrow><annotation encoding=\"application/x-tex\">r=4.383</annotation></semantics></math>) versus literature (<math alttext=\"r=2.577\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>2.577</mn></mrow><annotation encoding=\"application/x-tex\">r=2.577</annotation></semantics></math>). The higher compression for the Code reflects its greater repetitiveness. In the following sections, we show that these tendencies generalize across our datasets.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the following subsections, we concentrate on the three primary scale&#8208;free measures: Heaps&#8217; exponent <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, Taylor&#8217;s exponent <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, and gzip compression rate <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m3\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>. All of the resulting values are shown in Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.T2\" title=\"Table 2 &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The normalized vocabulary entropy <math alttext=\"H_{\\mathrm{norm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m4\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>norm</mi></msub><annotation encoding=\"application/x-tex\">H_{\\mathrm{norm}}</annotation></semantics></math> will be revisited in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.SS4\" title=\"6.4 Normalized Entropy and Compression&#8208;Rate Plane &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">6.4</span></a>, where we demonstrate that, alone, it does not reliably distinguish text types.</p>\n\n",
                "matched_terms": [
                    "text",
                    "vocabulary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These subdomain patterns highlight how scale&#8208;free metrics capture the nuanced drafting conventions across different branches of legal text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One previous work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib12\" title=\"\">12</a>]</cite> employs a two&#8208;dimensional plot of normalized vocabulary entropy <math alttext=\"H_{\\mathrm{norm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>norm</mi></msub><annotation encoding=\"application/x-tex\">H_{\\mathrm{norm}}</annotation></semantics></math> (See Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S3.SS2\" title=\"3.2 Entropy and Compression Rate Analysis &#8227; 3 Methodology &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>) against gzip compression rate <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p1.m2\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>, suggesting that legal and general&#8208;language texts occupy distinct regions in redundancy space. Crucially, this figure confines <math alttext=\"H_{\\mathrm{norm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>norm</mi></msub><annotation encoding=\"application/x-tex\">H_{\\mathrm{norm}}</annotation></semantics></math> (vertical axis) to a narrow interval 0.72&#8211;0.82, which visually amplifies small corpus&#8208;to&#8208;corpus differences. When we tried to replot our data points (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.F6.sf2\" title=\"In Figure 6 &#8227; 6.4 Normalized Entropy and Compression&#8208;Rate Plane &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>), these apparent separations vanished, revealing a single, continuous trend rather than discrete clusters. In other words, <math alttext=\"H_{\\mathrm{norm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>norm</mi></msub><annotation encoding=\"application/x-tex\">H_{\\mathrm{norm}}</annotation></semantics></math> does not distinguish different kinds of texts by itself.</p>\n\n",
                "matched_terms": [
                    "vocabulary",
                    "different"
                ]
            }
        ]
    },
    "S6.T2": {
        "source_file": "Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs",
        "caption": "Table 2: Average experimental results for different text categories (mean ±\\pm std).",
        "body": "Normalized Entropy\n\n\n(HnormH_{\\mathrm{norm}})",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Normalized Entropy</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">(<math alttext=\"H_{\\mathrm{norm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m6\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>norm</mi></msub><annotation encoding=\"application/x-tex\">H_{\\mathrm{norm}}</annotation></semantics></math>)</td>\n</tr>\n</table>\n",
        "informative_terms_identified": [
            "text",
            "entropy",
            "normalized",
            "different",
            "results",
            "average",
            "±pm",
            "hnormhmathrmnorm",
            "categories",
            "experimental",
            "std",
            "mean"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In the following subsections, we concentrate on the three primary scale&#8208;free measures: Heaps&#8217; exponent <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, Taylor&#8217;s exponent <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, and gzip compression rate <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m3\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>. All of the resulting values are shown in Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.T2\" title=\"Table 2 &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The normalized vocabulary entropy <math alttext=\"H_{\\mathrm{norm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m4\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>norm</mi></msub><annotation encoding=\"application/x-tex\">H_{\\mathrm{norm}}</annotation></semantics></math> will be revisited in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.SS4\" title=\"6.4 Normalized Entropy and Compression&#8208;Rate Plane &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">6.4</span></a>, where we demonstrate that, alone, it does not reliably distinguish text types.</p>\n\n",
            "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.F4\" title=\"Figure 4 &#8227; 6.2 Legal Subdomains &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> vs. <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> for statutes, cases, and deeds, with compression rates reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.T2\" title=\"Table 2 &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> (3rd column).\nStatutes (French Code, U.S. Code, Australian corpus) cluster at low <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m3\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> (0.521 - 0.674), high <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m4\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> (0.655 &#8211; 0.711), and the highest <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m5\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> (4.56 &#8211; 4.68), reflecting uniform, precise drafting. Case opinions (Harvard Case Law: <math alttext=\"\\beta=0.622\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#946;</mi><mo>=</mo><mn>0.622</mn></mrow><annotation encoding=\"application/x-tex\">\\beta=0.622</annotation></semantics></math>, <math alttext=\"\\alpha=0.647\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.647</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.647</annotation></semantics></math>, <math alttext=\"r=3.28\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>3.28</mn></mrow><annotation encoding=\"application/x-tex\">r=3.28</annotation></semantics></math>) sit slightly lower and on the right, indicating greater lexical flexibility balanced by moderate redundancy.</p>\n\n",
            "<p class=\"ltx_p\">characteristics of LLM outputs relative to human-authored legal texts. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.F5\" title=\"Figure 5 &#8227; 6.3 GPT-Generated vs. Human Text &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> compares legal corpora (green) with ChatGPT outputs (gray) in the <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS3.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>&#8211;<math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> plane (the compression rates are in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.T2\" title=\"Table 2 &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). GPT&#8208;generated texts shift toward higher <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS3.p2.m3\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> and lower <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS3.p2.m4\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, reflecting faster vocabulary turnover and weaker clustering. Moreover, their tight clustering (occupying a small, compact region) signals a lack of variance in generative behavior. Lower <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS3.p2.m5\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> values further underscore reduced redundancy compared to human&#8208;authored legal drafts.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present a comparative analysis of text complexity across domains using scale-free metrics. We quantify linguistic complexity via Heaps&#8217; exponent <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> (vocabulary growth), Taylor&#8217;s exponent <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> (word-frequency fluctuation scaling), compression rate <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> (redundancy), and entropy. Our corpora span three domains: legal documents (statutes, cases, deeds) as a specialized domain, general natural language texts (literature, Wikipedia), and AI-generated (GPT) text. We find that legal texts exhibit slower vocabulary growth (lower <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"m4\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>) and higher term consistency (higher <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"m5\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>) than general texts. Within legal domain, statutory codes have the lowest <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"m6\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> and highest <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"m7\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, reflecting strict drafting conventions, while cases and deeds show higher <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"m8\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> and lower <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"m9\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>. In contrast, GPT-generated text shows the statistics more aligning with general language patterns. These results demonstrate that legal texts exhibit domain-specific structures and complexities, which current generative models do not fully replicate.</p>\n\n",
                "matched_terms": [
                    "text",
                    "entropy",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section describes the methodology used to assess the differences in complexity between the text categories. We propose two types of metrics: scale-free properties (our original focus) and general metrics (compression rate), which have frequently been used in previous work on text complexity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib12\" title=\"\">12</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "categories"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Texts often exhibit universal statistical behaviors in the form of scale-free properties, meaning these properties hold regardless of text size. This scale-free quality is crucial because many statistical metrics, including entropy, typically depend on the corpus size&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib30\" title=\"\">30</a>]</cite>. A comprehensive summary of scale-free properties is given in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib29\" title=\"\">29</a>]</cite>, which categorizes them into frequency distribution and long-memory aspects. We will demonstrate that different text domains possess distinct characteristics in both of these aspects.</p>\n\n",
                "matched_terms": [
                    "text",
                    "entropy",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For long-memory properties, while traditional long-range correlation analysis can be problematic for text&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib29\" title=\"\">29</a>]</cite>, Taylor&#8217;s law provides an alternative. We divide a text into segments and measure the mean <math alttext=\"\\mu_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#956;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\mu_{w}</annotation></semantics></math> and standard deviation <math alttext=\"\\sigma_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#963;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\sigma_{w}</annotation></semantics></math> of occurrences of each word <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mi>w</mi><annotation encoding=\"application/x-tex\">w</annotation></semantics></math>. Across words, these follow:</p>\n\n",
                "matched_terms": [
                    "text",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Information&#8208;theoretic measures have long been used to investigate legal language complexity. Early work used Shannon entropy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib24\" title=\"\">24</a>]</cite> to distinguish legal from general&#8208;language corpora <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib15\" title=\"\">15</a>]</cite>. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib12\" title=\"\">12</a>]</cite> later proposed pairing a &#8220;normalized vocabulary entropy&#8221; with a gzip&#8208;based compression rate to separate text types.</p>\n\n",
                "matched_terms": [
                    "text",
                    "entropy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We will reserve discussion of this measure till Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.SS4\" title=\"6.4 Normalized Entropy and Compression&#8208;Rate Plane &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">6.4</span></a>, where we show that, by itself, <math alttext=\"H_{\\mathrm{norm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>norm</mi></msub><annotation encoding=\"application/x-tex\">H_{\\mathrm{norm}}</annotation></semantics></math> does not effectively distinguish text types. In the following sections, our primary analysis focuses on the scale&#8208;free exponents <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> (Sect.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S3.SS1\" title=\"3.1 Metrics Related to Scale-Free Properties &#8227; 3 Methodology &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) and the compression rate <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "hnormhmathrmnorm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before presenting our large-scale results, we illustrate the metrics in two examples. We selected one literature text (&#8220;Terminal Compromise&#8221; by Winn Schwartau; 265,780 words) and one legal code text (the &#8220;Abandoned Property Article&#8221; from the U.S. Code; 300,770 words). Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 Entropy and Compression Rate Analysis &#8227; 3 Methodology &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(a) and Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 Entropy and Compression Rate Analysis &#8227; 3 Methodology &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(c) show Heaps&#8217; analysis for these texts. The U.S. Code example has a much smaller <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> than the literature example, indicating slower vocabulary growth for that text. Similarly, Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 Entropy and Compression Rate Analysis &#8227; 3 Methodology &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(b) and Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 Entropy and Compression Rate Analysis &#8227; 3 Methodology &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>(d) show Taylor&#8217;s analysis: the literature example has a smaller <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, whereas the U.S. Code example&#8217;s larger <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> indicates more clustered word usage. We also computed the compression rate for these two examples: U.S. Code (<math alttext=\"r=4.383\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>4.383</mn></mrow><annotation encoding=\"application/x-tex\">r=4.383</annotation></semantics></math>) versus literature (<math alttext=\"r=2.577\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m5\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>2.577</mn></mrow><annotation encoding=\"application/x-tex\">r=2.577</annotation></semantics></math>). The higher compression for the Code reflects its greater repetitiveness. In the following sections, we show that these tendencies generalize across our datasets.</p>\n\n",
                "matched_terms": [
                    "text",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These subdomain patterns highlight how scale&#8208;free metrics capture the nuanced drafting conventions across different branches of legal text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One previous work <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#bib.bib12\" title=\"\">12</a>]</cite> employs a two&#8208;dimensional plot of normalized vocabulary entropy <math alttext=\"H_{\\mathrm{norm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>norm</mi></msub><annotation encoding=\"application/x-tex\">H_{\\mathrm{norm}}</annotation></semantics></math> (See Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S3.SS2\" title=\"3.2 Entropy and Compression Rate Analysis &#8227; 3 Methodology &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>) against gzip compression rate <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p1.m2\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>, suggesting that legal and general&#8208;language texts occupy distinct regions in redundancy space. Crucially, this figure confines <math alttext=\"H_{\\mathrm{norm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>norm</mi></msub><annotation encoding=\"application/x-tex\">H_{\\mathrm{norm}}</annotation></semantics></math> (vertical axis) to a narrow interval 0.72&#8211;0.82, which visually amplifies small corpus&#8208;to&#8208;corpus differences. When we tried to replot our data points (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.F6.sf2\" title=\"In Figure 6 &#8227; 6.4 Normalized Entropy and Compression&#8208;Rate Plane &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>), these apparent separations vanished, revealing a single, continuous trend rather than discrete clusters. In other words, <math alttext=\"H_{\\mathrm{norm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>norm</mi></msub><annotation encoding=\"application/x-tex\">H_{\\mathrm{norm}}</annotation></semantics></math> does not distinguish different kinds of texts by itself.</p>\n\n",
                "matched_terms": [
                    "hnormhmathrmnorm",
                    "entropy",
                    "normalized",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p3.m1\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>&#8217;s discriminative power, our scatterplot of Taylor&#8217;s <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p3.m2\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> versus Heaps&#8217; <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p3.m3\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.F2\" title=\"Figure 2 &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) reveals an even richer separation. GPT-generated texts form a tight, low-variance cluster in the <math alttext=\"(\\beta,\\alpha)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p3.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>&#946;</mi><mo>,</mo><mi>&#945;</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\beta,\\alpha)</annotation></semantics></math> plane, as discussed in Sec. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17367v1#S6.SS1\" title=\"6.1 Cross&#8208;Domain Comparison &#8227; 6 Results &#8227; Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs\"><span class=\"ltx_text ltx_ref_tag\">6.1</span></a>, clearly distinct from both literature and legal corpora. This compact clustering highlights GPT&#8217;s limited diversity in vocabulary growth and term consistency, whereas combining <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p3.m5\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math>, <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p3.m6\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math>, and <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS4.p3.m7\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> provides a multidimensional signature that robustly captures domain-specific drafting conventions&#8212;something normalized entropy alone cannot achieve.</p>\n\n",
                "matched_terms": [
                    "entropy",
                    "normalized"
                ]
            }
        ]
    }
}