{
    "S2.T1": {
        "caption": "Table 1: \nASR performance (WER) of baselines, our model, and upper bound.\nThe first group varies codebook size with fixed mean-repeat (MR) sampling.\nThe second group compares various sampling configurations using the best codebook size (NCB=1024N_{\\text{CB}}=1024.)",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">WER</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Baseline</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"5\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Codebook Size (<math alttext=\"N_{\\text{CB}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m4\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext class=\"ltx_mathvariant_bold\">CB</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{CB}}</annotation></semantics></math>) vs. WER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"6\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Sampling Strategies vs. WER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Upper Bound</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">VBDemand</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">Whisper</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">+Adapter</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">Speechless</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">128</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">256</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">1024</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">2048</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">MR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">MC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">CC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">CR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">TR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\">TC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"><math alttext=\"\\text{C}_{\\text{ZS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m5\" intent=\":literal\"><semantics><msub><mtext>C</mtext><mtext>ZS</mtext></msub><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{ZS}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"><math alttext=\"\\text{C}_{\\text{TR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m6\" intent=\":literal\"><semantics><msub><mtext>C</mtext><mtext>TR</mtext></msub><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{TR}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Validation</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">all</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">6.75</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">0.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">0.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">0.29</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">0.49</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">0.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.19</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">0.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">0.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_italic\">0.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">0.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">0.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">Test</span></td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:0.5pt 5.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_italic\">bus</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">12.05</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">10.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.86</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">1.99</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">3.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.91</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">1.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">1.86</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_italic\">cafe</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">25.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">4.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">6.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">5.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">4.97</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">5.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">4.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.88</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">4.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">5.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">4.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_italic\">living</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">8.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">4.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">3.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.30</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">3.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">4.77</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">3.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">4.03</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">3.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">3.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">3.72</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_italic\">office</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">9.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">1.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">1.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">2.98</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.57</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">1.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">1.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_italic\">pedestrian</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">13.44</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">1.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">3.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">2.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">1.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">2.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">1.46</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 5.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">all</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\">13.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">12.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\">3.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\">2.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\">2.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">3.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\">2.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\">2.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\">2.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\">2.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 5.0pt;\">2.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\">12.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 5.0pt;\">1.87</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "validation",
            "second",
            "wer",
            "configurations",
            "baselines",
            "group",
            "size",
            "living",
            "bound",
            "â†“downarrow",
            "various",
            "ncb1024ntextcb1024",
            "our",
            "sampling",
            "ncbntextcb",
            "strategies",
            "speechless",
            "test",
            "first",
            "performance",
            "meanrepeat",
            "bus",
            "codebook",
            "pedestrian",
            "asr",
            "compares",
            "czstextctextzs",
            "office",
            "varies",
            "fixed",
            "model",
            "best",
            "all",
            "upper",
            "whisper",
            "adapter",
            "cafe",
            "ctrtextctexttr",
            "vbdemand",
            "baseline"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The latent decoder <math alttext=\"D_{L}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mi>L</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D_{L}(\\cdot)</annotation></semantics></math> reconstructs Whisper-like embeddings from the quantized outputs. It consists of a linear projection <math alttext=\"P_{u}\\in\\mathbb{R}^{64\\times 1024}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mi>u</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>64</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1024</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">P_{u}\\in\\mathbb{R}^{64\\times 1024}</annotation></semantics></math>, an up-sampling module, and a transformer refinement block. <math alttext=\"P_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">P_{u}</annotation></semantics></math> restores the embedding dimension, and the up-sampler restores sequence length <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math>. We compare repeat and transposed 1D convolution for up-sampling. The simpler repeat method, where tokens are duplicated in time, performed better in our setup (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.T1\" title=\"Table 1 &#8227; 2.4.4 Noise Classification Loss &#8227; 2.4 Training Cost Function &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). The transformer refinement block aids in smoothing artifacts due to up-sampling and aligning with Whisper&#8217;s latent space.</p>\n\n",
            "<p class=\"ltx_p\">We use the frozen <span class=\"ltx_text ltx_font_italic\">whisper-medium</span> model on the VB-Demand test split to establish a performance baseline. Given that 1) Our method inserts adapter-style modules between the Whisper encoder and decoder without fine-tuning either and 2) a degree of semantic loss is inevitable in our approach due to quantization; We consider an additional baseline model (+Adapter in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.T1\" title=\"Table 1 &#8227; 2.4.4 Noise Classification Loss &#8227; 2.4 Training Cost Function &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) where we insert trainable adapters&#8212;without discretization&#8212;between encoder and decoder. Each adapter block consists of 2 MLP layers with a GELU activation function between them. This black-box adapter provides a non-interpretable baseline approach that does not suffer info loss due to quantization. Lastly, the authors of Speechless <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx14\" title=\"\">14</a>]</cite> show that VQ-VAE training with supervision only for semantic alignment results in poor performance in noisy conditions, hence we include results from their 2560-codebook model under similar conditions.</p>\n\n",
            "<p class=\"ltx_p\">In our first study, we fixed the downsampling to simple mean pooling and upsampling to repeat (MR), varying only the codebook size. After identifying <math alttext=\"N_{\\text{CB}}=1024\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mtext>CB</mtext></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">N_{\\text{CB}}=1024</annotation></semantics></math> as optimal, we studied the effect of different sampling strategies on ASR and NC performance. These strategies are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.T1\" title=\"Table 1 &#8227; 2.4.4 Noise Classification Loss &#8227; 2.4 Training Cost Function &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">Mean pooling relies heavily on MLP layers for embedding placement. Conv1D-based downsampling introduces additional learnable filters, while conv-transformer modules operate over both time and feature dimensions, enabling more complex token positioning. These results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.T1\" title=\"Table 1 &#8227; 2.4.4 Noise Classification Loss &#8227; 2.4 Training Cost Function &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. We further discuss these results in detail in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S4.SS1\" title=\"4.1 Quantitative Observations &#8227; 4 Results and Discussion &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">We present our findings in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.T1\" title=\"Table 1 &#8227; 2.4.4 Noise Classification Loss &#8227; 2.4 Training Cost Function &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The baseline whisper-medium model achieves a WER of 13.72% on the VBDemand noisy test set. Adding trainable linear adapters further reduces the WER to 3.78% (+Adapter; where no quantization info loss occurs). As shown, the majority of our configurations show performance gains compared to inserted linear MLP adapters, with our best approach (<math alttext=\"N_{\\text{CB}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>CB</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{CB}}</annotation></semantics></math>=1024, TR) achieving a WER of 2.47%. This shows a relative error reduction of 82% over the zero-shot whisper baseline, 35% over the trainable adapter and 80% over the base Speechless <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx14\" title=\"\">14</a>]</cite> model. The conv-transformer method enables strong embedding placement requiring 28% fewer epochs for convergence, while also showing the least amount of overfitting to the validation dataset.</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.T1\" title=\"Table 1 &#8227; 2.4.4 Noise Classification Loss &#8227; 2.4 Training Cost Function &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that our best model (TR) has a much lower WER of 1.87% with clean speech inputs (<math alttext=\"\\text{C}_{\\text{TR}}&gt;\\text{TR}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mtext>C</mtext><mtext>TR</mtext></msub><mo>&gt;</mo><mtext>TR</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{TR}}&gt;\\text{TR}</annotation></semantics></math>). This improved performance indicates that our model correctly encodes only semantic data, generalizing well to clean speech. Otherwise performance in clean speech would degrade due to information loss. We also perform inference on clean speech embeddings from the CHiME-4 dataset and as seen in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S4.F4\" title=\"Figure 4 &#8227; 4 Results and Discussion &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> (bottom-ii), disentangled noise embeddings are not class-separable when the model is input with clean speech. Whereas, as seen in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S4.F4\" title=\"Figure 4 &#8227; 4 Results and Discussion &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> (bottom-ii), even unseen noisy speech from CHiME-4 yields a good degree of class separability. These observations reinforce that our model separates out only valid noise information when available, while semantic information is correctly tokenized.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Discrete audio representations are gaining traction in speech modeling due to their interpretability and compatibility with large language models, but are not always optimized for noisy or real-world environments. Building on existing works that quantize Whisper embeddings for speech-to-unit modeling, we propose disentangling semantic speech content from background noise in the latent space. Our end-to-end model separates clean speech in the form of codebook tokens, while extracting interpretable noise vectors as quantization residue which are supervised via a lightweight classifier. We show that our approach improves alignment between clean/noisy speech and text, producing speech tokens that display a high degree of noise-invariance, and improves ASR performance. Keeping Whisper frozen, we show an 82% reduction in error rate compared to Whisper, and 35% improvement over baseline methods on the VBDemand test set. Further analyses show that the learned token space generalizes well to both seen and unseen acoustic conditions.</p>\n\n",
                "matched_terms": [
                    "our",
                    "codebook",
                    "model",
                    "whisper",
                    "asr",
                    "test",
                    "vbdemand",
                    "baseline",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in speech modeling have increasingly adopted discrete audio representations for tasks such as automatic speech recognition (ASR), text-to-speech (TTS), and speech-language modeling. Existing works on neural audio codecs, such as DAC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx1\" title=\"\">1</a>]</cite>, EnCodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx2\" title=\"\">2</a>]</cite>, and SoundStream <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx3\" title=\"\">3</a>]</cite>, focus on compressing audio into discrete tokens for high-quality reconstruction and low-bitrate transmission. Works like <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx5\" title=\"\">5</a>]</cite> demonstrate that with task-supervised training, these discrete representations can also yield competitive ASR results. Similarly, methods like <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx8\" title=\"\">8</a>]</cite> have shown that vector-quantized (VQ) tokens can capture compressed and interpretable speech units for TTS and speech generation. This trend is also motivated by the natural compatibility between discrete audio and tokenized sequences used in large language models (LLMs). For example, HuBERT-based speech representations <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx9\" title=\"\">9</a>]</cite> combined with LLMs can achieve strong ASR performance, as shown in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx10\" title=\"\">10</a>]</cite>. Unfortunately, a key issue with these VQ-based approaches is the information loss introduced by quantization, which can hinder downstream performance, particularly under noisy or adverse conditions.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most prior works focus on representing clean speech, which does not reflect real-world noisy scenarios, highlighting a gap in noise-robust discrete representations. Although OpenAI&#8217;s Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx11\" title=\"\">11</a>]</cite> exhibits strong performance under clean or moderately noisy conditions, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx13\" title=\"\">13</a>]</cite> show that its continuous latent representations still encode background noise, which may affect downstream tasks. This issue is echoed in Speechless <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx14\" title=\"\">14</a>]</cite>, which highlights a performance gap between clean and noisy inputs when using a similar VQ-module on Whisper embeddings. These observations suggest that quantized representations align well with clean inputs but fail to generalize well under noisy or real world settings, leading to semantic degradation. We posit that ASR supervision and semantic alignment alone are insufficient to address this issue. Instead, jointly supervising both semantic and noise representations provides a more effective path to robust speech modeling.</p>\n\n",
                "matched_terms": [
                    "speechless",
                    "whisper",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Aligned with this hypothesis, we introduce a vector quantization (VQ) module that separates clean speech features from noise in Whisper&#8217;s latent space. We freeze the Whisper encoder and decoder and train lightweight modules that align with clean targets while guiding the quantization residue to capture noise. Our key idea is to frame disentanglement as a vector difference: the quantization residue is treated as an explicit and interpretable representation of background noise.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to approaches that rely on partitioning or multiple codebooks, we use a single codebook to capture speech semantics. Although traditional RVQ methods quantize the residue at multiple stages, our setup uses a single-stage quantizer and interprets the unquantized residue as an explicit estimate of background noise. A lightweight classifier supervises this residue without mapping it to discrete tokens. While De&#8217;hubert relies on fine-tuned HuBERT embeddings and shows gains in ASR performance, our system benefits from Whisper&#8217;s ASR pretraining, providing a strong semantic prior.</p>\n\n",
                "matched_terms": [
                    "codebook",
                    "performance",
                    "asr",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose an end-to-end model that disentangles speech from noise in the latent space of Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx11\" title=\"\">11</a>]</cite> using a vector quantizer that captures semantic features and treats quantization error as noise.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite information loss due to imperfect disentanglement, our method achieves competitive ASR performance, preserving useful semantic information.</p>\n\n",
                "matched_terms": [
                    "our",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build on the work of Speechless <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx14\" title=\"\">14</a>]</cite>, a speech-to-unit model using residual vector quantization (RVQ). Our model uses the pretrained <span class=\"ltx_text ltx_font_italic\">whisper-medium</span> encoder to provide latent speech embeddings, and the Whisper encoder and decoder remain frozen during training. Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.F1\" title=\"Figure 1 &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the overall architecture, and in this section we explain the underlying modules.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechless",
                    "whisper",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.F2\" title=\"Figure 2 &#8227; 2.1.2 Latent Encoder &#8227; 2.1 Extended Encoder &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the latent encoder consists of a downsampling module, followed by a projection layer <math alttext=\"P_{d}\\in\\mathbb{R}^{1024\\times 64}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mi>d</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>1024</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>64</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">P_{d}\\in\\mathbb{R}^{1024\\times 64}</annotation></semantics></math>. Downsampling reduces the temporal resolution by a factor of 2, such that each token encodes &#160;40ms of audio. In the english language, phonemes range from a duration of 80-120ms with an average around 100ms <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx20\" title=\"\">20</a>]</cite>. This implies that our tokens are sub-phonetic, and sequences of tokens make up phonemes and words. We experiment with mean-pooling, strided 1D convolution, and conv-transformer from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx21\" title=\"\">21</a>]</cite>, with the latter performing best. As trainable parameters in the encoder are important for embedding placement, for pooling and conv1d, we further use an MLP block. The output is seen in (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.E1\" title=\"In 2.1.2 Latent Encoder &#8227; 2.1 Extended Encoder &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "best",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VQ creates a discrete bottleneck that captures task-relevant semantic information, yielding quantized embeddings <math alttext=\"Q(q_{e}(X))\\in\\mathbb{R}^{750\\times 64}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>q</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>750</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>64</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">Q(q_{e}(X))\\in\\mathbb{R}^{750\\times 64}</annotation></semantics></math>, derived from codebook <math alttext=\"\\mathcal{C}\\in\\mathbb{R}^{N_{\\text{CB}}\\times 64}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>N</mi><mtext>CB</mtext></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>64</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{C}\\in\\mathbb{R}^{N_{\\text{CB}}\\times 64}</annotation></semantics></math> of size <math alttext=\"N_{\\text{CB}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>CB</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{CB}}</annotation></semantics></math>. Here, <math alttext=\"q_{e}(X)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>q</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">q_{e}(X)</annotation></semantics></math> denotes the output of the latent encoder. <math alttext=\"Q(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q(\\cdot)</annotation></semantics></math> performs the nearest-neighbor lookup in <math alttext=\"\\mathcal{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><annotation encoding=\"application/x-tex\">\\mathcal{C}</annotation></semantics></math>, whose codes represent denoised speech embeddings, which are further used for noise disentanglement and in the latent decoder. As a straight-through estimator is used, gradients will not directly impact codebook embeddings. Instead estimated moving averages are used to update the codebook with a codebook decay of 0.9. The optimal value of <math alttext=\"N_{\\text{CB}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>CB</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{CB}}</annotation></semantics></math> varies with the size of the training corpus and its associated vocabulary. In our case <math alttext=\"N_{\\text{CB}}=1024\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mtext>CB</mtext></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">N_{\\text{CB}}=1024</annotation></semantics></math> showed the best results.</p>\n\n",
                "matched_terms": [
                    "varies",
                    "codebook",
                    "ncbntextcb",
                    "size",
                    "best",
                    "ncb1024ntextcb1024",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Sequence information is preserved by 1) using fixed codebook token as padding token and 2) restoring positional encoding; This is not required when down-sampling using a conv-transformer as we find it is able to retain sequence information and embed all padded timesteps close together.</p>\n\n",
                "matched_terms": [
                    "codebook",
                    "all",
                    "fixed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This encourages the noisy embeddings to stay close to their clean counterparts in latent space. The best performance was observed when <math alttext=\"\\mathcal{W}_{\\alpha}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119986;</mi><mi>&#945;</mi></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\mathcal{W}_{\\alpha}=0.5</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "best",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Legend:<span class=\"ltx_text ltx_font_upright\">\nWhisper = Whisper-medium zero-shot, +Adapter = Whisper-medium with trainable linear adapter; \nMR = (mean, repeat),\nMC = (mean, conv1d),\nCC = (conv1d, conv1d),\nCR = (conv1d, repeat),\nTR = (conv-transformer, repeat),\nTC = (conv-transformer, conv1d);\n<br class=\"ltx_break\"/><math alttext=\"\\text{C}_{\\text{ZS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m7\" intent=\":literal\"><semantics><msub><mtext>C</mtext><mtext>ZS</mtext></msub><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{ZS}}</annotation></semantics></math> = Whisper-medium zero-shot with <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>lean speech input\n<math alttext=\"\\text{C}_{\\text{TR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m8\" intent=\":literal\"><semantics><msub><mtext>C</mtext><mtext>TR</mtext></msub><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{TR}}</annotation></semantics></math> = Our model (1024-codebook TR setting) with <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>lean speech input</span></span>\n</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "whisper",
                    "adapter",
                    "ctrtextctexttr",
                    "czstextctextzs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VBDemand <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx25\" title=\"\">25</a>]</cite> is widely used for speech enhancement and robust ASR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx26\" title=\"\">26</a>]</cite>. It includes clean-noisy speech pairs with transcriptions. We combine two subsets: one with 11,572 utterances (9.5h, English accents) and another with 21,225 utterances (19h, US and Scottish accents). Of this 28.5h, we reserve 1.5h (disjoint speakers) for validation and use the rest for training. Each training and validation sample contains 1 of 10 noise types. The noisy test set has 5 unseen noise types: bus, cafe, living room, office, and pedestrian.</p>\n\n",
                "matched_terms": [
                    "office",
                    "validation",
                    "bus",
                    "pedestrian",
                    "living",
                    "asr",
                    "test",
                    "cafe",
                    "vbdemand"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the <span class=\"ltx_text ltx_font_typewriter\">test-real</span> and <span class=\"ltx_text ltx_font_typewriter\">test-simu</span> splits from CHiME-4 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx27\" title=\"\">27</a>]</cite> for additional qualitative analysis to assess the generalization ability of our model under out-of-distribution noise and speaker conditions, as the clean-noisy pairs are perfectly time-aligned with various noise types.</p>\n\n",
                "matched_terms": [
                    "model",
                    "various",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments were conducted on 2 A100 GPUs with a total batch size of 64 (32 per GPU). We used a linear learning rate warm-up for the first 500 steps, followed by a cosine annealing schedule. The initial learning rate was set to <math alttext=\"1\\text{e}^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mtext>e</mtext><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\text{e}^{-3}</annotation></semantics></math>, and optimization was performed using AdamW (<math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math>) until convergence. We initialize the whisper components using pretrained <span class=\"ltx_text ltx_font_italic\">whisper-medium</span> checkpoints from the official OpenAI Whisper repository<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span>. Codebooks were initialized using Kaiming noise as described in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx14\" title=\"\">14</a>]</cite>. All WER results presented are derived using autoregressive decoding with a beam-size of 10 and temperature of 0.0.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "size",
                    "all",
                    "whisper",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the Whisper decoder&#8217;s final objective is next-token prediction, we correlate ASR performance with the degree of semantic information retained in the latent embedding. We use word error rate (WER) as the primary evaluation metric for both our model and the baselines, and use relative error reduction (RER %) to compare performances.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model",
                    "baselines",
                    "performance",
                    "asr",
                    "whisper",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the performance with clean speech inputs as the upper-bound. The VBDemand clean test-set is used as the input and we tabulate zero-shot inference using Whisper-medium (<math alttext=\"\\text{C}_{\\text{ZS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m1\" intent=\":literal\"><semantics><msub><mtext>C</mtext><mtext>ZS</mtext></msub><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{ZS}}</annotation></semantics></math>) as well as our best model&#8217;s performance (<math alttext=\"\\text{C}_{\\text{TR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mtext>C</mtext><mtext>TR</mtext></msub><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{TR}}</annotation></semantics></math>: <math alttext=\"N_{\\text{CB}}=1024\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mtext>CB</mtext></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">N_{\\text{CB}}=1024</annotation></semantics></math> with conv-tranformer downsampling and repeat upsampling) with clean inputs.</p>\n\n",
                "matched_terms": [
                    "our",
                    "best",
                    "performance",
                    "ctrtextctexttr",
                    "vbdemand",
                    "ncb1024ntextcb1024",
                    "czstextctextzs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We show that three factors contribute to the effectiveness of our method: (1) the VQ codebook size, (2) the number of trainable parameters before VQ, and (3) the choice of downsampling and upsampling strategies. (2) and (3) are tightly coupled as layers preceding the VQ module are expected to improve embedding placement and help the model learn meaningful representations while also performing downsampling.</p>\n\n",
                "matched_terms": [
                    "codebook",
                    "strategies",
                    "model",
                    "size",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Further, in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S3.T2\" title=\"Table 2 &#8227; 3.3.3 Upper Bound &#8227; 3.3 Evaluation &#8227; 3 Experiments &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we analyse the significance of each loss term. The first setting only employs ASR task supervision resulting in a WER of 3.89% which is poorer than adapter training. An RER of 6.7% is achieved by adding only noise disentanglement. The complete loss function results in a WER of 2.87% and total RER of 20.9%, showing that each term of our proposed loss function plays a significant role.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "adapter",
                    "asr",
                    "first",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S4.F4\" title=\"Figure 4 &#8227; 4 Results and Discussion &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> (top) shows T-SNE projections of representations from the penultimate layers of the noise-classifier of noisy samples from the validation set after model convergence. We observe that the information removed via the quantization error is clearly noise-class separable, and correlates directly with a distribution that helps the classifier categorize the background noise. The final noise classification accuracy for the VBDemand validation dataset is 98.23%.</p>\n\n",
                "matched_terms": [
                    "vbdemand",
                    "validation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To examine the noise invariance of our latent representations, we examined embeddings of the same speech in the clean setting and with various noise samples mixed. Ideally the embeddings should be similar (invariant) at each time-step. To visualize this we make use of L2 distance of codebook embeddings of the clean and noisy speech. Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S4.F3\" title=\"Figure 3 &#8227; 4 Results and Discussion &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows both the clean audio sample from the CHiME-4 test dataset, along with L2 distances between each noisy sample and the original clean sample at each timestep. We see that the majority of embeddings and sequences of embedding show a high degree of noise invariance (dark purple). Patches of high embedding variance correlate directly with silence where the quantizer may not have enough information to generate useful embeddings (R1, R2, R3, R4). Localized errors occur across many noise types and seem to be due to fricative sounds and because CHiME-4 is outside of the training distribution.</p>\n\n",
                "matched_terms": [
                    "codebook",
                    "various",
                    "test",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a novel disentanglement framework to improve the noise-robustness of discrete speech representations, by separating semantic speech tokens from background noise using vector quantization over Whisper embeddings. Without fine-tuning the Whisper encoder, our method achieves an 82% error reduction compared to zero-shot Whisper and a 35% improvement over adapter-based baselines, demonstrating the benefit of dual supervision over semantic and noise representations. Unlike prior methods such as Speechless that align only semantic content, our model jointly learns semantic and noise representations, yielding significantly better performance across noisy conditions (80% reduction in WER). The quantization residue serves as an interpretable noise embedding, enabling accurate classification and an explainable latent space. Visual analysis confirms that the model generalizes well to unseen noise, while preserving clean speech performance.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model",
                    "baselines",
                    "speechless",
                    "performance",
                    "whisper",
                    "our"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Ablation study of loss terms. All configurations use Whisper-medium, NCB=512N_{\\text{CB}}=512, and (mean, repeat) DS/US setup.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Losses Used</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">DS/US</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><math alttext=\"N_{\\text{CB}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext class=\"ltx_mathvariant_bold\">CB</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{CB}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">WER (%)</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"5\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Ours</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Only VQ and <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">MR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3.89</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#8194;&#8202;&#8195;+ <math alttext=\"\\mathcal{L}_{\\text{NC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>NC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{NC}}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">MR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">512</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#8194;&#8202;&#8195;&#8195;+ Only <math alttext=\"\\mathcal{L}_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\alpha}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">MR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">512</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3.36</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#8194;&#8202;&#8195;&#8195;+ Only <math alttext=\"\\mathcal{L}_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\beta}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">MR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">512</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3.11</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#8194;&#8202;&#8195;&#8195;+ <math alttext=\"\\mathcal{L}_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\alpha}</annotation></semantics></math> + <math alttext=\"\\mathcal{L}_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\beta}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">MR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">2.87</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "wer",
            "ncb512ntextcb512",
            "configurations",
            "setup",
            "â†“downarrow",
            "â„’ncmathcalltextnc",
            "â„’asrmathcalltextasr",
            "study",
            "ncbntextcb",
            "repeat",
            "ablation",
            "terms",
            "loss",
            "used",
            "whispermedium",
            "losses",
            "mean",
            "only",
            "ours",
            "dsus",
            "use",
            "model",
            "â„’Î²mathcallbeta",
            "all",
            "â„’Î±mathcallalpha"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Through experiments, we found that <math alttext=\"\\mathcal{W}_{\\alpha}+\\mathcal{W}_{\\beta}=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119986;</mi><mi>&#945;</mi></msub><mo>+</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119986;</mi><mi>&#946;</mi></msub></mrow><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\mathcal{W}_{\\alpha}+\\mathcal{W}_{\\beta}=1.0</annotation></semantics></math> gave the most consistent results, and setting <math alttext=\"\\mathcal{W}_{\\text{ASR}}=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119986;</mi><mtext>ASR</mtext></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\mathcal{W}_{\\text{ASR}}=1.0</annotation></semantics></math> provides a stable training direction early-on when quantizer outputs are still noisy. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S3.T2\" title=\"Table 2 &#8227; 3.3.3 Upper Bound &#8227; 3.3 Evaluation &#8227; 3 Experiments &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the relevance of each loss term.</p>\n\n",
            "<p class=\"ltx_p\">To study the contribution of each loss term, we perform an experiment where loss components are added incrementally. This illustrates how each loss term contributes towards improving the model&#8217;s performance. Results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S3.T2\" title=\"Table 2 &#8227; 3.3.3 Upper Bound &#8227; 3.3 Evaluation &#8227; 3 Experiments &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">Further, in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S3.T2\" title=\"Table 2 &#8227; 3.3.3 Upper Bound &#8227; 3.3 Evaluation &#8227; 3 Experiments &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we analyse the significance of each loss term. The first setting only employs ASR task supervision resulting in a WER of 3.89% which is poorer than adapter training. An RER of 6.7% is achieved by adding only noise disentanglement. The complete loss function results in a WER of 2.87% and total RER of 20.9%, showing that each term of our proposed loss function plays a significant role.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in speech modeling have increasingly adopted discrete audio representations for tasks such as automatic speech recognition (ASR), text-to-speech (TTS), and speech-language modeling. Existing works on neural audio codecs, such as DAC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx1\" title=\"\">1</a>]</cite>, EnCodec <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx2\" title=\"\">2</a>]</cite>, and SoundStream <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx3\" title=\"\">3</a>]</cite>, focus on compressing audio into discrete tokens for high-quality reconstruction and low-bitrate transmission. Works like <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx5\" title=\"\">5</a>]</cite> demonstrate that with task-supervised training, these discrete representations can also yield competitive ASR results. Similarly, methods like <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx8\" title=\"\">8</a>]</cite> have shown that vector-quantized (VQ) tokens can capture compressed and interpretable speech units for TTS and speech generation. This trend is also motivated by the natural compatibility between discrete audio and tokenized sequences used in large language models (LLMs). For example, HuBERT-based speech representations <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx9\" title=\"\">9</a>]</cite> combined with LLMs can achieve strong ASR performance, as shown in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx10\" title=\"\">10</a>]</cite>. Unfortunately, a key issue with these VQ-based approaches is the information loss introduced by quantization, which can hinder downstream performance, particularly under noisy or adverse conditions.</p>\n\n",
                "matched_terms": [
                    "used",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to approaches that rely on partitioning or multiple codebooks, we use a single codebook to capture speech semantics. Although traditional RVQ methods quantize the residue at multiple stages, our setup uses a single-stage quantizer and interprets the unquantized residue as an explicit estimate of background noise. A lightweight classifier supervises this residue without mapping it to discrete tokens. While De&#8217;hubert relies on fine-tuned HuBERT embeddings and shows gains in ASR performance, our system benefits from Whisper&#8217;s ASR pretraining, providing a strong semantic prior.</p>\n\n",
                "matched_terms": [
                    "use",
                    "setup"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build on the work of Speechless <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx14\" title=\"\">14</a>]</cite>, a speech-to-unit model using residual vector quantization (RVQ). Our model uses the pretrained <span class=\"ltx_text ltx_font_italic\">whisper-medium</span> encoder to provide latent speech embeddings, and the Whisper encoder and decoder remain frozen during training. Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.F1\" title=\"Figure 1 &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the overall architecture, and in this section we explain the underlying modules.</p>\n\n",
                "matched_terms": [
                    "whispermedium",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use paired clean and noisy monophonic signals, <math alttext=\"X^{\\prime}\\in\\mathbb{R}^{1\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>X</mi><mo>&#8242;</mo></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X^{\\prime}\\in\\mathbb{R}^{1\\times T}</annotation></semantics></math> and <math alttext=\"X\\in\\mathbb{R}^{1\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X\\in\\mathbb{R}^{1\\times T}</annotation></semantics></math>, containing the same semantic content. The clean signal is only used for reference in loss computation, while only the noisy signal is passed through the model.</p>\n\n",
                "matched_terms": [
                    "use",
                    "model",
                    "loss",
                    "used",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VQ creates a discrete bottleneck that captures task-relevant semantic information, yielding quantized embeddings <math alttext=\"Q(q_{e}(X))\\in\\mathbb{R}^{750\\times 64}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>q</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>750</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>64</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">Q(q_{e}(X))\\in\\mathbb{R}^{750\\times 64}</annotation></semantics></math>, derived from codebook <math alttext=\"\\mathcal{C}\\in\\mathbb{R}^{N_{\\text{CB}}\\times 64}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msub><mi>N</mi><mtext>CB</mtext></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>64</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{C}\\in\\mathbb{R}^{N_{\\text{CB}}\\times 64}</annotation></semantics></math> of size <math alttext=\"N_{\\text{CB}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>CB</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{CB}}</annotation></semantics></math>. Here, <math alttext=\"q_{e}(X)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>q</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">q_{e}(X)</annotation></semantics></math> denotes the output of the latent encoder. <math alttext=\"Q(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q(\\cdot)</annotation></semantics></math> performs the nearest-neighbor lookup in <math alttext=\"\\mathcal{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><annotation encoding=\"application/x-tex\">\\mathcal{C}</annotation></semantics></math>, whose codes represent denoised speech embeddings, which are further used for noise disentanglement and in the latent decoder. As a straight-through estimator is used, gradients will not directly impact codebook embeddings. Instead estimated moving averages are used to update the codebook with a codebook decay of 0.9. The optimal value of <math alttext=\"N_{\\text{CB}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>CB</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{CB}}</annotation></semantics></math> varies with the size of the training corpus and its associated vocabulary. In our case <math alttext=\"N_{\\text{CB}}=1024\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mtext>CB</mtext></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">N_{\\text{CB}}=1024</annotation></semantics></math> showed the best results.</p>\n\n",
                "matched_terms": [
                    "ncbntextcb",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We guide this separation using auxiliary loss terms that push <math alttext=\"Q(q_{e}(X))\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>q</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Q(q_{e}(X))</annotation></semantics></math> toward clean semantics and <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p3.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> toward noise. Inspired by <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx12\" title=\"\">12</a>]</cite>, we apply a lightweight classifier (optionally transformer-augmented) to <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p3.m3\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> for noise classification. This supervision enforces extraction of noise features in <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p3.m4\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The latent decoder <math alttext=\"D_{L}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mi>L</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D_{L}(\\cdot)</annotation></semantics></math> reconstructs Whisper-like embeddings from the quantized outputs. It consists of a linear projection <math alttext=\"P_{u}\\in\\mathbb{R}^{64\\times 1024}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mi>u</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>64</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1024</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">P_{u}\\in\\mathbb{R}^{64\\times 1024}</annotation></semantics></math>, an up-sampling module, and a transformer refinement block. <math alttext=\"P_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>P</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">P_{u}</annotation></semantics></math> restores the embedding dimension, and the up-sampler restores sequence length <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math>. We compare repeat and transposed 1D convolution for up-sampling. The simpler repeat method, where tokens are duplicated in time, performed better in our setup (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.T1\" title=\"Table 1 &#8227; 2.4.4 Noise Classification Loss &#8227; 2.4 Training Cost Function &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). The transformer refinement block aids in smoothing artifacts due to up-sampling and aligning with Whisper&#8217;s latent space.</p>\n\n",
                "matched_terms": [
                    "repeat",
                    "setup"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a loss function that combines four components: ASR cross-entropy (<math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>), VQ commitment loss (<math alttext=\"\\mathcal{L}_{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>&#945;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\alpha}</annotation></semantics></math>), a semantic disentanglement loss (<math alttext=\"\\mathcal{L}_{\\beta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>&#946;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\beta}</annotation></semantics></math>), and an auxiliary noise classification loss (<math alttext=\"\\mathcal{L}_{\\text{NC}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>NC</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{NC}}</annotation></semantics></math>). The total cost is defined as:</p>\n\n",
                "matched_terms": [
                    "â„’ncmathcalltextnc",
                    "â„’asrmathcalltextasr",
                    "â„’Î²mathcallbeta",
                    "loss",
                    "â„’Î±mathcallalpha"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a noisy speech signal <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math>, which passes through the encoder, quantizer, and decoder to yield the model output <math alttext=\"q_{d}(Q(q_{e}(X)))\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>q</mi><mi>d</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>q</mi><mi>e</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">q_{d}(Q(q_{e}(X)))</annotation></semantics></math>, and a sequence of previous tokens <math alttext=\"y_{t-1},...,y_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">y_{t-1},...,y_{1}</annotation></semantics></math>, the cross-entropy loss aims to minimize the error in predicting the next token <math alttext=\"y_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">y_{t}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To explicitly model the noise component, we compute the quantization residue as seen in (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.E2\" title=\"In 2.2.1 Noise Disentanglement &#8227; 2.2 Vector Quantization &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) and pass it through a lightweight classifier <math alttext=\"NC(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.SSS4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">NC(\\cdot)</annotation></semantics></math> to predict the noise class <math alttext=\"Y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.SSS4.p1.m2\" intent=\":literal\"><semantics><msup><mi>Y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">Y^{\\prime}</annotation></semantics></math>. The loss is defined as:</p>\n\n",
                "matched_terms": [
                    "loss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_figure_panel ltx_align_center\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Legend:<span class=\"ltx_text ltx_font_upright\">\nWhisper = Whisper-medium zero-shot, +Adapter = Whisper-medium with trainable linear adapter; \nMR = (mean, repeat),\nMC = (mean, conv1d),\nCC = (conv1d, conv1d),\nCR = (conv1d, repeat),\nTR = (conv-transformer, repeat),\nTC = (conv-transformer, conv1d);\n<br class=\"ltx_break\"/><math alttext=\"\\text{C}_{\\text{ZS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m7\" intent=\":literal\"><semantics><msub><mtext>C</mtext><mtext>ZS</mtext></msub><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{ZS}}</annotation></semantics></math> = Whisper-medium zero-shot with <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>lean speech input\n<math alttext=\"\\text{C}_{\\text{TR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m8\" intent=\":literal\"><semantics><msub><mtext>C</mtext><mtext>TR</mtext></msub><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{TR}}</annotation></semantics></math> = Our model (1024-codebook TR setting) with <span class=\"ltx_text ltx_framed ltx_framed_underline\">C</span>lean speech input</span></span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "mean",
                    "whispermedium",
                    "repeat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VBDemand <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx25\" title=\"\">25</a>]</cite> is widely used for speech enhancement and robust ASR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx26\" title=\"\">26</a>]</cite>. It includes clean-noisy speech pairs with transcriptions. We combine two subsets: one with 11,572 utterances (9.5h, English accents) and another with 21,225 utterances (19h, US and Scottish accents). Of this 28.5h, we reserve 1.5h (disjoint speakers) for validation and use the rest for training. Each training and validation sample contains 1 of 10 noise types. The noisy test set has 5 unseen noise types: bus, cafe, living room, office, and pedestrian.</p>\n\n",
                "matched_terms": [
                    "used",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the <span class=\"ltx_text ltx_font_typewriter\">test-real</span> and <span class=\"ltx_text ltx_font_typewriter\">test-simu</span> splits from CHiME-4 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx27\" title=\"\">27</a>]</cite> for additional qualitative analysis to assess the generalization ability of our model under out-of-distribution noise and speaker conditions, as the clean-noisy pairs are perfectly time-aligned with various noise types.</p>\n\n",
                "matched_terms": [
                    "use",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments were conducted on 2 A100 GPUs with a total batch size of 64 (32 per GPU). We used a linear learning rate warm-up for the first 500 steps, followed by a cosine annealing schedule. The initial learning rate was set to <math alttext=\"1\\text{e}^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mtext>e</mtext><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\text{e}^{-3}</annotation></semantics></math>, and optimization was performed using AdamW (<math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.95</annotation></semantics></math>) until convergence. We initialize the whisper components using pretrained <span class=\"ltx_text ltx_font_italic\">whisper-medium</span> checkpoints from the official OpenAI Whisper repository<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span>. Codebooks were initialized using Kaiming noise as described in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx14\" title=\"\">14</a>]</cite>. All WER results presented are derived using autoregressive decoding with a beam-size of 10 and temperature of 0.0.</p>\n\n",
                "matched_terms": [
                    "all",
                    "used",
                    "whispermedium",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As the Whisper decoder&#8217;s final objective is next-token prediction, we correlate ASR performance with the degree of semantic information retained in the latent embedding. We use word error rate (WER) as the primary evaluation metric for both our model and the baselines, and use relative error reduction (RER %) to compare performances.</p>\n\n",
                "matched_terms": [
                    "use",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the frozen <span class=\"ltx_text ltx_font_italic\">whisper-medium</span> model on the VB-Demand test split to establish a performance baseline. Given that 1) Our method inserts adapter-style modules between the Whisper encoder and decoder without fine-tuning either and 2) a degree of semantic loss is inevitable in our approach due to quantization; We consider an additional baseline model (+Adapter in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.T1\" title=\"Table 1 &#8227; 2.4.4 Noise Classification Loss &#8227; 2.4 Training Cost Function &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) where we insert trainable adapters&#8212;without discretization&#8212;between encoder and decoder. Each adapter block consists of 2 MLP layers with a GELU activation function between them. This black-box adapter provides a non-interpretable baseline approach that does not suffer info loss due to quantization. Lastly, the authors of Speechless <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx14\" title=\"\">14</a>]</cite> show that VQ-VAE training with supervision only for semantic alignment results in poor performance in noisy conditions, hence we include results from their 2560-codebook model under similar conditions.</p>\n\n",
                "matched_terms": [
                    "use",
                    "model",
                    "loss",
                    "whispermedium",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider the performance with clean speech inputs as the upper-bound. The VBDemand clean test-set is used as the input and we tabulate zero-shot inference using Whisper-medium (<math alttext=\"\\text{C}_{\\text{ZS}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m1\" intent=\":literal\"><semantics><msub><mtext>C</mtext><mtext>ZS</mtext></msub><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{ZS}}</annotation></semantics></math>) as well as our best model&#8217;s performance (<math alttext=\"\\text{C}_{\\text{TR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mtext>C</mtext><mtext>TR</mtext></msub><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{TR}}</annotation></semantics></math>: <math alttext=\"N_{\\text{CB}}=1024\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mtext>CB</mtext></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">N_{\\text{CB}}=1024</annotation></semantics></math> with conv-tranformer downsampling and repeat upsampling) with clean inputs.</p>\n\n",
                "matched_terms": [
                    "used",
                    "whispermedium",
                    "repeat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our first study, we fixed the downsampling to simple mean pooling and upsampling to repeat (MR), varying only the codebook size. After identifying <math alttext=\"N_{\\text{CB}}=1024\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mtext>CB</mtext></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding=\"application/x-tex\">N_{\\text{CB}}=1024</annotation></semantics></math> as optimal, we studied the effect of different sampling strategies on ASR and NC performance. These strategies are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.T1\" title=\"Table 1 &#8227; 2.4.4 Noise Classification Loss &#8227; 2.4 Training Cost Function &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "study",
                    "mean",
                    "only",
                    "repeat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present our findings in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.T1\" title=\"Table 1 &#8227; 2.4.4 Noise Classification Loss &#8227; 2.4 Training Cost Function &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The baseline whisper-medium model achieves a WER of 13.72% on the VBDemand noisy test set. Adding trainable linear adapters further reduces the WER to 3.78% (+Adapter; where no quantization info loss occurs). As shown, the majority of our configurations show performance gains compared to inserted linear MLP adapters, with our best approach (<math alttext=\"N_{\\text{CB}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>N</mi><mtext>CB</mtext></msub><annotation encoding=\"application/x-tex\">N_{\\text{CB}}</annotation></semantics></math>=1024, TR) achieving a WER of 2.47%. This shows a relative error reduction of 82% over the zero-shot whisper baseline, 35% over the trainable adapter and 80% over the base Speechless <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#bib.bibx14\" title=\"\">14</a>]</cite> model. The conv-transformer method enables strong embedding placement requiring 28% fewer epochs for convergence, while also showing the least amount of overfitting to the validation dataset.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "configurations",
                    "model",
                    "loss",
                    "whispermedium"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S2.T1\" title=\"Table 1 &#8227; 2.4.4 Noise Classification Loss &#8227; 2.4 Training Cost Function &#8227; 2 Methodology &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that our best model (TR) has a much lower WER of 1.87% with clean speech inputs (<math alttext=\"\\text{C}_{\\text{TR}}&gt;\\text{TR}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mtext>C</mtext><mtext>TR</mtext></msub><mo>&gt;</mo><mtext>TR</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{C}_{\\text{TR}}&gt;\\text{TR}</annotation></semantics></math>). This improved performance indicates that our model correctly encodes only semantic data, generalizing well to clean speech. Otherwise performance in clean speech would degrade due to information loss. We also perform inference on clean speech embeddings from the CHiME-4 dataset and as seen in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S4.F4\" title=\"Figure 4 &#8227; 4 Results and Discussion &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> (bottom-ii), disentangled noise embeddings are not class-separable when the model is input with clean speech. Whereas, as seen in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.25150v1#S4.F4\" title=\"Figure 4 &#8227; 4 Results and Discussion &#8227; Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> (bottom-ii), even unseen noisy speech from CHiME-4 yields a good degree of class separability. These observations reinforce that our model separates out only valid noise information when available, while semantic information is correctly tokenized.</p>\n\n",
                "matched_terms": [
                    "model",
                    "only",
                    "loss",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a novel disentanglement framework to improve the noise-robustness of discrete speech representations, by separating semantic speech tokens from background noise using vector quantization over Whisper embeddings. Without fine-tuning the Whisper encoder, our method achieves an 82% error reduction compared to zero-shot Whisper and a 35% improvement over adapter-based baselines, demonstrating the benefit of dual supervision over semantic and noise representations. Unlike prior methods such as Speechless that align only semantic content, our model jointly learns semantic and noise representations, yielding significantly better performance across noisy conditions (80% reduction in WER). The quantization residue serves as an interpretable noise embedding, enabling accurate classification and an explainable latent space. Visual analysis confirms that the model generalizes well to unseen noise, while preserving clean speech performance.</p>\n\n",
                "matched_terms": [
                    "only",
                    "model",
                    "wer"
                ]
            }
        ]
    }
}