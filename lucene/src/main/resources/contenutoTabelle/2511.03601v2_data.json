{
    "S5.T1": {
        "caption": "Table 1: Performance of Step-Audio-EditX on Emotion and Speaking Style Editing.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">\n<span class=\"ltx_text ltx_font_bold\">Emotion</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">\n<span class=\"ltx_text ltx_font_bold\">Speaking Style</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">0</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">1</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">2</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">3</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">0</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">1</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">2</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">3</sub></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">73.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">77.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">68.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">English</td>\n<td class=\"ltx_td ltx_align_center\">51.2</td>\n<td class=\"ltx_td ltx_align_center\">60.0</td>\n<td class=\"ltx_td ltx_align_center\">63.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">64.2</span></td>\n<td class=\"ltx_td ltx_align_center\">48.8</td>\n<td class=\"ltx_td ltx_align_center\">63.4</td>\n<td class=\"ltx_td ltx_align_center\">62.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">64.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">Average</span></td>\n<td class=\"ltx_td ltx_align_center\">55.0</td>\n<td class=\"ltx_td ltx_align_center\">66.8</td>\n<td class=\"ltx_td ltx_align_center\">69.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">71.0</span></td>\n<td class=\"ltx_td ltx_align_center\">44.6</td>\n<td class=\"ltx_td ltx_align_center\">62.8</td>\n<td class=\"ltx_td ltx_align_center\">63.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">66.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Chinese (Prompt-Fixed)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">57.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">73.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">76.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">75.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">65.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">63.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">English (Prompt-Fixed)</td>\n<td class=\"ltx_td ltx_align_center\">49.7</td>\n<td class=\"ltx_td ltx_align_center\">60.4</td>\n<td class=\"ltx_td ltx_align_center\">61.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">62.8</span></td>\n<td class=\"ltx_td ltx_align_center\">50.0</td>\n<td class=\"ltx_td ltx_align_center\">63.4</td>\n<td class=\"ltx_td ltx_align_center\">63.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">63.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">Average</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">53.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">66.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">68.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">69.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">45.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">62.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">64.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">63.8</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "â†‘uparrow",
            "editing",
            "english",
            "language",
            "iter2",
            "iter3",
            "iter0",
            "average",
            "style",
            "promptfixed",
            "performance",
            "speaking",
            "iter1",
            "stepaudioeditx",
            "chinese",
            "emotion"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative Editing Results.</span> As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T1\" title=\"Table 1 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, there was a significant boost in both emotion and speaking style accuracy after the initial edit of the Iter<sub class=\"ltx_sub\">0</sub> audio. Furthermore, with successive iterations of editing, the accuracy for both emotion and speaking style was further enhanced.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Audio Ablation.</span> Since the performance improvement in later iterations (starting from Iter<sub class=\"ltx_sub\">2</sub>) were attributed to both the dual-code and the prompt audio. To isolate the effect of the prompt audio, we conducted an ablation study in which the prompt audio was held constant across all iterations. As presented in the Prompt-Fixed section of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T1\" title=\"Table 1 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the accuracy for both emotion and speaking style continues to improve with an increasing number of editing iterations. This clearly demonstrates the effectiveness of our large-margin method.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Step-Audio-EditX, the first open-source LLM-based reinforcement learning audio model excelling at expressive and iterative audio editing&#8212;encompassing emotion, speaking style, and paralinguistics&#8212;alongside robust zero-shot text-to-speech (TTS) capabilities. Our core innovation lies in leveraging only large-margin synthetic data in post-training, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks. Our code and models are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/stepfun-ai/Step-Audio-EditX\" title=\"\">https://github.com/stepfun-ai/Step-Audio-EditX</a>.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "style",
                    "speaking",
                    "stepaudioeditx",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite considerable progress in zero-shot TTS synthesis, attributes such as emotion, style, accent, and timbre in the synthesized speech are still derived directly from the reference audio. This inherent limitation restricts independent control over these attributes. Although prepending style or emotion instructions to the input text offers a certain degree of controllability and often performs well for in-domain speakers <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">seedtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fireredtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>]</cite>, this approach faces challenges in disentangling speech attributes. In particular, the cloned voice often fails to effectively follow the provided style or emotion instructions.</p>\n\n",
                "matched_terms": [
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many previous studies on speech disentanglement have relied on approaches such as adversarial training<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">naturalspeech3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2022crossspeakeremotiondisentanglingtransfer</span>]</cite>, feature engineering<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2023dddmvcdecoupleddenoisingdiffusion</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Anastassiou2024VoiceShopAU</span>]</cite>, and innovative network architectures<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Jia2022ZeroShotAC</span>]</cite> to achieve attribute decoupling. In contrast, we propose a simple yet stable data-driven method. Specifically, we design a pipeline for generating high-quality data pairs that preserve identical linguistic content while exhibiting clearly distinguishable variations in one or a few attributes, such as emotion, style, accent, and paralinguistic features. By training models on such data pairs, we achieve effective attribute disentanglement, enabling to edit the attribute of input speech. Moreover, by applying multiple iterative \"editing\" steps, the intensity of a target attribute can be progressively enhanced or reduced. Beyond emotion, style and paralinguistic editing, we demonstrate that this approach can be extended to other applications, including speed rate adjustment, speech denoising, and silence trimming.\nIn this report, we outline our contributions and findings:</p>\n\n",
                "matched_terms": [
                    "style",
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing, encompassing emotion, speaking style, and paralinguistics, alongside robust zero-shot TTS capabilities.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "style",
                    "speaking",
                    "stepaudioeditx",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results show that emotion and speaking style can be controlled through post-training with large-margin data alone, eliminating the need for extra presentation modeling or adapter modules.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our prior work, we introduced an Audio-Edit synthesis model in Step-Audio<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stepaudio</span>]</cite> for nuanced emotional expressions and diverse speaking styles data generation. In this report, we retain the previous model along with the same audio tokenizer. The key modifications include an expanded range of emotions and speaking styles, the addition of zero-shot TTS and paralinguistic editing capabilities, as well as a reduction in model parameters from 130B to 3B. Leveraging large-margin synthetic data, our 3B model demonstrates superior and more stable performance compared to the previous version.\n\n<br class=\"ltx_break\"/>Our system comprises three primary components: (1) a dual-codebook audio tokenizer, which converts reference or input audio into discrete tokens; (2) an audio LLM that generates dual-codebook token sequences; and (3) an audio decoder, which converts the dual-codebook token sequences predicted by the audio LLM back into audio waveforms using a flow matching approach.This integrated architecture enables the Step-Audio-EditX to perform zero-shot TTS and diverse editing tasks within a unified framework. Thus, it can directly capitalize on the rich ecosystem of post-training techniques developed for text LLMs.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "speaking",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ SFT to enable the Step-Audio-EditX model for zero-shot TTS and diverse audio editing tasks. The SFT data can be categorized into several parts: zero-shot TTS, emotion editing, speaking style editing and paralinguistic editing. Notably, the large-margin dataset targets editing tasks, particularly on the aspects of emotion and speaking style.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "style",
                    "speaking",
                    "stepaudioeditx",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a high-quality, professionally annotated in-house dataset, primarily in Chinese and English, for zero-shot TTS. Furthermore, a minimal amount of Cantonese and Sichuanese data is employed to elicit dialect capabilities. To ensure diverse and highly expressive styles and emotions in the synthesized speech, as well as robust zero-shot performance, The dataset captures vocal variations within individual speakers as well as across a broad speaker population, comprising approximately 60,000 unique individuals.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Emotion and speaking style present significant challenges for expressive text-to-speech systems, due to the inherent difficulties in both defining their categorical characteristics and collecting high-quality data. We propose a straightforward and efficient large-margin synthetic data approach, which performs zero-shot voice cloning across different emotions and speaking styles for the same speaker, while ensuring a sufficiently large margin between contrastive sample pairs. Only one prompt audio segment per emotion or speaking style is required, eliminating the need for costly data collection. Moreover, this method ingeniously converts complex emotion and style descriptions into a comparative pair-based data construction format. In the following, we introduce the proposed approach:</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Voice Actor Recording.</span> Voice actors recorded expressive emotions and speaking styles. For each actor, a single audio clip of approximately 10 seconds was captured for every emotion and style combination.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot Cloning.</span> A triplet <math alttext=\"\\langle text\\textsubscript{prompt},audio\\textsubscript{neutral},audio\\textsubscript{emotion,style}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">prompt</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">neutral</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">emotion,style</sub></mtext></mrow><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle text\\textsubscript{prompt},audio\\textsubscript{neutral},audio\\textsubscript{emotion,style}\\rangle</annotation></semantics></math> is constructed for each emotion and speaking style by selecting corresponding emotional and neutral audio clips from the same speaker as the prompt audio and processing them with the StepTTS voice cloning interface, using a text instruction that describes the target attribute.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the audio clips in each triplet are generated using the same emotional or stylistic text prompt, which encourages the model to focus solely on the variations in emotion and style itself during the SFT training process.</p>\n\n",
                "matched_terms": [
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> Model responses were scored on a 1-10 scale for emotion and speaking style editing by a comprehension model. Preference pairs were then generated from these scores, retaining only pairs with a score margin greater than 8 points in the final dataset.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The accurate and comprehensive evaluation of a model&#8217;s performance in synthesizing emotional, stylistic, and paralinguistic speech represents a substantial challenge. To address this, we first introduce the construction of a comprehensive and reproducible benchmark in Section 5.1. We then employ this benchmark in Section 5.2 to demonstrate the advantages of our Step-Audio-EditX model.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce Step-Audio-Edit-Benchmark, a benchmark that leverages LLM-as-a-judge model to evaluate model performance on emotion, speaking style, and paralinguistics. All evaluation audio is generated via zero-shot voice cloning and subsequently scored using the Gemini-2.5-Pro<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1b\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/</span></span></span> model.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "speaking",
                    "style",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Selection.</span> The speaker set for zero-shot cloning consisted of eight speakers (2 male and 2 female per language, for both Chinese and English). The Chinese speakers were sourced from the Wenet-Speech4TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts12800hourmandarintts</span>]</cite> corpus, whereas the English speakers were sourced from the open-source GLOBE-V2<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024globehighqualityenglishcorpus</span>]</cite> and Libri-Light<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kahn_2020</span>]</cite> datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "language",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion.</span> The emotional test set covers five categories: happiness, anger, sadness, fear, and surprise. Each category includes 50 Chinese and 50 English prompts, with the textual content of each prompt designed to be consistent with its corresponding target emotion.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking Style.</span> The test set includes seven speaking styles: childlike, elderly, exaggerated, recitative, passionate, coquettish, and whisper. Each style contains 50 Chinese and 50 English prompts, with content matched to its target style.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic.</span> The paralinguistic test set includes ten paralinguistic labels per speaker: breathing, laughter, surprise-oh, confirmation-en, uhm, surprise-ah, surprise-wa, sigh, question-ei, and dissatisfaction-hnn. Each label contains 50 relevant LLM-generated samples in Chinese and 50 in English.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion and Speaking Style Evaluation.</span> To evaluate emotion and speaking style, predefined category sets (5 emotions and 7 styles) are provided to the Gemini-2.5-Pro model in the prompts, instructing it to classify the audio. The final accuracy for each category is calculated as the average across all speakers.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "average",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Style Evaluation.</span> To evaluate the performance of paralinguistic editing, a specialized evaluation prompt has been designed for the Gemini-2.5-Pro model, employing a rigorous 1&#8211;3 scoring scale (3 = perfect, 2 = flawed, 1 = failed). The prompt directs the model to actively examine specific assessment points in the audio&#8212;such as whether annotations like [laughter] or [sigh] have been accurately inserted. Particular emphasis is placed on the most common failure mode, &#8220;omission,&#8221; where the audio may remain fluent but lacks required paralinguistic elements specified in the instructions. Finally, model performance in the paralinguistic editing task is assessed by calculating the overall average score generated by Gemini-2.5-Pro model.</p>\n\n",
                "matched_terms": [
                    "average",
                    "style",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details our model&#8217;s performance on the Step-Audio-Edit-Benchmark, benchmark and demonstrates its superior editing accuracy and scalability when used to edit audio generated by various closed-source TTS systems.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This evaluation employs an iterative approach to audio editing for emotion and speaking style. The process begins with a zero-shot clone as the initial audio <math alttext=\"iteration_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">iteration_{0}</annotation></semantics></math>, which then undergoes <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> rounds of iterative editing. The output of the N-th round is denoted as <math alttext=\"iteration_{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mi>N</mi></msub></mrow><annotation encoding=\"application/x-tex\">iteration_{N}</annotation></semantics></math>. In this specific setup, N is configured as 3. For most use cases, two editing iterations are sufficient to meet the desired criteria.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on Closed-Source Models.</span> The emotion and speaking style generalization of the Step-Audio-EditX model was evaluated on several leading closed-source TTS systems, including GPT-4o-mini-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span>, Eleven_Multilingual_v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://elevenlabs.io/docs/api-reference/text-to-speech/convert</span></span></span>, Doubao-Seed-TTS-2.0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://www.volcengine.com/docs/6561/1871062</span></span></span>, and MiniMax-speech-2.6-hd<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://platform.minimaxi.com/docs/api-reference/speech-t2a-http</span></span></span>. For each TTS system, one male and one female built-in voice were selected for direct speech synthesis of the source text. Subsequently, three iterations of editing were applied to the resultant audio outputs. As presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T2\" title=\"Table 2 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the built-in voices of these closed-source systems possess considerable in-context capabilities, allowing them to partially convey the emotions in the text. After a single editing round with Step-Audio-EditX, the emotion and style accuracy across all voice models exhibited significant improvement. Further enhancement was observed over the next two iterations, robustly demonstrating our model&#8217;s strong generalization.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "style",
                    "speaking",
                    "stepaudioeditx",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Control on Closed-Source Models.</span> Due to the limited availability of closed-source systems with emotion and speaking style control, this section presents a comparative evaluation of Doubao-Seed-TTS-2.0 and MiniMax-speech-2.6-hd, selected for their capability in both zero-shot cloning and emotion control. To meet the minimum audio length constraints of the closed-source models and to ensure a fair evaluation, the prompt audios for all the speakers in the Step-Audio-Edit-Benchmark were extended in duration. These extended audios were employed for zero-shot cloning followed by two emotion editing iterations. Additionally, the cloned voices were used to generate emotional speech via each closed-source model&#8217;s native emotion control.The outputs from this native emotion control subsequently underwent one round of editing with our model. It can be observed from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> that:</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One emotional editing iteration applied to the zero-shot cloned audio outperformed the results generated by the native emotion control functions of the closed-source models.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Paralinguistic editing can be considered a time-domain operation. We evaluated the effect of a single editing iteration using Step-Audio-EditX and assessed its generalization across other closed-source models.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Editing Results.</span> As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Paralinguistic Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, a significant performance gain is obtained by adding paralinguistic tags in a single editing iteration.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on Closed-Source Models.</span> The generalization evaluation was conducted identically to the prior one. For each closed-source model, we employed one female and one male built-in voice to synthesize speech from texts with paralinguistic labels removed. The resultant audio then underwent a single editing iteration. Additionally, for comparison, extra audio samples were synthesized by substituting paralinguistic tags with onomatopoeic words (e.g., \"[Laughter]\" &#8594; \"haha\"). After one iteration of paralinguistic editing with Step-Audio-EditX, the performance of paralinguistic reproduction is comparable to that achieved by the built-in voices of closed-source models when synthesizing native paralinguistic content directly.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results across emotion, speaking style, and paralinguistic editing tasks confirm that our simple yet powerful approach&#8212;large-margin learning with reinforcement learning enhancement&#8212;delivers high accuracy and strong generalization. This methodology demonstrates considerable promise for both advancing research and enabling practical applications.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This large-margin learning method can be straightforwardly extended to various downstream applications. By enforcing a sufficiently large margin between paired data samples, the model can rapidly acquire target editing capabilities through SFT. Reinforcement learning can then be seamlessly integrated to further enhance performance on challenging cases. This section details two practical extensions: (1) speed editing for speech rate control, and (2) denoising and silence trimming.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Background noise and silence segments in prompt audio can substantially influence the performance of zero-shot voice cloning. The model tends to interpret these acoustic features as part of the speaker&#8217;s characteristics, subsequently reproducing them in synthesized audio. While such imitation is desirable in some use cases, it is undesirable in others. To address this, we integrated denoising and silence trimming using a generative approach, which enables targeted editing of both the prompt and the synthesized audio.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present Step-Audio&#8209;EditX, an LLM-based audio model trained on large-margin data and enhanced through reinforcement learning. The model enables zero-shot TTS, iterative editing of emotion and speaking style, and paralinguistic editing. We have identified that the capabilities of LLMs and the use of large-margin data, which have often been overlooked in previous studies, allow the model to overcome the limitations of audio representations. Furthermore, the proposed framework can be easily extended to a variety of tasks, including dialect editing, accent editing, vocal editing, and imitation. Finally, it should be noted that our audio editing process is not strictly conventional \"editing\" in the traditional sense. Instead, it functions as a form of conditional regeneration or transfer. For tasks that require partial modifications while preserving the rest of the content, our approach provides a straightforward yet effective mask-based editing method by reconstructing paired data to ensure only specific portions of the edited tokens differ from the original sequence.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "editing",
                    "emotion"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Generalization of Emotion and Speaking Style Editing on Closed-Source Models.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">\n<span class=\"ltx_text ltx_font_bold\">Emotion  </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">\n<span class=\"ltx_text ltx_font_bold\">Speaking Style  </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Iter</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_bold\">0</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Iter</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_bold\">1</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Iter</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_bold\">2</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Iter</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_bold\">3</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Iter</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_bold\">0</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Iter</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_bold\">1</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Iter</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_bold\">2</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Iter</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_bold\">3</span></sub>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Chinese</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">MiniMax-2.6-hd</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">71.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">78.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">81.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">83.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">36.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">58.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">63.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">67.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Doubao-Seed-TTS-2.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">67.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">77.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">80.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">82.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">38.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">60.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">65.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">64.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">GPT-4o-mini-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">62.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">76.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">77.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">81.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">45.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">64.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">65.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">69.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">ElevenLabs-v2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">60.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">74.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">77.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">79.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">43.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">63.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">69.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">70.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text ltx_font_bold\">English</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">MiniMax-2.6-hd</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">55.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">64.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">64.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">66.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">51.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">60.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">62.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">64.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Doubao-Seed-TTS-2.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">53.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">65.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">65.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">66.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">47.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">62.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">62.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">62.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">GPT-4o-mini-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">56.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">61.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">64.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">65.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">52.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">62.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">62.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">63.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">ElevenLabs-v2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">51.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">61.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">64.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">65.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">51.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">62.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">62.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">64.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text ltx_font_bold\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">Average</span></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">MiniMax-2.6-hd</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">63.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">71.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">72.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">74.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">44.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">59.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">62.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">65.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Doubao-Seed-TTS-2.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">60.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">71.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">73.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">74.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">42.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">61.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">63.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">63.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">GPT-4o-mini-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">59.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">68.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">70.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">73.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">49.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">63.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">64.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">66.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">ElevenLabs-v2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">55.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">67.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">70.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">72.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">47.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">62.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">66.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">67.4</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "minimax26hd",
            "iter2",
            "iter0",
            "style",
            "speaking",
            "gpt4ominitts",
            "iter3",
            "average",
            "iter1",
            "elevenlabsv2",
            "english",
            "language",
            "emotion",
            "â†‘uparrow",
            "editing",
            "model",
            "generalization",
            "chinese",
            "doubaoseedtts20",
            "closedsource"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on Closed-Source Models.</span> The emotion and speaking style generalization of the Step-Audio-EditX model was evaluated on several leading closed-source TTS systems, including GPT-4o-mini-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span>, Eleven_Multilingual_v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://elevenlabs.io/docs/api-reference/text-to-speech/convert</span></span></span>, Doubao-Seed-TTS-2.0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://www.volcengine.com/docs/6561/1871062</span></span></span>, and MiniMax-speech-2.6-hd<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://platform.minimaxi.com/docs/api-reference/speech-t2a-http</span></span></span>. For each TTS system, one male and one female built-in voice were selected for direct speech synthesis of the source text. Subsequently, three iterations of editing were applied to the resultant audio outputs. As presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T2\" title=\"Table 2 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the built-in voices of these closed-source systems possess considerable in-context capabilities, allowing them to partially convey the emotions in the text. After a single editing round with Step-Audio-EditX, the emotion and style accuracy across all voice models exhibited significant improvement. Further enhancement was observed over the next two iterations, robustly demonstrating our model&#8217;s strong generalization.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Step-Audio-EditX, the first open-source LLM-based reinforcement learning audio model excelling at expressive and iterative audio editing&#8212;encompassing emotion, speaking style, and paralinguistics&#8212;alongside robust zero-shot text-to-speech (TTS) capabilities. Our core innovation lies in leveraging only large-margin synthetic data in post-training, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks. Our code and models are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/stepfun-ai/Step-Audio-EditX\" title=\"\">https://github.com/stepfun-ai/Step-Audio-EditX</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "minimax26hd",
                    "editing",
                    "model",
                    "style",
                    "speaking",
                    "doubaoseedtts20",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, TTS technology has advanced significantly. A notable development is zero-shot TTS models, which can generate high-quality, natural-sounding speech by mimicking the timbre, prosody, and style of a reference speech prompt. Generally, current zero-shot TTS systems fall into three main categories: those that utilize LLMs to model discrete or continuous acoustic tokens <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vall-e</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">valle2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">melle</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">maskgct</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sparktts</span>]</cite>, those employing diffusion or flow matching models to learn direct text-to-speech mapping <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2024voicebox</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">voiceflow</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matchatts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">seedtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">f5tts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">f5rtts</span>]</cite>, and hybrid coarse-to-fine systems, where LLMs first convert text tokens into coarse speech tokens, which are then refined by a diffusion or flow matching model to render fine-grained speech details<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fireredtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fireredtts2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024speaking</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2025cosyvoice3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jia2025ditar</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025indextts</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite considerable progress in zero-shot TTS synthesis, attributes such as emotion, style, accent, and timbre in the synthesized speech are still derived directly from the reference audio. This inherent limitation restricts independent control over these attributes. Although prepending style or emotion instructions to the input text offers a certain degree of controllability and often performs well for in-domain speakers <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">seedtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fireredtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>]</cite>, this approach faces challenges in disentangling speech attributes. In particular, the cloned voice often fails to effectively follow the provided style or emotion instructions.</p>\n\n",
                "matched_terms": [
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many previous studies on speech disentanglement have relied on approaches such as adversarial training<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">naturalspeech3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2022crossspeakeremotiondisentanglingtransfer</span>]</cite>, feature engineering<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2023dddmvcdecoupleddenoisingdiffusion</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Anastassiou2024VoiceShopAU</span>]</cite>, and innovative network architectures<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Jia2022ZeroShotAC</span>]</cite> to achieve attribute decoupling. In contrast, we propose a simple yet stable data-driven method. Specifically, we design a pipeline for generating high-quality data pairs that preserve identical linguistic content while exhibiting clearly distinguishable variations in one or a few attributes, such as emotion, style, accent, and paralinguistic features. By training models on such data pairs, we achieve effective attribute disentanglement, enabling to edit the attribute of input speech. Moreover, by applying multiple iterative \"editing\" steps, the intensity of a target attribute can be progressively enhanced or reduced. Beyond emotion, style and paralinguistic editing, we demonstrate that this approach can be extended to other applications, including speed rate adjustment, speech denoising, and silence trimming.\nIn this report, we outline our contributions and findings:</p>\n\n",
                "matched_terms": [
                    "models",
                    "editing",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing, encompassing emotion, speaking style, and paralinguistics, alongside robust zero-shot TTS capabilities.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "model",
                    "style",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our results show that emotion and speaking style can be controlled through post-training with large-margin data alone, eliminating the need for extra presentation modeling or adapter modules.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our prior work, we introduced an Audio-Edit synthesis model in Step-Audio<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stepaudio</span>]</cite> for nuanced emotional expressions and diverse speaking styles data generation. In this report, we retain the previous model along with the same audio tokenizer. The key modifications include an expanded range of emotions and speaking styles, the addition of zero-shot TTS and paralinguistic editing capabilities, as well as a reduction in model parameters from 130B to 3B. Leveraging large-margin synthetic data, our 3B model demonstrates superior and more stable performance compared to the previous version.\n\n<br class=\"ltx_break\"/>Our system comprises three primary components: (1) a dual-codebook audio tokenizer, which converts reference or input audio into discrete tokens; (2) an audio LLM that generates dual-codebook token sequences; and (3) an audio decoder, which converts the dual-codebook token sequences predicted by the audio LLM back into audio waveforms using a flow matching approach.This integrated architecture enables the Step-Audio-EditX to perform zero-shot TTS and diverse editing tasks within a unified framework. Thus, it can directly capitalize on the rich ecosystem of post-training techniques developed for text LLMs.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "model",
                    "speaking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio LLM uses the same architecture as our prior Audio-Edit model, differing only in its smaller parameter size of 3B. To capitalize on the powerful language capabilities of pre-trained text LLMs, the 3B model is initialized with a text-based LLM, followed by training on a blended dataset with a 1:1 ratio of text data to audio dual-codebook tokens. The audio LLM processes text tokens along with their corresponding dual-codebook audio tokens in a chat format, subsequently generating dual-codebook tokens as the sole output.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ SFT to enable the Step-Audio-EditX model for zero-shot TTS and diverse audio editing tasks. The SFT data can be categorized into several parts: zero-shot TTS, emotion editing, speaking style editing and paralinguistic editing. Notably, the large-margin dataset targets editing tasks, particularly on the aspects of emotion and speaking style.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "model",
                    "style",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a high-quality, professionally annotated in-house dataset, primarily in Chinese and English, for zero-shot TTS. Furthermore, a minimal amount of Cantonese and Sichuanese data is employed to elicit dialect capabilities. To ensure diverse and highly expressive styles and emotions in the synthesized speech, as well as robust zero-shot performance, The dataset captures vocal variations within individual speakers as well as across a broad speaker population, comprising approximately 60,000 unique individuals.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Emotion and speaking style present significant challenges for expressive text-to-speech systems, due to the inherent difficulties in both defining their categorical characteristics and collecting high-quality data. We propose a straightforward and efficient large-margin synthetic data approach, which performs zero-shot voice cloning across different emotions and speaking styles for the same speaker, while ensuring a sufficiently large margin between contrastive sample pairs. Only one prompt audio segment per emotion or speaking style is required, eliminating the need for costly data collection. Moreover, this method ingeniously converts complex emotion and style descriptions into a comparative pair-based data construction format. In the following, we introduce the proposed approach:</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Voice Actor Recording.</span> Voice actors recorded expressive emotions and speaking styles. For each actor, a single audio clip of approximately 10 seconds was captured for every emotion and style combination.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot Cloning.</span> A triplet <math alttext=\"\\langle text\\textsubscript{prompt},audio\\textsubscript{neutral},audio\\textsubscript{emotion,style}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">prompt</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">neutral</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">emotion,style</sub></mtext></mrow><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle text\\textsubscript{prompt},audio\\textsubscript{neutral},audio\\textsubscript{emotion,style}\\rangle</annotation></semantics></math> is constructed for each emotion and speaking style by selecting corresponding emotional and neutral audio clips from the same speaker as the prompt audio and processing them with the StepTTS voice cloning interface, using a text instruction that describes the target attribute.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the audio clips in each triplet are generated using the same emotional or stylistic text prompt, which encourages the model to focus solely on the variations in emotion and style itself during the SFT training process.</p>\n\n",
                "matched_terms": [
                    "model",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Paralinguistic cues, such as breathing, laughter, and filled pauses (e.g., \"uhm\"), are crucial for enhancing the naturalness and expressiveness of synthesized speech. We achieved paralinguistic editing capability by using a \"semi-synthetic\" strategy, which leverages the NVSpeech dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2025nvspeech</span>]</cite>, a highly expressive speech corpus whose rich annotations for numerous paralinguistic types enabled the construction of comparative quadruplets for model training. The quadruplet <math alttext=\"\\langle text\\textsubscript{without\\_tags},audio\\textsubscript{without\\_tags},text\\textsubscript{nv\\_source},audio\\textsubscript{nv\\_source}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">without_tags</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">without_tags</sub></mtext></mrow><mo>,</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">nv_source</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">nv_source</sub></mtext></mrow><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle text\\textsubscript{without\\_tags},audio\\textsubscript{without\\_tags},text\\textsubscript{nv\\_source},audio\\textsubscript{nv\\_source}\\rangle</annotation></semantics></math> construction differs from the triplet by using the NVSpeech original audio and transcript as the target output and the StepTTS voice cloning generated audio as the input, which is synthesized using the original transcript after paralinguistic tag removal.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As paralinguistic editing is an editing task performed in the time domain and exhibits substantial intrinsic margin differences, a margin scoring model is not required for data selection. A small set of quadruplet data is sufficient to effectively elicit the model&#8217;s paralinguistic editing capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> Model responses were scored on a 1-10 scale for emotion and speaking style editing by a comprehension model. Preference pairs were then generated from these scores, retaining only pairs with a score margin greater than 8 points in the final dataset.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "model",
                    "style",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SFT stage enhances the model&#8217;s zero-shot text-to-speech synthesis and editing capabilities through the use of distinct system prompts in a chat format. In the zero-shot TTS task, the prompt waveform is encoded into dual-codebook tokens, which are subsequently detokenized into string format and incorporated into the speaker information within the system prompt. The text to be synthesized serves as the user prompt in a chat-based format, and the generated dual-code tokens are returned as the system&#8217;s response. For the editing task, all operations are defined under a unified system prompt. The user prompt includes both the original audio and a descriptive command for the editing operation, and the system response delivers the resulting edited audio tokens. The model is finetuned for one epoch with learning rate from <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Reinforcement learning has further amplified the model&#8217;s stability in zero-shot TTS, as well as its capability and expressiveness in following editing instructions. These enhancements are particularly noticeable when there is a substantial divergence between the emotional and stylistic characteristics of the source prompt waveform and the target editing output, such as generating sad speech from a happy prompt or converting loud speech into a whisper. This reinforcement learning approach offers a novel perspective to address these challenges by shifting the focus from achieving ideal speech representation disentanglement to improving both the construction of large-margin pairs and the efficacy of the reward model evaluation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce Step-Audio-Edit-Benchmark, a benchmark that leverages LLM-as-a-judge model to evaluate model performance on emotion, speaking style, and paralinguistics. All evaluation audio is generated via zero-shot voice cloning and subsequently scored using the Gemini-2.5-Pro<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1b\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/</span></span></span> model.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "model",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Selection.</span> The speaker set for zero-shot cloning consisted of eight speakers (2 male and 2 female per language, for both Chinese and English). The Chinese speakers were sourced from the Wenet-Speech4TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts12800hourmandarintts</span>]</cite> corpus, whereas the English speakers were sourced from the open-source GLOBE-V2<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024globehighqualityenglishcorpus</span>]</cite> and Libri-Light<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kahn_2020</span>]</cite> datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "language",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion.</span> The emotional test set covers five categories: happiness, anger, sadness, fear, and surprise. Each category includes 50 Chinese and 50 English prompts, with the textual content of each prompt designed to be consistent with its corresponding target emotion.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking Style.</span> The test set includes seven speaking styles: childlike, elderly, exaggerated, recitative, passionate, coquettish, and whisper. Each style contains 50 Chinese and 50 English prompts, with content matched to its target style.</p>\n\n",
                "matched_terms": [
                    "speaking",
                    "english",
                    "style",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic.</span> The paralinguistic test set includes ten paralinguistic labels per speaker: breathing, laughter, surprise-oh, confirmation-en, uhm, surprise-ah, surprise-wa, sigh, question-ei, and dissatisfaction-hnn. Each label contains 50 relevant LLM-generated samples in Chinese and 50 in English.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion and Speaking Style Evaluation.</span> To evaluate emotion and speaking style, predefined category sets (5 emotions and 7 styles) are provided to the Gemini-2.5-Pro model in the prompts, instructing it to classify the audio. The final accuracy for each category is calculated as the average across all speakers.</p>\n\n",
                "matched_terms": [
                    "average",
                    "model",
                    "style",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Style Evaluation.</span> To evaluate the performance of paralinguistic editing, a specialized evaluation prompt has been designed for the Gemini-2.5-Pro model, employing a rigorous 1&#8211;3 scoring scale (3 = perfect, 2 = flawed, 1 = failed). The prompt directs the model to actively examine specific assessment points in the audio&#8212;such as whether annotations like [laughter] or [sigh] have been accurately inserted. Particular emphasis is placed on the most common failure mode, &#8220;omission,&#8221; where the audio may remain fluent but lacks required paralinguistic elements specified in the instructions. Finally, model performance in the paralinguistic editing task is assessed by calculating the overall average score generated by Gemini-2.5-Pro model.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "average",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details our model&#8217;s performance on the Step-Audio-Edit-Benchmark, benchmark and demonstrates its superior editing accuracy and scalability when used to edit audio generated by various closed-source TTS systems.</p>\n\n",
                "matched_terms": [
                    "closedsource",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This evaluation employs an iterative approach to audio editing for emotion and speaking style. The process begins with a zero-shot clone as the initial audio <math alttext=\"iteration_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">iteration_{0}</annotation></semantics></math>, which then undergoes <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> rounds of iterative editing. The output of the N-th round is denoted as <math alttext=\"iteration_{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mi>N</mi></msub></mrow><annotation encoding=\"application/x-tex\">iteration_{N}</annotation></semantics></math>. In this specific setup, N is configured as 3. For most use cases, two editing iterations are sufficient to meet the desired criteria.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "speaking",
                    "style",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative Editing Results.</span> As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T1\" title=\"Table 1 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, there was a significant boost in both emotion and speaking style accuracy after the initial edit of the Iter<sub class=\"ltx_sub\">0</sub> audio. Furthermore, with successive iterations of editing, the accuracy for both emotion and speaking style was further enhanced.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "iter0",
                    "style",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Audio Ablation.</span> Since the performance improvement in later iterations (starting from Iter<sub class=\"ltx_sub\">2</sub>) were attributed to both the dual-code and the prompt audio. To isolate the effect of the prompt audio, we conducted an ablation study in which the prompt audio was held constant across all iterations. As presented in the Prompt-Fixed section of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T1\" title=\"Table 1 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the accuracy for both emotion and speaking style continues to improve with an increasing number of editing iterations. This clearly demonstrates the effectiveness of our large-margin method.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "iter2",
                    "style",
                    "speaking",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Control on Closed-Source Models.</span> Due to the limited availability of closed-source systems with emotion and speaking style control, this section presents a comparative evaluation of Doubao-Seed-TTS-2.0 and MiniMax-speech-2.6-hd, selected for their capability in both zero-shot cloning and emotion control. To meet the minimum audio length constraints of the closed-source models and to ensure a fair evaluation, the prompt audios for all the speakers in the Step-Audio-Edit-Benchmark were extended in duration. These extended audios were employed for zero-shot cloning followed by two emotion editing iterations. Additionally, the cloned voices were used to generate emotional speech via each closed-source model&#8217;s native emotion control.The outputs from this native emotion control subsequently underwent one round of editing with our model. It can be observed from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> that:</p>\n\n",
                "matched_terms": [
                    "models",
                    "editing",
                    "model",
                    "style",
                    "speaking",
                    "doubaoseedtts20",
                    "closedsource",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One emotional editing iteration applied to the zero-shot cloned audio outperformed the results generated by the native emotion control functions of the closed-source models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "closedsource",
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Paralinguistic editing can be considered a time-domain operation. We evaluated the effect of a single editing iteration using Step-Audio-EditX and assessed its generalization across other closed-source models.</p>\n\n",
                "matched_terms": [
                    "generalization",
                    "models",
                    "closedsource",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on Closed-Source Models.</span> The generalization evaluation was conducted identically to the prior one. For each closed-source model, we employed one female and one male built-in voice to synthesize speech from texts with paralinguistic labels removed. The resultant audio then underwent a single editing iteration. Additionally, for comparison, extra audio samples were synthesized by substituting paralinguistic tags with onomatopoeic words (e.g., \"[Laughter]\" &#8594; \"haha\"). After one iteration of paralinguistic editing with Step-Audio-EditX, the performance of paralinguistic reproduction is comparable to that achieved by the built-in voices of closed-source models when synthesizing native paralinguistic content directly.</p>\n\n",
                "matched_terms": [
                    "models",
                    "editing",
                    "model",
                    "generalization",
                    "closedsource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results across emotion, speaking style, and paralinguistic editing tasks confirm that our simple yet powerful approach&#8212;large-margin learning with reinforcement learning enhancement&#8212;delivers high accuracy and strong generalization. This methodology demonstrates considerable promise for both advancing research and enabling practical applications.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "style",
                    "speaking",
                    "generalization",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This large-margin learning method can be straightforwardly extended to various downstream applications. By enforcing a sufficiently large margin between paired data samples, the model can rapidly acquire target editing capabilities through SFT. Reinforcement learning can then be seamlessly integrated to further enhance performance on challenging cases. This section details two practical extensions: (1) speed editing for speech rate control, and (2) denoising and silence trimming.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Background noise and silence segments in prompt audio can substantially influence the performance of zero-shot voice cloning. The model tends to interpret these acoustic features as part of the speaker&#8217;s characteristics, subsequently reproducing them in synthesized audio. While such imitation is desirable in some use cases, it is undesirable in others. To address this, we integrated denoising and silence trimming using a generative approach, which enables targeted editing of both the prompt and the synthesized audio.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present Step-Audio&#8209;EditX, an LLM-based audio model trained on large-margin data and enhanced through reinforcement learning. The model enables zero-shot TTS, iterative editing of emotion and speaking style, and paralinguistic editing. We have identified that the capabilities of LLMs and the use of large-margin data, which have often been overlooked in previous studies, allow the model to overcome the limitations of audio representations. Furthermore, the proposed framework can be easily extended to a variety of tasks, including dialect editing, accent editing, vocal editing, and imitation. Finally, it should be noted that our audio editing process is not strictly conventional \"editing\" in the traditional sense. Instead, it functions as a form of conditional regeneration or transfer. For tasks that require partial modifications while preserving the rest of the content, our approach provides a straightforward yet effective mask-based editing method by reconstructing paired data to ensure only specific portions of the edited tokens differ from the original sequence.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "model",
                    "style",
                    "speaking",
                    "emotion"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Performance Comparison Between Step-Audio-EditX and Closed-Source Models on Emotion Editing.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">\n<span class=\"ltx_text ltx_font_bold\">Emotion</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">0</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">1</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">2</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">3</sub></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"5\">Chinese</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Step-Audio-EditX</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">72.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">77.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiniMax-2.6-hd (Clone)</td>\n<td class=\"ltx_td ltx_align_center\">49.4</td>\n<td class=\"ltx_td ltx_align_center\">72.1</td>\n<td class=\"ltx_td ltx_align_center\">75.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">78.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiniMax-2.6-hd (Emotion Control)</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">59.9</td>\n<td class=\"ltx_td ltx_align_center\">75.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">78.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Doubao-Seed-TTS-2.0 (Clone)</td>\n<td class=\"ltx_td ltx_align_center\">50.8</td>\n<td class=\"ltx_td ltx_align_center\">70.9</td>\n<td class=\"ltx_td ltx_align_center\">75.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">76.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Doubao-Seed-TTS-2.0 (Emotion Control)</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">51.8</td>\n<td class=\"ltx_td ltx_align_center\">68.9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">75.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"5\">English</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Step-Audio-EditX</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">51.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">61.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">63.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">64.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiniMax-2.6-hd (Clone)</td>\n<td class=\"ltx_td ltx_align_center\">50.6</td>\n<td class=\"ltx_td ltx_align_center\">60.2</td>\n<td class=\"ltx_td ltx_align_center\">62.9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">63.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiniMax-2.6-hd (Emotion Control)</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">54.0</td>\n<td class=\"ltx_td ltx_align_center\">61.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">62.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Doubao-Seed-TTS-2.0 (Clone)</td>\n<td class=\"ltx_td ltx_align_center\">47.1</td>\n<td class=\"ltx_td ltx_align_center\">57.7</td>\n<td class=\"ltx_td ltx_align_center\">61.9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">64.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Doubao-Seed-TTS-2.0 (Emotion Control)</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">47.9</td>\n<td class=\"ltx_td ltx_align_center\">60.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">61.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">Average</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Step-Audio-EditX</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">66.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">71.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiniMax-2.6-hd (Clone)</td>\n<td class=\"ltx_td ltx_align_center\">50.0</td>\n<td class=\"ltx_td ltx_align_center\">66.2</td>\n<td class=\"ltx_td ltx_align_center\">69.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">70.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiniMax-2.6-hd (Emotion Control)</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">57.0</td>\n<td class=\"ltx_td ltx_align_center\">68.4</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">70.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Doubao-Seed-TTS-2.0 (Clone)</td>\n<td class=\"ltx_td ltx_align_center\">49.0</td>\n<td class=\"ltx_td ltx_align_center\">64.3</td>\n<td class=\"ltx_td ltx_align_center\">68.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">70.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Doubao-Seed-TTS-2.0 (Emotion Control)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">49.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">64.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">68.8</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "minimax26hd",
            "iter2",
            "iter0",
            "control",
            "clone",
            "comparison",
            "iter3",
            "average",
            "iter1",
            "between",
            "performance",
            "english",
            "language",
            "emotion",
            "â†‘uparrow",
            "editing",
            "model",
            "stepaudioeditx",
            "chinese",
            "doubaoseedtts20",
            "closedsource"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Control on Closed-Source Models.</span> Due to the limited availability of closed-source systems with emotion and speaking style control, this section presents a comparative evaluation of Doubao-Seed-TTS-2.0 and MiniMax-speech-2.6-hd, selected for their capability in both zero-shot cloning and emotion control. To meet the minimum audio length constraints of the closed-source models and to ensure a fair evaluation, the prompt audios for all the speakers in the Step-Audio-Edit-Benchmark were extended in duration. These extended audios were employed for zero-shot cloning followed by two emotion editing iterations. Additionally, the cloned voices were used to generate emotional speech via each closed-source model&#8217;s native emotion control.The outputs from this native emotion control subsequently underwent one round of editing with our model. It can be observed from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> that:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Step-Audio-EditX, the first open-source LLM-based reinforcement learning audio model excelling at expressive and iterative audio editing&#8212;encompassing emotion, speaking style, and paralinguistics&#8212;alongside robust zero-shot text-to-speech (TTS) capabilities. Our core innovation lies in leveraging only large-margin synthetic data in post-training, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks. Our code and models are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/stepfun-ai/Step-Audio-EditX\" title=\"\">https://github.com/stepfun-ai/Step-Audio-EditX</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "minimax26hd",
                    "editing",
                    "model",
                    "control",
                    "stepaudioeditx",
                    "doubaoseedtts20",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, TTS technology has advanced significantly. A notable development is zero-shot TTS models, which can generate high-quality, natural-sounding speech by mimicking the timbre, prosody, and style of a reference speech prompt. Generally, current zero-shot TTS systems fall into three main categories: those that utilize LLMs to model discrete or continuous acoustic tokens <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vall-e</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">valle2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">melle</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">maskgct</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sparktts</span>]</cite>, those employing diffusion or flow matching models to learn direct text-to-speech mapping <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2024voicebox</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">voiceflow</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matchatts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">seedtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">f5tts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">f5rtts</span>]</cite>, and hybrid coarse-to-fine systems, where LLMs first convert text tokens into coarse speech tokens, which are then refined by a diffusion or flow matching model to render fine-grained speech details<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fireredtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fireredtts2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024speaking</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2025cosyvoice3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jia2025ditar</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025indextts</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite considerable progress in zero-shot TTS synthesis, attributes such as emotion, style, accent, and timbre in the synthesized speech are still derived directly from the reference audio. This inherent limitation restricts independent control over these attributes. Although prepending style or emotion instructions to the input text offers a certain degree of controllability and often performs well for in-domain speakers <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">seedtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fireredtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>]</cite>, this approach faces challenges in disentangling speech attributes. In particular, the cloned voice often fails to effectively follow the provided style or emotion instructions.</p>\n\n",
                "matched_terms": [
                    "control",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many previous studies on speech disentanglement have relied on approaches such as adversarial training<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">naturalspeech3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2022crossspeakeremotiondisentanglingtransfer</span>]</cite>, feature engineering<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2023dddmvcdecoupleddenoisingdiffusion</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Anastassiou2024VoiceShopAU</span>]</cite>, and innovative network architectures<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Jia2022ZeroShotAC</span>]</cite> to achieve attribute decoupling. In contrast, we propose a simple yet stable data-driven method. Specifically, we design a pipeline for generating high-quality data pairs that preserve identical linguistic content while exhibiting clearly distinguishable variations in one or a few attributes, such as emotion, style, accent, and paralinguistic features. By training models on such data pairs, we achieve effective attribute disentanglement, enabling to edit the attribute of input speech. Moreover, by applying multiple iterative \"editing\" steps, the intensity of a target attribute can be progressively enhanced or reduced. Beyond emotion, style and paralinguistic editing, we demonstrate that this approach can be extended to other applications, including speed rate adjustment, speech denoising, and silence trimming.\nIn this report, we outline our contributions and findings:</p>\n\n",
                "matched_terms": [
                    "models",
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing, encompassing emotion, speaking style, and paralinguistics, alongside robust zero-shot TTS capabilities.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "model",
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our prior work, we introduced an Audio-Edit synthesis model in Step-Audio<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stepaudio</span>]</cite> for nuanced emotional expressions and diverse speaking styles data generation. In this report, we retain the previous model along with the same audio tokenizer. The key modifications include an expanded range of emotions and speaking styles, the addition of zero-shot TTS and paralinguistic editing capabilities, as well as a reduction in model parameters from 130B to 3B. Leveraging large-margin synthetic data, our 3B model demonstrates superior and more stable performance compared to the previous version.\n\n<br class=\"ltx_break\"/>Our system comprises three primary components: (1) a dual-codebook audio tokenizer, which converts reference or input audio into discrete tokens; (2) an audio LLM that generates dual-codebook token sequences; and (3) an audio decoder, which converts the dual-codebook token sequences predicted by the audio LLM back into audio waveforms using a flow matching approach.This integrated architecture enables the Step-Audio-EditX to perform zero-shot TTS and diverse editing tasks within a unified framework. Thus, it can directly capitalize on the rich ecosystem of post-training techniques developed for text LLMs.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "model",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio LLM uses the same architecture as our prior Audio-Edit model, differing only in its smaller parameter size of 3B. To capitalize on the powerful language capabilities of pre-trained text LLMs, the 3B model is initialized with a text-based LLM, followed by training on a blended dataset with a 1:1 ratio of text data to audio dual-codebook tokens. The audio LLM processes text tokens along with their corresponding dual-codebook audio tokens in a chat format, subsequently generating dual-codebook tokens as the sole output.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ SFT to enable the Step-Audio-EditX model for zero-shot TTS and diverse audio editing tasks. The SFT data can be categorized into several parts: zero-shot TTS, emotion editing, speaking style editing and paralinguistic editing. Notably, the large-margin dataset targets editing tasks, particularly on the aspects of emotion and speaking style.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "model",
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a high-quality, professionally annotated in-house dataset, primarily in Chinese and English, for zero-shot TTS. Furthermore, a minimal amount of Cantonese and Sichuanese data is employed to elicit dialect capabilities. To ensure diverse and highly expressive styles and emotions in the synthesized speech, as well as robust zero-shot performance, The dataset captures vocal variations within individual speakers as well as across a broad speaker population, comprising approximately 60,000 unique individuals.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Emotion and speaking style present significant challenges for expressive text-to-speech systems, due to the inherent difficulties in both defining their categorical characteristics and collecting high-quality data. We propose a straightforward and efficient large-margin synthetic data approach, which performs zero-shot voice cloning across different emotions and speaking styles for the same speaker, while ensuring a sufficiently large margin between contrastive sample pairs. Only one prompt audio segment per emotion or speaking style is required, eliminating the need for costly data collection. Moreover, this method ingeniously converts complex emotion and style descriptions into a comparative pair-based data construction format. In the following, we introduce the proposed approach:</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, the audio clips in each triplet are generated using the same emotional or stylistic text prompt, which encourages the model to focus solely on the variations in emotion and style itself during the SFT training process.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Paralinguistic cues, such as breathing, laughter, and filled pauses (e.g., \"uhm\"), are crucial for enhancing the naturalness and expressiveness of synthesized speech. We achieved paralinguistic editing capability by using a \"semi-synthetic\" strategy, which leverages the NVSpeech dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2025nvspeech</span>]</cite>, a highly expressive speech corpus whose rich annotations for numerous paralinguistic types enabled the construction of comparative quadruplets for model training. The quadruplet <math alttext=\"\\langle text\\textsubscript{without\\_tags},audio\\textsubscript{without\\_tags},text\\textsubscript{nv\\_source},audio\\textsubscript{nv\\_source}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">without_tags</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">without_tags</sub></mtext></mrow><mo>,</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">nv_source</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">nv_source</sub></mtext></mrow><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle text\\textsubscript{without\\_tags},audio\\textsubscript{without\\_tags},text\\textsubscript{nv\\_source},audio\\textsubscript{nv\\_source}\\rangle</annotation></semantics></math> construction differs from the triplet by using the NVSpeech original audio and transcript as the target output and the StepTTS voice cloning generated audio as the input, which is synthesized using the original transcript after paralinguistic tag removal.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As paralinguistic editing is an editing task performed in the time domain and exhibits substantial intrinsic margin differences, a margin scoring model is not required for data selection. A small set of quadruplet data is sufficient to effectively elicit the model&#8217;s paralinguistic editing capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> Model responses were scored on a 1-10 scale for emotion and speaking style editing by a comprehension model. Preference pairs were then generated from these scores, retaining only pairs with a score margin greater than 8 points in the final dataset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SFT stage enhances the model&#8217;s zero-shot text-to-speech synthesis and editing capabilities through the use of distinct system prompts in a chat format. In the zero-shot TTS task, the prompt waveform is encoded into dual-codebook tokens, which are subsequently detokenized into string format and incorporated into the speaker information within the system prompt. The text to be synthesized serves as the user prompt in a chat-based format, and the generated dual-code tokens are returned as the system&#8217;s response. For the editing task, all operations are defined under a unified system prompt. The user prompt includes both the original audio and a descriptive command for the editing operation, and the system response delivers the resulting edited audio tokens. The model is finetuned for one epoch with learning rate from <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Reinforcement learning has further amplified the model&#8217;s stability in zero-shot TTS, as well as its capability and expressiveness in following editing instructions. These enhancements are particularly noticeable when there is a substantial divergence between the emotional and stylistic characteristics of the source prompt waveform and the target editing output, such as generating sad speech from a happy prompt or converting loud speech into a whisper. This reinforcement learning approach offers a novel perspective to address these challenges by shifting the focus from achieving ideal speech representation disentanglement to improving both the construction of large-margin pairs and the efficacy of the reward model evaluation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The accurate and comprehensive evaluation of a model&#8217;s performance in synthesizing emotional, stylistic, and paralinguistic speech represents a substantial challenge. To address this, we first introduce the construction of a comprehensive and reproducible benchmark in Section 5.1. We then employ this benchmark in Section 5.2 to demonstrate the advantages of our Step-Audio-EditX model.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce Step-Audio-Edit-Benchmark, a benchmark that leverages LLM-as-a-judge model to evaluate model performance on emotion, speaking style, and paralinguistics. All evaluation audio is generated via zero-shot voice cloning and subsequently scored using the Gemini-2.5-Pro<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1b\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/</span></span></span> model.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Selection.</span> The speaker set for zero-shot cloning consisted of eight speakers (2 male and 2 female per language, for both Chinese and English). The Chinese speakers were sourced from the Wenet-Speech4TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts12800hourmandarintts</span>]</cite> corpus, whereas the English speakers were sourced from the open-source GLOBE-V2<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024globehighqualityenglishcorpus</span>]</cite> and Libri-Light<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kahn_2020</span>]</cite> datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "language",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion.</span> The emotional test set covers five categories: happiness, anger, sadness, fear, and surprise. Each category includes 50 Chinese and 50 English prompts, with the textual content of each prompt designed to be consistent with its corresponding target emotion.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking Style.</span> The test set includes seven speaking styles: childlike, elderly, exaggerated, recitative, passionate, coquettish, and whisper. Each style contains 50 Chinese and 50 English prompts, with content matched to its target style.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic.</span> The paralinguistic test set includes ten paralinguistic labels per speaker: breathing, laughter, surprise-oh, confirmation-en, uhm, surprise-ah, surprise-wa, sigh, question-ei, and dissatisfaction-hnn. Each label contains 50 relevant LLM-generated samples in Chinese and 50 in English.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion and Speaking Style Evaluation.</span> To evaluate emotion and speaking style, predefined category sets (5 emotions and 7 styles) are provided to the Gemini-2.5-Pro model in the prompts, instructing it to classify the audio. The final accuracy for each category is calculated as the average across all speakers.</p>\n\n",
                "matched_terms": [
                    "average",
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Style Evaluation.</span> To evaluate the performance of paralinguistic editing, a specialized evaluation prompt has been designed for the Gemini-2.5-Pro model, employing a rigorous 1&#8211;3 scoring scale (3 = perfect, 2 = flawed, 1 = failed). The prompt directs the model to actively examine specific assessment points in the audio&#8212;such as whether annotations like [laughter] or [sigh] have been accurately inserted. Particular emphasis is placed on the most common failure mode, &#8220;omission,&#8221; where the audio may remain fluent but lacks required paralinguistic elements specified in the instructions. Finally, model performance in the paralinguistic editing task is assessed by calculating the overall average score generated by Gemini-2.5-Pro model.</p>\n\n",
                "matched_terms": [
                    "average",
                    "model",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details our model&#8217;s performance on the Step-Audio-Edit-Benchmark, benchmark and demonstrates its superior editing accuracy and scalability when used to edit audio generated by various closed-source TTS systems.</p>\n\n",
                "matched_terms": [
                    "closedsource",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This evaluation employs an iterative approach to audio editing for emotion and speaking style. The process begins with a zero-shot clone as the initial audio <math alttext=\"iteration_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">iteration_{0}</annotation></semantics></math>, which then undergoes <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> rounds of iterative editing. The output of the N-th round is denoted as <math alttext=\"iteration_{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mi>N</mi></msub></mrow><annotation encoding=\"application/x-tex\">iteration_{N}</annotation></semantics></math>. In this specific setup, N is configured as 3. For most use cases, two editing iterations are sufficient to meet the desired criteria.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "clone",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative Editing Results.</span> As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T1\" title=\"Table 1 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, there was a significant boost in both emotion and speaking style accuracy after the initial edit of the Iter<sub class=\"ltx_sub\">0</sub> audio. Furthermore, with successive iterations of editing, the accuracy for both emotion and speaking style was further enhanced.</p>\n\n",
                "matched_terms": [
                    "iter0",
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Audio Ablation.</span> Since the performance improvement in later iterations (starting from Iter<sub class=\"ltx_sub\">2</sub>) were attributed to both the dual-code and the prompt audio. To isolate the effect of the prompt audio, we conducted an ablation study in which the prompt audio was held constant across all iterations. As presented in the Prompt-Fixed section of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T1\" title=\"Table 1 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the accuracy for both emotion and speaking style continues to improve with an increasing number of editing iterations. This clearly demonstrates the effectiveness of our large-margin method.</p>\n\n",
                "matched_terms": [
                    "iter2",
                    "emotion",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on Closed-Source Models.</span> The emotion and speaking style generalization of the Step-Audio-EditX model was evaluated on several leading closed-source TTS systems, including GPT-4o-mini-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span>, Eleven_Multilingual_v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://elevenlabs.io/docs/api-reference/text-to-speech/convert</span></span></span>, Doubao-Seed-TTS-2.0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://www.volcengine.com/docs/6561/1871062</span></span></span>, and MiniMax-speech-2.6-hd<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://platform.minimaxi.com/docs/api-reference/speech-t2a-http</span></span></span>. For each TTS system, one male and one female built-in voice were selected for direct speech synthesis of the source text. Subsequently, three iterations of editing were applied to the resultant audio outputs. As presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T2\" title=\"Table 2 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the built-in voices of these closed-source systems possess considerable in-context capabilities, allowing them to partially convey the emotions in the text. After a single editing round with Step-Audio-EditX, the emotion and style accuracy across all voice models exhibited significant improvement. Further enhancement was observed over the next two iterations, robustly demonstrating our model&#8217;s strong generalization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "editing",
                    "model",
                    "stepaudioeditx",
                    "closedsource",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our Step-Audio-EditX demonstrates better emotional accuracy in its zero-shot cloning capability compared to the other two models.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One emotional editing iteration applied to the zero-shot cloned audio outperformed the results generated by the native emotion control functions of the closed-source models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "editing",
                    "control",
                    "closedsource",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Paralinguistic editing can be considered a time-domain operation. We evaluated the effect of a single editing iteration using Step-Audio-EditX and assessed its generalization across other closed-source models.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "models",
                    "closedsource",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Editing Results.</span> As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Paralinguistic Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, a significant performance gain is obtained by adding paralinguistic tags in a single editing iteration.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on Closed-Source Models.</span> The generalization evaluation was conducted identically to the prior one. For each closed-source model, we employed one female and one male built-in voice to synthesize speech from texts with paralinguistic labels removed. The resultant audio then underwent a single editing iteration. Additionally, for comparison, extra audio samples were synthesized by substituting paralinguistic tags with onomatopoeic words (e.g., \"[Laughter]\" &#8594; \"haha\"). After one iteration of paralinguistic editing with Step-Audio-EditX, the performance of paralinguistic reproduction is comparable to that achieved by the built-in voices of closed-source models when synthesizing native paralinguistic content directly.</p>\n\n",
                "matched_terms": [
                    "models",
                    "editing",
                    "model",
                    "stepaudioeditx",
                    "comparison",
                    "closedsource",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results across emotion, speaking style, and paralinguistic editing tasks confirm that our simple yet powerful approach&#8212;large-margin learning with reinforcement learning enhancement&#8212;delivers high accuracy and strong generalization. This methodology demonstrates considerable promise for both advancing research and enabling practical applications.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This large-margin learning method can be straightforwardly extended to various downstream applications. By enforcing a sufficiently large margin between paired data samples, the model can rapidly acquire target editing capabilities through SFT. Reinforcement learning can then be seamlessly integrated to further enhance performance on challenging cases. This section details two practical extensions: (1) speed editing for speech rate control, and (2) denoising and silence trimming.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "model",
                    "control",
                    "between",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Background noise and silence segments in prompt audio can substantially influence the performance of zero-shot voice cloning. The model tends to interpret these acoustic features as part of the speaker&#8217;s characteristics, subsequently reproducing them in synthesized audio. While such imitation is desirable in some use cases, it is undesirable in others. To address this, we integrated denoising and silence trimming using a generative approach, which enables targeted editing of both the prompt and the synthesized audio.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present Step-Audio&#8209;EditX, an LLM-based audio model trained on large-margin data and enhanced through reinforcement learning. The model enables zero-shot TTS, iterative editing of emotion and speaking style, and paralinguistic editing. We have identified that the capabilities of LLMs and the use of large-margin data, which have often been overlooked in previous studies, allow the model to overcome the limitations of audio representations. Furthermore, the proposed framework can be easily extended to a variety of tasks, including dialect editing, accent editing, vocal editing, and imitation. Finally, it should be noted that our audio editing process is not strictly conventional \"editing\" in the traditional sense. Instead, it functions as a form of conditional regeneration or transfer. For tasks that require partial modifications while preserving the rest of the content, our approach provides a straightforward yet effective mask-based editing method by reconstructing paired data to ensure only specific portions of the edited tokens differ from the original sequence.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing",
                    "emotion"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Performance of Step-Audio-EditX on Paralinguistic Editing.\n(Evaluated by LLM-Judge on a 1-3 scale)",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">\n<span class=\"ltx_text ltx_font_bold\">Paralinguistic</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">0</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">1</sub></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.89</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">English</td>\n<td class=\"ltx_td ltx_align_center\">2.02</td>\n<td class=\"ltx_td ltx_align_center\">2.89</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">Average</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">2.89</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "â†‘uparrow",
            "evaluated",
            "editing",
            "english",
            "scale",
            "language",
            "average",
            "llmjudge",
            "iter0",
            "paralinguistic",
            "iter1",
            "stepaudioeditx",
            "chinese",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Editing Results.</span> As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Paralinguistic Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, a significant performance gain is obtained by adding paralinguistic tags in a single editing iteration.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Step-Audio-EditX, the first open-source LLM-based reinforcement learning audio model excelling at expressive and iterative audio editing&#8212;encompassing emotion, speaking style, and paralinguistics&#8212;alongside robust zero-shot text-to-speech (TTS) capabilities. Our core innovation lies in leveraging only large-margin synthetic data in post-training, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks. Our code and models are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/stepfun-ai/Step-Audio-EditX\" title=\"\">https://github.com/stepfun-ai/Step-Audio-EditX</a>.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many previous studies on speech disentanglement have relied on approaches such as adversarial training<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">naturalspeech3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2022crossspeakeremotiondisentanglingtransfer</span>]</cite>, feature engineering<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2023dddmvcdecoupleddenoisingdiffusion</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Anastassiou2024VoiceShopAU</span>]</cite>, and innovative network architectures<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Jia2022ZeroShotAC</span>]</cite> to achieve attribute decoupling. In contrast, we propose a simple yet stable data-driven method. Specifically, we design a pipeline for generating high-quality data pairs that preserve identical linguistic content while exhibiting clearly distinguishable variations in one or a few attributes, such as emotion, style, accent, and paralinguistic features. By training models on such data pairs, we achieve effective attribute disentanglement, enabling to edit the attribute of input speech. Moreover, by applying multiple iterative \"editing\" steps, the intensity of a target attribute can be progressively enhanced or reduced. Beyond emotion, style and paralinguistic editing, we demonstrate that this approach can be extended to other applications, including speed rate adjustment, speech denoising, and silence trimming.\nIn this report, we outline our contributions and findings:</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing, encompassing emotion, speaking style, and paralinguistics, alongside robust zero-shot TTS capabilities.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our prior work, we introduced an Audio-Edit synthesis model in Step-Audio<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stepaudio</span>]</cite> for nuanced emotional expressions and diverse speaking styles data generation. In this report, we retain the previous model along with the same audio tokenizer. The key modifications include an expanded range of emotions and speaking styles, the addition of zero-shot TTS and paralinguistic editing capabilities, as well as a reduction in model parameters from 130B to 3B. Leveraging large-margin synthetic data, our 3B model demonstrates superior and more stable performance compared to the previous version.\n\n<br class=\"ltx_break\"/>Our system comprises three primary components: (1) a dual-codebook audio tokenizer, which converts reference or input audio into discrete tokens; (2) an audio LLM that generates dual-codebook token sequences; and (3) an audio decoder, which converts the dual-codebook token sequences predicted by the audio LLM back into audio waveforms using a flow matching approach.This integrated architecture enables the Step-Audio-EditX to perform zero-shot TTS and diverse editing tasks within a unified framework. Thus, it can directly capitalize on the rich ecosystem of post-training techniques developed for text LLMs.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "paralinguistic",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ SFT to enable the Step-Audio-EditX model for zero-shot TTS and diverse audio editing tasks. The SFT data can be categorized into several parts: zero-shot TTS, emotion editing, speaking style editing and paralinguistic editing. Notably, the large-margin dataset targets editing tasks, particularly on the aspects of emotion and speaking style.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "paralinguistic",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a high-quality, professionally annotated in-house dataset, primarily in Chinese and English, for zero-shot TTS. Furthermore, a minimal amount of Cantonese and Sichuanese data is employed to elicit dialect capabilities. To ensure diverse and highly expressive styles and emotions in the synthesized speech, as well as robust zero-shot performance, The dataset captures vocal variations within individual speakers as well as across a broad speaker population, comprising approximately 60,000 unique individuals.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Paralinguistic cues, such as breathing, laughter, and filled pauses (e.g., \"uhm\"), are crucial for enhancing the naturalness and expressiveness of synthesized speech. We achieved paralinguistic editing capability by using a \"semi-synthetic\" strategy, which leverages the NVSpeech dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2025nvspeech</span>]</cite>, a highly expressive speech corpus whose rich annotations for numerous paralinguistic types enabled the construction of comparative quadruplets for model training. The quadruplet <math alttext=\"\\langle text\\textsubscript{without\\_tags},audio\\textsubscript{without\\_tags},text\\textsubscript{nv\\_source},audio\\textsubscript{nv\\_source}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">without_tags</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">without_tags</sub></mtext></mrow><mo>,</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">nv_source</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">nv_source</sub></mtext></mrow><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle text\\textsubscript{without\\_tags},audio\\textsubscript{without\\_tags},text\\textsubscript{nv\\_source},audio\\textsubscript{nv\\_source}\\rangle</annotation></semantics></math> construction differs from the triplet by using the NVSpeech original audio and transcript as the target output and the StepTTS voice cloning generated audio as the input, which is synthesized using the original transcript after paralinguistic tag removal.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As paralinguistic editing is an editing task performed in the time domain and exhibits substantial intrinsic margin differences, a margin scoring model is not required for data selection. A small set of quadruplet data is sufficient to effectively elicit the model&#8217;s paralinguistic editing capabilities.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> Model responses were scored on a 1-10 scale for emotion and speaking style editing by a comprehension model. Preference pairs were then generated from these scores, retaining only pairs with a score margin greater than 8 points in the final dataset.</p>\n\n",
                "matched_terms": [
                    "scale",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The accurate and comprehensive evaluation of a model&#8217;s performance in synthesizing emotional, stylistic, and paralinguistic speech represents a substantial challenge. To address this, we first introduce the construction of a comprehensive and reproducible benchmark in Section 5.1. We then employ this benchmark in Section 5.2 to demonstrate the advantages of our Step-Audio-EditX model.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "paralinguistic",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Selection.</span> The speaker set for zero-shot cloning consisted of eight speakers (2 male and 2 female per language, for both Chinese and English). The Chinese speakers were sourced from the Wenet-Speech4TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts12800hourmandarintts</span>]</cite> corpus, whereas the English speakers were sourced from the open-source GLOBE-V2<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024globehighqualityenglishcorpus</span>]</cite> and Libri-Light<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kahn_2020</span>]</cite> datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "language",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion.</span> The emotional test set covers five categories: happiness, anger, sadness, fear, and surprise. Each category includes 50 Chinese and 50 English prompts, with the textual content of each prompt designed to be consistent with its corresponding target emotion.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking Style.</span> The test set includes seven speaking styles: childlike, elderly, exaggerated, recitative, passionate, coquettish, and whisper. Each style contains 50 Chinese and 50 English prompts, with content matched to its target style.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic.</span> The paralinguistic test set includes ten paralinguistic labels per speaker: breathing, laughter, surprise-oh, confirmation-en, uhm, surprise-ah, surprise-wa, sigh, question-ei, and dissatisfaction-hnn. Each label contains 50 relevant LLM-generated samples in Chinese and 50 in English.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Style Evaluation.</span> To evaluate the performance of paralinguistic editing, a specialized evaluation prompt has been designed for the Gemini-2.5-Pro model, employing a rigorous 1&#8211;3 scoring scale (3 = perfect, 2 = flawed, 1 = failed). The prompt directs the model to actively examine specific assessment points in the audio&#8212;such as whether annotations like [laughter] or [sigh] have been accurately inserted. Particular emphasis is placed on the most common failure mode, &#8220;omission,&#8221; where the audio may remain fluent but lacks required paralinguistic elements specified in the instructions. Finally, model performance in the paralinguistic editing task is assessed by calculating the overall average score generated by Gemini-2.5-Pro model.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "scale",
                    "average",
                    "paralinguistic",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details our model&#8217;s performance on the Step-Audio-Edit-Benchmark, benchmark and demonstrates its superior editing accuracy and scalability when used to edit audio generated by various closed-source TTS systems.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative Editing Results.</span> As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T1\" title=\"Table 1 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, there was a significant boost in both emotion and speaking style accuracy after the initial edit of the Iter<sub class=\"ltx_sub\">0</sub> audio. Furthermore, with successive iterations of editing, the accuracy for both emotion and speaking style was further enhanced.</p>\n\n",
                "matched_terms": [
                    "iter0",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Audio Ablation.</span> Since the performance improvement in later iterations (starting from Iter<sub class=\"ltx_sub\">2</sub>) were attributed to both the dual-code and the prompt audio. To isolate the effect of the prompt audio, we conducted an ablation study in which the prompt audio was held constant across all iterations. As presented in the Prompt-Fixed section of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T1\" title=\"Table 1 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the accuracy for both emotion and speaking style continues to improve with an increasing number of editing iterations. This clearly demonstrates the effectiveness of our large-margin method.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on Closed-Source Models.</span> The emotion and speaking style generalization of the Step-Audio-EditX model was evaluated on several leading closed-source TTS systems, including GPT-4o-mini-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span>, Eleven_Multilingual_v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://elevenlabs.io/docs/api-reference/text-to-speech/convert</span></span></span>, Doubao-Seed-TTS-2.0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://www.volcengine.com/docs/6561/1871062</span></span></span>, and MiniMax-speech-2.6-hd<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://platform.minimaxi.com/docs/api-reference/speech-t2a-http</span></span></span>. For each TTS system, one male and one female built-in voice were selected for direct speech synthesis of the source text. Subsequently, three iterations of editing were applied to the resultant audio outputs. As presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T2\" title=\"Table 2 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the built-in voices of these closed-source systems possess considerable in-context capabilities, allowing them to partially convey the emotions in the text. After a single editing round with Step-Audio-EditX, the emotion and style accuracy across all voice models exhibited significant improvement. Further enhancement was observed over the next two iterations, robustly demonstrating our model&#8217;s strong generalization.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "evaluated",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Paralinguistic editing can be considered a time-domain operation. We evaluated the effect of a single editing iteration using Step-Audio-EditX and assessed its generalization across other closed-source models.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "paralinguistic",
                    "evaluated",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on Closed-Source Models.</span> The generalization evaluation was conducted identically to the prior one. For each closed-source model, we employed one female and one male built-in voice to synthesize speech from texts with paralinguistic labels removed. The resultant audio then underwent a single editing iteration. Additionally, for comparison, extra audio samples were synthesized by substituting paralinguistic tags with onomatopoeic words (e.g., \"[Laughter]\" &#8594; \"haha\"). After one iteration of paralinguistic editing with Step-Audio-EditX, the performance of paralinguistic reproduction is comparable to that achieved by the built-in voices of closed-source models when synthesizing native paralinguistic content directly.</p>\n\n",
                "matched_terms": [
                    "stepaudioeditx",
                    "paralinguistic",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results across emotion, speaking style, and paralinguistic editing tasks confirm that our simple yet powerful approach&#8212;large-margin learning with reinforcement learning enhancement&#8212;delivers high accuracy and strong generalization. This methodology demonstrates considerable promise for both advancing research and enabling practical applications.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This large-margin learning method can be straightforwardly extended to various downstream applications. By enforcing a sufficiently large margin between paired data samples, the model can rapidly acquire target editing capabilities through SFT. Reinforcement learning can then be seamlessly integrated to further enhance performance on challenging cases. This section details two practical extensions: (1) speed editing for speech rate control, and (2) denoising and silence trimming.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Background noise and silence segments in prompt audio can substantially influence the performance of zero-shot voice cloning. The model tends to interpret these acoustic features as part of the speaker&#8217;s characteristics, subsequently reproducing them in synthesized audio. While such imitation is desirable in some use cases, it is undesirable in others. To address this, we integrated denoising and silence trimming using a generative approach, which enables targeted editing of both the prompt and the synthesized audio.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present Step-Audio&#8209;EditX, an LLM-based audio model trained on large-margin data and enhanced through reinforcement learning. The model enables zero-shot TTS, iterative editing of emotion and speaking style, and paralinguistic editing. We have identified that the capabilities of LLMs and the use of large-margin data, which have often been overlooked in previous studies, allow the model to overcome the limitations of audio representations. Furthermore, the proposed framework can be easily extended to a variety of tasks, including dialect editing, accent editing, vocal editing, and imitation. Finally, it should be noted that our audio editing process is not strictly conventional \"editing\" in the traditional sense. Instead, it functions as a form of conditional regeneration or transfer. For tasks that require partial modifications while preserving the rest of the content, our approach provides a straightforward yet effective mask-based editing method by reconstructing paired data to ensure only specific portions of the edited tokens differ from the original sequence.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "editing"
                ]
            }
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Generalization of Paralinguistic Editing on Close-Source Models.\n(Evaluated by LLM-Judge on a 1-3 scale)",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">\n<span class=\"ltx_text ltx_font_bold\">Paralinguistic</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">0</sub></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Substitution</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Iter<sub class=\"ltx_sub\">1</sub></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\">Chinese</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">MiniMax-speech-2.6-hd</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Doubao-Seed-TTS-2.0</td>\n<td class=\"ltx_td ltx_align_center\">1.67</td>\n<td class=\"ltx_td ltx_align_center\">2.81</td>\n<td class=\"ltx_td ltx_align_center\">2.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GPT-4o-mini-TTS</td>\n<td class=\"ltx_td ltx_align_center\">1.71</td>\n<td class=\"ltx_td ltx_align_center\">2.88</td>\n<td class=\"ltx_td ltx_align_center\">2.93</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ElevenLabs-v2</td>\n<td class=\"ltx_td ltx_align_center\">1.70</td>\n<td class=\"ltx_td ltx_align_center\">2.71</td>\n<td class=\"ltx_td ltx_align_center\">2.92</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\">English</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">MiniMax-speech-2.6-hd</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Doubao-Seed-TTS-2.0</td>\n<td class=\"ltx_td ltx_align_center\">1.72</td>\n<td class=\"ltx_td ltx_align_center\">2.75</td>\n<td class=\"ltx_td ltx_align_center\">2.92</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GPT-4o-mini-TTS</td>\n<td class=\"ltx_td ltx_align_center\">1.90</td>\n<td class=\"ltx_td ltx_align_center\">2.90</td>\n<td class=\"ltx_td ltx_align_center\">2.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ElevenLabs-v2</td>\n<td class=\"ltx_td ltx_align_center\">1.93</td>\n<td class=\"ltx_td ltx_align_center\">2.87</td>\n<td class=\"ltx_td ltx_align_center\">2.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">Average</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">MiniMax-speech-2.6-hd</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.89</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Doubao-Seed-TTS-2.0</td>\n<td class=\"ltx_td ltx_align_center\">1.70</td>\n<td class=\"ltx_td ltx_align_center\">2.78</td>\n<td class=\"ltx_td ltx_align_center\">2.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GPT-4o-mini-TTS</td>\n<td class=\"ltx_td ltx_align_center\">1.81</td>\n<td class=\"ltx_td ltx_align_center\">2.89</td>\n<td class=\"ltx_td ltx_align_center\">2.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">ElevenLabs-v2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.90</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "evaluated",
            "llmjudge",
            "iter0",
            "closesource",
            "gpt4ominitts",
            "average",
            "substitution",
            "iter1",
            "elevenlabsv2",
            "english",
            "scale",
            "language",
            "â†‘uparrow",
            "editing",
            "model",
            "paralinguistic",
            "minimaxspeech26hd",
            "generalization",
            "chinese",
            "doubaoseedtts20"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Step-Audio-EditX, the first open-source LLM-based reinforcement learning audio model excelling at expressive and iterative audio editing&#8212;encompassing emotion, speaking style, and paralinguistics&#8212;alongside robust zero-shot text-to-speech (TTS) capabilities. Our core innovation lies in leveraging only large-margin synthetic data in post-training, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks. Our code and models are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/stepfun-ai/Step-Audio-EditX\" title=\"\">https://github.com/stepfun-ai/Step-Audio-EditX</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "doubaoseedtts20",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, TTS technology has advanced significantly. A notable development is zero-shot TTS models, which can generate high-quality, natural-sounding speech by mimicking the timbre, prosody, and style of a reference speech prompt. Generally, current zero-shot TTS systems fall into three main categories: those that utilize LLMs to model discrete or continuous acoustic tokens <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vall-e</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">valle2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">melle</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">maskgct</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sparktts</span>]</cite>, those employing diffusion or flow matching models to learn direct text-to-speech mapping <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2024voicebox</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">voiceflow</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">matchatts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">seedtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">f5tts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">f5rtts</span>]</cite>, and hybrid coarse-to-fine systems, where LLMs first convert text tokens into coarse speech tokens, which are then refined by a diffusion or flow matching model to render fine-grained speech details<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cosyvoice</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fireredtts</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">fireredtts2</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2024speaking</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2025cosyvoice3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jia2025ditar</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">deng2025indextts</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many previous studies on speech disentanglement have relied on approaches such as adversarial training<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">naturalspeech3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2022crossspeakeremotiondisentanglingtransfer</span>]</cite>, feature engineering<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">choi2023dddmvcdecoupleddenoisingdiffusion</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Anastassiou2024VoiceShopAU</span>]</cite>, and innovative network architectures<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Jia2022ZeroShotAC</span>]</cite> to achieve attribute decoupling. In contrast, we propose a simple yet stable data-driven method. Specifically, we design a pipeline for generating high-quality data pairs that preserve identical linguistic content while exhibiting clearly distinguishable variations in one or a few attributes, such as emotion, style, accent, and paralinguistic features. By training models on such data pairs, we achieve effective attribute disentanglement, enabling to edit the attribute of input speech. Moreover, by applying multiple iterative \"editing\" steps, the intensity of a target attribute can be progressively enhanced or reduced. Beyond emotion, style and paralinguistic editing, we demonstrate that this approach can be extended to other applications, including speed rate adjustment, speech denoising, and silence trimming.\nIn this report, we outline our contributions and findings:</p>\n\n",
                "matched_terms": [
                    "models",
                    "paralinguistic",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing, encompassing emotion, speaking style, and paralinguistics, alongside robust zero-shot TTS capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our prior work, we introduced an Audio-Edit synthesis model in Step-Audio<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stepaudio</span>]</cite> for nuanced emotional expressions and diverse speaking styles data generation. In this report, we retain the previous model along with the same audio tokenizer. The key modifications include an expanded range of emotions and speaking styles, the addition of zero-shot TTS and paralinguistic editing capabilities, as well as a reduction in model parameters from 130B to 3B. Leveraging large-margin synthetic data, our 3B model demonstrates superior and more stable performance compared to the previous version.\n\n<br class=\"ltx_break\"/>Our system comprises three primary components: (1) a dual-codebook audio tokenizer, which converts reference or input audio into discrete tokens; (2) an audio LLM that generates dual-codebook token sequences; and (3) an audio decoder, which converts the dual-codebook token sequences predicted by the audio LLM back into audio waveforms using a flow matching approach.This integrated architecture enables the Step-Audio-EditX to perform zero-shot TTS and diverse editing tasks within a unified framework. Thus, it can directly capitalize on the rich ecosystem of post-training techniques developed for text LLMs.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio LLM uses the same architecture as our prior Audio-Edit model, differing only in its smaller parameter size of 3B. To capitalize on the powerful language capabilities of pre-trained text LLMs, the 3B model is initialized with a text-based LLM, followed by training on a blended dataset with a 1:1 ratio of text data to audio dual-codebook tokens. The audio LLM processes text tokens along with their corresponding dual-codebook audio tokens in a chat format, subsequently generating dual-codebook tokens as the sole output.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ SFT to enable the Step-Audio-EditX model for zero-shot TTS and diverse audio editing tasks. The SFT data can be categorized into several parts: zero-shot TTS, emotion editing, speaking style editing and paralinguistic editing. Notably, the large-margin dataset targets editing tasks, particularly on the aspects of emotion and speaking style.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a high-quality, professionally annotated in-house dataset, primarily in Chinese and English, for zero-shot TTS. Furthermore, a minimal amount of Cantonese and Sichuanese data is employed to elicit dialect capabilities. To ensure diverse and highly expressive styles and emotions in the synthesized speech, as well as robust zero-shot performance, The dataset captures vocal variations within individual speakers as well as across a broad speaker population, comprising approximately 60,000 unique individuals.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Margin Scoring.</span> To evaluate the triplet generated, we developed a scoring model using a small, human-annotated dataset. The model evaluates audio pairs on a 1-10 scale, with higher margin scores corresponding to more desirable outcomes.</p>\n\n",
                "matched_terms": [
                    "scale",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Paralinguistic cues, such as breathing, laughter, and filled pauses (e.g., \"uhm\"), are crucial for enhancing the naturalness and expressiveness of synthesized speech. We achieved paralinguistic editing capability by using a \"semi-synthetic\" strategy, which leverages the NVSpeech dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2025nvspeech</span>]</cite>, a highly expressive speech corpus whose rich annotations for numerous paralinguistic types enabled the construction of comparative quadruplets for model training. The quadruplet <math alttext=\"\\langle text\\textsubscript{without\\_tags},audio\\textsubscript{without\\_tags},text\\textsubscript{nv\\_source},audio\\textsubscript{nv\\_source}\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">&#10216;</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">without_tags</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">without_tags</sub></mtext></mrow><mo>,</mo><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">nv_source</sub></mtext></mrow><mo>,</mo><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext><sub class=\"ltx_sub\">nv_source</sub></mtext></mrow><mo stretchy=\"false\">&#10217;</mo></mrow><annotation encoding=\"application/x-tex\">\\langle text\\textsubscript{without\\_tags},audio\\textsubscript{without\\_tags},text\\textsubscript{nv\\_source},audio\\textsubscript{nv\\_source}\\rangle</annotation></semantics></math> construction differs from the triplet by using the NVSpeech original audio and transcript as the target output and the StepTTS voice cloning generated audio as the input, which is synthesized using the original transcript after paralinguistic tag removal.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As paralinguistic editing is an editing task performed in the time domain and exhibits substantial intrinsic margin differences, a margin scoring model is not required for data selection. A small set of quadruplet data is sufficient to effectively elicit the model&#8217;s paralinguistic editing capabilities.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span> We first collected real-world prompt audio and corresponding text prompts from users, and generated 20 candidate responses using the SFT model. We then constructed chosen/rejected pairs by having human annotators rate each of the 20 responses on a 5-point scale based on the criteria of correctness, prosody, and naturalness. Only pairs with a score margin greater than 3 were selected.</p>\n\n",
                "matched_terms": [
                    "scale",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> Model responses were scored on a 1-10 scale for emotion and speaking style editing by a comprehension model. Preference pairs were then generated from these scores, retaining only pairs with a score margin greater than 8 points in the final dataset.</p>\n\n",
                "matched_terms": [
                    "scale",
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SFT stage enhances the model&#8217;s zero-shot text-to-speech synthesis and editing capabilities through the use of distinct system prompts in a chat format. In the zero-shot TTS task, the prompt waveform is encoded into dual-codebook tokens, which are subsequently detokenized into string format and incorporated into the speaker information within the system prompt. The text to be synthesized serves as the user prompt in a chat-based format, and the generated dual-code tokens are returned as the system&#8217;s response. For the editing task, all operations are defined under a unified system prompt. The user prompt includes both the original audio and a descriptive command for the editing operation, and the system response delivers the resulting edited audio tokens. The model is finetuned for one epoch with learning rate from <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> to <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Reinforcement learning has further amplified the model&#8217;s stability in zero-shot TTS, as well as its capability and expressiveness in following editing instructions. These enhancements are particularly noticeable when there is a substantial divergence between the emotional and stylistic characteristics of the source prompt waveform and the target editing output, such as generating sad speech from a happy prompt or converting loud speech into a whisper. This reinforcement learning approach offers a novel perspective to address these challenges by shifting the focus from achieving ideal speech representation disentanglement to improving both the construction of large-margin pairs and the efficacy of the reward model evaluation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The accurate and comprehensive evaluation of a model&#8217;s performance in synthesizing emotional, stylistic, and paralinguistic speech represents a substantial challenge. To address this, we first introduce the construction of a comprehensive and reproducible benchmark in Section 5.1. We then employ this benchmark in Section 5.2 to demonstrate the advantages of our Step-Audio-EditX model.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Selection.</span> The speaker set for zero-shot cloning consisted of eight speakers (2 male and 2 female per language, for both Chinese and English). The Chinese speakers were sourced from the Wenet-Speech4TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts12800hourmandarintts</span>]</cite> corpus, whereas the English speakers were sourced from the open-source GLOBE-V2<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024globehighqualityenglishcorpus</span>]</cite> and Libri-Light<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kahn_2020</span>]</cite> datasets, respectively.</p>\n\n",
                "matched_terms": [
                    "language",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion.</span> The emotional test set covers five categories: happiness, anger, sadness, fear, and surprise. Each category includes 50 Chinese and 50 English prompts, with the textual content of each prompt designed to be consistent with its corresponding target emotion.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaking Style.</span> The test set includes seven speaking styles: childlike, elderly, exaggerated, recitative, passionate, coquettish, and whisper. Each style contains 50 Chinese and 50 English prompts, with content matched to its target style.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic.</span> The paralinguistic test set includes ten paralinguistic labels per speaker: breathing, laughter, surprise-oh, confirmation-en, uhm, surprise-ah, surprise-wa, sigh, question-ei, and dissatisfaction-hnn. Each label contains 50 relevant LLM-generated samples in Chinese and 50 in English.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "english",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion and Speaking Style Evaluation.</span> To evaluate emotion and speaking style, predefined category sets (5 emotions and 7 styles) are provided to the Gemini-2.5-Pro model in the prompts, instructing it to classify the audio. The final accuracy for each category is calculated as the average across all speakers.</p>\n\n",
                "matched_terms": [
                    "average",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Style Evaluation.</span> To evaluate the performance of paralinguistic editing, a specialized evaluation prompt has been designed for the Gemini-2.5-Pro model, employing a rigorous 1&#8211;3 scoring scale (3 = perfect, 2 = flawed, 1 = failed). The prompt directs the model to actively examine specific assessment points in the audio&#8212;such as whether annotations like [laughter] or [sigh] have been accurately inserted. Particular emphasis is placed on the most common failure mode, &#8220;omission,&#8221; where the audio may remain fluent but lacks required paralinguistic elements specified in the instructions. Finally, model performance in the paralinguistic editing task is assessed by calculating the overall average score generated by Gemini-2.5-Pro model.</p>\n\n",
                "matched_terms": [
                    "editing",
                    "scale",
                    "average",
                    "model",
                    "paralinguistic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative Editing Results.</span> As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T1\" title=\"Table 1 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, there was a significant boost in both emotion and speaking style accuracy after the initial edit of the Iter<sub class=\"ltx_sub\">0</sub> audio. Furthermore, with successive iterations of editing, the accuracy for both emotion and speaking style was further enhanced.</p>\n\n",
                "matched_terms": [
                    "iter0",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on Closed-Source Models.</span> The emotion and speaking style generalization of the Step-Audio-EditX model was evaluated on several leading closed-source TTS systems, including GPT-4o-mini-TTS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://platform.openai.com/docs/guides/text-to-speech</span></span></span>, Eleven_Multilingual_v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://elevenlabs.io/docs/api-reference/text-to-speech/convert</span></span></span>, Doubao-Seed-TTS-2.0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://www.volcengine.com/docs/6561/1871062</span></span></span>, and MiniMax-speech-2.6-hd<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://platform.minimaxi.com/docs/api-reference/speech-t2a-http</span></span></span>. For each TTS system, one male and one female built-in voice were selected for direct speech synthesis of the source text. Subsequently, three iterations of editing were applied to the resultant audio outputs. As presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T2\" title=\"Table 2 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the built-in voices of these closed-source systems possess considerable in-context capabilities, allowing them to partially convey the emotions in the text. After a single editing round with Step-Audio-EditX, the emotion and style accuracy across all voice models exhibited significant improvement. Further enhancement was observed over the next two iterations, robustly demonstrating our model&#8217;s strong generalization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluated",
                    "editing",
                    "model",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Control on Closed-Source Models.</span> Due to the limited availability of closed-source systems with emotion and speaking style control, this section presents a comparative evaluation of Doubao-Seed-TTS-2.0 and MiniMax-speech-2.6-hd, selected for their capability in both zero-shot cloning and emotion control. To meet the minimum audio length constraints of the closed-source models and to ensure a fair evaluation, the prompt audios for all the speakers in the Step-Audio-Edit-Benchmark were extended in duration. These extended audios were employed for zero-shot cloning followed by two emotion editing iterations. Additionally, the cloned voices were used to generate emotional speech via each closed-source model&#8217;s native emotion control.The outputs from this native emotion control subsequently underwent one round of editing with our model. It can be observed from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T3\" title=\"Table 3 &#8227; 5.2.1 Emotion and Speaking Style Editing Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> that:</p>\n\n",
                "matched_terms": [
                    "models",
                    "editing",
                    "model",
                    "minimaxspeech26hd",
                    "doubaoseedtts20"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One emotional editing iteration applied to the zero-shot cloned audio outperformed the results generated by the native emotion control functions of the closed-source models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Paralinguistic editing can be considered a time-domain operation. We evaluated the effect of a single editing iteration using Step-Audio-EditX and assessed its generalization across other closed-source models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluated",
                    "editing",
                    "paralinguistic",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Editing Results.</span> As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.03601v2#S5.T4\" title=\"Table 4 &#8227; 5.2.2 Paralinguistic Results &#8227; 5.2 Evaluation Results &#8227; 5 Evaluation &#8227; Step-Audio-EditX Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, a significant performance gain is obtained by adding paralinguistic tags in a single editing iteration.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generalization on Closed-Source Models.</span> The generalization evaluation was conducted identically to the prior one. For each closed-source model, we employed one female and one male built-in voice to synthesize speech from texts with paralinguistic labels removed. The resultant audio then underwent a single editing iteration. Additionally, for comparison, extra audio samples were synthesized by substituting paralinguistic tags with onomatopoeic words (e.g., \"[Laughter]\" &#8594; \"haha\"). After one iteration of paralinguistic editing with Step-Audio-EditX, the performance of paralinguistic reproduction is comparable to that achieved by the built-in voices of closed-source models when synthesizing native paralinguistic content directly.</p>\n\n",
                "matched_terms": [
                    "models",
                    "editing",
                    "model",
                    "paralinguistic",
                    "generalization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation results across emotion, speaking style, and paralinguistic editing tasks confirm that our simple yet powerful approach&#8212;large-margin learning with reinforcement learning enhancement&#8212;delivers high accuracy and strong generalization. This methodology demonstrates considerable promise for both advancing research and enabling practical applications.</p>\n\n",
                "matched_terms": [
                    "generalization",
                    "paralinguistic",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This large-margin learning method can be straightforwardly extended to various downstream applications. By enforcing a sufficiently large margin between paired data samples, the model can rapidly acquire target editing capabilities through SFT. Reinforcement learning can then be seamlessly integrated to further enhance performance on challenging cases. This section details two practical extensions: (1) speed editing for speech rate control, and (2) denoising and silence trimming.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Background noise and silence segments in prompt audio can substantially influence the performance of zero-shot voice cloning. The model tends to interpret these acoustic features as part of the speaker&#8217;s characteristics, subsequently reproducing them in synthesized audio. While such imitation is desirable in some use cases, it is undesirable in others. To address this, we integrated denoising and silence trimming using a generative approach, which enables targeted editing of both the prompt and the synthesized audio.</p>\n\n",
                "matched_terms": [
                    "model",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present Step-Audio&#8209;EditX, an LLM-based audio model trained on large-margin data and enhanced through reinforcement learning. The model enables zero-shot TTS, iterative editing of emotion and speaking style, and paralinguistic editing. We have identified that the capabilities of LLMs and the use of large-margin data, which have often been overlooked in previous studies, allow the model to overcome the limitations of audio representations. Furthermore, the proposed framework can be easily extended to a variety of tasks, including dialect editing, accent editing, vocal editing, and imitation. Finally, it should be noted that our audio editing process is not strictly conventional \"editing\" in the traditional sense. Instead, it functions as a form of conditional regeneration or transfer. For tasks that require partial modifications while preserving the rest of the content, our approach provides a straightforward yet effective mask-based editing method by reconstructing paired data to ensure only specific portions of the edited tokens differ from the original sequence.</p>\n\n",
                "matched_terms": [
                    "paralinguistic",
                    "model",
                    "editing"
                ]
            }
        ]
    }
}