{
    "p3": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_top\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Joshua Wolfe Brook, Ilia Markov</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Computational Linguistics &amp; Text Mining Lab</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Vrije Universiteit Amsterdam</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">j.w.brook@student.vu.nl, i.markov@vu.nl</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "jwbrookstudentvunl",
            "joshua",
            "universiteit",
            "ilia",
            "wolfe",
            "amsterdam",
            "imarkovvunl",
            "brook",
            "computational",
            "mining",
            "linguistics",
            "lab",
            "text",
            "vrije",
            "markov"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "markov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "text",
                    "markov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "text",
                    "markov"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "computational",
                    "text"
                ]
            }
        ]
    },
    "S3.T1": {
        "caption": "Table 1: Macro & positive class (hate) F1 scores for binary HSD and macro F1 scores for implicit multi-class prediction on the Latent Hatred dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Context</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Incorporation</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Strategy</span></span></span>\n</span> </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Binary</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">Multi-Class</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">Macro F1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">Hate F1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">Macro F1</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Zero-Context</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.72</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.65</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.51</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">REL</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ConceptNet</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.74</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.67</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Prediction</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.65</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.26</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" rowspan=\"4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Named Entities</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.73</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.66</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Context-Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.57</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.42</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.23</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Enhance</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.72</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.64</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.49</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" rowspan=\"4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Full Text</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.73</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.66</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.53</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.75</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.69</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.53</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Context-Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.64</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Enhance</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "full",
            "positive",
            "macro",
            "strategy",
            "embed",
            "concat",
            "entities",
            "enhance",
            "implicit",
            "class",
            "context",
            "llm",
            "prediction",
            "append",
            "conceptnet",
            "text",
            "scores",
            "hatred",
            "hsd",
            "rel",
            "incorporation",
            "binary",
            "multiclass",
            "latent",
            "named",
            "contextembed",
            "hate",
            "zerocontext",
            "dataset"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "named",
                    "entities",
                    "hatred",
                    "hate",
                    "implicit",
                    "hsd",
                    "context",
                    "zerocontext",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Leveraging LLMs for Context-Aware Implicit\n<br class=\"ltx_break\"/>Textual and Multimodal Hate Speech Detection</span>\n</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "implicit",
                    "hsd",
                    "context",
                    "zerocontext",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "hate",
                    "implicit",
                    "hsd",
                    "llm",
                    "context",
                    "binary",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To what extent is a pre-trained LLM more effective than static entity linking at generating background context for implicit HSD?</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hsd",
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How can background context be effectively incorporated into a HSD model input?</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How useful are these generation and incorporation methods for textual and multimodal HSD across binary and multi-class settings?</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "multiclass",
                    "binary",
                    "hsd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While hate speech is inherently contextual <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, previous studies have shown that incorporating generic additional context into the model input often leads to a negligible or negative impact on model performance <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>. These findings suggest that context must be carefully selected, represented, and integrated to avoid introducing noise. This section discusses previous work relevant to HSD in context retrieval and incorporation, the use of LLMs, and the challenges surrounding multimodality.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "hsd",
                    "context",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "hate",
                    "hsd",
                    "context",
                    "incorporation",
                    "binary",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "named",
                    "entities",
                    "hate",
                    "hsd",
                    "rel",
                    "macro",
                    "context",
                    "prediction",
                    "binary",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs have been used in HSD previously, generally replacing classifiers themselves, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>)</cite>, who directly compare a generative LLM against two pre-trained models: BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib25\" title=\"\">2019</a>)</cite>. They test various prompting strategies (few-shot, chain-of-thought, etc.) and demonstrate that a generalist LLM with simple prompts can comfortably outperform transformer-based encoders. These results are not confirmed, however, by numerous other studies, who find that LLMs either match or fall short of the performance of encoder-only classifiers, especially without extensive prompt engineering <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Albladi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib1\" title=\"\">2025</a>)</cite> discuss these results, highlighting the fact that encoder models are fine-tuned on the training data and can potentially capture the peculiarities of a given dataset better than generalist LLMs.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "llm",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "hsd",
                    "context",
                    "append",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "implicit",
                    "hsd",
                    "context",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "named",
                    "full",
                    "entities",
                    "llm",
                    "context",
                    "incorporation",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "latent",
                    "hatred",
                    "hate",
                    "implicit",
                    "class",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our second source is <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, used to detect misogynous memes. This dataset is comprised of 10,995 distinct memes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Five duplicate memes were discovered in MAMI during preprocessing and subsequently removed.</span></span></span> scraped from social media platforms and meme-creation websites.\nThe high level annotation is a simple binary distinction between misogynous<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>A meme is defined as misogynous if it &#8220;conceptually describes an offensive, sexist or hateful scene [&#8230;] having as target a woman or a group of women&#8221;.</span></span></span> and non-misogynous memes.\nIn the second level annotation, misogynous memes are labelled with one or more subtypes of misogyny: shaming, stereotyping, objectification, and violence. These labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T5\" title=\"Table 5 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, with example memes and their labels given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.\nWe again split the data in an 80/20 ratio.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Posts are first analysed for named entities (in one branch of the pipeline), then contextual information is generated and incorporated into vectorised representations, before finally being classified.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the named-entity-based context generation approach, we identify named entities using the uncased <span class=\"ltx_text ltx_font_typewriter\">dslim/bert-base-NER</span> model <cite class=\"ltx_cite ltx_citemacro_cite\">Lim (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib4\" title=\"\">2021</a>)</cite>, a fine-tuned BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> system trained on Named Entity Recognition (NER), chosen based on its high performance on the CONLL-2003 benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">Tjong Kim&#160;Sang and De&#160;Meulder (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib38\" title=\"\">2003</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Context:</span> The first configuration uses only the raw post, simply creating SBERT embeddings from each post in the dataset and feeding them directly into the input of the MLP classification model.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "zerocontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">REL:</span> The second baseline replicates <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> entity-linking strategy, augmenting post text with Wikipedia-derived summaries based on named entities. Posts are processed using REL <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite>, with identified entities mapped to corresponding Wikipedia articles. The first two sentences of these articles are then appended to the original post. The combination is encoded using SBERT and given to the classifier, leaving posts without (discovered) named entities unchanged.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities",
                    "rel",
                    "strategy",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ConceptNet:</span> The third baseline augments posts using ConceptNet&#8217;s Numberbatch embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>, replicating the methodology outlined by <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>. Entities are extracted using n-gram matching before being mapped to ConceptNet embeddings. These vectors are then averaged and normalised, forming a single contextual representation which is concatenated with the SBERT embedding of the original text, resulting in a 1,068-dimensional input vector.</p>\n\n",
                "matched_terms": [
                    "text",
                    "conceptnet",
                    "entities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Prediction:</span> The final baseline system forgoes embeddings and training entirely, instead using the generative LLM directly as a classifier.\nThis setup aims to determine whether, if given access to an LLM like <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>, such a model is better utilised for generating context or directly for prediction. The LLM is prompted solely with each post and the potential labels and asked to predict binary, multi-class and multi-label classes.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>Full prompt texts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS5\" title=\"E. LLM Prompts for Context Generation and Incorporation &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "llm",
                    "context",
                    "prediction",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "named",
                    "full",
                    "entities",
                    "hsd",
                    "context",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "named",
                    "full",
                    "entities",
                    "hatred",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once context has been generated, we test four strategies to incorporate it into the model input, dubbed i) <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>, ii) <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>, iii) <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, and iv) <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "enhance",
                    "llm",
                    "context",
                    "append",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "text",
                    "append",
                    "context",
                    "embed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embed &amp; Concat</span> tests the inverse of the previous strategy, vectorising the post and context separately, then concatenating them. This preserves a greater separation between post and context, potentially aiding the classifier in distinguishing their respective contributions.</p>\n\n",
                "matched_terms": [
                    "context",
                    "strategy",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context-Embed</span> follows <cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> methodology, fusing post and context hierarchically at the embedding layer before encoding, leading to a deeper integration of context and post than simple concatenation at the input or embedding level. In this configuration, the context is encoded with SBERT, while the post is tokenized and passed through the word embedding matrix of another SBERT model. The context vector is projected to match the dimensionality of token embeddings and prepended to the post embedding sequence. This expanded sequence is then passed through the remaining layers of the encoder. The resulting representation is then pooled before being passed to the classifier.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context",
                    "hate",
                    "enhance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each experiment is run five times with averaged results reported. The primary evaluation metric used in this study is macro F1. We also report positive class F1 in the binary settings and per-class F1 for the multi-class and multi-label settings.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "positive",
                    "class",
                    "macro",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context",
                    "hatred",
                    "latent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities",
                    "rel",
                    "context",
                    "strategy",
                    "llm",
                    "prediction",
                    "zerocontext",
                    "binary",
                    "embed",
                    "concat",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "contextembed",
                    "full",
                    "entities",
                    "context",
                    "llm",
                    "prediction",
                    "append",
                    "binary",
                    "embed",
                    "concat",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the influence of incorporating LLM generated context on model behaviour, we compare variance between the predictions of the zero context model and the best-performing <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, rather than solely against the true labels.</p>\n\n",
                "matched_terms": [
                    "context",
                    "llm",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "hsd",
                    "positive",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, 187 non-hateful posts (4.4%) were incorrectly classified by the Embed &amp; Concat system but not by the zero-context one.\nPrompting the LLM to relate generated context to potential hate speech may lead to inventing hateful context where none exists, consequently leading to the over-estimation of non-hateful posts as hateful.\nThe post &#8220;<span class=\"ltx_text ltx_font_italic\">this is what becoming a groyper does to your life people, it is an irreconcilable decision</span>&#8221; is a warning of the damage that falling into right-wing conspiracist groups can do to someone, but is mistakenly classified as hateful once lengthy context about <span class=\"ltx_text ltx_font_italic\">groypers</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Hawley (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib19\" title=\"\">2021</a>)</cite> is added, associating them with hateful ideologies and leading the classifier to over-represent this inferred hate.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "llm",
                    "context",
                    "zerocontext",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">68 misogynous memes (3.1%) were classified correctly with no context but incorrectly by the Embed &amp; Concat system. These generally fall under the aforementioned issue of adding irrelevant or incorrect context that obfuscates previously clearer hate.\nIn the example in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F1\" title=\"Figure 1 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the generated context does not recognise Bill Cosby in the image and misses the implication of Drug-Facilitated Sexual Assault <cite class=\"ltx_cite ltx_citemacro_cite\">Butler and Welch (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib4\" title=\"\">2009</a>)</cite>, leading the model to misclassify it as not misogynous.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "context",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "context",
                    "embed",
                    "concat",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while our results demonstrates the promise of using LLM-generated context in HSD, our error analysis highlights some of its pitfalls. Most pressingly, we observe that additional context can cause semantic drift in the original post, occasionally neutralising hateful intent or inventing false hateful connections. This underscores the need for careful context selection and integration to maximise benefits and mitigate errors.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond demonstrating increased performance metrics, this research addresses important considerations regarding the role of background context in HSD.\nWhile full-text prompting outperformed named-entity-based approaches, both occasionally introduced spurious associations, impacting performance and suggesting that overcontextualisation, where generated background information dilutes or reinterprets the original intent, can be just as problematic as undercontextualisation.\nError analysis reinforces this view, with many false positives arising from benign posts being supplemented with highly charged or misleading contextual information, while false negatives often occurred when clearly hateful statements were augmented with neutralising or overly-explanatory context. These patterns reveal a trade-off between providing richer background information and maintaining original intent.\nFuture models may aim to strike the right balance through dynamic weighting or more sophisticated attention mechanisms to weigh the impacts of post and context.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, future work in this domain could explore a wider range of prompting strategies, in particular, utilising online-search and reasoning-capable LLMs to generate higher-quality background context.\nFurthermore, hybrid systems that combine generated context with verifiable domain knowledge may help to detect hateful references that even LLMs can overlook and guarantee more factual grounding <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. For instance, known extremist references could be identified by linking to curated hate-related knowledge bases, such as <cite class=\"ltx_cite ltx_citemacro_citet\">GPAHE (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib3\" title=\"\">2023</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Global Extremist Symbols Database</span> and integrating discovered information into the LLM prompt.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "hatred",
                    "hate",
                    "implicit",
                    "hsd",
                    "llm",
                    "context",
                    "zerocontext",
                    "binary",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research investigated the use of LLMs as dynamic knowledge bases to generate background context aimed at improving the detection of (implicit) hate speech across textual and multimodal domains. By comparing two prompting strategies for context generation and four distinct methods of integrating context into a classification pipeline, we systematically evaluated the impact of contextual enhancement on HSD performance.</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hsd",
                    "context",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing our research questions, our findings indicate, firstly, that LLM-based context generation can outperform static entity-linking-based approaches, as evidenced by consistent improvements across both binary and fine-grained classification tasks in textual and multimodal settings (RQ1).\nWhile named-entity-based strategies showed marginal gains, incorporating context generated via full-text prompting outperformed baselines in the vast majority of settings.\nResults show that both the quality of generated context and the method by which it is incorporated into model inputs are critical to achieving improvements.</p>\n\n",
                "matched_terms": [
                    "binary",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, we find that embedding-level concatenation (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) outperformed other incorporation methods, demonstrating the importance of maintaining separation between original content and additional background context (RQ2).\nIntegrating context directly into the original text through use of an LLM (<span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>) also beat baseline scores in the multimodal setting and lead to strong results when incorporating named-entity context in the textual setting, underscoring the future potential of this approach.</p>\n\n",
                "matched_terms": [
                    "enhance",
                    "llm",
                    "context",
                    "incorporation",
                    "embed",
                    "concat",
                    "text",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "strategy",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite promising results, this research is subject to limitations that should be considered when interpreting the findings or generalising them to broader settings.\nPrimarily, we relied on a single LLM (<span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>) and a fixed set of simple, natural-language prompts to generate background context. Alternative models, prompt formulations, or temperature settings could produce substantially different context and thus alter downstream performance.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our evaluation is limited to two English-language datasets collected over relatively brief time spans. This could limit generalisability to multilingual or non-English settings, as well as temporal generalisability, particularly as the language and references used in online posts and memes changes rapidly and dramatically over time (see <cite class=\"ltx_cite ltx_citemacro_citet\">Kostadinovska-Stojchevska and Shalevska (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib23\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Joshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib21\" title=\"\">2024</a>)</cite> for more on this).\nMoreover, human annotations, particularly for implicit hate, are inherently subjective, which may introduce bias into annotated datasets.\nOur error analysis revealed several instances that appeared mislabelled or open to multiple valid interpretations in the Latent Hatred dataset. These issues are discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS4\" title=\"D. Doubtful Labels in Latent Hatred &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "hate",
                    "hatred",
                    "implicit",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, reliance on macro F1 scores, though useful as an overall performance indicator, can obscure important class-specific trends and failure modes.\nFuture evaluations may benefit from more fine-grained analysis or the development of dedicated challenge datasets to better assess robustness to various phenomena, including the aforementioned linguistic development in memes.</p>\n\n",
                "matched_terms": [
                    "macro",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our replication of <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> methodology was unable to validate their results, instead demonstrating a slight drop in macro F1 score from the zero-context baseline for both binary and multi-class classification.\nOne major issue with Lin&#8217;s approach is REL itself, which struggles to identify entities that are not capitalised in the &#8220;proper&#8221; way &#8212; a substantial issue when working with informal social media data. Lin does not explicitly mention the number of entities discovered, but our reproduction finds that only 9,595 of 21,480 posts (44.67%) were linked to one or more entities.\nManual inspection of a random sample of 100 posts (containing 79 identified tags) found 28 extra named entities missed by REL in Lin&#8217;s approach, along with 18 incorrectly linked named entities.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "named",
                    "entities",
                    "rel",
                    "macro",
                    "zerocontext",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "named",
                    "entities",
                    "hatred",
                    "hate",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "latent",
                    "hate",
                    "hatred",
                    "class",
                    "dataset"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Macro & positive class (misogynous) F1 scores for binary HSD and macro F1 scores for multi-label classification on the MAMI dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Context</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Incorporation</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Strategy</span></span></span>\n</span> </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Binary</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">Multi-Label</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">Macro F1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">Hate F1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">Macro F1</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Zero-Context</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.79</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.79</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">REL</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.78</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.78</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.57</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ConceptNet</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.60</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Prediction</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.86</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.87</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.60</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" rowspan=\"4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Full Text</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.85</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.85</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.63</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Context-Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Enhance</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:68.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "full",
            "positive",
            "misogynous",
            "macro",
            "strategy",
            "embed",
            "concat",
            "enhance",
            "class",
            "mami",
            "context",
            "llm",
            "prediction",
            "append",
            "conceptnet",
            "text",
            "scores",
            "hsd",
            "rel",
            "incorporation",
            "classification",
            "binary",
            "contextembed",
            "hate",
            "zerocontext",
            "dataset",
            "multilabel"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "hsd",
                    "mami",
                    "misogynous",
                    "context",
                    "zerocontext",
                    "classification",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "hsd",
                    "zerocontext",
                    "context",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "hate",
                    "hsd",
                    "mami",
                    "misogynous",
                    "llm",
                    "context",
                    "classification",
                    "binary",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To what extent is a pre-trained LLM more effective than static entity linking at generating background context for implicit HSD?</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How can background context be effectively incorporated into a HSD model input?</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How useful are these generation and incorporation methods for textual and multimodal HSD across binary and multi-class settings?</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "hsd",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While hate speech is inherently contextual <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, previous studies have shown that incorporating generic additional context into the model input often leads to a negligible or negative impact on model performance <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>. These findings suggest that context must be carefully selected, represented, and integrated to avoid introducing noise. This section discusses previous work relevant to HSD in context retrieval and incorporation, the use of LLMs, and the challenges surrounding multimodality.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "hsd",
                    "context",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "hsd",
                    "context",
                    "incorporation",
                    "classification",
                    "binary",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "hate",
                    "hsd",
                    "rel",
                    "macro",
                    "context",
                    "prediction",
                    "binary",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs have been used in HSD previously, generally replacing classifiers themselves, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>)</cite>, who directly compare a generative LLM against two pre-trained models: BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib25\" title=\"\">2019</a>)</cite>. They test various prompting strategies (few-shot, chain-of-thought, etc.) and demonstrate that a generalist LLM with simple prompts can comfortably outperform transformer-based encoders. These results are not confirmed, however, by numerous other studies, who find that LLMs either match or fall short of the performance of encoder-only classifiers, especially without extensive prompt engineering <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Albladi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib1\" title=\"\">2025</a>)</cite> discuss these results, highlighting the fact that encoder models are fine-tuned on the training data and can potentially capture the peculiarities of a given dataset better than generalist LLMs.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "llm",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "hsd",
                    "context",
                    "classification",
                    "append",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "text",
                    "context",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "full",
                    "llm",
                    "context",
                    "incorporation",
                    "classification",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n",
                "matched_terms": [
                    "class",
                    "classification",
                    "dataset",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our second source is <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, used to detect misogynous memes. This dataset is comprised of 10,995 distinct memes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Five duplicate memes were discovered in MAMI during preprocessing and subsequently removed.</span></span></span> scraped from social media platforms and meme-creation websites.\nThe high level annotation is a simple binary distinction between misogynous<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>A meme is defined as misogynous if it &#8220;conceptually describes an offensive, sexist or hateful scene [&#8230;] having as target a woman or a group of women&#8221;.</span></span></span> and non-misogynous memes.\nIn the second level annotation, misogynous memes are labelled with one or more subtypes of misogyny: shaming, stereotyping, objectification, and violence. These labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T5\" title=\"Table 5 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, with example memes and their labels given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.\nWe again split the data in an 80/20 ratio.</p>\n\n",
                "matched_terms": [
                    "binary",
                    "dataset",
                    "mami",
                    "misogynous"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Context:</span> The first configuration uses only the raw post, simply creating SBERT embeddings from each post in the dataset and feeding them directly into the input of the MLP classification model.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "classification",
                    "zerocontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">REL:</span> The second baseline replicates <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> entity-linking strategy, augmenting post text with Wikipedia-derived summaries based on named entities. Posts are processed using REL <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite>, with identified entities mapped to corresponding Wikipedia articles. The first two sentences of these articles are then appended to the original post. The combination is encoded using SBERT and given to the classifier, leaving posts without (discovered) named entities unchanged.</p>\n\n",
                "matched_terms": [
                    "rel",
                    "text",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ConceptNet:</span> The third baseline augments posts using ConceptNet&#8217;s Numberbatch embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>, replicating the methodology outlined by <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>. Entities are extracted using n-gram matching before being mapped to ConceptNet embeddings. These vectors are then averaged and normalised, forming a single contextual representation which is concatenated with the SBERT embedding of the original text, resulting in a 1,068-dimensional input vector.</p>\n\n",
                "matched_terms": [
                    "conceptnet",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Prediction:</span> The final baseline system forgoes embeddings and training entirely, instead using the generative LLM directly as a classifier.\nThis setup aims to determine whether, if given access to an LLM like <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>, such a model is better utilised for generating context or directly for prediction. The LLM is prompted solely with each post and the potential labels and asked to predict binary, multi-class and multi-label classes.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>Full prompt texts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS5\" title=\"E. LLM Prompts for Context Generation and Incorporation &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "llm",
                    "context",
                    "prediction",
                    "binary",
                    "multilabel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "full",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "full",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once context has been generated, we test four strategies to incorporate it into the model input, dubbed i) <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>, ii) <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>, iii) <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, and iv) <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "enhance",
                    "llm",
                    "context",
                    "append",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "text",
                    "append",
                    "context",
                    "embed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embed &amp; Concat</span> tests the inverse of the previous strategy, vectorising the post and context separately, then concatenating them. This preserves a greater separation between post and context, potentially aiding the classifier in distinguishing their respective contributions.</p>\n\n",
                "matched_terms": [
                    "context",
                    "strategy",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context-Embed</span> follows <cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> methodology, fusing post and context hierarchically at the embedding layer before encoding, leading to a deeper integration of context and post than simple concatenation at the input or embedding level. In this configuration, the context is encoded with SBERT, while the post is tokenized and passed through the word embedding matrix of another SBERT model. The context vector is projected to match the dimensionality of token embeddings and prepended to the post embedding sequence. This expanded sequence is then passed through the remaining layers of the encoder. The resulting representation is then pooled before being passed to the classifier.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context",
                    "hate",
                    "enhance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "mami",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each experiment is run five times with averaged results reported. The primary evaluation metric used in this study is macro F1. We also report positive class F1 in the binary settings and per-class F1 for the multi-class and multi-label settings.</p>\n\n",
                "matched_terms": [
                    "positive",
                    "class",
                    "macro",
                    "binary",
                    "multilabel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "mami",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "macro",
                    "embed",
                    "concat",
                    "enhance",
                    "context",
                    "llm",
                    "prediction",
                    "conceptnet",
                    "text",
                    "scores",
                    "hsd",
                    "rel",
                    "incorporation",
                    "classification",
                    "binary",
                    "contextembed",
                    "hate",
                    "zerocontext",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "full",
                    "llm",
                    "context",
                    "prediction",
                    "classification",
                    "append",
                    "binary",
                    "embed",
                    "concat",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the influence of incorporating LLM generated context on model behaviour, we compare variance between the predictions of the zero context model and the best-performing <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, rather than solely against the true labels.</p>\n\n",
                "matched_terms": [
                    "context",
                    "llm",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "hsd",
                    "positive",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, 187 non-hateful posts (4.4%) were incorrectly classified by the Embed &amp; Concat system but not by the zero-context one.\nPrompting the LLM to relate generated context to potential hate speech may lead to inventing hateful context where none exists, consequently leading to the over-estimation of non-hateful posts as hateful.\nThe post &#8220;<span class=\"ltx_text ltx_font_italic\">this is what becoming a groyper does to your life people, it is an irreconcilable decision</span>&#8221; is a warning of the damage that falling into right-wing conspiracist groups can do to someone, but is mistakenly classified as hateful once lengthy context about <span class=\"ltx_text ltx_font_italic\">groypers</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Hawley (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib19\" title=\"\">2021</a>)</cite> is added, associating them with hateful ideologies and leading the classifier to over-represent this inferred hate.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "llm",
                    "context",
                    "zerocontext",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">68 misogynous memes (3.1%) were classified correctly with no context but incorrectly by the Embed &amp; Concat system. These generally fall under the aforementioned issue of adding irrelevant or incorrect context that obfuscates previously clearer hate.\nIn the example in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F1\" title=\"Figure 1 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the generated context does not recognise Bill Cosby in the image and misses the implication of Drug-Facilitated Sexual Assault <cite class=\"ltx_cite ltx_citemacro_cite\">Butler and Welch (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib4\" title=\"\">2009</a>)</cite>, leading the model to misclassify it as not misogynous.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "misogynous",
                    "context",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "context",
                    "embed",
                    "concat",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while our results demonstrates the promise of using LLM-generated context in HSD, our error analysis highlights some of its pitfalls. Most pressingly, we observe that additional context can cause semantic drift in the original post, occasionally neutralising hateful intent or inventing false hateful connections. This underscores the need for careful context selection and integration to maximise benefits and mitigate errors.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond demonstrating increased performance metrics, this research addresses important considerations regarding the role of background context in HSD.\nWhile full-text prompting outperformed named-entity-based approaches, both occasionally introduced spurious associations, impacting performance and suggesting that overcontextualisation, where generated background information dilutes or reinterprets the original intent, can be just as problematic as undercontextualisation.\nError analysis reinforces this view, with many false positives arising from benign posts being supplemented with highly charged or misleading contextual information, while false negatives often occurred when clearly hateful statements were augmented with neutralising or overly-explanatory context. These patterns reveal a trade-off between providing richer background information and maintaining original intent.\nFuture models may aim to strike the right balance through dynamic weighting or more sophisticated attention mechanisms to weigh the impacts of post and context.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, future work in this domain could explore a wider range of prompting strategies, in particular, utilising online-search and reasoning-capable LLMs to generate higher-quality background context.\nFurthermore, hybrid systems that combine generated context with verifiable domain knowledge may help to detect hateful references that even LLMs can overlook and guarantee more factual grounding <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. For instance, known extremist references could be identified by linking to curated hate-related knowledge bases, such as <cite class=\"ltx_cite ltx_citemacro_citet\">GPAHE (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib3\" title=\"\">2023</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Global Extremist Symbols Database</span> and integrating discovered information into the LLM prompt.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "hsd",
                    "llm",
                    "context",
                    "zerocontext",
                    "classification",
                    "binary",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research investigated the use of LLMs as dynamic knowledge bases to generate background context aimed at improving the detection of (implicit) hate speech across textual and multimodal domains. By comparing two prompting strategies for context generation and four distinct methods of integrating context into a classification pipeline, we systematically evaluated the impact of contextual enhancement on HSD performance.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "classification",
                    "context",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing our research questions, our findings indicate, firstly, that LLM-based context generation can outperform static entity-linking-based approaches, as evidenced by consistent improvements across both binary and fine-grained classification tasks in textual and multimodal settings (RQ1).\nWhile named-entity-based strategies showed marginal gains, incorporating context generated via full-text prompting outperformed baselines in the vast majority of settings.\nResults show that both the quality of generated context and the method by which it is incorporated into model inputs are critical to achieving improvements.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "binary",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, we find that embedding-level concatenation (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) outperformed other incorporation methods, demonstrating the importance of maintaining separation between original content and additional background context (RQ2).\nIntegrating context directly into the original text through use of an LLM (<span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>) also beat baseline scores in the multimodal setting and lead to strong results when incorporating named-entity context in the textual setting, underscoring the future potential of this approach.</p>\n\n",
                "matched_terms": [
                    "enhance",
                    "llm",
                    "context",
                    "incorporation",
                    "embed",
                    "concat",
                    "text",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "strategy",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite promising results, this research is subject to limitations that should be considered when interpreting the findings or generalising them to broader settings.\nPrimarily, we relied on a single LLM (<span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>) and a fixed set of simple, natural-language prompts to generate background context. Alternative models, prompt formulations, or temperature settings could produce substantially different context and thus alter downstream performance.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our evaluation is limited to two English-language datasets collected over relatively brief time spans. This could limit generalisability to multilingual or non-English settings, as well as temporal generalisability, particularly as the language and references used in online posts and memes changes rapidly and dramatically over time (see <cite class=\"ltx_cite ltx_citemacro_citet\">Kostadinovska-Stojchevska and Shalevska (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib23\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Joshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib21\" title=\"\">2024</a>)</cite> for more on this).\nMoreover, human annotations, particularly for implicit hate, are inherently subjective, which may introduce bias into annotated datasets.\nOur error analysis revealed several instances that appeared mislabelled or open to multiple valid interpretations in the Latent Hatred dataset. These issues are discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS4\" title=\"D. Doubtful Labels in Latent Hatred &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, reliance on macro F1 scores, though useful as an overall performance indicator, can obscure important class-specific trends and failure modes.\nFuture evaluations may benefit from more fine-grained analysis or the development of dedicated challenge datasets to better assess robustness to various phenomena, including the aforementioned linguistic development in memes.</p>\n\n",
                "matched_terms": [
                    "macro",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our replication of <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> methodology was unable to validate their results, instead demonstrating a slight drop in macro F1 score from the zero-context baseline for both binary and multi-class classification.\nOne major issue with Lin&#8217;s approach is REL itself, which struggles to identify entities that are not capitalised in the &#8220;proper&#8221; way &#8212; a substantial issue when working with informal social media data. Lin does not explicitly mention the number of entities discovered, but our reproduction finds that only 9,595 of 21,480 posts (44.67%) were linked to one or more entities.\nManual inspection of a random sample of 100 posts (containing 79 identified tags) found 28 extra named entities missed by REL in Lin&#8217;s approach, along with 18 incorrectly linked named entities.</p>\n\n",
                "matched_terms": [
                    "rel",
                    "zerocontext",
                    "macro",
                    "classification",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "hate",
                    "mami",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "class",
                    "dataset",
                    "hate"
                ]
            }
        ]
    },
    "S5.F1": {
        "caption": "Figure 1: Example false negative meme alongside its LLM-generated context.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:212.5pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"376\" id=\"S5.F1.g1\" src=\"images/4970.jpg\" width=\"538\"/>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:203.8pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\">&#8220;The meme features a picture of Carl Winslow and his wife Harriette from the sitcom <span class=\"ltx_text ltx_font_italic\">Family Matters</span>. The text suggests a scenario where someone is initially happy or carefree (\"laughing\") but will face negative consequences or regret after consuming the drink. It is likely used to humorously warn against overindulgence or actions that seem fun at the moment but will lead to a negative outcome.&#8221;</span></span>\n</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "consuming",
            "seem",
            "fun",
            "the",
            "laughing",
            "false",
            "sitcom",
            "example",
            "matters",
            "actions",
            "meme",
            "winslow",
            "likely",
            "overindulgence",
            "its",
            "picture",
            "against",
            "context",
            "suggests",
            "his",
            "someone",
            "face",
            "after",
            "from",
            "lead",
            "consequences",
            "used",
            "harriette",
            "text",
            "wife",
            "outcome",
            "drink",
            "carl",
            "scenario",
            "moment",
            "happy",
            "alongside",
            "llmgenerated",
            "carefree",
            "warn",
            "where",
            "negative",
            "humorously",
            "features",
            "initially",
            "will",
            "regret",
            "family"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">68 misogynous memes (3.1%) were classified correctly with no context but incorrectly by the Embed &amp; Concat system. These generally fall under the aforementioned issue of adding irrelevant or incorrect context that obfuscates previously clearer hate.\nIn the example in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F1\" title=\"Figure 1 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the generated context does not recognise Bill Cosby in the image and misses the implication of Drug-Facilitated Sexual Assault <cite class=\"ltx_cite ltx_citemacro_cite\">Butler and Welch (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib4\" title=\"\">2009</a>)</cite>, leading the model to misclassify it as not misogynous.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "context",
                    "from",
                    "used",
                    "text",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While hate speech is inherently contextual <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, previous studies have shown that incorporating generic additional context into the model input often leads to a negligible or negative impact on model performance <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>. These findings suggest that context must be carefully selected, represented, and integrated to avoid introducing noise. This section discusses previous work relevant to HSD in context retrieval and incorporation, the use of LLMs, and the challenges surrounding multimodality.</p>\n\n",
                "matched_terms": [
                    "context",
                    "negative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "used",
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs have been used in HSD previously, generally replacing classifiers themselves, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>)</cite>, who directly compare a generative LLM against two pre-trained models: BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib25\" title=\"\">2019</a>)</cite>. They test various prompting strategies (few-shot, chain-of-thought, etc.) and demonstrate that a generalist LLM with simple prompts can comfortably outperform transformer-based encoders. These results are not confirmed, however, by numerous other studies, who find that LLMs either match or fall short of the performance of encoder-only classifiers, especially without extensive prompt engineering <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Albladi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib1\" title=\"\">2025</a>)</cite> discuss these results, highlighting the fact that encoder models are fine-tuned on the training data and can potentially capture the peculiarities of a given dataset better than generalist LLMs.</p>\n\n",
                "matched_terms": [
                    "against",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While replacing traditional classifiers entirely is a promising approach, this research instead focuses on treating LLMs as dynamic knowledge bases from which to extract contextual information. <cite class=\"ltx_cite ltx_citemacro_citet\">Petroni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib32\" title=\"\">2019</a>)</cite> were among the first to systematically evaluate whether generative models can store factual and relational knowledge. Using cloze-style prompts, they demonstrate that pre-trained models can recall factual information with high accuracy, often outperforming traditional relation extraction systems.\nBuilding on this idea, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib41\" title=\"\">2020</a>)</cite> propose a method for building structured knowledge graphs directly from LLMs, suggesting that larger models can be used to effectively store and retrieve world knowledge.</p>\n\n",
                "matched_terms": [
                    "used",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "seem",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our second source is <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, used to detect misogynous memes. This dataset is comprised of 10,995 distinct memes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Five duplicate memes were discovered in MAMI during preprocessing and subsequently removed.</span></span></span> scraped from social media platforms and meme-creation websites.\nThe high level annotation is a simple binary distinction between misogynous<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>A meme is defined as misogynous if it &#8220;conceptually describes an offensive, sexist or hateful scene [&#8230;] having as target a woman or a group of women&#8221;.</span></span></span> and non-misogynous memes.\nIn the second level annotation, misogynous memes are labelled with one or more subtypes of misogyny: shaming, stereotyping, objectification, and violence. These labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T5\" title=\"Table 5 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, with example memes and their labels given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.\nWe again split the data in an 80/20 ratio.</p>\n\n",
                "matched_terms": [
                    "meme",
                    "used",
                    "from",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the named-entity-based context generation approach, we identify named entities using the uncased <span class=\"ltx_text ltx_font_typewriter\">dslim/bert-base-NER</span> model <cite class=\"ltx_cite ltx_citemacro_cite\">Lim (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib4\" title=\"\">2021</a>)</cite>, a fine-tuned BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> system trained on Named Entity Recognition (NER), chosen based on its high performance on the CONLL-2003 benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">Tjong Kim&#160;Sang and De&#160;Meulder (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib38\" title=\"\">2003</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ Google&#8217;s <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> <cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib2\" title=\"\">2024</a>)</cite> model for all experiments; a decision driven primarily by its speed and cost-efficiency.\nThis model can be prompted through a useful batch API, allowing for efficient processing of many requests in parallel. This was considered essential, given the need to generate context for thousands of instances, often multiple times to experiment with consistency.\nWhile comparable LLMs, such as <cite class=\"ltx_cite ltx_citemacro_citet\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib5\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span> or <cite class=\"ltx_cite ltx_citemacro_citet\">Anthropic (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib1\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">Claude Sonnet 3.7</span>, may offer similar capabilities, Gemini&#8217;s high score-to-cost ratio, highlighted by LMArena&#8217;s benchmark evaluations <cite class=\"ltx_cite ltx_citemacro_cite\">Chiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib5\" title=\"\">2024</a>)</cite>, made it an attractive choice.</p>\n\n",
                "matched_terms": [
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "llmgenerated",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "used",
                    "from",
                    "context",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "against",
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "context",
                    "llmgenerated",
                    "from",
                    "meme",
                    "used",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "against",
                    "llmgenerated",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "text",
                    "likely",
                    "lead",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "against",
                    "context",
                    "llmgenerated",
                    "from",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "text",
                    "llmgenerated",
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the influence of incorporating LLM generated context on model behaviour, we compare variance between the predictions of the zero context model and the best-performing <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, rather than solely against the true labels.</p>\n\n",
                "matched_terms": [
                    "against",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "false",
                    "example",
                    "negative",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, 187 non-hateful posts (4.4%) were incorrectly classified by the Embed &amp; Concat system but not by the zero-context one.\nPrompting the LLM to relate generated context to potential hate speech may lead to inventing hateful context where none exists, consequently leading to the over-estimation of non-hateful posts as hateful.\nThe post &#8220;<span class=\"ltx_text ltx_font_italic\">this is what becoming a groyper does to your life people, it is an irreconcilable decision</span>&#8221; is a warning of the damage that falling into right-wing conspiracist groups can do to someone, but is mistakenly classified as hateful once lengthy context about <span class=\"ltx_text ltx_font_italic\">groypers</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Hawley (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib19\" title=\"\">2021</a>)</cite> is added, associating them with hateful ideologies and leading the classifier to over-represent this inferred hate.</p>\n\n",
                "matched_terms": [
                    "where",
                    "context",
                    "lead",
                    "someone"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n",
                "matched_terms": [
                    "false",
                    "context",
                    "from",
                    "meme",
                    "used",
                    "negative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while our results demonstrates the promise of using LLM-generated context in HSD, our error analysis highlights some of its pitfalls. Most pressingly, we observe that additional context can cause semantic drift in the original post, occasionally neutralising hateful intent or inventing false hateful connections. This underscores the need for careful context selection and integration to maximise benefits and mitigate errors.</p>\n\n",
                "matched_terms": [
                    "false",
                    "llmgenerated",
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond demonstrating increased performance metrics, this research addresses important considerations regarding the role of background context in HSD.\nWhile full-text prompting outperformed named-entity-based approaches, both occasionally introduced spurious associations, impacting performance and suggesting that overcontextualisation, where generated background information dilutes or reinterprets the original intent, can be just as problematic as undercontextualisation.\nError analysis reinforces this view, with many false positives arising from benign posts being supplemented with highly charged or misleading contextual information, while false negatives often occurred when clearly hateful statements were augmented with neutralising or overly-explanatory context. These patterns reveal a trade-off between providing richer background information and maintaining original intent.\nFuture models may aim to strike the right balance through dynamic weighting or more sophisticated attention mechanisms to weigh the impacts of post and context.</p>\n\n",
                "matched_terms": [
                    "where",
                    "false",
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "llmgenerated",
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, we find that embedding-level concatenation (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) outperformed other incorporation methods, demonstrating the importance of maintaining separation between original content and additional background context (RQ2).\nIntegrating context directly into the original text through use of an LLM (<span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>) also beat baseline scores in the multimodal setting and lead to strong results when incorporating named-entity context in the textual setting, underscoring the future potential of this approach.</p>\n\n",
                "matched_terms": [
                    "text",
                    "context",
                    "lead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "llmgenerated",
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "false",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "context",
                    "from",
                    "meme",
                    "used",
                    "text",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "meme",
                    "text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "will",
                    "where"
                ]
            }
        ]
    },
    "S5.F2": {
        "caption": "Figure 2: Example false positive meme alongside its LLM-generated context.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:242.8pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"596\" id=\"S5.F2.g1\" src=\"images/8206.jpg\" width=\"598\"/>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:164.8pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:70%;\">&#8220;The meme is a four-panel image featuring historical artwork. Each panel displays a different image of a woman, accompanied by a caption describing an action or state of being that purportedly empowers women. It is likely designed to express the idea that feminism supports a variety of choices for women, as their empowerment may come from diverse actions, behaviours, and lifestyles.&#8221;</span></span>\n</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "positive",
            "the",
            "false",
            "their",
            "come",
            "example",
            "feminism",
            "each",
            "actions",
            "purportedly",
            "meme",
            "likely",
            "accompanied",
            "featuring",
            "panel",
            "caption",
            "its",
            "being",
            "lifestyles",
            "context",
            "from",
            "displays",
            "supports",
            "image",
            "action",
            "empowers",
            "behaviours",
            "alongside",
            "designed",
            "variety",
            "llmgenerated",
            "choices",
            "empowerment",
            "historical",
            "describing",
            "state",
            "women",
            "express",
            "fourpanel",
            "woman",
            "idea",
            "artwork",
            "diverse",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "different",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "their",
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While replacing traditional classifiers entirely is a promising approach, this research instead focuses on treating LLMs as dynamic knowledge bases from which to extract contextual information. <cite class=\"ltx_cite ltx_citemacro_citet\">Petroni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib32\" title=\"\">2019</a>)</cite> were among the first to systematically evaluate whether generative models can store factual and relational knowledge. Using cloze-style prompts, they demonstrate that pre-trained models can recall factual information with high accuracy, often outperforming traditional relation extraction systems.\nBuilding on this idea, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib41\" title=\"\">2020</a>)</cite> propose a method for building structured knowledge graphs directly from LLMs, suggesting that larger models can be used to effectively store and retrieve world knowledge.</p>\n\n",
                "matched_terms": [
                    "from",
                    "idea"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "each",
                    "context",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "image",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "their",
                    "image",
                    "different",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our second source is <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, used to detect misogynous memes. This dataset is comprised of 10,995 distinct memes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Five duplicate memes were discovered in MAMI during preprocessing and subsequently removed.</span></span></span> scraped from social media platforms and meme-creation websites.\nThe high level annotation is a simple binary distinction between misogynous<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>A meme is defined as misogynous if it &#8220;conceptually describes an offensive, sexist or hateful scene [&#8230;] having as target a woman or a group of women&#8221;.</span></span></span> and non-misogynous memes.\nIn the second level annotation, misogynous memes are labelled with one or more subtypes of misogyny: shaming, stereotyping, objectification, and violence. These labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T5\" title=\"Table 5 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, with example memes and their labels given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.\nWe again split the data in an 80/20 ratio.</p>\n\n",
                "matched_terms": [
                    "their",
                    "woman",
                    "example",
                    "from",
                    "meme"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the named-entity-based context generation approach, we identify named entities using the uncased <span class=\"ltx_text ltx_font_typewriter\">dslim/bert-base-NER</span> model <cite class=\"ltx_cite ltx_citemacro_cite\">Lim (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib4\" title=\"\">2021</a>)</cite>, a fine-tuned BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> system trained on Named Entity Recognition (NER), chosen based on its high performance on the CONLL-2003 benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">Tjong Kim&#160;Sang and De&#160;Meulder (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib38\" title=\"\">2003</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ Google&#8217;s <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> <cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib2\" title=\"\">2024</a>)</cite> model for all experiments; a decision driven primarily by its speed and cost-efficiency.\nThis model can be prompted through a useful batch API, allowing for efficient processing of many requests in parallel. This was considered essential, given the need to generate context for thousands of instances, often multiple times to experiment with consistency.\nWhile comparable LLMs, such as <cite class=\"ltx_cite ltx_citemacro_citet\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib5\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span> or <cite class=\"ltx_cite ltx_citemacro_citet\">Anthropic (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib1\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">Claude Sonnet 3.7</span>, may offer similar capabilities, Gemini&#8217;s high score-to-cost ratio, highlighted by LMArena&#8217;s benchmark evaluations <cite class=\"ltx_cite ltx_citemacro_cite\">Chiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib5\" title=\"\">2024</a>)</cite>, made it an attractive choice.</p>\n\n",
                "matched_terms": [
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Context:</span> The first configuration uses only the raw post, simply creating SBERT embeddings from each post in the dataset and feeding them directly into the input of the MLP classification model.</p>\n\n",
                "matched_terms": [
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Prediction:</span> The final baseline system forgoes embeddings and training entirely, instead using the generative LLM directly as a classifier.\nThis setup aims to determine whether, if given access to an LLM like <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>, such a model is better utilised for generating context or directly for prediction. The LLM is prompted solely with each post and the potential labels and asked to predict binary, multi-class and multi-label classes.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>Full prompt texts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS5\" title=\"E. LLM Prompts for Context Generation and Incorporation &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "each",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "llmgenerated",
                    "different",
                    "context",
                    "designed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "each",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "their",
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embed &amp; Concat</span> tests the inverse of the previous strategy, vectorising the post and context separately, then concatenating them. This preserves a greater separation between post and context, potentially aiding the classifier in distinguishing their respective contributions.</p>\n\n",
                "matched_terms": [
                    "their",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context-Embed</span> follows <cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> methodology, fusing post and context hierarchically at the embedding layer before encoding, leading to a deeper integration of context and post than simple concatenation at the input or embedding level. In this configuration, the context is encoded with SBERT, while the post is tokenized and passed through the word embedding matrix of another SBERT model. The context vector is projected to match the dimensionality of token embeddings and prepended to the post embedding sequence. This expanded sequence is then passed through the remaining layers of the encoder. The resulting representation is then pooled before being passed to the classifier.</p>\n\n",
                "matched_terms": [
                    "being",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "context",
                    "llmgenerated",
                    "each",
                    "from",
                    "meme",
                    "image"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each experiment is run five times with averaged results reported. The primary evaluation metric used in this study is macro F1. We also report positive class F1 in the binary settings and per-class F1 for the multi-class and multi-label settings.</p>\n\n",
                "matched_terms": [
                    "positive",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "llmgenerated",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "their",
                    "likely",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "llmgenerated",
                    "from",
                    "likely",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "llmgenerated",
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "false",
                    "positive",
                    "example",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">68 misogynous memes (3.1%) were classified correctly with no context but incorrectly by the Embed &amp; Concat system. These generally fall under the aforementioned issue of adding irrelevant or incorrect context that obfuscates previously clearer hate.\nIn the example in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F1\" title=\"Figure 1 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the generated context does not recognise Bill Cosby in the image and misses the implication of Drug-Facilitated Sexual Assault <cite class=\"ltx_cite ltx_citemacro_cite\">Butler and Welch (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib4\" title=\"\">2009</a>)</cite>, leading the model to misclassify it as not misogynous.</p>\n\n",
                "matched_terms": [
                    "image",
                    "example",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while our results demonstrates the promise of using LLM-generated context in HSD, our error analysis highlights some of its pitfalls. Most pressingly, we observe that additional context can cause semantic drift in the original post, occasionally neutralising hateful intent or inventing false hateful connections. This underscores the need for careful context selection and integration to maximise benefits and mitigate errors.</p>\n\n",
                "matched_terms": [
                    "false",
                    "llmgenerated",
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond demonstrating increased performance metrics, this research addresses important considerations regarding the role of background context in HSD.\nWhile full-text prompting outperformed named-entity-based approaches, both occasionally introduced spurious associations, impacting performance and suggesting that overcontextualisation, where generated background information dilutes or reinterprets the original intent, can be just as problematic as undercontextualisation.\nError analysis reinforces this view, with many false positives arising from benign posts being supplemented with highly charged or misleading contextual information, while false negatives often occurred when clearly hateful statements were augmented with neutralising or overly-explanatory context. These patterns reveal a trade-off between providing richer background information and maintaining original intent.\nFuture models may aim to strike the right balance through dynamic weighting or more sophisticated attention mechanisms to weigh the impacts of post and context.</p>\n\n",
                "matched_terms": [
                    "false",
                    "being",
                    "context",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "llmgenerated",
                    "from",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "variety",
                    "llmgenerated",
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite promising results, this research is subject to limitations that should be considered when interpreting the findings or generalising them to broader settings.\nPrimarily, we relied on a single LLM (<span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>) and a fixed set of simple, natural-language prompts to generate background context. Alternative models, prompt formulations, or temperature settings could produce substantially different context and thus alter downstream performance.</p>\n\n",
                "matched_terms": [
                    "different",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As argued by <cite class=\"ltx_cite ltx_citemacro_citet\">Wong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib43\" title=\"\">2024</a>)</cite>, technical HSD research has advanced rapidly but studies often underemphasise ethical and societal implications, leading to a lack of real-world impact.\nWhile automatic HSD systems are undoubtedly valuable for identifying and combating hateful content online, their deployment raises substantial ethical concerns that must be addressed alongside technical development.</p>\n\n",
                "matched_terms": [
                    "their",
                    "alongside"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Firstly and most importantly, HSD systems, if implemented inadequately, can limit freedom of expression, a fundamental right <cite class=\"ltx_cite ltx_citemacro_cite\">Council of Europe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nWithout frequent assessment and transparent appeal mechanisms, there is a risk of these models being weaponised to suppress lawful content&#8212;either inadvertently through biased data and imprecise definitions, or deliberately by actors seeking to limit public discourse on certain topics <cite class=\"ltx_cite ltx_citemacro_cite\">Funk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib14\" title=\"\">2023</a>)</cite>.\nThe ease with which a seemingly robust HSD system can be adapted to identify and suppress any kind of <span class=\"ltx_text ltx_font_italic\">undesirable</span> content, merely by retraining it with different labels, should not be underestimated.</p>\n\n",
                "matched_terms": [
                    "different",
                    "being"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "false",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our replication of <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> methodology was unable to validate their results, instead demonstrating a slight drop in macro F1 score from the zero-context baseline for both binary and multi-class classification.\nOne major issue with Lin&#8217;s approach is REL itself, which struggles to identify entities that are not capitalised in the &#8220;proper&#8221; way &#8212; a substantial issue when working with informal social media data. Lin does not explicitly mention the number of entities discovered, but our reproduction finds that only 9,595 of 21,480 posts (44.67%) were linked to one or more entities.\nManual inspection of a random sample of 100 posts (containing 79 identified tags) found 28 extra named entities missed by REL in Lin&#8217;s approach, along with 18 incorrectly linked named entities.</p>\n\n",
                "matched_terms": [
                    "their",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "context",
                    "from",
                    "meme",
                    "image",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "meme",
                    "image",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "their",
                    "women",
                    "each"
                ]
            }
        ]
    },
    "Sx6.T3": {
        "caption": "Table 3: System, request, and input(s) for HSD-related prompts.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Request</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Input(s)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system that extracts text from memes.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Extract and return only the text directly from this image. Ignore any watermarks. Respond with just the extracted text.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">Meme (JPEG)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system that generates descriptions of images.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">In one sentence, describe who or what is in this image and what they are doing. Ignore all text.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">Meme (JPEG)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system that finds and adds background context to tweets, in order to detect whether they are likely to contain hate speech.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">What is the likely context of the following tweet?</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">Post</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system that finds and adds context to tweets, in order to detect whether they are likely to contain hate speech.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Below is a list of Named Entities (and their tags) extracted from a tweet. Provide background context about these entities.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent\" id=\"Sx6.T3.p1\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_top\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">List of</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Entities</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/></span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system that finds and adds background context to memes, in order to detect whether they are likely to be misogynous.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">What is the likely context of this meme?</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">Meme (JPEG)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system that finds and adds context to tweets, in order to detect whether they are likely to contain hate speech.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Given the following tweet and some context about its named entities, incorporate this context back into the original tweet, so that the whole string can be passed to a hate speech detection model. Keep the original structure and intent intact while embedding additional information. Respond with only the newly-generated tweet.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent\" id=\"Sx6.T3.p2\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_top\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Post,</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Context</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/></span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system that finds and adds context to tweets, in order to detect whether they are likely to contain hate speech.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Given the following tweet and its context, incorporate this context back into the original tweet, so that the whole string can be passed to a hate speech detection model. Keep the tweet&#8217;s original structure and intent intact while embedding additional information. Respond with only the newly-generated tweet.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent\" id=\"Sx6.T3.p3\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_top\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Post,</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Context</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/></span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system that finds and adds context to memes, in order to detect whether they are likely to contain misogynous speech.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">The following text has been extracted directly from a meme using OCR, and also includes a brief description of the meme&#8217;s image and some background context. Incorporate these into one unified text that represents the meme and its context, so that the whole string can be passed to a misogyny detection model. Keep some of the meme text&#8217;s original structure and intent intact while embedding additional information. Respond with only the newly-generated textual meme representation.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_inline-logical-block ltx_align_top\">\n<span class=\"ltx_para ltx_noindent\" id=\"Sx6.T3.p4\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_top\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Extracted</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Text,</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Image</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Description,</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Context</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/></span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system that detects hate speech in tweets.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Is the following tweet considered hate speech? Respond with one word: &#8217;yes&#8217; or &#8217;no&#8217;.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">Post</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system that classifies implicitly hateful tweets into the following classes. You respond only with the name of one class.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Which of the following classes does this tweet best fit into? (&#8217;White Grievance&#8217;, &#8217;Incitement&#8217;, &#8217;Stereotypical&#8217;, &#8217;Inferiority&#8217;, &#8217;Irony&#8217;, Threatening&#8217;, &#8217;Other&#8217;).</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">Post</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system that detects misogyny in memes.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Is this meme misogynous? Respond only with one word: &#8217;yes&#8217; or &#8217;no&#8217;.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">Meme (JPEG)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:142.3pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">You are a system performs multi-label classification of misogynous memes. You respond only with the names of all suitable classes.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:227.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Which of the following classes does this misogynous meme fit into? (&#8217;Shaming&#8217;, &#8217;Stereotype&#8217;, &#8217;Objectification&#8217;, &#8217;Violence&#8217;). If it is not misogynous, respond &#8217;None&#8217;.</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">Meme (JPEG)</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "does",
            "white",
            "prompts",
            "their",
            "unified",
            "detection",
            "yes",
            "hateful",
            "meme",
            "textual",
            "considered",
            "who",
            "likely",
            "information",
            "list",
            "also",
            "from",
            "inferiority",
            "jpeg",
            "back",
            "contain",
            "represents",
            "stereotype",
            "no",
            "intact",
            "intent",
            "names",
            "only",
            "images",
            "representation",
            "given",
            "passed",
            "other",
            "whether",
            "shaming",
            "they",
            "watermarks",
            "embedding",
            "memes",
            "you",
            "misogynous",
            "brief",
            "describe",
            "whole",
            "not",
            "entities",
            "its",
            "fit",
            "provide",
            "classes",
            "request",
            "background",
            "generates",
            "context",
            "classifies",
            "system",
            "descriptions",
            "image",
            "text",
            "memes",
            "sentence",
            "about",
            "tweet",
            "tags",
            "ocr",
            "tweets",
            "objectification",
            "detect",
            "some",
            "additional",
            "description",
            "incitement",
            "irony",
            "order",
            "respond",
            "tweets",
            "word",
            "what",
            "which",
            "any",
            "detects",
            "directly",
            "newlygenerated",
            "stereotypical",
            "return",
            "into",
            "structure",
            "suitable",
            "extracts",
            "ignore",
            "misogyny",
            "doing",
            "classification",
            "extract",
            "finds",
            "speech",
            "below",
            "keep",
            "name",
            "named",
            "implicitly",
            "hate",
            "adds",
            "model",
            "all",
            "grievance",
            "multilabel",
            "incorporate",
            "hsdrelated",
            "while",
            "original",
            "texts",
            "inputs",
            "extracted",
            "performs",
            "just",
            "has",
            "none",
            "class",
            "threatening",
            "violence",
            "following",
            "includes",
            "one",
            "post",
            "best",
            "string"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "incorporate",
                    "embedding",
                    "misogynous",
                    "which",
                    "detection",
                    "textual",
                    "information",
                    "entities",
                    "context",
                    "background",
                    "from",
                    "system",
                    "text",
                    "memes",
                    "into",
                    "classification",
                    "one",
                    "speech",
                    "named",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Leveraging LLMs for Context-Aware Implicit\n<br class=\"ltx_break\"/>Textual and Multimodal Hate Speech Detection</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "textual",
                    "hate",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "incorporate",
                    "while",
                    "detection",
                    "hateful",
                    "has",
                    "not",
                    "any",
                    "context",
                    "background",
                    "from",
                    "text",
                    "into",
                    "structure",
                    "intent",
                    "one",
                    "speech",
                    "implicitly",
                    "post",
                    "detect",
                    "hate",
                    "additional",
                    "representation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "incorporate",
                    "while",
                    "into",
                    "hate",
                    "following",
                    "misogynous",
                    "context",
                    "background",
                    "which",
                    "misogyny",
                    "model",
                    "classification",
                    "whether",
                    "extract",
                    "speech",
                    "contain",
                    "about",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To what extent is a pre-trained LLM more effective than static entity linking at generating background context for implicit HSD?</p>\n\n",
                "matched_terms": [
                    "context",
                    "background",
                    "what"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How can background context be effectively incorporated into a HSD model input?</p>\n\n",
                "matched_terms": [
                    "model",
                    "into",
                    "background",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While hate speech is inherently contextual <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, previous studies have shown that incorporating generic additional context into the model input often leads to a negligible or negative impact on model performance <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>. These findings suggest that context must be carefully selected, represented, and integrated to avoid introducing noise. This section discusses previous work relevant to HSD in context retrieval and incorporation, the use of LLMs, and the challenges surrounding multimodality.</p>\n\n",
                "matched_terms": [
                    "while",
                    "into",
                    "hate",
                    "additional",
                    "model",
                    "context",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "its",
                    "post",
                    "hate",
                    "provide",
                    "context",
                    "classification",
                    "speech",
                    "considered"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "incorporate",
                    "named",
                    "tweets",
                    "entities",
                    "hate",
                    "some",
                    "also",
                    "their",
                    "context",
                    "background",
                    "extracted",
                    "from",
                    "extract",
                    "descriptions",
                    "about"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs have been used in HSD previously, generally replacing classifiers themselves, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>)</cite>, who directly compare a generative LLM against two pre-trained models: BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib25\" title=\"\">2019</a>)</cite>. They test various prompting strategies (few-shot, chain-of-thought, etc.) and demonstrate that a generalist LLM with simple prompts can comfortably outperform transformer-based encoders. These results are not confirmed, however, by numerous other studies, who find that LLMs either match or fall short of the performance of encoder-only classifiers, especially without extensive prompt engineering <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Albladi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib1\" title=\"\">2025</a>)</cite> discuss these results, highlighting the fact that encoder models are fine-tuned on the training data and can potentially capture the peculiarities of a given dataset better than generalist LLMs.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "given",
                    "directly",
                    "not",
                    "who",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While replacing traditional classifiers entirely is a promising approach, this research instead focuses on treating LLMs as dynamic knowledge bases from which to extract contextual information. <cite class=\"ltx_cite ltx_citemacro_citet\">Petroni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib32\" title=\"\">2019</a>)</cite> were among the first to systematically evaluate whether generative models can store factual and relational knowledge. Using cloze-style prompts, they demonstrate that pre-trained models can recall factual information with high accuracy, often outperforming traditional relation extraction systems.\nBuilding on this idea, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib41\" title=\"\">2020</a>)</cite> propose a method for building structured knowledge graphs directly from LLMs, suggesting that larger models can be used to effectively store and retrieve world knowledge.</p>\n\n",
                "matched_terms": [
                    "while",
                    "prompts",
                    "which",
                    "directly",
                    "from",
                    "whether",
                    "extract",
                    "information",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "incorporate",
                    "embedding",
                    "does",
                    "tweets",
                    "word",
                    "unified",
                    "who",
                    "context",
                    "from",
                    "system",
                    "text",
                    "into",
                    "classification",
                    "one",
                    "post",
                    "representation",
                    "model",
                    "all",
                    "whether"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "implicitly",
                    "memes",
                    "post",
                    "hate",
                    "which",
                    "context",
                    "background",
                    "intent",
                    "hateful",
                    "speech",
                    "just",
                    "textual",
                    "image",
                    "text",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "incorporate",
                    "named",
                    "entities",
                    "sentence",
                    "into",
                    "their",
                    "representation",
                    "context",
                    "background",
                    "passed",
                    "classification",
                    "all",
                    "one",
                    "textual",
                    "image",
                    "text",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n",
                "matched_terms": [
                    "tweets",
                    "while",
                    "into",
                    "hate",
                    "class",
                    "classification",
                    "from",
                    "speech",
                    "textual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our second source is <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, used to detect misogynous memes. This dataset is comprised of 10,995 distinct memes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Five duplicate memes were discovered in MAMI during preprocessing and subsequently removed.</span></span></span> scraped from social media platforms and meme-creation websites.\nThe high level annotation is a simple binary distinction between misogynous<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>A meme is defined as misogynous if it &#8220;conceptually describes an offensive, sexist or hateful scene [&#8230;] having as target a woman or a group of women&#8221;.</span></span></span> and non-misogynous memes.\nIn the second level annotation, misogynous memes are labelled with one or more subtypes of misogyny: shaming, stereotyping, objectification, and violence. These labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T5\" title=\"Table 5 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, with example memes and their labels given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.\nWe again split the data in an 80/20 ratio.</p>\n\n",
                "matched_terms": [
                    "detect",
                    "their",
                    "misogynous",
                    "misogyny",
                    "given",
                    "hateful",
                    "one",
                    "from",
                    "meme",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Posts are first analysed for named entities (in one branch of the pipeline), then contextual information is generated and incorporated into vectorised representations, before finally being classified.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities",
                    "into",
                    "one",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the named-entity-based context generation approach, we identify named entities using the uncased <span class=\"ltx_text ltx_font_typewriter\">dslim/bert-base-NER</span> model <cite class=\"ltx_cite ltx_citemacro_cite\">Lim (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib4\" title=\"\">2021</a>)</cite>, a fine-tuned BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> system trained on Named Entity Recognition (NER), chosen based on its high performance on the CONLL-2003 benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">Tjong Kim&#160;Sang and De&#160;Meulder (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib38\" title=\"\">2003</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "named",
                    "its",
                    "entities",
                    "model",
                    "context",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ Google&#8217;s <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> <cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib2\" title=\"\">2024</a>)</cite> model for all experiments; a decision driven primarily by its speed and cost-efficiency.\nThis model can be prompted through a useful batch API, allowing for efficient processing of many requests in parallel. This was considered essential, given the need to generate context for thousands of instances, often multiple times to experiment with consistency.\nWhile comparable LLMs, such as <cite class=\"ltx_cite ltx_citemacro_citet\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib5\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span> or <cite class=\"ltx_cite ltx_citemacro_citet\">Anthropic (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib1\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">Claude Sonnet 3.7</span>, may offer similar capabilities, Gemini&#8217;s high score-to-cost ratio, highlighted by LMArena&#8217;s benchmark evaluations <cite class=\"ltx_cite ltx_citemacro_cite\">Chiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib5\" title=\"\">2024</a>)</cite>, made it an attractive choice.</p>\n\n",
                "matched_terms": [
                    "its",
                    "while",
                    "model",
                    "context",
                    "given",
                    "all",
                    "considered"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Posts are encoded using the SBERT <span class=\"ltx_text ltx_font_typewriter\">all-mpnet-base-v2</span> model, which creates normalised 768-length embeddings from sentences up to 384 tokens and is the highest-performing pre-trained generalist system from Sentence Transformers <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "from",
                    "system",
                    "sentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Context:</span> The first configuration uses only the raw post, simply creating SBERT embeddings from each post in the dataset and feeding them directly into the input of the MLP classification model.</p>\n\n",
                "matched_terms": [
                    "post",
                    "into",
                    "model",
                    "directly",
                    "classification",
                    "from",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">REL:</span> The second baseline replicates <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> entity-linking strategy, augmenting post text with Wikipedia-derived summaries based on named entities. Posts are processed using REL <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite>, with identified entities mapped to corresponding Wikipedia articles. The first two sentences of these articles are then appended to the original post. The combination is encoded using SBERT and given to the classifier, leaving posts without (discovered) named entities unchanged.</p>\n\n",
                "matched_terms": [
                    "named",
                    "post",
                    "entities",
                    "original",
                    "given",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ConceptNet:</span> The third baseline augments posts using ConceptNet&#8217;s Numberbatch embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>, replicating the methodology outlined by <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>. Entities are extracted using n-gram matching before being mapped to ConceptNet embeddings. These vectors are then averaged and normalised, forming a single contextual representation which is concatenated with the SBERT embedding of the original text, resulting in a 1,068-dimensional input vector.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "entities",
                    "original",
                    "representation",
                    "which",
                    "extracted",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Prediction:</span> The final baseline system forgoes embeddings and training entirely, instead using the generative LLM directly as a classifier.\nThis setup aims to determine whether, if given access to an LLM like <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>, such a model is better utilised for generating context or directly for prediction. The LLM is prompted solely with each post and the potential labels and asked to predict binary, multi-class and multi-label classes.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>Full prompt texts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS5\" title=\"E. LLM Prompts for Context Generation and Incorporation &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "post",
                    "model",
                    "context",
                    "directly",
                    "given",
                    "whether",
                    "system",
                    "multilabel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities",
                    "post",
                    "context",
                    "extracted",
                    "textual",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities",
                    "post",
                    "model",
                    "context",
                    "background",
                    "request",
                    "given",
                    "extracted",
                    "one",
                    "only",
                    "about",
                    "text",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once context has been generated, we test four strategies to incorporate it into the model input, dubbed i) <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>, ii) <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>, iii) <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, and iv) <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>.</p>\n\n",
                "matched_terms": [
                    "incorporate",
                    "into",
                    "model",
                    "context",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "post",
                    "original",
                    "following",
                    "their",
                    "context",
                    "directly",
                    "from",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embed &amp; Concat</span> tests the inverse of the previous strategy, vectorising the post and context separately, then concatenating them. This preserves a greater separation between post and context, potentially aiding the classifier in distinguishing their respective contributions.</p>\n\n",
                "matched_terms": [
                    "their",
                    "post",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context-Embed</span> follows <cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> methodology, fusing post and context hierarchically at the embedding layer before encoding, leading to a deeper integration of context and post than simple concatenation at the input or embedding level. In this configuration, the context is encoded with SBERT, while the post is tokenized and passed through the word embedding matrix of another SBERT model. The context vector is projected to match the dimensionality of token embeddings and prepended to the post embedding sequence. This expanded sequence is then passed through the remaining layers of the encoder. The resulting representation is then pooled before being passed to the classifier.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "post",
                    "while",
                    "word",
                    "model",
                    "context",
                    "representation",
                    "passed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "its",
                    "original",
                    "post",
                    "structure",
                    "hate",
                    "context",
                    "whether",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "watermarks",
                    "original",
                    "extracted",
                    "meme",
                    "whole",
                    "textual",
                    "not",
                    "context",
                    "background",
                    "directly",
                    "from",
                    "image",
                    "text",
                    "memes",
                    "some",
                    "additional",
                    "representation",
                    "model",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each experiment is run five times with averaged results reported. The primary evaluation metric used in this study is macro F1. We also report positive class F1 in the binary settings and per-class F1 for the multi-class and multi-label settings.</p>\n\n",
                "matched_terms": [
                    "also",
                    "class",
                    "multilabel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "misogyny",
                    "context",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "does",
                    "entities",
                    "hate",
                    "their",
                    "representation",
                    "given",
                    "context",
                    "which",
                    "directly",
                    "model",
                    "classification",
                    "all",
                    "just",
                    "speech",
                    "text",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "named",
                    "does",
                    "entities",
                    "any",
                    "while",
                    "classes",
                    "model",
                    "context",
                    "all",
                    "classification",
                    "from",
                    "performs",
                    "system",
                    "memes",
                    "only",
                    "not",
                    "likely",
                    "multilabel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "entities",
                    "into",
                    "while",
                    "post",
                    "context",
                    "extracted",
                    "classification",
                    "from",
                    "performs",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the influence of incorporating LLM generated context on model behaviour, we compare variance between the predictions of the zero context model and the best-performing <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, rather than solely against the true labels.</p>\n\n",
                "matched_terms": [
                    "model",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "while",
                    "does",
                    "original",
                    "post",
                    "additional",
                    "model",
                    "which",
                    "context",
                    "all",
                    "hateful",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, 187 non-hateful posts (4.4%) were incorrectly classified by the Embed &amp; Concat system but not by the zero-context one.\nPrompting the LLM to relate generated context to potential hate speech may lead to inventing hateful context where none exists, consequently leading to the over-estimation of non-hateful posts as hateful.\nThe post &#8220;<span class=\"ltx_text ltx_font_italic\">this is what becoming a groyper does to your life people, it is an irreconcilable decision</span>&#8221; is a warning of the damage that falling into right-wing conspiracist groups can do to someone, but is mistakenly classified as hateful once lengthy context about <span class=\"ltx_text ltx_font_italic\">groypers</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Hawley (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib19\" title=\"\">2021</a>)</cite> is added, associating them with hateful ideologies and leading the classifier to over-represent this inferred hate.</p>\n\n",
                "matched_terms": [
                    "does",
                    "into",
                    "post",
                    "what",
                    "hate",
                    "context",
                    "hateful",
                    "one",
                    "system",
                    "speech",
                    "about",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">68 misogynous memes (3.1%) were classified correctly with no context but incorrectly by the Embed &amp; Concat system. These generally fall under the aforementioned issue of adding irrelevant or incorrect context that obfuscates previously clearer hate.\nIn the example in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F1\" title=\"Figure 1 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the generated context does not recognise Bill Cosby in the image and misses the implication of Drug-Facilitated Sexual Assault <cite class=\"ltx_cite ltx_citemacro_cite\">Butler and Welch (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib4\" title=\"\">2009</a>)</cite>, leading the model to misclassify it as not misogynous.</p>\n\n",
                "matched_terms": [
                    "does",
                    "hate",
                    "misogynous",
                    "context",
                    "model",
                    "system",
                    "image",
                    "not",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "context",
                    "one",
                    "from",
                    "meme",
                    "not",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while our results demonstrates the promise of using LLM-generated context in HSD, our error analysis highlights some of its pitfalls. Most pressingly, we observe that additional context can cause semantic drift in the original post, occasionally neutralising hateful intent or inventing false hateful connections. This underscores the need for careful context selection and integration to maximise benefits and mitigate errors.</p>\n\n",
                "matched_terms": [
                    "its",
                    "while",
                    "original",
                    "post",
                    "some",
                    "additional",
                    "context",
                    "intent",
                    "hateful"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond demonstrating increased performance metrics, this research addresses important considerations regarding the role of background context in HSD.\nWhile full-text prompting outperformed named-entity-based approaches, both occasionally introduced spurious associations, impacting performance and suggesting that overcontextualisation, where generated background information dilutes or reinterprets the original intent, can be just as problematic as undercontextualisation.\nError analysis reinforces this view, with many false positives arising from benign posts being supplemented with highly charged or misleading contextual information, while false negatives often occurred when clearly hateful statements were augmented with neutralising or overly-explanatory context. These patterns reveal a trade-off between providing richer background information and maintaining original intent.\nFuture models may aim to strike the right balance through dynamic weighting or more sophisticated attention mechanisms to weigh the impacts of post and context.</p>\n\n",
                "matched_terms": [
                    "while",
                    "original",
                    "post",
                    "context",
                    "background",
                    "intent",
                    "hateful",
                    "from",
                    "just",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, future work in this domain could explore a wider range of prompting strategies, in particular, utilising online-search and reasoning-capable LLMs to generate higher-quality background context.\nFurthermore, hybrid systems that combine generated context with verifiable domain knowledge may help to detect hateful references that even LLMs can overlook and guarantee more factual grounding <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. For instance, known extremist references could be identified by linking to curated hate-related knowledge bases, such as <cite class=\"ltx_cite ltx_citemacro_citet\">GPAHE (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib3\" title=\"\">2023</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Global Extremist Symbols Database</span> and integrating discovered information into the LLM prompt.</p>\n\n",
                "matched_terms": [
                    "into",
                    "detect",
                    "context",
                    "background",
                    "hateful",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "does",
                    "while",
                    "hate",
                    "classes",
                    "model",
                    "context",
                    "background",
                    "classification",
                    "from",
                    "not",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research investigated the use of LLMs as dynamic knowledge bases to generate background context aimed at improving the detection of (implicit) hate speech across textual and multimodal domains. By comparing two prompting strategies for context generation and four distinct methods of integrating context into a classification pipeline, we systematically evaluated the impact of contextual enhancement on HSD performance.</p>\n\n",
                "matched_terms": [
                    "into",
                    "hate",
                    "context",
                    "background",
                    "detection",
                    "classification",
                    "speech",
                    "textual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing our research questions, our findings indicate, firstly, that LLM-based context generation can outperform static entity-linking-based approaches, as evidenced by consistent improvements across both binary and fine-grained classification tasks in textual and multimodal settings (RQ1).\nWhile named-entity-based strategies showed marginal gains, incorporating context generated via full-text prompting outperformed baselines in the vast majority of settings.\nResults show that both the quality of generated context and the method by which it is incorporated into model inputs are critical to achieving improvements.</p>\n\n",
                "matched_terms": [
                    "while",
                    "into",
                    "inputs",
                    "model",
                    "which",
                    "context",
                    "classification",
                    "textual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, we find that embedding-level concatenation (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) outperformed other incorporation methods, demonstrating the importance of maintaining separation between original content and additional background context (RQ2).\nIntegrating context directly into the original text through use of an LLM (<span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>) also beat baseline scores in the multimodal setting and lead to strong results when incorporating named-entity context in the textual setting, underscoring the future potential of this approach.</p>\n\n",
                "matched_terms": [
                    "original",
                    "into",
                    "also",
                    "additional",
                    "context",
                    "background",
                    "directly",
                    "textual",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "textual",
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite promising results, this research is subject to limitations that should be considered when interpreting the findings or generalising them to broader settings.\nPrimarily, we relied on a single LLM (<span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>) and a fixed set of simple, natural-language prompts to generate background context. Alternative models, prompt formulations, or temperature settings could produce substantially different context and thus alter downstream performance.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "considered",
                    "context",
                    "background"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our evaluation is limited to two English-language datasets collected over relatively brief time spans. This could limit generalisability to multilingual or non-English settings, as well as temporal generalisability, particularly as the language and references used in online posts and memes changes rapidly and dramatically over time (see <cite class=\"ltx_cite ltx_citemacro_citet\">Kostadinovska-Stojchevska and Shalevska (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib23\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Joshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib21\" title=\"\">2024</a>)</cite> for more on this).\nMoreover, human annotations, particularly for implicit hate, are inherently subjective, which may introduce bias into annotated datasets.\nOur error analysis revealed several instances that appeared mislabelled or open to multiple valid interpretations in the Latent Hatred dataset. These issues are discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS4\" title=\"D. Doubtful Labels in Latent Hatred &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "into",
                    "hate",
                    "brief",
                    "which",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, reliance on macro F1 scores, though useful as an overall performance indicator, can obscure important class-specific trends and failure modes.\nFuture evaluations may benefit from more fine-grained analysis or the development of dedicated challenge datasets to better assess robustness to various phenomena, including the aforementioned linguistic development in memes.</p>\n\n",
                "matched_terms": [
                    "from",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As argued by <cite class=\"ltx_cite ltx_citemacro_citet\">Wong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib43\" title=\"\">2024</a>)</cite>, technical HSD research has advanced rapidly but studies often underemphasise ethical and societal implications, leading to a lack of real-world impact.\nWhile automatic HSD systems are undoubtedly valuable for identifying and combating hateful content online, their deployment raises substantial ethical concerns that must be addressed alongside technical development.</p>\n\n",
                "matched_terms": [
                    "has",
                    "their",
                    "hateful",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Firstly and most importantly, HSD systems, if implemented inadequately, can limit freedom of expression, a fundamental right <cite class=\"ltx_cite ltx_citemacro_cite\">Council of Europe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nWithout frequent assessment and transparent appeal mechanisms, there is a risk of these models being weaponised to suppress lawful content&#8212;either inadvertently through biased data and imprecise definitions, or deliberately by actors seeking to limit public discourse on certain topics <cite class=\"ltx_cite ltx_citemacro_cite\">Funk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib14\" title=\"\">2023</a>)</cite>.\nThe ease with which a seemingly robust HSD system can be adapted to identify and suppress any kind of <span class=\"ltx_text ltx_font_italic\">undesirable</span> content, merely by retraining it with different labels, should not be underestimated.</p>\n\n",
                "matched_terms": [
                    "not",
                    "which",
                    "any",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "into",
                    "context",
                    "directly",
                    "background",
                    "classification",
                    "about",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Processed data, including generated background context, and code to reproduce our experiments are available in an anonymous repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a>.</p>\n\n",
                "matched_terms": [
                    "context",
                    "background"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our replication of <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> methodology was unable to validate their results, instead demonstrating a slight drop in macro F1 score from the zero-context baseline for both binary and multi-class classification.\nOne major issue with Lin&#8217;s approach is REL itself, which struggles to identify entities that are not capitalised in the &#8220;proper&#8221; way &#8212; a substantial issue when working with informal social media data. Lin does not explicitly mention the number of entities discovered, but our reproduction finds that only 9,595 of 21,480 posts (44.67%) were linked to one or more entities.\nManual inspection of a random sample of 100 posts (containing 79 identified tags) found 28 extra named entities missed by REL in Lin&#8217;s approach, along with 18 incorrectly linked named entities.</p>\n\n",
                "matched_terms": [
                    "named",
                    "tags",
                    "does",
                    "entities",
                    "their",
                    "which",
                    "classification",
                    "one",
                    "from",
                    "finds",
                    "only",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "while",
                    "which",
                    "extracted",
                    "meme",
                    "textual",
                    "likely",
                    "entities",
                    "any",
                    "context",
                    "background",
                    "directly",
                    "from",
                    "descriptions",
                    "image",
                    "text",
                    "memes",
                    "one",
                    "speech",
                    "only",
                    "named",
                    "hate",
                    "given",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "into",
                    "provide",
                    "inputs",
                    "only",
                    "which",
                    "directly",
                    "description",
                    "extracted",
                    "from",
                    "meme",
                    "textual",
                    "image",
                    "text",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "tweet",
                    "tweets",
                    "does",
                    "while",
                    "hate",
                    "classes",
                    "their",
                    "class",
                    "model",
                    "hateful",
                    "just",
                    "has",
                    "not",
                    "they"
                ]
            }
        ]
    },
    "Sx6.T4": {
        "caption": "Table 4: Distribution of the Latent Hatred datasets implicit hate classes.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">White</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Grievance</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Incitement</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Stereo-</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">typing</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Inferiority</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Irony</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Threat-</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">ening</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Other</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Count</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1,538</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1,269</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1,133</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">863</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">797</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">666</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">80</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Occurrence</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.24%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.99%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.85%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.60%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.56%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.49%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.26%</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "threat",
            "occurrence",
            "incitement",
            "implicit",
            "classes",
            "datasets",
            "distribution",
            "count",
            "hatred",
            "inferiority",
            "typing",
            "irony",
            "stereo",
            "latent",
            "grievance",
            "white",
            "hate",
            "other",
            "ening"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "hate",
                    "hatred",
                    "implicit",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Leveraging LLMs for Context-Aware Implicit\n<br class=\"ltx_break\"/>Textual and Multimodal Hate Speech Detection</span>\n</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "irony",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "incitement",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "hatred"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "hatred"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "hate",
                    "hatred",
                    "implicit",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "other",
                    "classes",
                    "distribution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "hate",
                    "hatred",
                    "implicit",
                    "classes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research investigated the use of LLMs as dynamic knowledge bases to generate background context aimed at improving the detection of (implicit) hate speech across textual and multimodal domains. By comparing two prompting strategies for context generation and four distinct methods of integrating context into a classification pipeline, we systematically evaluated the impact of contextual enhancement on HSD performance.</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our evaluation is limited to two English-language datasets collected over relatively brief time spans. This could limit generalisability to multilingual or non-English settings, as well as temporal generalisability, particularly as the language and references used in online posts and memes changes rapidly and dramatically over time (see <cite class=\"ltx_cite ltx_citemacro_citet\">Kostadinovska-Stojchevska and Shalevska (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib23\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Joshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib21\" title=\"\">2024</a>)</cite> for more on this).\nMoreover, human annotations, particularly for implicit hate, are inherently subjective, which may introduce bias into annotated datasets.\nOur error analysis revealed several instances that appeared mislabelled or open to multiple valid interpretations in the Latent Hatred dataset. These issues are discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS4\" title=\"D. Doubtful Labels in Latent Hatred &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "latent",
                    "hate",
                    "hatred"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "hate",
                    "hatred"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "incitement",
                    "grievance",
                    "white",
                    "hatred",
                    "hate",
                    "classes",
                    "datasets"
                ]
            }
        ]
    },
    "Sx6.T5": {
        "caption": "Table 5: Distribution of misogynous labels in the MAMI dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Stereotyping</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Objectification</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Shaming</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Violence</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Count</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3,160</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2,549</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1,417</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1,106</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Occurrence</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.46%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.35%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.76%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.11%</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "count",
            "labels",
            "shaming",
            "objectification",
            "mami",
            "misogynous",
            "stereotyping",
            "violence",
            "occurrence",
            "dataset",
            "distribution"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our second source is <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, used to detect misogynous memes. This dataset is comprised of 10,995 distinct memes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Five duplicate memes were discovered in MAMI during preprocessing and subsequently removed.</span></span></span> scraped from social media platforms and meme-creation websites.\nThe high level annotation is a simple binary distinction between misogynous<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>A meme is defined as misogynous if it &#8220;conceptually describes an offensive, sexist or hateful scene [&#8230;] having as target a woman or a group of women&#8221;.</span></span></span> and non-misogynous memes.\nIn the second level annotation, misogynous memes are labelled with one or more subtypes of misogyny: shaming, stereotyping, objectification, and violence. These labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T5\" title=\"Table 5 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, with example memes and their labels given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.\nWe again split the data in an 80/20 ratio.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "mami",
                    "misogynous"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "stereotyping",
                    "mami",
                    "misogynous",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n",
                "matched_terms": [
                    "stereotyping",
                    "violence",
                    "labels",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "mami"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "distribution",
                    "labels"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "mami"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "labels"
                ]
            }
        ]
    },
    "Sx6.T6": {
        "caption": "Table 6: Comparison of Named Entity context vs. Full-Text context.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Post</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;report </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">trump</span><span class=\"ltx_text\" style=\"font-size:90%;\"> sought visas to import at least 1100 foreign workers into </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">usa</span><span class=\"ltx_text\" style=\"font-size:90%;\">&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_top\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Named</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Entity</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Context</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;A trump is a playing card which is elevated above its usual rank in trick-taking games. Typically an entire suit is nominated as a trump suit; these cards then outrank all cards of plain (non-trump) suits. The United States of America (USA), commonly known as the United States (U.S.) or America, is a country primarily located in North America. It is a federal union of 50 states and a federal capital district, Washington, D.C.&#8220;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_top\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Full-Text</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Context</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;The tweet references Donald Trump&#8217;s past business practices of seeking H-2B visas to employ foreign workers at his various properties, including Mar-a-Lago. The H-2B visa program allows employers to temporarily hire non-immigrant foreign workers to perform non-agricultural labour or services in the United States. Trump&#8217;s use of the program has been controversial, given his rhetoric on immigration and American jobs. Critics have argued that he should prioritize hiring American workers instead of seeking foreign labour.&#8221;</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "foreign",
            "services",
            "allows",
            "jobs",
            "immigration",
            "various",
            "washington",
            "above",
            "past",
            "h2b",
            "outrank",
            "temporarily",
            "workers",
            "usa",
            "references",
            "plain",
            "games",
            "entire",
            "commonly",
            "rank",
            "use",
            "given",
            "controversial",
            "sought",
            "nonimmigrant",
            "states",
            "maralago",
            "its",
            "argued",
            "context",
            "visas",
            "country",
            "least",
            "dc",
            "tweet",
            "card",
            "critics",
            "capital",
            "american",
            "suit",
            "cards",
            "fulltext",
            "tricktaking",
            "suits",
            "usual",
            "primarily",
            "labour",
            "usa",
            "the",
            "prioritize",
            "which",
            "nonagricultural",
            "hiring",
            "united",
            "employ",
            "union",
            "instead",
            "located",
            "elevated",
            "labour",
            "into",
            "known",
            "perform",
            "seeking",
            "visa",
            "report",
            "named",
            "donald",
            "including",
            "all",
            "north",
            "trumps",
            "typically",
            "have",
            "program",
            "has",
            "comparison",
            "hire",
            "import",
            "his",
            "trump",
            "nominated",
            "practices",
            "playing",
            "nontrump",
            "federal",
            "employers",
            "america",
            "then",
            "business",
            "rhetoric",
            "district",
            "post",
            "properties",
            "entity"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "named",
                    "into",
                    "which",
                    "context",
                    "including",
                    "fulltext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "post",
                    "into",
                    "context",
                    "perform",
                    "entity",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "into",
                    "which",
                    "typically",
                    "context",
                    "have",
                    "various"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To what extent is a pre-trained LLM more effective than static entity linking at generating background context for implicit HSD?</p>\n\n",
                "matched_terms": [
                    "entity",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How can background context be effectively incorporated into a HSD model input?</p>\n\n",
                "matched_terms": [
                    "into",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While hate speech is inherently contextual <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, previous studies have shown that incorporating generic additional context into the model input often leads to a negligible or negative impact on model performance <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>. These findings suggest that context must be carefully selected, represented, and integrated to avoid introducing noise. This section discusses previous work relevant to HSD in context retrieval and incorporation, the use of LLMs, and the challenges surrounding multimodality.</p>\n\n",
                "matched_terms": [
                    "have",
                    "use",
                    "into",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "post",
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "named",
                    "context",
                    "entity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs have been used in HSD previously, generally replacing classifiers themselves, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>)</cite>, who directly compare a generative LLM against two pre-trained models: BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib25\" title=\"\">2019</a>)</cite>. They test various prompting strategies (few-shot, chain-of-thought, etc.) and demonstrate that a generalist LLM with simple prompts can comfortably outperform transformer-based encoders. These results are not confirmed, however, by numerous other studies, who find that LLMs either match or fall short of the performance of encoder-only classifiers, especially without extensive prompt engineering <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Albladi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib1\" title=\"\">2025</a>)</cite> discuss these results, highlighting the fact that encoder models are fine-tuned on the training data and can potentially capture the peculiarities of a given dataset better than generalist LLMs.</p>\n\n",
                "matched_terms": [
                    "have",
                    "various",
                    "given"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While replacing traditional classifiers entirely is a promising approach, this research instead focuses on treating LLMs as dynamic knowledge bases from which to extract contextual information. <cite class=\"ltx_cite ltx_citemacro_citet\">Petroni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib32\" title=\"\">2019</a>)</cite> were among the first to systematically evaluate whether generative models can store factual and relational knowledge. Using cloze-style prompts, they demonstrate that pre-trained models can recall factual information with high accuracy, often outperforming traditional relation extraction systems.\nBuilding on this idea, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib41\" title=\"\">2020</a>)</cite> propose a method for building structured knowledge graphs directly from LLMs, suggesting that larger models can be used to effectively store and retrieve world knowledge.</p>\n\n",
                "matched_terms": [
                    "instead",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "all",
                    "post",
                    "into",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "post",
                    "which",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "named",
                    "into",
                    "context",
                    "all",
                    "fulltext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n",
                "matched_terms": [
                    "known",
                    "into",
                    "including"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Posts are first analysed for named entities (in one branch of the pipeline), then contextual information is generated and incorporated into vectorised representations, before finally being classified.</p>\n\n",
                "matched_terms": [
                    "then",
                    "named",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the named-entity-based context generation approach, we identify named entities using the uncased <span class=\"ltx_text ltx_font_typewriter\">dslim/bert-base-NER</span> model <cite class=\"ltx_cite ltx_citemacro_cite\">Lim (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib4\" title=\"\">2021</a>)</cite>, a fine-tuned BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> system trained on Named Entity Recognition (NER), chosen based on its high performance on the CONLL-2003 benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">Tjong Kim&#160;Sang and De&#160;Meulder (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib38\" title=\"\">2003</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "named",
                    "its",
                    "context",
                    "entity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ Google&#8217;s <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> <cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib2\" title=\"\">2024</a>)</cite> model for all experiments; a decision driven primarily by its speed and cost-efficiency.\nThis model can be prompted through a useful batch API, allowing for efficient processing of many requests in parallel. This was considered essential, given the need to generate context for thousands of instances, often multiple times to experiment with consistency.\nWhile comparable LLMs, such as <cite class=\"ltx_cite ltx_citemacro_citet\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib5\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span> or <cite class=\"ltx_cite ltx_citemacro_citet\">Anthropic (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib1\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">Claude Sonnet 3.7</span>, may offer similar capabilities, Gemini&#8217;s high score-to-cost ratio, highlighted by LMArena&#8217;s benchmark evaluations <cite class=\"ltx_cite ltx_citemacro_cite\">Chiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib5\" title=\"\">2024</a>)</cite>, made it an attractive choice.</p>\n\n",
                "matched_terms": [
                    "its",
                    "employ",
                    "primarily",
                    "context",
                    "given",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Context:</span> The first configuration uses only the raw post, simply creating SBERT embeddings from each post in the dataset and feeding them directly into the input of the MLP classification model.</p>\n\n",
                "matched_terms": [
                    "post",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">REL:</span> The second baseline replicates <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> entity-linking strategy, augmenting post text with Wikipedia-derived summaries based on named entities. Posts are processed using REL <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite>, with identified entities mapped to corresponding Wikipedia articles. The first two sentences of these articles are then appended to the original post. The combination is encoded using SBERT and given to the classifier, leaving posts without (discovered) named entities unchanged.</p>\n\n",
                "matched_terms": [
                    "then",
                    "named",
                    "post",
                    "given"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ConceptNet:</span> The third baseline augments posts using ConceptNet&#8217;s Numberbatch embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>, replicating the methodology outlined by <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>. Entities are extracted using n-gram matching before being mapped to ConceptNet embeddings. These vectors are then averaged and normalised, forming a single contextual representation which is concatenated with the SBERT embedding of the original text, resulting in a 1,068-dimensional input vector.</p>\n\n",
                "matched_terms": [
                    "then",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Prediction:</span> The final baseline system forgoes embeddings and training entirely, instead using the generative LLM directly as a classifier.\nThis setup aims to determine whether, if given access to an LLM like <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>, such a model is better utilised for generating context or directly for prediction. The LLM is prompted solely with each post and the potential labels and asked to predict binary, multi-class and multi-label classes.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>Full prompt texts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS5\" title=\"E. LLM Prompts for Context Generation and Incorporation &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "given",
                    "instead",
                    "post",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "named",
                    "post",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "named",
                    "post",
                    "context",
                    "given",
                    "entity",
                    "least",
                    "instead"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once context has been generated, we test four strategies to incorporate it into the model input, dubbed i) <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>, ii) <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>, iii) <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, and iv) <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>.</p>\n\n",
                "matched_terms": [
                    "has",
                    "into",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "post",
                    "context",
                    "union"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embed &amp; Concat</span> tests the inverse of the previous strategy, vectorising the post and context separately, then concatenating them. This preserves a greater separation between post and context, potentially aiding the classifier in distinguishing their respective contributions.</p>\n\n",
                "matched_terms": [
                    "then",
                    "post",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context-Embed</span> follows <cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> methodology, fusing post and context hierarchically at the embedding layer before encoding, leading to a deeper integration of context and post than simple concatenation at the input or embedding level. In this configuration, the context is encoded with SBERT, while the post is tokenized and passed through the word embedding matrix of another SBERT model. The context vector is projected to match the dimensionality of token embeddings and prepended to the post embedding sequence. This expanded sequence is then passed through the remaining layers of the encoder. The resulting representation is then pooled before being passed to the classifier.</p>\n\n",
                "matched_terms": [
                    "then",
                    "post",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "post",
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "fulltext",
                    "use",
                    "context",
                    "including"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "which",
                    "given",
                    "context",
                    "all",
                    "fulltext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "named",
                    "context",
                    "all",
                    "entity",
                    "fulltext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "post",
                    "into",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "all",
                    "post",
                    "which",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, 187 non-hateful posts (4.4%) were incorrectly classified by the Embed &amp; Concat system but not by the zero-context one.\nPrompting the LLM to relate generated context to potential hate speech may lead to inventing hateful context where none exists, consequently leading to the over-estimation of non-hateful posts as hateful.\nThe post &#8220;<span class=\"ltx_text ltx_font_italic\">this is what becoming a groyper does to your life people, it is an irreconcilable decision</span>&#8221; is a warning of the damage that falling into right-wing conspiracist groups can do to someone, but is mistakenly classified as hateful once lengthy context about <span class=\"ltx_text ltx_font_italic\">groypers</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Hawley (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib19\" title=\"\">2021</a>)</cite> is added, associating them with hateful ideologies and leading the classifier to over-represent this inferred hate.</p>\n\n",
                "matched_terms": [
                    "post",
                    "into",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n",
                "matched_terms": [
                    "which",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while our results demonstrates the promise of using LLM-generated context in HSD, our error analysis highlights some of its pitfalls. Most pressingly, we observe that additional context can cause semantic drift in the original post, occasionally neutralising hateful intent or inventing false hateful connections. This underscores the need for careful context selection and integration to maximise benefits and mitigate errors.</p>\n\n",
                "matched_terms": [
                    "post",
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond demonstrating increased performance metrics, this research addresses important considerations regarding the role of background context in HSD.\nWhile full-text prompting outperformed named-entity-based approaches, both occasionally introduced spurious associations, impacting performance and suggesting that overcontextualisation, where generated background information dilutes or reinterprets the original intent, can be just as problematic as undercontextualisation.\nError analysis reinforces this view, with many false positives arising from benign posts being supplemented with highly charged or misleading contextual information, while false negatives often occurred when clearly hateful statements were augmented with neutralising or overly-explanatory context. These patterns reveal a trade-off between providing richer background information and maintaining original intent.\nFuture models may aim to strike the right balance through dynamic weighting or more sophisticated attention mechanisms to weigh the impacts of post and context.</p>\n\n",
                "matched_terms": [
                    "fulltext",
                    "post",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, future work in this domain could explore a wider range of prompting strategies, in particular, utilising online-search and reasoning-capable LLMs to generate higher-quality background context.\nFurthermore, hybrid systems that combine generated context with verifiable domain knowledge may help to detect hateful references that even LLMs can overlook and guarantee more factual grounding <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. For instance, known extremist references could be identified by linking to curated hate-related knowledge bases, such as <cite class=\"ltx_cite ltx_citemacro_citet\">GPAHE (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib3\" title=\"\">2023</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Global Extremist Symbols Database</span> and integrating discovered information into the LLM prompt.</p>\n\n",
                "matched_terms": [
                    "references",
                    "known",
                    "into",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research investigated the use of LLMs as dynamic knowledge bases to generate background context aimed at improving the detection of (implicit) hate speech across textual and multimodal domains. By comparing two prompting strategies for context generation and four distinct methods of integrating context into a classification pipeline, we systematically evaluated the impact of contextual enhancement on HSD performance.</p>\n\n",
                "matched_terms": [
                    "use",
                    "into",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing our research questions, our findings indicate, firstly, that LLM-based context generation can outperform static entity-linking-based approaches, as evidenced by consistent improvements across both binary and fine-grained classification tasks in textual and multimodal settings (RQ1).\nWhile named-entity-based strategies showed marginal gains, incorporating context generated via full-text prompting outperformed baselines in the vast majority of settings.\nResults show that both the quality of generated context and the method by which it is incorporated into model inputs are critical to achieving improvements.</p>\n\n",
                "matched_terms": [
                    "fulltext",
                    "into",
                    "which",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, we find that embedding-level concatenation (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) outperformed other incorporation methods, demonstrating the importance of maintaining separation between original content and additional background context (RQ2).\nIntegrating context directly into the original text through use of an LLM (<span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>) also beat baseline scores in the multimodal setting and lead to strong results when incorporating named-entity context in the textual setting, underscoring the future potential of this approach.</p>\n\n",
                "matched_terms": [
                    "use",
                    "into",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite promising results, this research is subject to limitations that should be considered when interpreting the findings or generalising them to broader settings.\nPrimarily, we relied on a single LLM (<span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>) and a fixed set of simple, natural-language prompts to generate background context. Alternative models, prompt formulations, or temperature settings could produce substantially different context and thus alter downstream performance.</p>\n\n",
                "matched_terms": [
                    "primarily",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our evaluation is limited to two English-language datasets collected over relatively brief time spans. This could limit generalisability to multilingual or non-English settings, as well as temporal generalisability, particularly as the language and references used in online posts and memes changes rapidly and dramatically over time (see <cite class=\"ltx_cite ltx_citemacro_citet\">Kostadinovska-Stojchevska and Shalevska (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib23\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Joshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib21\" title=\"\">2024</a>)</cite> for more on this).\nMoreover, human annotations, particularly for implicit hate, are inherently subjective, which may introduce bias into annotated datasets.\nOur error analysis revealed several instances that appeared mislabelled or open to multiple valid interpretations in the Latent Hatred dataset. These issues are discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS4\" title=\"D. Doubtful Labels in Latent Hatred &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "references",
                    "into",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, reliance on macro F1 scores, though useful as an overall performance indicator, can obscure important class-specific trends and failure modes.\nFuture evaluations may benefit from more fine-grained analysis or the development of dedicated challenge datasets to better assess robustness to various phenomena, including the aforementioned linguistic development in memes.</p>\n\n",
                "matched_terms": [
                    "various",
                    "including"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As argued by <cite class=\"ltx_cite ltx_citemacro_citet\">Wong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib43\" title=\"\">2024</a>)</cite>, technical HSD research has advanced rapidly but studies often underemphasise ethical and societal implications, leading to a lack of real-world impact.\nWhile automatic HSD systems are undoubtedly valuable for identifying and combating hateful content online, their deployment raises substantial ethical concerns that must be addressed alongside technical development.</p>\n\n",
                "matched_terms": [
                    "has",
                    "argued"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Firstly and most importantly, HSD systems, if implemented inadequately, can limit freedom of expression, a fundamental right <cite class=\"ltx_cite ltx_citemacro_cite\">Council of Europe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nWithout frequent assessment and transparent appeal mechanisms, there is a risk of these models being weaponised to suppress lawful content&#8212;either inadvertently through biased data and imprecise definitions, or deliberately by actors seeking to limit public discourse on certain topics <cite class=\"ltx_cite ltx_citemacro_cite\">Funk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib14\" title=\"\">2023</a>)</cite>.\nThe ease with which a seemingly robust HSD system can be adapted to identify and suppress any kind of <span class=\"ltx_text ltx_font_italic\">undesirable</span> content, merely by retraining it with different labels, should not be underestimated.</p>\n\n",
                "matched_terms": [
                    "seeking",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "use",
                    "into",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Processed data, including generated background context, and code to reproduce our experiments are available in an anonymous repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a>.</p>\n\n",
                "matched_terms": [
                    "context",
                    "including"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our replication of <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> methodology was unable to validate their results, instead demonstrating a slight drop in macro F1 score from the zero-context baseline for both binary and multi-class classification.\nOne major issue with Lin&#8217;s approach is REL itself, which struggles to identify entities that are not capitalised in the &#8220;proper&#8221; way &#8212; a substantial issue when working with informal social media data. Lin does not explicitly mention the number of entities discovered, but our reproduction finds that only 9,595 of 21,480 posts (44.67%) were linked to one or more entities.\nManual inspection of a random sample of 100 posts (containing 79 identified tags) found 28 extra named entities missed by REL in Lin&#8217;s approach, along with 18 incorrectly linked named entities.</p>\n\n",
                "matched_terms": [
                    "named",
                    "instead",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "named",
                    "references",
                    "given",
                    "context",
                    "which",
                    "have",
                    "entity",
                    "least",
                    "fulltext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "into",
                    "primarily",
                    "allows",
                    "which",
                    "have"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "tweet",
                    "references",
                    "suit",
                    "has",
                    "instead",
                    "various"
                ]
            }
        ]
    },
    "Sx6.T7": {
        "caption": "Table 7: Post and generated context of Latent Hatred error analysis examples.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Post</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Context</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;berlin is the new jerusalem: more and more israeli jews moving to germany </span><span class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:1.8pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:7.7pt;height:8.1pt;vertical-align:-1.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text\" style=\"font-size:90%;\">swrm via&#8230;&#8221;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">The tweet references a perceived increase in Israeli Jews moving to Berlin, Germany. The hashtag &#8220;</span><span class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:1.8pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:7.7pt;height:8.1pt;vertical-align:-1.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text\" style=\"font-size:90%;\">swrm&#8221; likely refers to S&#252;dwestrundfunk, a German public broadcaster, suggesting the information may originate from a news report or program by this outlet. Some see Berlin as an attractive destination due to its relatively low cost of living compared to Israel, as well as its vibrant cultural scene and historical significance.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;</span><span class=\"ltx_text\" style=\"font-size:90%;position:relative; bottom:1.8pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:7.7pt;height:8.1pt;vertical-align:-1.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text\" style=\"font-size:90%;\">aryan men &amp; women will either rise up to fight for their heritage children &amp; future or the white race will cease to exist. choose now.&#8221;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">The tweet uses the term &#8220;aryan&#8221;, which is associated with white supremacist ideology, and warns that the &#8220;white race will cease to exist&#8221; unless people &#8220;rise up to fight&#8221;. This language aligns with the &#8220;great replacement&#8221; conspiracy theory, a white nationalist idea that claims white people are being intentionally replaced by non-white people. Such rhetoric is often used to promote fear and incite violence against minority groups.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_top\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#8220;Fight me,</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">communists.&#8221;</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">The tweet &#8220;Fight me, communists&#8221; likely comes from someone with anti-communist views, engaging in political antagonism. Communism is a political and economic ideology advocating a classless society in which the community owns all property and resources. The tweet could be interpreted as a challenge to those who espouse communist beliefs, potentially escalating into online or real-world conflict.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_top\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#8220;yes kill all</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">christians&#8221;</span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">The statement &#8220;yes kill all christians&#8221; expresses support for violence against Christians, a religious group with billions of adherents worldwide. Christianity has a long and complex history, and Christians have been both perpetrators and victims of violence throughout history. Calls for violence against religious groups are considered hate speech by many definitions.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:99.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;this is what becoming a groyper does to your life people it is an irreconcilable decision&#8221;</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;Groyper&#8221; is a term used to describe members of the Groypers, a far-right, white nationalist, and alt-right movement in the United States. Groypers are known for their support of white supremacist and anti-Semitic ideologies, as well as their attempts to inject these views into mainstream conservative politics. The tweet likely refers to the consequences or changes in lifestyle and relationships that may occur when someone becomes involved with this movement.</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "does",
            "supremacist",
            "great",
            "their",
            "inject",
            "history",
            "news",
            "now",
            "expresses",
            "living",
            "relatively",
            "group",
            "beliefs",
            "considered",
            "who",
            "likely",
            "information",
            "error",
            "aryan",
            "conflict",
            "being",
            "against",
            "low",
            "promote",
            "such",
            "from",
            "christianity",
            "cost",
            "aligns",
            "billions",
            "changes",
            "jews",
            "realworld",
            "community",
            "language",
            "references",
            "analysis",
            "well",
            "term",
            "both",
            "conspiracy",
            "society",
            "intentionally",
            "lifestyle",
            "rise",
            "ideology",
            "people",
            "adherents",
            "groyper",
            "either",
            "states",
            "uses",
            "moving",
            "espouse",
            "owns",
            "nationalist",
            "yes",
            "worldwide",
            "complex",
            "nonwhite",
            "victims",
            "ideologies",
            "conservative",
            "violence",
            "describe",
            "exist",
            "members",
            "examples",
            "comes",
            "its",
            "property",
            "aryan",
            "theory",
            "context",
            "someone",
            "fight",
            "classless",
            "interpreted",
            "christians",
            "used",
            "attempts",
            "new",
            "kill",
            "tweet",
            "could",
            "hatred",
            "attractive",
            "altright",
            "becomes",
            "increase",
            "antisemitic",
            "men",
            "communism",
            "online",
            "sdwestrundfunk",
            "will",
            "white",
            "some",
            "israel",
            "associated",
            "christians",
            "political",
            "broadcaster",
            "swrm",
            "statement",
            "significance",
            "choose",
            "communists",
            "life",
            "often",
            "what",
            "escalating",
            "hashtag",
            "which",
            "relationships",
            "economic",
            "when",
            "united",
            "swrm",
            "exist",
            "resources",
            "german",
            "scene",
            "vibrant",
            "future",
            "religious",
            "becoming",
            "destination",
            "jerusalem",
            "into",
            "movement",
            "see",
            "views",
            "known",
            "replaced",
            "definitions",
            "perpetrators",
            "speech",
            "historical",
            "challenge",
            "latent",
            "hate",
            "race",
            "idea",
            "support",
            "more",
            "unless",
            "fear",
            "all",
            "throughout",
            "incite",
            "refers",
            "advocating",
            "anticommunist",
            "groypers",
            "potentially",
            "cultural",
            "white",
            "perceived",
            "fight",
            "antagonism",
            "have",
            "program",
            "has",
            "claims",
            "warns",
            "cease",
            "farright",
            "germany",
            "public",
            "this",
            "your",
            "engaging",
            "fight",
            "berlin",
            "long",
            "groyper",
            "consequences",
            "outlet",
            "suggesting",
            "due",
            "politics",
            "report",
            "mainstream",
            "women",
            "children",
            "groups",
            "calls",
            "involved",
            "via",
            "rhetoric",
            "post",
            "originate",
            "communist",
            "many",
            "heritage",
            "minority",
            "compared",
            "occur",
            "israeli",
            "generated",
            "rise",
            "irreconcilable",
            "berlin",
            "decision",
            "replacement"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "into",
                    "hatred",
                    "information",
                    "hate",
                    "language",
                    "compared",
                    "context",
                    "which",
                    "from",
                    "both",
                    "speech",
                    "examples"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">Leveraging LLMs for Context-Aware Implicit\n<br class=\"ltx_break\"/>Textual and Multimodal Hate Speech Detection</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "complex",
                    "often",
                    "potentially",
                    "relatively",
                    "has",
                    "context",
                    "such",
                    "from",
                    "used",
                    "aligns",
                    "due",
                    "into",
                    "language",
                    "well",
                    "speech",
                    "post",
                    "hate",
                    "many",
                    "more"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "due",
                    "into",
                    "hate",
                    "language",
                    "well",
                    "context",
                    "which",
                    "resources",
                    "have",
                    "generated",
                    "such",
                    "speech",
                    "online",
                    "new"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To what extent is a pre-trained LLM more effective than static entity linking at generating background context for implicit HSD?</p>\n\n",
                "matched_terms": [
                    "more",
                    "context",
                    "what"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How can background context be effectively incorporated into a HSD model input?</p>\n\n",
                "matched_terms": [
                    "into",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While hate speech is inherently contextual <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, previous studies have shown that incorporating generic additional context into the model input often leads to a negligible or negative impact on model performance <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>. These findings suggest that context must be carefully selected, represented, and integrated to avoid introducing noise. This section discusses previous work relevant to HSD in context retrieval and incorporation, the use of LLMs, and the challenges surrounding multimodality.</p>\n\n",
                "matched_terms": [
                    "into",
                    "often",
                    "hate",
                    "context",
                    "have",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "its",
                    "post",
                    "hate",
                    "refers",
                    "context",
                    "news",
                    "such",
                    "speech",
                    "used",
                    "considered"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "hate",
                    "some",
                    "their",
                    "context",
                    "from",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs have been used in HSD previously, generally replacing classifiers themselves, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>)</cite>, who directly compare a generative LLM against two pre-trained models: BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib25\" title=\"\">2019</a>)</cite>. They test various prompting strategies (few-shot, chain-of-thought, etc.) and demonstrate that a generalist LLM with simple prompts can comfortably outperform transformer-based encoders. These results are not confirmed, however, by numerous other studies, who find that LLMs either match or fall short of the performance of encoder-only classifiers, especially without extensive prompt engineering <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Albladi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib1\" title=\"\">2025</a>)</cite> discuss these results, highlighting the fact that encoder models are fine-tuned on the training data and can potentially capture the peculiarities of a given dataset better than generalist LLMs.</p>\n\n",
                "matched_terms": [
                    "potentially",
                    "against",
                    "have",
                    "either",
                    "used",
                    "who"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While replacing traditional classifiers entirely is a promising approach, this research instead focuses on treating LLMs as dynamic knowledge bases from which to extract contextual information. <cite class=\"ltx_cite ltx_citemacro_citet\">Petroni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib32\" title=\"\">2019</a>)</cite> were among the first to systematically evaluate whether generative models can store factual and relational knowledge. Using cloze-style prompts, they demonstrate that pre-trained models can recall factual information with high accuracy, often outperforming traditional relation extraction systems.\nBuilding on this idea, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib41\" title=\"\">2020</a>)</cite> propose a method for building structured knowledge graphs directly from LLMs, suggesting that larger models can be used to effectively store and retrieve world knowledge.</p>\n\n",
                "matched_terms": [
                    "often",
                    "which",
                    "idea",
                    "from",
                    "suggesting",
                    "used",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "does",
                    "into",
                    "often",
                    "post",
                    "context",
                    "all",
                    "more",
                    "such",
                    "from",
                    "who"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "post",
                    "often",
                    "hate",
                    "many",
                    "cultural",
                    "which",
                    "context",
                    "relatively",
                    "both",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "into",
                    "their",
                    "context",
                    "all",
                    "both",
                    "online"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "into",
                    "hatred",
                    "hate",
                    "white",
                    "language",
                    "known",
                    "group",
                    "from",
                    "speech",
                    "violence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our second source is <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, used to detect misogynous memes. This dataset is comprised of 10,995 distinct memes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Five duplicate memes were discovered in MAMI during preprocessing and subsequently removed.</span></span></span> scraped from social media platforms and meme-creation websites.\nThe high level annotation is a simple binary distinction between misogynous<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>A meme is defined as misogynous if it &#8220;conceptually describes an offensive, sexist or hateful scene [&#8230;] having as target a woman or a group of women&#8221;.</span></span></span> and non-misogynous memes.\nIn the second level annotation, misogynous memes are labelled with one or more subtypes of misogyny: shaming, stereotyping, objectification, and violence. These labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T5\" title=\"Table 5 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, with example memes and their labels given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.\nWe again split the data in an 80/20 ratio.</p>\n\n",
                "matched_terms": [
                    "their",
                    "group",
                    "more",
                    "from",
                    "scene",
                    "violence",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Posts are first analysed for named entities (in one branch of the pipeline), then contextual information is generated and incorporated into vectorised representations, before finally being classified.</p>\n\n",
                "matched_terms": [
                    "being",
                    "into",
                    "information",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the named-entity-based context generation approach, we identify named entities using the uncased <span class=\"ltx_text ltx_font_typewriter\">dslim/bert-base-NER</span> model <cite class=\"ltx_cite ltx_citemacro_cite\">Lim (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib4\" title=\"\">2021</a>)</cite>, a fine-tuned BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> system trained on Named Entity Recognition (NER), chosen based on its high performance on the CONLL-2003 benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">Tjong Kim&#160;Sang and De&#160;Meulder (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib38\" title=\"\">2003</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ Google&#8217;s <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> <cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib2\" title=\"\">2024</a>)</cite> model for all experiments; a decision driven primarily by its speed and cost-efficiency.\nThis model can be prompted through a useful batch API, allowing for efficient processing of many requests in parallel. This was considered essential, given the need to generate context for thousands of instances, often multiple times to experiment with consistency.\nWhile comparable LLMs, such as <cite class=\"ltx_cite ltx_citemacro_citet\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib5\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span> or <cite class=\"ltx_cite ltx_citemacro_citet\">Anthropic (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib1\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">Claude Sonnet 3.7</span>, may offer similar capabilities, Gemini&#8217;s high score-to-cost ratio, highlighted by LMArena&#8217;s benchmark evaluations <cite class=\"ltx_cite ltx_citemacro_cite\">Chiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib5\" title=\"\">2024</a>)</cite>, made it an attractive choice.</p>\n\n",
                "matched_terms": [
                    "its",
                    "often",
                    "many",
                    "attractive",
                    "context",
                    "all",
                    "such",
                    "considered"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Posts are encoded using the SBERT <span class=\"ltx_text ltx_font_typewriter\">all-mpnet-base-v2</span> model, which creates normalised 768-length embeddings from sentences up to 384 tokens and is the highest-performing pre-trained generalist system from Sentence Transformers <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Context:</span> The first configuration uses only the raw post, simply creating SBERT embeddings from each post in the dataset and feeding them directly into the input of the MLP classification model.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "post",
                    "from",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ConceptNet:</span> The third baseline augments posts using ConceptNet&#8217;s Numberbatch embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>, replicating the methodology outlined by <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>. Entities are extracted using n-gram matching before being mapped to ConceptNet embeddings. These vectors are then averaged and normalised, forming a single contextual representation which is concatenated with the SBERT embedding of the original text, resulting in a 1,068-dimensional input vector.</p>\n\n",
                "matched_terms": [
                    "being",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Prediction:</span> The final baseline system forgoes embeddings and training entirely, instead using the generative LLM directly as a classifier.\nThis setup aims to determine whether, if given access to an LLM like <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>, such a model is better utilised for generating context or directly for prediction. The LLM is prompted solely with each post and the potential labels and asked to predict binary, multi-class and multi-label classes.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>Full prompt texts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS5\" title=\"E. LLM Prompts for Context Generation and Incorporation &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "such",
                    "post",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "either",
                    "post",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "post",
                    "hatred",
                    "context",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once context has been generated, we test four strategies to incorporate it into the model input, dubbed i) <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>, ii) <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>, iii) <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, and iv) <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>.</p>\n\n",
                "matched_terms": [
                    "has",
                    "into",
                    "context",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "post",
                    "their",
                    "context",
                    "generated",
                    "more",
                    "from",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embed &amp; Concat</span> tests the inverse of the previous strategy, vectorising the post and context separately, then concatenating them. This preserves a greater separation between post and context, potentially aiding the classifier in distinguishing their respective contributions.</p>\n\n",
                "matched_terms": [
                    "their",
                    "post",
                    "context",
                    "potentially"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context-Embed</span> follows <cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> methodology, fusing post and context hierarchically at the embedding layer before encoding, leading to a deeper integration of context and post than simple concatenation at the input or embedding level. In this configuration, the context is encoded with SBERT, while the post is tokenized and passed through the word embedding matrix of another SBERT model. The context vector is projected to match the dimensionality of token embeddings and prepended to the post embedding sequence. This expanded sequence is then passed through the remaining layers of the encoder. The resulting representation is then pooled before being passed to the classifier.</p>\n\n",
                "matched_terms": [
                    "post",
                    "being",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "post",
                    "its",
                    "hate",
                    "against",
                    "context",
                    "generated",
                    "more",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "some",
                    "context",
                    "relatively",
                    "from",
                    "both",
                    "used",
                    "new"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each experiment is run five times with averaged results reported. The primary evaluation metric used in this study is macro F1. We also report positive class F1 in the binary settings and per-class F1 for the multi-class and multi-label settings.</p>\n\n",
                "matched_terms": [
                    "report",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "against",
                    "context",
                    "hatred",
                    "latent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "does",
                    "hatred",
                    "hate",
                    "their",
                    "compared",
                    "context",
                    "which",
                    "all",
                    "such",
                    "both",
                    "when",
                    "speech",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "due",
                    "does",
                    "against",
                    "minority",
                    "context",
                    "all",
                    "from",
                    "both",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "post",
                    "into",
                    "complex",
                    "context",
                    "from",
                    "both",
                    "when",
                    "suggesting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the influence of incorporating LLM generated context on model behaviour, we compare variance between the predictions of the zero context model and the best-performing <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, rather than solely against the true labels.</p>\n\n",
                "matched_terms": [
                    "against",
                    "context",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "communists",
                    "yes",
                    "does",
                    "post",
                    "many",
                    "which",
                    "context",
                    "fight",
                    "all",
                    "more",
                    "such",
                    "when",
                    "kill"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, 187 non-hateful posts (4.4%) were incorrectly classified by the Embed &amp; Concat system but not by the zero-context one.\nPrompting the LLM to relate generated context to potential hate speech may lead to inventing hateful context where none exists, consequently leading to the over-estimation of non-hateful posts as hateful.\nThe post &#8220;<span class=\"ltx_text ltx_font_italic\">this is what becoming a groyper does to your life people, it is an irreconcilable decision</span>&#8221; is a warning of the damage that falling into right-wing conspiracist groups can do to someone, but is mistakenly classified as hateful once lengthy context about <span class=\"ltx_text ltx_font_italic\">groypers</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Hawley (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib19\" title=\"\">2021</a>)</cite> is added, associating them with hateful ideologies and leading the classifier to over-represent this inferred hate.</p>\n\n",
                "matched_terms": [
                    "groypers",
                    "does",
                    "life",
                    "what",
                    "ideologies",
                    "this",
                    "your",
                    "context",
                    "someone",
                    "becoming",
                    "into",
                    "speech",
                    "groups",
                    "post",
                    "hate",
                    "people",
                    "generated",
                    "irreconcilable",
                    "groyper",
                    "decision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">68 misogynous memes (3.1%) were classified correctly with no context but incorrectly by the Embed &amp; Concat system. These generally fall under the aforementioned issue of adding irrelevant or incorrect context that obfuscates previously clearer hate.\nIn the example in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F1\" title=\"Figure 1 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the generated context does not recognise Bill Cosby in the image and misses the implication of Drug-Facilitated Sexual Assault <cite class=\"ltx_cite ltx_citemacro_cite\">Butler and Welch (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib4\" title=\"\">2009</a>)</cite>, leading the model to misclassify it as not misogynous.</p>\n\n",
                "matched_terms": [
                    "does",
                    "context",
                    "hate",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n",
                "matched_terms": [
                    "often",
                    "potentially",
                    "many",
                    "context",
                    "which",
                    "from",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while our results demonstrates the promise of using LLM-generated context in HSD, our error analysis highlights some of its pitfalls. Most pressingly, we observe that additional context can cause semantic drift in the original post, occasionally neutralising hateful intent or inventing false hateful connections. This underscores the need for careful context selection and integration to maximise benefits and mitigate errors.</p>\n\n",
                "matched_terms": [
                    "post",
                    "its",
                    "some",
                    "analysis",
                    "context",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond demonstrating increased performance metrics, this research addresses important considerations regarding the role of background context in HSD.\nWhile full-text prompting outperformed named-entity-based approaches, both occasionally introduced spurious associations, impacting performance and suggesting that overcontextualisation, where generated background information dilutes or reinterprets the original intent, can be just as problematic as undercontextualisation.\nError analysis reinforces this view, with many false positives arising from benign posts being supplemented with highly charged or misleading contextual information, while false negatives often occurred when clearly hateful statements were augmented with neutralising or overly-explanatory context. These patterns reveal a trade-off between providing richer background information and maintaining original intent.\nFuture models may aim to strike the right balance through dynamic weighting or more sophisticated attention mechanisms to weigh the impacts of post and context.</p>\n\n",
                "matched_terms": [
                    "being",
                    "post",
                    "often",
                    "many",
                    "analysis",
                    "context",
                    "generated",
                    "more",
                    "from",
                    "both",
                    "when",
                    "suggesting",
                    "future",
                    "information",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, future work in this domain could explore a wider range of prompting strategies, in particular, utilising online-search and reasoning-capable LLMs to generate higher-quality background context.\nFurthermore, hybrid systems that combine generated context with verifiable domain knowledge may help to detect hateful references that even LLMs can overlook and guarantee more factual grounding <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. For instance, known extremist references could be identified by linking to curated hate-related knowledge bases, such as <cite class=\"ltx_cite ltx_citemacro_citet\">GPAHE (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib3\" title=\"\">2023</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Global Extremist Symbols Database</span> and integrating discovered information into the LLM prompt.</p>\n\n",
                "matched_terms": [
                    "could",
                    "into",
                    "references",
                    "known",
                    "context",
                    "generated",
                    "more",
                    "such",
                    "future",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "does",
                    "hatred",
                    "hate",
                    "potentially",
                    "context",
                    "generated",
                    "more",
                    "such",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research investigated the use of LLMs as dynamic knowledge bases to generate background context aimed at improving the detection of (implicit) hate speech across textual and multimodal domains. By comparing two prompting strategies for context generation and four distinct methods of integrating context into a classification pipeline, we systematically evaluated the impact of contextual enhancement on HSD performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "into",
                    "context",
                    "hate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing our research questions, our findings indicate, firstly, that LLM-based context generation can outperform static entity-linking-based approaches, as evidenced by consistent improvements across both binary and fine-grained classification tasks in textual and multimodal settings (RQ1).\nWhile named-entity-based strategies showed marginal gains, incorporating context generated via full-text prompting outperformed baselines in the vast majority of settings.\nResults show that both the quality of generated context and the method by which it is incorporated into model inputs are critical to achieving improvements.</p>\n\n",
                "matched_terms": [
                    "into",
                    "which",
                    "context",
                    "generated",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, we find that embedding-level concatenation (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) outperformed other incorporation methods, demonstrating the importance of maintaining separation between original content and additional background context (RQ2).\nIntegrating context directly into the original text through use of an LLM (<span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>) also beat baseline scores in the multimodal setting and lead to strong results when incorporating named-entity context in the textual setting, underscoring the future potential of this approach.</p>\n\n",
                "matched_terms": [
                    "future",
                    "into",
                    "context",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "its",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite promising results, this research is subject to limitations that should be considered when interpreting the findings or generalising them to broader settings.\nPrimarily, we relied on a single LLM (<span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>) and a fixed set of simple, natural-language prompts to generate background context. Alternative models, prompt formulations, or temperature settings could produce substantially different context and thus alter downstream performance.</p>\n\n",
                "matched_terms": [
                    "could",
                    "considered",
                    "context",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our evaluation is limited to two English-language datasets collected over relatively brief time spans. This could limit generalisability to multilingual or non-English settings, as well as temporal generalisability, particularly as the language and references used in online posts and memes changes rapidly and dramatically over time (see <cite class=\"ltx_cite ltx_citemacro_citet\">Kostadinovska-Stojchevska and Shalevska (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib23\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Joshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib21\" title=\"\">2024</a>)</cite> for more on this).\nMoreover, human annotations, particularly for implicit hate, are inherently subjective, which may introduce bias into annotated datasets.\nOur error analysis revealed several instances that appeared mislabelled or open to multiple valid interpretations in the Latent Hatred dataset. These issues are discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS4\" title=\"D. Doubtful Labels in Latent Hatred &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "changes",
                    "could",
                    "into",
                    "hatred",
                    "see",
                    "language",
                    "references",
                    "analysis",
                    "hate",
                    "which",
                    "well",
                    "more",
                    "relatively",
                    "used",
                    "online",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, reliance on macro F1 scores, though useful as an overall performance indicator, can obscure important class-specific trends and failure modes.\nFuture evaluations may benefit from more fine-grained analysis or the development of dedicated challenge datasets to better assess robustness to various phenomena, including the aforementioned linguistic development in memes.</p>\n\n",
                "matched_terms": [
                    "challenge",
                    "analysis",
                    "more",
                    "from",
                    "future"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As argued by <cite class=\"ltx_cite ltx_citemacro_citet\">Wong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib43\" title=\"\">2024</a>)</cite>, technical HSD research has advanced rapidly but studies often underemphasise ethical and societal implications, leading to a lack of real-world impact.\nWhile automatic HSD systems are undoubtedly valuable for identifying and combating hateful content online, their deployment raises substantial ethical concerns that must be addressed alongside technical development.</p>\n\n",
                "matched_terms": [
                    "often",
                    "realworld",
                    "their",
                    "has",
                    "online"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Firstly and most importantly, HSD systems, if implemented inadequately, can limit freedom of expression, a fundamental right <cite class=\"ltx_cite ltx_citemacro_cite\">Council of Europe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nWithout frequent assessment and transparent appeal mechanisms, there is a risk of these models being weaponised to suppress lawful content&#8212;either inadvertently through biased data and imprecise definitions, or deliberately by actors seeking to limit public discourse on certain topics <cite class=\"ltx_cite ltx_citemacro_cite\">Funk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib14\" title=\"\">2023</a>)</cite>.\nThe ease with which a seemingly robust HSD system can be adapted to identify and suppress any kind of <span class=\"ltx_text ltx_font_italic\">undesirable</span> content, merely by retraining it with different labels, should not be underestimated.</p>\n\n",
                "matched_terms": [
                    "being",
                    "which",
                    "definitions",
                    "public"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "could",
                    "into",
                    "analysis",
                    "context",
                    "generated",
                    "more",
                    "such",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Processed data, including generated background context, and code to reproduce our experiments are available in an anonymous repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a>.</p>\n\n",
                "matched_terms": [
                    "context",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our replication of <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> methodology was unable to validate their results, instead demonstrating a slight drop in macro F1 score from the zero-context baseline for both binary and multi-class classification.\nOne major issue with Lin&#8217;s approach is REL itself, which struggles to identify entities that are not capitalised in the &#8220;proper&#8221; way &#8212; a substantial issue when working with informal social media data. Lin does not explicitly mention the number of entities discovered, but our reproduction finds that only 9,595 of 21,480 posts (44.67%) were linked to one or more entities.\nManual inspection of a random sample of 100 posts (containing 79 identified tags) found 28 extra named entities missed by REL in Lin&#8217;s approach, along with 18 incorrectly linked named entities.</p>\n\n",
                "matched_terms": [
                    "does",
                    "their",
                    "which",
                    "more",
                    "from",
                    "both",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "hatred",
                    "often",
                    "hate",
                    "references",
                    "political",
                    "context",
                    "which",
                    "generated",
                    "have",
                    "either",
                    "both",
                    "from",
                    "speech",
                    "used",
                    "likely",
                    "examples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "into",
                    "which",
                    "have",
                    "from",
                    "cost",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "does",
                    "often",
                    "potentially",
                    "their",
                    "news",
                    "now",
                    "has",
                    "exist",
                    "aryan",
                    "error",
                    "cease",
                    "germany",
                    "swrm",
                    "fight",
                    "future",
                    "new",
                    "tweet",
                    "jews",
                    "jerusalem",
                    "hatred",
                    "references",
                    "analysis",
                    "men",
                    "women",
                    "children",
                    "latent",
                    "will",
                    "white",
                    "rise",
                    "hate",
                    "race",
                    "many",
                    "heritage",
                    "israeli",
                    "more",
                    "either",
                    "berlin",
                    "choose",
                    "moving"
                ]
            }
        ]
    },
    "Sx6.T8": {
        "caption": "Table 8: Generated image descriptions and context of example MAMI memes.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">Meme</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">Image Description</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Context</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"584\" id=\"Sx6.T8.g1\" src=\"images/4866.jpg\" width=\"598\"/>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;A cat reaches up to high-five a person&#8217;s hand.&#8221;</span></span>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;The meme uses an image of a cat \"high-fiving\" a person&#8217;s hand. Text is overlaid on the image to indicate the cat is setting the table and the person is the poster&#8217;s mom, who is cooking the food. The context is likely intended to be humorous, portraying the traditional division of labour within a household, where the mother cooks and other family members handle simpler tasks. It is a lighthearted observation of domestic roles.&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"598\" id=\"Sx6.T8.g2\" src=\"images/10984.jpg\" width=\"598\"/>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;A crying woman is holding her hand to her forehead.&#8221;</span></span>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;This meme is using the \"Woman Crying\" meme format. The text suggests a joke about domestic violence against women. The humor relies on the idea that the woman has been physically abused (given two black eyes) and that further communication would be pointless or repetitive.&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" height=\"794\" id=\"Sx6.T8.g3\" src=\"images/1322.jpg\" width=\"598\"/>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;A woman in red heels and jean shorts leans over and looks at the exposed engine of a car.&#8221;</span></span>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;The meme features a woman in a pin-up style outfit looking at a car engine. The text \"What am I looking at? This isn&#8217;t the kitchen\" is superimposed on the image. It is a joke based on a sexist stereotype that a woman&#8217;s place is in the kitchen, and therefore she would not know anything about cars.&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"418\" id=\"Sx6.T8.g4\" src=\"images/4970.jpg\" width=\"598\"/>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;A smiling woman holds a mug while a man drinks from one.&#8221;</span></span>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;The meme features a picture of Carl Winslow and his wife Harriette from the sitcom <span class=\"ltx_text ltx_font_italic\">Family Matters</span>. The text suggests a scenario where someone is initially happy or carefree (\"laughing\") but will face negative consequences or regret after consuming the drink. It is likely used to humorously warn against overindulgence or actions that seem fun at the moment but will lead to a negative outcome.&#8221;</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"596\" id=\"Sx6.T8.g5\" src=\"images/8206.jpg\" width=\"598\"/>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:85.4pt;\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;This image shows four illustrations of women, each in a different style and context, conveying the idea that different things empower different women.&#8221;</span></span>\n</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8220;The meme is a four-panel image featuring historical artwork. Each panel displays a different image of a woman, accompanied by a caption describing an action or state of being that purportedly empowers women. It is likely designed to express the idea that feminism supports a variety of choices for women, as their empowerment may come from diverse actions, behaviours, and lifestyles.&#8221;</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "four",
            "fun",
            "their",
            "leans",
            "actions",
            "each",
            "meme",
            "who",
            "likely",
            "accompanied",
            "featuring",
            "being",
            "picture",
            "mother",
            "observation",
            "crying",
            "against",
            "face",
            "one",
            "from",
            "displays",
            "looking",
            "over",
            "moment",
            "happy",
            "designed",
            "her",
            "further",
            "carefree",
            "would",
            "empowerment",
            "where",
            "state",
            "negative",
            "heels",
            "woman",
            "given",
            "smiling",
            "food",
            "other",
            "uses",
            "empower",
            "person",
            "stereotype",
            "roles",
            "format",
            "laughing",
            "pointless",
            "therefore",
            "come",
            "example",
            "mug",
            "man",
            "purportedly",
            "violence",
            "members",
            "not",
            "panel",
            "overindulgence",
            "highfive",
            "shorts",
            "cooking",
            "mami",
            "know",
            "context",
            "suggests",
            "jean",
            "someone",
            "after",
            "lifestyles",
            "descriptions",
            "image",
            "text",
            "memes",
            "about",
            "humor",
            "used",
            "carl",
            "scenario",
            "engine",
            "variety",
            "sexist",
            "cat",
            "overlaid",
            "portraying",
            "exposed",
            "express",
            "fourpanel",
            "initially",
            "will",
            "outfit",
            "persons",
            "description",
            "posters",
            "labour",
            "shows",
            "car",
            "simpler",
            "handle",
            "the",
            "mom",
            "what",
            "sitcom",
            "setting",
            "pinup",
            "illustrations",
            "winslow",
            "black",
            "lighthearted",
            "isnt",
            "tasks",
            "lead",
            "car",
            "supports",
            "harriette",
            "intended",
            "reaches",
            "anything",
            "warn",
            "historical",
            "describing",
            "humorously",
            "features",
            "regret",
            "hand",
            "within",
            "red",
            "idea",
            "artwork",
            "communication",
            "highfiving",
            "eyes",
            "family",
            "cooks",
            "different",
            "diverse",
            "consuming",
            "holding",
            "seem",
            "while",
            "hand",
            "matters",
            "style",
            "feminism",
            "has",
            "two",
            "looks",
            "things",
            "drinks",
            "she",
            "caption",
            "based",
            "household",
            "this",
            "his",
            "joke",
            "consequences",
            "wife",
            "outcome",
            "drink",
            "physically",
            "action",
            "forehead",
            "empowers",
            "indicate",
            "behaviours",
            "holds",
            "place",
            "traditional",
            "relies",
            "cars",
            "conveying",
            "women",
            "choices",
            "superimposed",
            "women",
            "abused",
            "division",
            "humorous",
            "repetitive",
            "generated",
            "kitchen",
            "womans",
            "domestic"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "four",
                    "based",
                    "mami",
                    "context",
                    "setting",
                    "other",
                    "from",
                    "text",
                    "memes",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "while",
                    "based",
                    "context",
                    "text",
                    "from",
                    "has",
                    "used",
                    "not",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "while",
                    "based",
                    "mami",
                    "context",
                    "setting",
                    "traditional",
                    "generated",
                    "further",
                    "about",
                    "different",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To what extent is a pre-trained LLM more effective than static entity linking at generating background context for implicit HSD?</p>\n\n",
                "matched_terms": [
                    "context",
                    "what"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While hate speech is inherently contextual <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, previous studies have shown that incorporating generic additional context into the model input often leads to a negligible or negative impact on model performance <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>. These findings suggest that context must be carefully selected, represented, and integrated to avoid introducing noise. This section discusses previous work relevant to HSD in context retrieval and incorporation, the use of LLMs, and the challenges surrounding multimodality.</p>\n\n",
                "matched_terms": [
                    "while",
                    "context",
                    "negative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "descriptions",
                    "their",
                    "context",
                    "sexist",
                    "from",
                    "used",
                    "about"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs have been used in HSD previously, generally replacing classifiers themselves, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>)</cite>, who directly compare a generative LLM against two pre-trained models: BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib25\" title=\"\">2019</a>)</cite>. They test various prompting strategies (few-shot, chain-of-thought, etc.) and demonstrate that a generalist LLM with simple prompts can comfortably outperform transformer-based encoders. These results are not confirmed, however, by numerous other studies, who find that LLMs either match or fall short of the performance of encoder-only classifiers, especially without extensive prompt engineering <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Albladi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib1\" title=\"\">2025</a>)</cite> discuss these results, highlighting the fact that encoder models are fine-tuned on the training data and can potentially capture the peculiarities of a given dataset better than generalist LLMs.</p>\n\n",
                "matched_terms": [
                    "against",
                    "who",
                    "given",
                    "other",
                    "used",
                    "not",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While replacing traditional classifiers entirely is a promising approach, this research instead focuses on treating LLMs as dynamic knowledge bases from which to extract contextual information. <cite class=\"ltx_cite ltx_citemacro_citet\">Petroni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib32\" title=\"\">2019</a>)</cite> were among the first to systematically evaluate whether generative models can store factual and relational knowledge. Using cloze-style prompts, they demonstrate that pre-trained models can recall factual information with high accuracy, often outperforming traditional relation extraction systems.\nBuilding on this idea, <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib41\" title=\"\">2020</a>)</cite> propose a method for building structured knowledge graphs directly from LLMs, suggesting that larger models can be used to effectively store and retrieve world knowledge.</p>\n\n",
                "matched_terms": [
                    "while",
                    "idea",
                    "traditional",
                    "from",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "who",
                    "context",
                    "setting",
                    "each",
                    "from",
                    "text",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "seem",
                    "setting",
                    "context",
                    "image",
                    "text",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "four",
                    "based",
                    "their",
                    "context",
                    "setting",
                    "other",
                    "different",
                    "image",
                    "text",
                    "memes",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research employs two open-source, English-language datasets used to train and evaluate our approaches over various setups.</p>\n\n",
                "matched_terms": [
                    "over",
                    "used",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n",
                "matched_terms": [
                    "violence",
                    "while",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our second source is <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, used to detect misogynous memes. This dataset is comprised of 10,995 distinct memes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Five duplicate memes were discovered in MAMI during preprocessing and subsequently removed.</span></span></span> scraped from social media platforms and meme-creation websites.\nThe high level annotation is a simple binary distinction between misogynous<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>A meme is defined as misogynous if it &#8220;conceptually describes an offensive, sexist or hateful scene [&#8230;] having as target a woman or a group of women&#8221;.</span></span></span> and non-misogynous memes.\nIn the second level annotation, misogynous memes are labelled with one or more subtypes of misogyny: shaming, stereotyping, objectification, and violence. These labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T5\" title=\"Table 5 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, with example memes and their labels given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.\nWe again split the data in an 80/20 ratio.</p>\n\n",
                "matched_terms": [
                    "their",
                    "mami",
                    "example",
                    "woman",
                    "given",
                    "sexist",
                    "women",
                    "from",
                    "meme",
                    "violence",
                    "used",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Posts are first analysed for named entities (in one branch of the pipeline), then contextual information is generated and incorporated into vectorised representations, before finally being classified.</p>\n\n",
                "matched_terms": [
                    "being",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the named-entity-based context generation approach, we identify named entities using the uncased <span class=\"ltx_text ltx_font_typewriter\">dslim/bert-base-NER</span> model <cite class=\"ltx_cite ltx_citemacro_cite\">Lim (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib4\" title=\"\">2021</a>)</cite>, a fine-tuned BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> system trained on Named Entity Recognition (NER), chosen based on its high performance on the CONLL-2003 benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">Tjong Kim&#160;Sang and De&#160;Meulder (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib38\" title=\"\">2003</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "based",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ Google&#8217;s <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> <cite class=\"ltx_cite ltx_citemacro_cite\">DeepMind (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib2\" title=\"\">2024</a>)</cite> model for all experiments; a decision driven primarily by its speed and cost-efficiency.\nThis model can be prompted through a useful batch API, allowing for efficient processing of many requests in parallel. This was considered essential, given the need to generate context for thousands of instances, often multiple times to experiment with consistency.\nWhile comparable LLMs, such as <cite class=\"ltx_cite ltx_citemacro_citet\">OpenAI (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib5\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span> or <cite class=\"ltx_cite ltx_citemacro_citet\">Anthropic (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib1\" title=\"\">2024</a>)</cite> <span class=\"ltx_text ltx_font_typewriter\">Claude Sonnet 3.7</span>, may offer similar capabilities, Gemini&#8217;s high score-to-cost ratio, highlighted by LMArena&#8217;s benchmark evaluations <cite class=\"ltx_cite ltx_citemacro_cite\">Chiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib5\" title=\"\">2024</a>)</cite>, made it an attractive choice.</p>\n\n",
                "matched_terms": [
                    "given",
                    "while",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Maintaining consistency with previous work, we implement a simple MLP classifier, mirroring the architecture outlined by <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The network uses three hidden layers of dimension 512 with ReLU activations, and was trained in each experiment for 500 epochs with the Adam optimiser and a learning rate of 0.001.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the effectiveness of our LLM-based context-enhanced approaches and compare against the results obtained by replicating the approaches of previous studies, we establish four baseline systems.</p>\n\n",
                "matched_terms": [
                    "against",
                    "four"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Context:</span> The first configuration uses only the raw post, simply creating SBERT embeddings from each post in the dataset and feeding them directly into the input of the MLP classification model.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">REL:</span> The second baseline replicates <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> entity-linking strategy, augmenting post text with Wikipedia-derived summaries based on named entities. Posts are processed using REL <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite>, with identified entities mapped to corresponding Wikipedia articles. The first two sentences of these articles are then appended to the original post. The combination is encoded using SBERT and given to the classifier, leaving posts without (discovered) named entities unchanged.</p>\n\n",
                "matched_terms": [
                    "given",
                    "text",
                    "based",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ConceptNet:</span> The third baseline augments posts using ConceptNet&#8217;s Numberbatch embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>, replicating the methodology outlined by <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>. Entities are extracted using n-gram matching before being mapped to ConceptNet embeddings. These vectors are then averaged and normalised, forming a single contextual representation which is concatenated with the SBERT embedding of the original text, resulting in a 1,068-dimensional input vector.</p>\n\n",
                "matched_terms": [
                    "text",
                    "being"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Prediction:</span> The final baseline system forgoes embeddings and training entirely, instead using the generative LLM directly as a classifier.\nThis setup aims to determine whether, if given access to an LLM like <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>, such a model is better utilised for generating context or directly for prediction. The LLM is prompted solely with each post and the potential labels and asked to predict binary, multi-class and multi-label classes.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>Full prompt texts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS5\" title=\"E. LLM Prompts for Context Generation and Incorporation &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "given",
                    "each",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "based",
                    "context",
                    "designed",
                    "text",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "given",
                    "context",
                    "each",
                    "about",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once context has been generated, we test four strategies to incorporate it into the model input, dubbed i) <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>, ii) <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>, iii) <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, and iv) <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>.</p>\n\n",
                "matched_terms": [
                    "has",
                    "four",
                    "context",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "their",
                    "context",
                    "generated",
                    "from",
                    "used",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embed &amp; Concat</span> tests the inverse of the previous strategy, vectorising the post and context separately, then concatenating them. This preserves a greater separation between post and context, potentially aiding the classifier in distinguishing their respective contributions.</p>\n\n",
                "matched_terms": [
                    "their",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context-Embed</span> follows <cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> methodology, fusing post and context hierarchically at the embedding layer before encoding, leading to a deeper integration of context and post than simple concatenation at the input or embedding level. In this configuration, the context is encoded with SBERT, while the post is tokenized and passed through the word embedding matrix of another SBERT model. The context vector is projected to match the dimensionality of token embeddings and prepended to the post embedding sequence. This expanded sequence is then passed through the remaining layers of the encoder. The resulting representation is then pooled before being passed to the classifier.</p>\n\n",
                "matched_terms": [
                    "while",
                    "context",
                    "being"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "against",
                    "context",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "four",
                    "mami",
                    "context",
                    "setting",
                    "description",
                    "text",
                    "each",
                    "from",
                    "further",
                    "meme",
                    "used",
                    "image",
                    "not",
                    "memes",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each experiment is run five times with averaged results reported. The primary evaluation metric used in this study is macro F1. We also report positive class F1 in the binary settings and per-class F1 for the multi-class and multi-label settings.</p>\n\n",
                "matched_terms": [
                    "used",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "against",
                    "mami",
                    "context",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "their",
                    "given",
                    "context",
                    "setting",
                    "other",
                    "lead",
                    "text",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "over",
                    "while",
                    "within",
                    "against",
                    "context",
                    "setting",
                    "other",
                    "from",
                    "tasks",
                    "further",
                    "memes",
                    "not",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "simpler",
                    "while",
                    "indicate",
                    "context",
                    "from",
                    "tasks",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the influence of incorporating LLM generated context on model behaviour, we compare variance between the predictions of the zero context model and the best-performing <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, rather than solely against the true labels.</p>\n\n",
                "matched_terms": [
                    "against",
                    "context",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "while",
                    "example",
                    "context",
                    "not",
                    "negative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, 187 non-hateful posts (4.4%) were incorrectly classified by the Embed &amp; Concat system but not by the zero-context one.\nPrompting the LLM to relate generated context to potential hate speech may lead to inventing hateful context where none exists, consequently leading to the over-estimation of non-hateful posts as hateful.\nThe post &#8220;<span class=\"ltx_text ltx_font_italic\">this is what becoming a groyper does to your life people, it is an irreconcilable decision</span>&#8221; is a warning of the damage that falling into right-wing conspiracist groups can do to someone, but is mistakenly classified as hateful once lengthy context about <span class=\"ltx_text ltx_font_italic\">groypers</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Hawley (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib19\" title=\"\">2021</a>)</cite> is added, associating them with hateful ideologies and leading the classifier to over-represent this inferred hate.</p>\n\n",
                "matched_terms": [
                    "what",
                    "this",
                    "context",
                    "someone",
                    "generated",
                    "lead",
                    "about",
                    "not",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">68 misogynous memes (3.1%) were classified correctly with no context but incorrectly by the Embed &amp; Concat system. These generally fall under the aforementioned issue of adding irrelevant or incorrect context that obfuscates previously clearer hate.\nIn the example in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F1\" title=\"Figure 1 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the generated context does not recognise Bill Cosby in the image and misses the implication of Drug-Facilitated Sexual Assault <cite class=\"ltx_cite ltx_citemacro_cite\">Butler and Welch (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib4\" title=\"\">2009</a>)</cite>, leading the model to misclassify it as not misogynous.</p>\n\n",
                "matched_terms": [
                    "example",
                    "context",
                    "generated",
                    "image",
                    "not",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n",
                "matched_terms": [
                    "context",
                    "from",
                    "meme",
                    "used",
                    "not",
                    "memes",
                    "two",
                    "negative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while our results demonstrates the promise of using LLM-generated context in HSD, our error analysis highlights some of its pitfalls. Most pressingly, we observe that additional context can cause semantic drift in the original post, occasionally neutralising hateful intent or inventing false hateful connections. This underscores the need for careful context selection and integration to maximise benefits and mitigate errors.</p>\n\n",
                "matched_terms": [
                    "while",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond demonstrating increased performance metrics, this research addresses important considerations regarding the role of background context in HSD.\nWhile full-text prompting outperformed named-entity-based approaches, both occasionally introduced spurious associations, impacting performance and suggesting that overcontextualisation, where generated background information dilutes or reinterprets the original intent, can be just as problematic as undercontextualisation.\nError analysis reinforces this view, with many false positives arising from benign posts being supplemented with highly charged or misleading contextual information, while false negatives often occurred when clearly hateful statements were augmented with neutralising or overly-explanatory context. These patterns reveal a trade-off between providing richer background information and maintaining original intent.\nFuture models may aim to strike the right balance through dynamic weighting or more sophisticated attention mechanisms to weigh the impacts of post and context.</p>\n\n",
                "matched_terms": [
                    "while",
                    "being",
                    "context",
                    "generated",
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, future work in this domain could explore a wider range of prompting strategies, in particular, utilising online-search and reasoning-capable LLMs to generate higher-quality background context.\nFurthermore, hybrid systems that combine generated context with verifiable domain knowledge may help to detect hateful references that even LLMs can overlook and guarantee more factual grounding <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. For instance, known extremist references could be identified by linking to curated hate-related knowledge bases, such as <cite class=\"ltx_cite ltx_citemacro_citet\">GPAHE (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib3\" title=\"\">2023</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Global Extremist Symbols Database</span> and integrating discovered information into the LLM prompt.</p>\n\n",
                "matched_terms": [
                    "context",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "over",
                    "while",
                    "context",
                    "generated",
                    "tasks",
                    "from",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research investigated the use of LLMs as dynamic knowledge bases to generate background context aimed at improving the detection of (implicit) hate speech across textual and multimodal domains. By comparing two prompting strategies for context generation and four distinct methods of integrating context into a classification pipeline, we systematically evaluated the impact of contextual enhancement on HSD performance.</p>\n\n",
                "matched_terms": [
                    "four",
                    "context",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing our research questions, our findings indicate, firstly, that LLM-based context generation can outperform static entity-linking-based approaches, as evidenced by consistent improvements across both binary and fine-grained classification tasks in textual and multimodal settings (RQ1).\nWhile named-entity-based strategies showed marginal gains, incorporating context generated via full-text prompting outperformed baselines in the vast majority of settings.\nResults show that both the quality of generated context and the method by which it is incorporated into model inputs are critical to achieving improvements.</p>\n\n",
                "matched_terms": [
                    "while",
                    "indicate",
                    "context",
                    "generated",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, we find that embedding-level concatenation (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) outperformed other incorporation methods, demonstrating the importance of maintaining separation between original content and additional background context (RQ2).\nIntegrating context directly into the original text through use of an LLM (<span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>) also beat baseline scores in the multimodal setting and lead to strong results when incorporating named-entity context in the textual setting, underscoring the future potential of this approach.</p>\n\n",
                "matched_terms": [
                    "setting",
                    "context",
                    "other",
                    "lead",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "variety",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite promising results, this research is subject to limitations that should be considered when interpreting the findings or generalising them to broader settings.\nPrimarily, we relied on a single LLM (<span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>) and a fixed set of simple, natural-language prompts to generate background context. Alternative models, prompt formulations, or temperature settings could produce substantially different context and thus alter downstream performance.</p>\n\n",
                "matched_terms": [
                    "different",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our evaluation is limited to two English-language datasets collected over relatively brief time spans. This could limit generalisability to multilingual or non-English settings, as well as temporal generalisability, particularly as the language and references used in online posts and memes changes rapidly and dramatically over time (see <cite class=\"ltx_cite ltx_citemacro_citet\">Kostadinovska-Stojchevska and Shalevska (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib23\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Joshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib21\" title=\"\">2024</a>)</cite> for more on this).\nMoreover, human annotations, particularly for implicit hate, are inherently subjective, which may introduce bias into annotated datasets.\nOur error analysis revealed several instances that appeared mislabelled or open to multiple valid interpretations in the Latent Hatred dataset. These issues are discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS4\" title=\"D. Doubtful Labels in Latent Hatred &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "over",
                    "further",
                    "used",
                    "memes",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, reliance on macro F1 scores, though useful as an overall performance indicator, can obscure important class-specific trends and failure modes.\nFuture evaluations may benefit from more fine-grained analysis or the development of dedicated challenge datasets to better assess robustness to various phenomena, including the aforementioned linguistic development in memes.</p>\n\n",
                "matched_terms": [
                    "from",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As argued by <cite class=\"ltx_cite ltx_citemacro_citet\">Wong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib43\" title=\"\">2024</a>)</cite>, technical HSD research has advanced rapidly but studies often underemphasise ethical and societal implications, leading to a lack of real-world impact.\nWhile automatic HSD systems are undoubtedly valuable for identifying and combating hateful content online, their deployment raises substantial ethical concerns that must be addressed alongside technical development.</p>\n\n",
                "matched_terms": [
                    "has",
                    "their",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Firstly and most importantly, HSD systems, if implemented inadequately, can limit freedom of expression, a fundamental right <cite class=\"ltx_cite ltx_citemacro_cite\">Council of Europe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib7\" title=\"\">2025</a>)</cite>.\nWithout frequent assessment and transparent appeal mechanisms, there is a risk of these models being weaponised to suppress lawful content&#8212;either inadvertently through biased data and imprecise definitions, or deliberately by actors seeking to limit public discourse on certain topics <cite class=\"ltx_cite ltx_citemacro_cite\">Funk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib14\" title=\"\">2023</a>)</cite>.\nThe ease with which a seemingly robust HSD system can be adapted to identify and suppress any kind of <span class=\"ltx_text ltx_font_italic\">undesirable</span> content, merely by retraining it with different labels, should not be underestimated.</p>\n\n",
                "matched_terms": [
                    "different",
                    "not",
                    "being"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "setting",
                    "context",
                    "generated",
                    "shows",
                    "about",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Processed data, including generated background context, and code to reproduce our experiments are available in an anonymous repository at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a>.</p>\n\n",
                "matched_terms": [
                    "context",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our replication of <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> methodology was unable to validate their results, instead demonstrating a slight drop in macro F1 score from the zero-context baseline for both binary and multi-class classification.\nOne major issue with Lin&#8217;s approach is REL itself, which struggles to identify entities that are not capitalised in the &#8220;proper&#8221; way &#8212; a substantial issue when working with informal social media data. Lin does not explicitly mention the number of entities discovered, but our reproduction finds that only 9,595 of 21,480 posts (44.67%) were linked to one or more entities.\nManual inspection of a random sample of 100 posts (containing 79 identified tags) found 28 extra named entities missed by REL in Lin&#8217;s approach, along with 18 incorrectly linked named entities.</p>\n\n",
                "matched_terms": [
                    "their",
                    "not",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "while",
                    "based",
                    "used",
                    "within",
                    "mami",
                    "given",
                    "context",
                    "generated",
                    "shows",
                    "from",
                    "memes",
                    "meme",
                    "descriptions",
                    "image",
                    "text",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "description",
                    "from",
                    "meme",
                    "would",
                    "image",
                    "text",
                    "memes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "will",
                    "while",
                    "their",
                    "women",
                    "each",
                    "has",
                    "not",
                    "where"
                ]
            }
        ]
    },
    "Sx6.T9": {
        "caption": "Table 9: Macro-averaged Precision, Recall, and F1 scores for binary and implicit HSD on the Latent Hatred dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Context</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Incorporation</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Strategy</span></span></span>\n</span> </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Binary</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multi-Class</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">P</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">R</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">F1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">P</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">R</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">F1</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Zero-Context</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.73</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.72</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.72</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.51</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.51</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.51</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">REL </span><cite class=\"ltx_cite ltx_citemacro_cite\">Lin <span class=\"ltx_text\" style=\"font-size:90%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.48</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ConceptNet </span><cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. <span class=\"ltx_text\" style=\"font-size:90%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.74</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.74</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.74</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Prediction</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.74</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.40</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.28</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.26</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Named Entities</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.73</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.73</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.73</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Context-Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.26</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.24</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.25</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Enhance</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.72</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.49</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.49</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.49</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Full Text</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.73</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.72</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.73</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.54</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.53</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.53</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.75</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.75</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.75</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.52</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.52</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.52</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Context-Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Enhance</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.71</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:23.6pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "full",
            "strategy",
            "embed",
            "concat",
            "entities",
            "enhance",
            "implicit",
            "context",
            "llm",
            "prediction",
            "append",
            "conceptnet",
            "text",
            "scores",
            "lin",
            "hatred",
            "hsd",
            "rel",
            "elsherief",
            "incorporation",
            "recall",
            "binary",
            "latent",
            "multiclass",
            "named",
            "contextembed",
            "zerocontext",
            "macroaveraged",
            "precision",
            "dataset"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "named",
                    "entities",
                    "hatred",
                    "implicit",
                    "hsd",
                    "context",
                    "zerocontext",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "implicit",
                    "hsd",
                    "elsherief",
                    "context",
                    "zerocontext",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "implicit",
                    "hsd",
                    "elsherief",
                    "llm",
                    "context",
                    "binary",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To what extent is a pre-trained LLM more effective than static entity linking at generating background context for implicit HSD?</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hsd",
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How can background context be effectively incorporated into a HSD model input?</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How useful are these generation and incorporation methods for textual and multimodal HSD across binary and multi-class settings?</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "multiclass",
                    "binary",
                    "hsd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While hate speech is inherently contextual <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, previous studies have shown that incorporating generic additional context into the model input often leads to a negligible or negative impact on model performance <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>. These findings suggest that context must be carefully selected, represented, and integrated to avoid introducing noise. This section discusses previous work relevant to HSD in context retrieval and incorporation, the use of LLMs, and the challenges surrounding multimodality.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "hsd",
                    "context",
                    "incorporation",
                    "binary",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "lin",
                    "multiclass",
                    "named",
                    "entities",
                    "hsd",
                    "rel",
                    "elsherief",
                    "context",
                    "prediction",
                    "binary",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs have been used in HSD previously, generally replacing classifiers themselves, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>)</cite>, who directly compare a generative LLM against two pre-trained models: BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib25\" title=\"\">2019</a>)</cite>. They test various prompting strategies (few-shot, chain-of-thought, etc.) and demonstrate that a generalist LLM with simple prompts can comfortably outperform transformer-based encoders. These results are not confirmed, however, by numerous other studies, who find that LLMs either match or fall short of the performance of encoder-only classifiers, especially without extensive prompt engineering <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Albladi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib1\" title=\"\">2025</a>)</cite> discuss these results, highlighting the fact that encoder models are fine-tuned on the training data and can potentially capture the peculiarities of a given dataset better than generalist LLMs.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "llm",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "hsd",
                    "context",
                    "append",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hsd",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "named",
                    "full",
                    "entities",
                    "llm",
                    "context",
                    "incorporation",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "multiclass",
                    "hatred",
                    "implicit",
                    "elsherief",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our second source is <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, used to detect misogynous memes. This dataset is comprised of 10,995 distinct memes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Five duplicate memes were discovered in MAMI during preprocessing and subsequently removed.</span></span></span> scraped from social media platforms and meme-creation websites.\nThe high level annotation is a simple binary distinction between misogynous<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>A meme is defined as misogynous if it &#8220;conceptually describes an offensive, sexist or hateful scene [&#8230;] having as target a woman or a group of women&#8221;.</span></span></span> and non-misogynous memes.\nIn the second level annotation, misogynous memes are labelled with one or more subtypes of misogyny: shaming, stereotyping, objectification, and violence. These labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T5\" title=\"Table 5 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, with example memes and their labels given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.\nWe again split the data in an 80/20 ratio.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Posts are first analysed for named entities (in one branch of the pipeline), then contextual information is generated and incorporated into vectorised representations, before finally being classified.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the named-entity-based context generation approach, we identify named entities using the uncased <span class=\"ltx_text ltx_font_typewriter\">dslim/bert-base-NER</span> model <cite class=\"ltx_cite ltx_citemacro_cite\">Lim (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib4\" title=\"\">2021</a>)</cite>, a fine-tuned BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> system trained on Named Entity Recognition (NER), chosen based on its high performance on the CONLL-2003 benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">Tjong Kim&#160;Sang and De&#160;Meulder (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib38\" title=\"\">2003</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Context:</span> The first configuration uses only the raw post, simply creating SBERT embeddings from each post in the dataset and feeding them directly into the input of the MLP classification model.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "zerocontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">REL:</span> The second baseline replicates <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> entity-linking strategy, augmenting post text with Wikipedia-derived summaries based on named entities. Posts are processed using REL <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite>, with identified entities mapped to corresponding Wikipedia articles. The first two sentences of these articles are then appended to the original post. The combination is encoded using SBERT and given to the classifier, leaving posts without (discovered) named entities unchanged.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "named",
                    "entities",
                    "rel",
                    "strategy",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ConceptNet:</span> The third baseline augments posts using ConceptNet&#8217;s Numberbatch embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>, replicating the methodology outlined by <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>. Entities are extracted using n-gram matching before being mapped to ConceptNet embeddings. These vectors are then averaged and normalised, forming a single contextual representation which is concatenated with the SBERT embedding of the original text, resulting in a 1,068-dimensional input vector.</p>\n\n",
                "matched_terms": [
                    "text",
                    "conceptnet",
                    "entities",
                    "elsherief"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Prediction:</span> The final baseline system forgoes embeddings and training entirely, instead using the generative LLM directly as a classifier.\nThis setup aims to determine whether, if given access to an LLM like <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>, such a model is better utilised for generating context or directly for prediction. The LLM is prompted solely with each post and the potential labels and asked to predict binary, multi-class and multi-label classes.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>Full prompt texts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS5\" title=\"E. LLM Prompts for Context Generation and Incorporation &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "llm",
                    "context",
                    "prediction",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "named",
                    "full",
                    "entities",
                    "hsd",
                    "context",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "named",
                    "full",
                    "entities",
                    "hatred",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once context has been generated, we test four strategies to incorporate it into the model input, dubbed i) <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>, ii) <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>, iii) <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, and iv) <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "enhance",
                    "llm",
                    "context",
                    "append",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "text",
                    "append",
                    "context",
                    "embed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embed &amp; Concat</span> tests the inverse of the previous strategy, vectorising the post and context separately, then concatenating them. This preserves a greater separation between post and context, potentially aiding the classifier in distinguishing their respective contributions.</p>\n\n",
                "matched_terms": [
                    "context",
                    "strategy",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context-Embed</span> follows <cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> methodology, fusing post and context hierarchically at the embedding layer before encoding, leading to a deeper integration of context and post than simple concatenation at the input or embedding level. In this configuration, the context is encoded with SBERT, while the post is tokenized and passed through the word embedding matrix of another SBERT model. The context vector is projected to match the dimensionality of token embeddings and prepended to the post embedding sequence. This expanded sequence is then passed through the remaining layers of the encoder. The resulting representation is then pooled before being passed to the classifier.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context",
                    "enhance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each experiment is run five times with averaged results reported. The primary evaluation metric used in this study is macro F1. We also report positive class F1 in the binary settings and per-class F1 for the multi-class and multi-label settings.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context",
                    "hatred",
                    "latent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "embed",
                    "concat",
                    "entities",
                    "enhance",
                    "implicit",
                    "context",
                    "llm",
                    "prediction",
                    "conceptnet",
                    "text",
                    "scores",
                    "hatred",
                    "hsd",
                    "rel",
                    "incorporation",
                    "recall",
                    "binary",
                    "latent",
                    "multiclass",
                    "contextembed",
                    "zerocontext",
                    "precision",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities",
                    "rel",
                    "context",
                    "strategy",
                    "llm",
                    "prediction",
                    "zerocontext",
                    "binary",
                    "embed",
                    "concat",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "contextembed",
                    "full",
                    "entities",
                    "context",
                    "llm",
                    "prediction",
                    "append",
                    "binary",
                    "embed",
                    "concat",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the influence of incorporating LLM generated context on model behaviour, we compare variance between the predictions of the zero context model and the best-performing <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, rather than solely against the true labels.</p>\n\n",
                "matched_terms": [
                    "context",
                    "llm",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, 187 non-hateful posts (4.4%) were incorrectly classified by the Embed &amp; Concat system but not by the zero-context one.\nPrompting the LLM to relate generated context to potential hate speech may lead to inventing hateful context where none exists, consequently leading to the over-estimation of non-hateful posts as hateful.\nThe post &#8220;<span class=\"ltx_text ltx_font_italic\">this is what becoming a groyper does to your life people, it is an irreconcilable decision</span>&#8221; is a warning of the damage that falling into right-wing conspiracist groups can do to someone, but is mistakenly classified as hateful once lengthy context about <span class=\"ltx_text ltx_font_italic\">groypers</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Hawley (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib19\" title=\"\">2021</a>)</cite> is added, associating them with hateful ideologies and leading the classifier to over-represent this inferred hate.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "llm",
                    "context",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">68 misogynous memes (3.1%) were classified correctly with no context but incorrectly by the Embed &amp; Concat system. These generally fall under the aforementioned issue of adding irrelevant or incorrect context that obfuscates previously clearer hate.\nIn the example in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F1\" title=\"Figure 1 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the generated context does not recognise Bill Cosby in the image and misses the implication of Drug-Facilitated Sexual Assault <cite class=\"ltx_cite ltx_citemacro_cite\">Butler and Welch (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib4\" title=\"\">2009</a>)</cite>, leading the model to misclassify it as not misogynous.</p>\n\n",
                "matched_terms": [
                    "context",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "context",
                    "embed",
                    "concat",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while our results demonstrates the promise of using LLM-generated context in HSD, our error analysis highlights some of its pitfalls. Most pressingly, we observe that additional context can cause semantic drift in the original post, occasionally neutralising hateful intent or inventing false hateful connections. This underscores the need for careful context selection and integration to maximise benefits and mitigate errors.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond demonstrating increased performance metrics, this research addresses important considerations regarding the role of background context in HSD.\nWhile full-text prompting outperformed named-entity-based approaches, both occasionally introduced spurious associations, impacting performance and suggesting that overcontextualisation, where generated background information dilutes or reinterprets the original intent, can be just as problematic as undercontextualisation.\nError analysis reinforces this view, with many false positives arising from benign posts being supplemented with highly charged or misleading contextual information, while false negatives often occurred when clearly hateful statements were augmented with neutralising or overly-explanatory context. These patterns reveal a trade-off between providing richer background information and maintaining original intent.\nFuture models may aim to strike the right balance through dynamic weighting or more sophisticated attention mechanisms to weigh the impacts of post and context.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, future work in this domain could explore a wider range of prompting strategies, in particular, utilising online-search and reasoning-capable LLMs to generate higher-quality background context.\nFurthermore, hybrid systems that combine generated context with verifiable domain knowledge may help to detect hateful references that even LLMs can overlook and guarantee more factual grounding <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. For instance, known extremist references could be identified by linking to curated hate-related knowledge bases, such as <cite class=\"ltx_cite ltx_citemacro_citet\">GPAHE (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib3\" title=\"\">2023</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Global Extremist Symbols Database</span> and integrating discovered information into the LLM prompt.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "hatred",
                    "implicit",
                    "hsd",
                    "llm",
                    "context",
                    "zerocontext",
                    "binary",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research investigated the use of LLMs as dynamic knowledge bases to generate background context aimed at improving the detection of (implicit) hate speech across textual and multimodal domains. By comparing two prompting strategies for context generation and four distinct methods of integrating context into a classification pipeline, we systematically evaluated the impact of contextual enhancement on HSD performance.</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing our research questions, our findings indicate, firstly, that LLM-based context generation can outperform static entity-linking-based approaches, as evidenced by consistent improvements across both binary and fine-grained classification tasks in textual and multimodal settings (RQ1).\nWhile named-entity-based strategies showed marginal gains, incorporating context generated via full-text prompting outperformed baselines in the vast majority of settings.\nResults show that both the quality of generated context and the method by which it is incorporated into model inputs are critical to achieving improvements.</p>\n\n",
                "matched_terms": [
                    "binary",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, we find that embedding-level concatenation (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) outperformed other incorporation methods, demonstrating the importance of maintaining separation between original content and additional background context (RQ2).\nIntegrating context directly into the original text through use of an LLM (<span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>) also beat baseline scores in the multimodal setting and lead to strong results when incorporating named-entity context in the textual setting, underscoring the future potential of this approach.</p>\n\n",
                "matched_terms": [
                    "enhance",
                    "llm",
                    "context",
                    "incorporation",
                    "embed",
                    "concat",
                    "text",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "strategy",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite promising results, this research is subject to limitations that should be considered when interpreting the findings or generalising them to broader settings.\nPrimarily, we relied on a single LLM (<span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>) and a fixed set of simple, natural-language prompts to generate background context. Alternative models, prompt formulations, or temperature settings could produce substantially different context and thus alter downstream performance.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our evaluation is limited to two English-language datasets collected over relatively brief time spans. This could limit generalisability to multilingual or non-English settings, as well as temporal generalisability, particularly as the language and references used in online posts and memes changes rapidly and dramatically over time (see <cite class=\"ltx_cite ltx_citemacro_citet\">Kostadinovska-Stojchevska and Shalevska (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib23\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Joshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib21\" title=\"\">2024</a>)</cite> for more on this).\nMoreover, human annotations, particularly for implicit hate, are inherently subjective, which may introduce bias into annotated datasets.\nOur error analysis revealed several instances that appeared mislabelled or open to multiple valid interpretations in the Latent Hatred dataset. These issues are discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS4\" title=\"D. Doubtful Labels in Latent Hatred &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "latent",
                    "dataset",
                    "hatred"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our replication of <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> methodology was unable to validate their results, instead demonstrating a slight drop in macro F1 score from the zero-context baseline for both binary and multi-class classification.\nOne major issue with Lin&#8217;s approach is REL itself, which struggles to identify entities that are not capitalised in the &#8220;proper&#8221; way &#8212; a substantial issue when working with informal social media data. Lin does not explicitly mention the number of entities discovered, but our reproduction finds that only 9,595 of 21,480 posts (44.67%) were linked to one or more entities.\nManual inspection of a random sample of 100 posts (containing 79 identified tags) found 28 extra named entities missed by REL in Lin&#8217;s approach, along with 18 incorrectly linked named entities.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "multiclass",
                    "named",
                    "entities",
                    "rel",
                    "zerocontext",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "named",
                    "entities",
                    "hatred",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "latent",
                    "multiclass",
                    "hatred"
                ]
            }
        ]
    },
    "Sx6.T10": {
        "caption": "Table 10: Per-class F1 of implicit multi-class HSD on the Latent Hatred dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Context</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Incorporation</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Strategy</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Incite-</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">ment</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Infer-</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">iority</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Irony</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Stereo-</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">typing</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Threat-</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">ening</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">White</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Grievance</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Zero-Context</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.61</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">REL </span><cite class=\"ltx_cite ltx_citemacro_cite\">Lin <span class=\"ltx_text\" style=\"font-size:90%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.51</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.52</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.57</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ConceptNet </span><cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. <span class=\"ltx_text\" style=\"font-size:90%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.54</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Prediction</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.24</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.39</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.11</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Named Entities</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.52</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.48</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.52</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Context-Embed</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.35</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.19</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.43</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.40</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Enhance</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.52</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.60</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Full Text</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.58</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.57</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Context-Embed</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.52</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.51</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Enhance</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "iority",
            "full",
            "infer",
            "threat",
            "strategy",
            "perclass",
            "embed",
            "concat",
            "entities",
            "enhance",
            "implicit",
            "context",
            "llm",
            "prediction",
            "append",
            "conceptnet",
            "text",
            "lin",
            "ment",
            "hatred",
            "hsd",
            "rel",
            "elsherief",
            "typing",
            "irony",
            "incorporation",
            "stereo",
            "multiclass",
            "latent",
            "grievance",
            "named",
            "white",
            "contextembed",
            "zerocontext",
            "incite",
            "dataset",
            "ening"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "named",
                    "entities",
                    "hatred",
                    "implicit",
                    "hsd",
                    "context",
                    "zerocontext",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "implicit",
                    "hsd",
                    "elsherief",
                    "context",
                    "zerocontext",
                    "irony",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "implicit",
                    "hsd",
                    "elsherief",
                    "llm",
                    "context",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To what extent is a pre-trained LLM more effective than static entity linking at generating background context for implicit HSD?</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hsd",
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How can background context be effectively incorporated into a HSD model input?</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How useful are these generation and incorporation methods for textual and multimodal HSD across binary and multi-class settings?</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "multiclass",
                    "hsd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While hate speech is inherently contextual <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, previous studies have shown that incorporating generic additional context into the model input often leads to a negligible or negative impact on model performance <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>. These findings suggest that context must be carefully selected, represented, and integrated to avoid introducing noise. This section discusses previous work relevant to HSD in context retrieval and incorporation, the use of LLMs, and the challenges surrounding multimodality.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "hsd",
                    "context",
                    "multiclass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "lin",
                    "multiclass",
                    "named",
                    "entities",
                    "hsd",
                    "rel",
                    "elsherief",
                    "context",
                    "prediction",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs have been used in HSD previously, generally replacing classifiers themselves, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>)</cite>, who directly compare a generative LLM against two pre-trained models: BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib25\" title=\"\">2019</a>)</cite>. They test various prompting strategies (few-shot, chain-of-thought, etc.) and demonstrate that a generalist LLM with simple prompts can comfortably outperform transformer-based encoders. These results are not confirmed, however, by numerous other studies, who find that LLMs either match or fall short of the performance of encoder-only classifiers, especially without extensive prompt engineering <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Albladi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib1\" title=\"\">2025</a>)</cite> discuss these results, highlighting the fact that encoder models are fine-tuned on the training data and can potentially capture the peculiarities of a given dataset better than generalist LLMs.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "llm",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "hsd",
                    "context",
                    "append",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hsd",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "named",
                    "full",
                    "entities",
                    "llm",
                    "context",
                    "incorporation",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "latent",
                    "grievance",
                    "white",
                    "hatred",
                    "implicit",
                    "elsherief",
                    "irony",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Posts are first analysed for named entities (in one branch of the pipeline), then contextual information is generated and incorporated into vectorised representations, before finally being classified.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the named-entity-based context generation approach, we identify named entities using the uncased <span class=\"ltx_text ltx_font_typewriter\">dslim/bert-base-NER</span> model <cite class=\"ltx_cite ltx_citemacro_cite\">Lim (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib4\" title=\"\">2021</a>)</cite>, a fine-tuned BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> system trained on Named Entity Recognition (NER), chosen based on its high performance on the CONLL-2003 benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">Tjong Kim&#160;Sang and De&#160;Meulder (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib38\" title=\"\">2003</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Context:</span> The first configuration uses only the raw post, simply creating SBERT embeddings from each post in the dataset and feeding them directly into the input of the MLP classification model.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "zerocontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">REL:</span> The second baseline replicates <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> entity-linking strategy, augmenting post text with Wikipedia-derived summaries based on named entities. Posts are processed using REL <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite>, with identified entities mapped to corresponding Wikipedia articles. The first two sentences of these articles are then appended to the original post. The combination is encoded using SBERT and given to the classifier, leaving posts without (discovered) named entities unchanged.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "named",
                    "entities",
                    "rel",
                    "strategy",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ConceptNet:</span> The third baseline augments posts using ConceptNet&#8217;s Numberbatch embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>, replicating the methodology outlined by <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>. Entities are extracted using n-gram matching before being mapped to ConceptNet embeddings. These vectors are then averaged and normalised, forming a single contextual representation which is concatenated with the SBERT embedding of the original text, resulting in a 1,068-dimensional input vector.</p>\n\n",
                "matched_terms": [
                    "text",
                    "conceptnet",
                    "entities",
                    "elsherief"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Prediction:</span> The final baseline system forgoes embeddings and training entirely, instead using the generative LLM directly as a classifier.\nThis setup aims to determine whether, if given access to an LLM like <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>, such a model is better utilised for generating context or directly for prediction. The LLM is prompted solely with each post and the potential labels and asked to predict binary, multi-class and multi-label classes.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>Full prompt texts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS5\" title=\"E. LLM Prompts for Context Generation and Incorporation &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "llm",
                    "context",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "named",
                    "full",
                    "entities",
                    "hsd",
                    "context",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "named",
                    "full",
                    "entities",
                    "hatred",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once context has been generated, we test four strategies to incorporate it into the model input, dubbed i) <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>, ii) <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>, iii) <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, and iv) <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "enhance",
                    "llm",
                    "context",
                    "append",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "text",
                    "append",
                    "context",
                    "embed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embed &amp; Concat</span> tests the inverse of the previous strategy, vectorising the post and context separately, then concatenating them. This preserves a greater separation between post and context, potentially aiding the classifier in distinguishing their respective contributions.</p>\n\n",
                "matched_terms": [
                    "context",
                    "strategy",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context-Embed</span> follows <cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> methodology, fusing post and context hierarchically at the embedding layer before encoding, leading to a deeper integration of context and post than simple concatenation at the input or embedding level. In this configuration, the context is encoded with SBERT, while the post is tokenized and passed through the word embedding matrix of another SBERT model. The context vector is projected to match the dimensionality of token embeddings and prepended to the post embedding sequence. This expanded sequence is then passed through the remaining layers of the encoder. The resulting representation is then pooled before being passed to the classifier.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context",
                    "enhance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each experiment is run five times with averaged results reported. The primary evaluation metric used in this study is macro F1. We also report positive class F1 in the binary settings and per-class F1 for the multi-class and multi-label settings.</p>\n\n",
                "matched_terms": [
                    "perclass",
                    "multiclass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context",
                    "hatred",
                    "latent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "perclass",
                    "embed",
                    "concat",
                    "entities",
                    "enhance",
                    "implicit",
                    "context",
                    "llm",
                    "prediction",
                    "conceptnet",
                    "text",
                    "hatred",
                    "hsd",
                    "rel",
                    "incorporation",
                    "multiclass",
                    "latent",
                    "contextembed",
                    "zerocontext",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "named",
                    "entities",
                    "rel",
                    "context",
                    "strategy",
                    "llm",
                    "prediction",
                    "zerocontext",
                    "embed",
                    "concat",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "contextembed",
                    "full",
                    "entities",
                    "context",
                    "llm",
                    "prediction",
                    "append",
                    "embed",
                    "concat",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the influence of incorporating LLM generated context on model behaviour, we compare variance between the predictions of the zero context model and the best-performing <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, rather than solely against the true labels.</p>\n\n",
                "matched_terms": [
                    "context",
                    "llm",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, 187 non-hateful posts (4.4%) were incorrectly classified by the Embed &amp; Concat system but not by the zero-context one.\nPrompting the LLM to relate generated context to potential hate speech may lead to inventing hateful context where none exists, consequently leading to the over-estimation of non-hateful posts as hateful.\nThe post &#8220;<span class=\"ltx_text ltx_font_italic\">this is what becoming a groyper does to your life people, it is an irreconcilable decision</span>&#8221; is a warning of the damage that falling into right-wing conspiracist groups can do to someone, but is mistakenly classified as hateful once lengthy context about <span class=\"ltx_text ltx_font_italic\">groypers</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Hawley (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib19\" title=\"\">2021</a>)</cite> is added, associating them with hateful ideologies and leading the classifier to over-represent this inferred hate.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "llm",
                    "context",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">68 misogynous memes (3.1%) were classified correctly with no context but incorrectly by the Embed &amp; Concat system. These generally fall under the aforementioned issue of adding irrelevant or incorrect context that obfuscates previously clearer hate.\nIn the example in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F1\" title=\"Figure 1 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the generated context does not recognise Bill Cosby in the image and misses the implication of Drug-Facilitated Sexual Assault <cite class=\"ltx_cite ltx_citemacro_cite\">Butler and Welch (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib4\" title=\"\">2009</a>)</cite>, leading the model to misclassify it as not misogynous.</p>\n\n",
                "matched_terms": [
                    "context",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "context",
                    "embed",
                    "concat",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while our results demonstrates the promise of using LLM-generated context in HSD, our error analysis highlights some of its pitfalls. Most pressingly, we observe that additional context can cause semantic drift in the original post, occasionally neutralising hateful intent or inventing false hateful connections. This underscores the need for careful context selection and integration to maximise benefits and mitigate errors.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond demonstrating increased performance metrics, this research addresses important considerations regarding the role of background context in HSD.\nWhile full-text prompting outperformed named-entity-based approaches, both occasionally introduced spurious associations, impacting performance and suggesting that overcontextualisation, where generated background information dilutes or reinterprets the original intent, can be just as problematic as undercontextualisation.\nError analysis reinforces this view, with many false positives arising from benign posts being supplemented with highly charged or misleading contextual information, while false negatives often occurred when clearly hateful statements were augmented with neutralising or overly-explanatory context. These patterns reveal a trade-off between providing richer background information and maintaining original intent.\nFuture models may aim to strike the right balance through dynamic weighting or more sophisticated attention mechanisms to weigh the impacts of post and context.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, future work in this domain could explore a wider range of prompting strategies, in particular, utilising online-search and reasoning-capable LLMs to generate higher-quality background context.\nFurthermore, hybrid systems that combine generated context with verifiable domain knowledge may help to detect hateful references that even LLMs can overlook and guarantee more factual grounding <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. For instance, known extremist references could be identified by linking to curated hate-related knowledge bases, such as <cite class=\"ltx_cite ltx_citemacro_citet\">GPAHE (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib3\" title=\"\">2023</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Global Extremist Symbols Database</span> and integrating discovered information into the LLM prompt.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "hatred",
                    "implicit",
                    "hsd",
                    "llm",
                    "context",
                    "zerocontext",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research investigated the use of LLMs as dynamic knowledge bases to generate background context aimed at improving the detection of (implicit) hate speech across textual and multimodal domains. By comparing two prompting strategies for context generation and four distinct methods of integrating context into a classification pipeline, we systematically evaluated the impact of contextual enhancement on HSD performance.</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, we find that embedding-level concatenation (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) outperformed other incorporation methods, demonstrating the importance of maintaining separation between original content and additional background context (RQ2).\nIntegrating context directly into the original text through use of an LLM (<span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>) also beat baseline scores in the multimodal setting and lead to strong results when incorporating named-entity context in the textual setting, underscoring the future potential of this approach.</p>\n\n",
                "matched_terms": [
                    "enhance",
                    "llm",
                    "context",
                    "incorporation",
                    "embed",
                    "concat",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "strategy",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite promising results, this research is subject to limitations that should be considered when interpreting the findings or generalising them to broader settings.\nPrimarily, we relied on a single LLM (<span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>) and a fixed set of simple, natural-language prompts to generate background context. Alternative models, prompt formulations, or temperature settings could produce substantially different context and thus alter downstream performance.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our evaluation is limited to two English-language datasets collected over relatively brief time spans. This could limit generalisability to multilingual or non-English settings, as well as temporal generalisability, particularly as the language and references used in online posts and memes changes rapidly and dramatically over time (see <cite class=\"ltx_cite ltx_citemacro_citet\">Kostadinovska-Stojchevska and Shalevska (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib23\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Joshi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib21\" title=\"\">2024</a>)</cite> for more on this).\nMoreover, human annotations, particularly for implicit hate, are inherently subjective, which may introduce bias into annotated datasets.\nOur error analysis revealed several instances that appeared mislabelled or open to multiple valid interpretations in the Latent Hatred dataset. These issues are discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS4\" title=\"D. Doubtful Labels in Latent Hatred &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "implicit",
                    "latent",
                    "dataset",
                    "hatred"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our replication of <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> methodology was unable to validate their results, instead demonstrating a slight drop in macro F1 score from the zero-context baseline for both binary and multi-class classification.\nOne major issue with Lin&#8217;s approach is REL itself, which struggles to identify entities that are not capitalised in the &#8220;proper&#8221; way &#8212; a substantial issue when working with informal social media data. Lin does not explicitly mention the number of entities discovered, but our reproduction finds that only 9,595 of 21,480 posts (44.67%) were linked to one or more entities.\nManual inspection of a random sample of 100 posts (containing 79 identified tags) found 28 extra named entities missed by REL in Lin&#8217;s approach, along with 18 incorrectly linked named entities.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "multiclass",
                    "named",
                    "entities",
                    "rel",
                    "zerocontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "named",
                    "entities",
                    "hatred",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through performing an error analysis on the Latent Hatred dataset, we discovered a substantial number of posts with doubtful labels.\nFor instance, &#8220;<span class=\"ltx_text ltx_font_italic\">Berlin is the new Jerusalem: more and more Israeli Jews moving to Germany <span class=\"ltx_text\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span>swrm via &#8230;</span>&#8221; is labelled as <span class=\"ltx_text ltx_font_italic\">hate</span> and <span class=\"ltx_text ltx_font_italic\">stereotypical</span>, but the reasoning behind this is unclear. Presumably, this is just a tweet linking to a news article, and does not appear to be hateful, instead just reporting a trend.\nPotentially inaccurate or misleading annotations are particularly noticeable in the dataset&#8217;s multi-class labels, where each instance is constrained to a single category despite often exhibiting characteristics of multiple classes.\nFor instance, many tweets containing general references to race and whiteness (e.g., &#8220;<span class=\"ltx_text ltx_font_italic\" style=\"position:relative; bottom:1.9pt;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.2pt;height:8.800000000000001pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">#</span>\n</span></span></span><span class=\"ltx_text ltx_font_italic\">aryan men &amp; women will either rise up to fight for their heritage, children &amp; future or the white race will cease to exist. choose now.</span>&#8221;) are labelled as <span class=\"ltx_text ltx_font_italic\">white grievance</span>, while they may better suit another class (<span class=\"ltx_text ltx_font_italic\">incitement</span> in this case).\nThis prevalence of doubtful labels in hate-speech-related datasets has been observed in various previous studies <cite class=\"ltx_cite ltx_citemacro_cite\">van Aken et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib39\" title=\"\">2018</a>); Markov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib27\" title=\"\">2022</a>)</cite>, and is often proposed as a leading cause of lower-than-expected model performance.</p>\n\n",
                "matched_terms": [
                    "multiclass",
                    "latent",
                    "grievance",
                    "white",
                    "hatred",
                    "dataset"
                ]
            }
        ]
    },
    "Sx6.T11": {
        "caption": "Table 11: Macro-averaged Precision, Recall, and F1 scores for binary and multi-label HSD on the MAMI dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Context</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Incorporation</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Strategy</span></span></span>\n</span> </span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Binary</span></td>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Multi-Label</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">P</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">R</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">F1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">P</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">R</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">F1</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Zero-Context</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.79</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.79</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.79</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">REL </span><cite class=\"ltx_cite ltx_citemacro_cite\">Lin <span class=\"ltx_text\" style=\"font-size:90%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.78</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.78</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.78</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.56</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.57</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ConceptNet </span><cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. <span class=\"ltx_text\" style=\"font-size:90%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.80</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.60</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Prediction</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.86</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.86</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.86</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.56</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.68</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.60</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Full Text</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.84</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.85</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.85</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.85</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.63</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.63</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Context-Embed</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:79.7pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Enhance</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\" style=\"font-size:90%;\">0.64</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:22.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "full",
            "strategy",
            "embed",
            "concat",
            "enhance",
            "mami",
            "context",
            "llm",
            "prediction",
            "append",
            "conceptnet",
            "text",
            "scores",
            "lin",
            "hsd",
            "rel",
            "elsherief",
            "incorporation",
            "recall",
            "binary",
            "contextembed",
            "zerocontext",
            "macroaveraged",
            "precision",
            "dataset",
            "multilabel"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "mami",
                    "zerocontext",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "hsd",
                    "elsherief",
                    "context",
                    "zerocontext",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "mami",
                    "elsherief",
                    "llm",
                    "context",
                    "binary",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To what extent is a pre-trained LLM more effective than static entity linking at generating background context for implicit HSD?</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How can background context be effectively incorporated into a HSD model input?</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">How useful are these generation and incorporation methods for textual and multimodal HSD across binary and multi-class settings?</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "hsd",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While hate speech is inherently contextual <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, previous studies have shown that incorporating generic additional context into the model input often leads to a negligible or negative impact on model performance <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>. These findings suggest that context must be carefully selected, represented, and integrated to avoid introducing noise. This section discusses previous work relevant to HSD in context retrieval and incorporation, the use of LLMs, and the challenges surrounding multimodality.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context",
                    "incorporation",
                    "binary",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "lin",
                    "hsd",
                    "rel",
                    "elsherief",
                    "context",
                    "prediction",
                    "binary",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs have been used in HSD previously, generally replacing classifiers themselves, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>)</cite>, who directly compare a generative LLM against two pre-trained models: BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib25\" title=\"\">2019</a>)</cite>. They test various prompting strategies (few-shot, chain-of-thought, etc.) and demonstrate that a generalist LLM with simple prompts can comfortably outperform transformer-based encoders. These results are not confirmed, however, by numerous other studies, who find that LLMs either match or fall short of the performance of encoder-only classifiers, especially without extensive prompt engineering <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Albladi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib1\" title=\"\">2025</a>)</cite> discuss these results, highlighting the fact that encoder models are fine-tuned on the training data and can potentially capture the peculiarities of a given dataset better than generalist LLMs.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "llm",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "hsd",
                    "context",
                    "append",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "full",
                    "llm",
                    "context",
                    "incorporation",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "elsherief"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our second source is <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, used to detect misogynous memes. This dataset is comprised of 10,995 distinct memes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Five duplicate memes were discovered in MAMI during preprocessing and subsequently removed.</span></span></span> scraped from social media platforms and meme-creation websites.\nThe high level annotation is a simple binary distinction between misogynous<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>A meme is defined as misogynous if it &#8220;conceptually describes an offensive, sexist or hateful scene [&#8230;] having as target a woman or a group of women&#8221;.</span></span></span> and non-misogynous memes.\nIn the second level annotation, misogynous memes are labelled with one or more subtypes of misogyny: shaming, stereotyping, objectification, and violence. These labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T5\" title=\"Table 5 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, with example memes and their labels given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.\nWe again split the data in an 80/20 ratio.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "mami",
                    "binary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Context:</span> The first configuration uses only the raw post, simply creating SBERT embeddings from each post in the dataset and feeding them directly into the input of the MLP classification model.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "zerocontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">REL:</span> The second baseline replicates <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> entity-linking strategy, augmenting post text with Wikipedia-derived summaries based on named entities. Posts are processed using REL <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite>, with identified entities mapped to corresponding Wikipedia articles. The first two sentences of these articles are then appended to the original post. The combination is encoded using SBERT and given to the classifier, leaving posts without (discovered) named entities unchanged.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "rel",
                    "text",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ConceptNet:</span> The third baseline augments posts using ConceptNet&#8217;s Numberbatch embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>, replicating the methodology outlined by <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>. Entities are extracted using n-gram matching before being mapped to ConceptNet embeddings. These vectors are then averaged and normalised, forming a single contextual representation which is concatenated with the SBERT embedding of the original text, resulting in a 1,068-dimensional input vector.</p>\n\n",
                "matched_terms": [
                    "elsherief",
                    "conceptnet",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Prediction:</span> The final baseline system forgoes embeddings and training entirely, instead using the generative LLM directly as a classifier.\nThis setup aims to determine whether, if given access to an LLM like <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>, such a model is better utilised for generating context or directly for prediction. The LLM is prompted solely with each post and the potential labels and asked to predict binary, multi-class and multi-label classes.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>Full prompt texts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS5\" title=\"E. LLM Prompts for Context Generation and Incorporation &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "llm",
                    "context",
                    "prediction",
                    "binary",
                    "multilabel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "full",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "full",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once context has been generated, we test four strategies to incorporate it into the model input, dubbed i) <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>, ii) <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>, iii) <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, and iv) <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "enhance",
                    "llm",
                    "context",
                    "append",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "text",
                    "append",
                    "context",
                    "embed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embed &amp; Concat</span> tests the inverse of the previous strategy, vectorising the post and context separately, then concatenating them. This preserves a greater separation between post and context, potentially aiding the classifier in distinguishing their respective contributions.</p>\n\n",
                "matched_terms": [
                    "context",
                    "strategy",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context-Embed</span> follows <cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> methodology, fusing post and context hierarchically at the embedding layer before encoding, leading to a deeper integration of context and post than simple concatenation at the input or embedding level. In this configuration, the context is encoded with SBERT, while the post is tokenized and passed through the word embedding matrix of another SBERT model. The context vector is projected to match the dimensionality of token embeddings and prepended to the post embedding sequence. This expanded sequence is then passed through the remaining layers of the encoder. The resulting representation is then pooled before being passed to the classifier.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context",
                    "enhance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "mami",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each experiment is run five times with averaged results reported. The primary evaluation metric used in this study is macro F1. We also report positive class F1 in the binary settings and per-class F1 for the multi-class and multi-label settings.</p>\n\n",
                "matched_terms": [
                    "binary",
                    "multilabel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "mami",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "enhance",
                    "hsd",
                    "rel",
                    "llm",
                    "context",
                    "prediction",
                    "zerocontext",
                    "incorporation",
                    "recall",
                    "binary",
                    "precision",
                    "embed",
                    "concat",
                    "dataset",
                    "conceptnet",
                    "text",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "full",
                    "llm",
                    "context",
                    "prediction",
                    "append",
                    "binary",
                    "embed",
                    "concat",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the influence of incorporating LLM generated context on model behaviour, we compare variance between the predictions of the zero context model and the best-performing <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, rather than solely against the true labels.</p>\n\n",
                "matched_terms": [
                    "context",
                    "llm",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, 187 non-hateful posts (4.4%) were incorrectly classified by the Embed &amp; Concat system but not by the zero-context one.\nPrompting the LLM to relate generated context to potential hate speech may lead to inventing hateful context where none exists, consequently leading to the over-estimation of non-hateful posts as hateful.\nThe post &#8220;<span class=\"ltx_text ltx_font_italic\">this is what becoming a groyper does to your life people, it is an irreconcilable decision</span>&#8221; is a warning of the damage that falling into right-wing conspiracist groups can do to someone, but is mistakenly classified as hateful once lengthy context about <span class=\"ltx_text ltx_font_italic\">groypers</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Hawley (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib19\" title=\"\">2021</a>)</cite> is added, associating them with hateful ideologies and leading the classifier to over-represent this inferred hate.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "llm",
                    "context",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">68 misogynous memes (3.1%) were classified correctly with no context but incorrectly by the Embed &amp; Concat system. These generally fall under the aforementioned issue of adding irrelevant or incorrect context that obfuscates previously clearer hate.\nIn the example in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F1\" title=\"Figure 1 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the generated context does not recognise Bill Cosby in the image and misses the implication of Drug-Facilitated Sexual Assault <cite class=\"ltx_cite ltx_citemacro_cite\">Butler and Welch (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib4\" title=\"\">2009</a>)</cite>, leading the model to misclassify it as not misogynous.</p>\n\n",
                "matched_terms": [
                    "context",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "context",
                    "embed",
                    "concat",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, while our results demonstrates the promise of using LLM-generated context in HSD, our error analysis highlights some of its pitfalls. Most pressingly, we observe that additional context can cause semantic drift in the original post, occasionally neutralising hateful intent or inventing false hateful connections. This underscores the need for careful context selection and integration to maximise benefits and mitigate errors.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond demonstrating increased performance metrics, this research addresses important considerations regarding the role of background context in HSD.\nWhile full-text prompting outperformed named-entity-based approaches, both occasionally introduced spurious associations, impacting performance and suggesting that overcontextualisation, where generated background information dilutes or reinterprets the original intent, can be just as problematic as undercontextualisation.\nError analysis reinforces this view, with many false positives arising from benign posts being supplemented with highly charged or misleading contextual information, while false negatives often occurred when clearly hateful statements were augmented with neutralising or overly-explanatory context. These patterns reveal a trade-off between providing richer background information and maintaining original intent.\nFuture models may aim to strike the right balance through dynamic weighting or more sophisticated attention mechanisms to weigh the impacts of post and context.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, future work in this domain could explore a wider range of prompting strategies, in particular, utilising online-search and reasoning-capable LLMs to generate higher-quality background context.\nFurthermore, hybrid systems that combine generated context with verifiable domain knowledge may help to detect hateful references that even LLMs can overlook and guarantee more factual grounding <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. For instance, known extremist references could be identified by linking to curated hate-related knowledge bases, such as <cite class=\"ltx_cite ltx_citemacro_citet\">GPAHE (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib3\" title=\"\">2023</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Global Extremist Symbols Database</span> and integrating discovered information into the LLM prompt.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "zerocontext",
                    "llm",
                    "context",
                    "binary",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research investigated the use of LLMs as dynamic knowledge bases to generate background context aimed at improving the detection of (implicit) hate speech across textual and multimodal domains. By comparing two prompting strategies for context generation and four distinct methods of integrating context into a classification pipeline, we systematically evaluated the impact of contextual enhancement on HSD performance.</p>\n\n",
                "matched_terms": [
                    "hsd",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing our research questions, our findings indicate, firstly, that LLM-based context generation can outperform static entity-linking-based approaches, as evidenced by consistent improvements across both binary and fine-grained classification tasks in textual and multimodal settings (RQ1).\nWhile named-entity-based strategies showed marginal gains, incorporating context generated via full-text prompting outperformed baselines in the vast majority of settings.\nResults show that both the quality of generated context and the method by which it is incorporated into model inputs are critical to achieving improvements.</p>\n\n",
                "matched_terms": [
                    "binary",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, we find that embedding-level concatenation (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) outperformed other incorporation methods, demonstrating the importance of maintaining separation between original content and additional background context (RQ2).\nIntegrating context directly into the original text through use of an LLM (<span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>) also beat baseline scores in the multimodal setting and lead to strong results when incorporating named-entity context in the textual setting, underscoring the future potential of this approach.</p>\n\n",
                "matched_terms": [
                    "enhance",
                    "llm",
                    "context",
                    "incorporation",
                    "embed",
                    "concat",
                    "text",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "strategy",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite promising results, this research is subject to limitations that should be considered when interpreting the findings or generalising them to broader settings.\nPrimarily, we relied on a single LLM (<span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>) and a fixed set of simple, natural-language prompts to generate background context. Alternative models, prompt formulations, or temperature settings could produce substantially different context and thus alter downstream performance.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our replication of <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> methodology was unable to validate their results, instead demonstrating a slight drop in macro F1 score from the zero-context baseline for both binary and multi-class classification.\nOne major issue with Lin&#8217;s approach is REL itself, which struggles to identify entities that are not capitalised in the &#8220;proper&#8221; way &#8212; a substantial issue when working with informal social media data. Lin does not explicitly mention the number of entities discovered, but our reproduction finds that only 9,595 of 21,480 posts (44.67%) were linked to one or more entities.\nManual inspection of a random sample of 100 posts (containing 79 identified tags) found 28 extra named entities missed by REL in Lin&#8217;s approach, along with 18 incorrectly linked named entities.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "rel",
                    "binary",
                    "zerocontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "mami",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "text"
                ]
            }
        ]
    },
    "Sx6.T12": {
        "caption": "Table 12: Per-label F1 for multi-label classification on the MAMI dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Context</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Incorporation</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Strategy</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Shaming</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Stereo-</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">typing</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Object-</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">ification</span></span></span>\n</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Violence</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Zero-Context</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.54</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">REL </span><cite class=\"ltx_cite ltx_citemacro_cite\">Lin <span class=\"ltx_text\" style=\"font-size:90%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.42</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.53</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ConceptNet </span><cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. <span class=\"ltx_text\" style=\"font-size:90%;\">(</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a><span class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.43</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Prediction</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.46</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Full Text</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Append &amp; Embed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Embed &amp; Concat</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.65</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Context-Embed</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.45</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLM Enhance</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "full",
            "strategy",
            "embed",
            "concat",
            "violence",
            "object",
            "enhance",
            "mami",
            "context",
            "llm",
            "prediction",
            "append",
            "conceptnet",
            "text",
            "lin",
            "shaming",
            "perlabel",
            "rel",
            "elsherief",
            "typing",
            "incorporation",
            "classification",
            "stereo",
            "contextembed",
            "ification",
            "zerocontext",
            "dataset",
            "multilabel"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This research introduces a novel approach to textual and multimodal Hate Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge bases to generate background context and incorporate it into the input of HSD classifiers.\nTwo context generation strategies are examined: one focused on named entities and the other on full-text prompting.\nFour methods of incorporating context into the classifier input are compared: text concatenation, embedding concatenation, a hierarchical transformer-based fusion, and LLM-driven text enhancement. Experiments are conducted on the textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset of implicit hate speech and applied in a multimodal setting on the <span class=\"ltx_text ltx_font_italic\">MAMI</span> dataset of misogynous memes.\nResults suggest that both the contextual information and the method by which it is incorporated are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups respectively, from a zero-context baseline to the highest-performing system, based on embedding concatenation.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Hate Speech Detection, Context-Aware Classification, Multimodality, Large Language Models\n\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Content Warning:</span> This paper discusses and contains examples of hate speech, including explicit language and content that may be misogynistic, racist, or otherwise offensive.</p>\n\n",
                "matched_terms": [
                    "mami",
                    "zerocontext",
                    "context",
                    "classification",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Robustly detecting hate speech is a complex and context-dependent task, as such speech is often obfuscated through irony, euphemism, or coded language <cite class=\"ltx_cite ltx_citemacro_cite\">Henderson and McCready (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib20\" title=\"\">2018</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. While current models perform relatively well at flagging explicit hate speech, implicit hate, where hostility is veiled or implied, remains far more difficult to detect <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nHate Speech Detection (HSD) has most frequently been performed in zero-context settings <cite class=\"ltx_cite ltx_citemacro_cite\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>); Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite>, predicting potentially hateful content based solely on a representation of the raw post. While this approach simplifies model design and aligns with the structure of many available datasets, it can often fail to capture the underlying intent behind implicitly hateful language.\nOne method used to improve implicit HSD systems is to incorporate additional context into the model input <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite>. This context can be identified from the surrounding conversation (e.g., replies to a target post) <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> or through augmenting target text with external background knowledge <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>. The former is not always feasible to implement, as available labelled datasets are generally composed of isolated posts without any surrounding discourse <cite class=\"ltx_cite ltx_citemacro_cite\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>, and the latter methodology often struggles to identify relevant context due to poor entity detection or irrelevant linked data <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "elsherief",
                    "zerocontext",
                    "context",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing the aforementioned limitations, this research investigates how Large Language Models (LLMs) can serve as dynamic knowledge bases to extract background context about texts that may contain (implicit) hate speech. While prior studies have integrated generative LLMs into the HSD pipeline, these models have typically been utilised as direct replacements for classifiers, leading to mixed results <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>); Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nIn contrast, we focus on leveraging the rich contextual knowledge embedded in pre-trained LLMs, without abandoning the simplicity and robustness of traditional classification models.\nAs such, rather than relying on generic pre-structured external resources (e.g., ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>), we explore prompting an LLM to generate background context related to hate speech for online posts.\nFurther, we test several methods to incorporate this generated context into the input of a classification model and evaluate performance across various experimental setups.\nApproaches are first tested in a binary hate or non-hate classification setup, followed by multi-class identification of fine-grained implicit hate speech categories (e.g., <span class=\"ltx_text ltx_font_italic\">incitement</span>, <span class=\"ltx_text ltx_font_italic\">stereotyping</span>), based on <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> typology for &#8220;characterising and detecting different forms of implicit hate&#8221;. Additionally, we adapt our approaches to a multimodal setting, using <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, focused on identifying misogynous memes, to determine how well our methodology generalises to a new setting and domain.\nOverall, we investigate whether the shortcomings of previous context-aware HSD systems are due to the quality of the context itself or the method by which it is incorporated, leading us to address the following research questions:</p>\n\n",
                "matched_terms": [
                    "mami",
                    "elsherief",
                    "llm",
                    "context",
                    "classification",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To what extent is a pre-trained LLM more effective than static entity linking at generating background context for implicit HSD?</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While hate speech is inherently contextual <cite class=\"ltx_cite ltx_citemacro_cite\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, previous studies have shown that incorporating generic additional context into the model input often leads to a negligible or negative impact on model performance <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>. These findings suggest that context must be carefully selected, represented, and integrated to avoid introducing noise. This section discusses previous work relevant to HSD in context retrieval and incorporation, the use of LLMs, and the challenges surrounding multimodality.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversational context refers to content structurally or temporally linked to the target post, such as parent posts or prior replies. <cite class=\"ltx_cite ltx_citemacro_citet\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>)</cite> incorporated usernames and article titles to improve F1 scores by 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>)</cite> added previous replies to Facebook comments targeting migrants, achieving a 6-point F1 gain in detecting the targets of hate speech. <cite class=\"ltx_cite ltx_citemacro_citet\">P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite> used news article titles and prior comments, boosting binary classification F1 by 2 points and multi-class HSD by 6. Such context can provide thematic grounding and discourse cues, though its relevance can vary and its incorporation must be carefully considered <cite class=\"ltx_cite ltx_citemacro_cite\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "classification",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If conversational context is unavailable, entity linking can be used to retrieve background knowledge from external sources. <cite class=\"ltx_cite ltx_citemacro_citet\">Sharifirad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib36\" title=\"\">2018</a>)</cite> augment sexist Tweets with knowledge extracted from ConceptNet <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Wikidata (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib6\" title=\"\">2025</a>)</cite>, boosting prediction accuracy by ca. 5 points. <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> test a similar approach on their own dataset, also extracting entities and linking to ConceptNet and Wikidata to incorporate background context about a wide range of entities.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> expands on this, using the Radboud Entity Linker (REL) <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite> to extract Wikipedia descriptions of named entities, concatenating these with tweets and encoding them via SBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib34\" title=\"\">2019</a>)</cite>. This approach improved binary HSD macro F1 by 10 points but reduced multi-class performance on fine-grained hate categories by 11 points.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We discuss some potential shortcomings of REL and Lin&#8217;s methodology in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS1\" title=\"A. A Note on the Radboud Entity Linker &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "lin",
                    "rel",
                    "elsherief",
                    "context",
                    "prediction",
                    "dataset",
                    "conceptnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs have been used in HSD previously, generally replacing classifiers themselves, as in <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib17\" title=\"\">2024</a>)</cite>, who directly compare a generative LLM against two pre-trained models: BERT <cite class=\"ltx_cite ltx_citemacro_cite\">Devlin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib9\" title=\"\">2019</a>)</cite> and RoBERTa <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib25\" title=\"\">2019</a>)</cite>. They test various prompting strategies (few-shot, chain-of-thought, etc.) and demonstrate that a generalist LLM with simple prompts can comfortably outperform transformer-based encoders. These results are not confirmed, however, by numerous other studies, who find that LLMs either match or fall short of the performance of encoder-only classifiers, especially without extensive prompt engineering <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib6\" title=\"\">2022</a>); Roy et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib35\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Albladi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib1\" title=\"\">2025</a>)</cite> discuss these results, highlighting the fact that encoder models are fine-tuned on the training data and can potentially capture the peculiarities of a given dataset better than generalist LLMs.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most approaches integrate context by simply appending it to the target text <cite class=\"ltx_cite ltx_citemacro_cite\">Gao and Huang (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib15\" title=\"\">2017</a>); Markov and Daelemans (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib26\" title=\"\">2022</a>); P&#233;rez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib31\" title=\"\">2023</a>)</cite>, relying on the embedding model to distinguish content from context. <cite class=\"ltx_cite ltx_citemacro_citet\">Menini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib28\" title=\"\">2021</a>)</cite> take such a route, reannotating a dataset from <cite class=\"ltx_cite ltx_citemacro_citet\">Founta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib13\" title=\"\">2018</a>)</cite> in a context-aware setting. Providing context to annotators reduced the number of posts labelled as abusive (from 18% to 10% of the data). Using this reannotated dataset, the authors append all posts deemed contextually relevant to each target post before classification. This approach reduced performance, demonstrating that naively adding large amounts of context does more harm than good to a HSD system. Similar results were observed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pavlopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib30\" title=\"\">2020</a>)</cite>, who investigate whether context is as important as is often assumed in HSD.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> propose a novel approach to incorporate conversational context into representations of Tweets. This approach, dubbed <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, passes the context through an encoder-only model and the post through the word embedding matrix of a separate transformer, before concatenating the two and passing the unified representation through the rest of the second transformer&#8217;s layers. This method was shown to generally outperform elementary concatenation, leading us to implement a variation of it for one of our own experimental setups.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "context",
                    "classification",
                    "append",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detecting hate speech in multimodal formats (e.g., memes, which combine visual and textual elements) shares many of the same challenges as implicit HSD <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. In both cases, the hateful intent can be subtle, obfuscated, or reliant on external context, making it difficult to identify using surface-level cues alone. Often, a meme&#8217;s text may appear benign until paired with a provocative image, or vice versa <cite class=\"ltx_cite ltx_citemacro_cite\">Kiela et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib22\" title=\"\">2021</a>)</cite>, just as an implicitly hateful post may seem innocuous without background knowledge or cultural awareness. This conceptual overlap makes multimodal HSD a relatively natural extension of implicit HSD and a compelling setting for evaluating the effectiveness of context-enhanced models.</p>\n\n",
                "matched_terms": [
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research explores the potential of LLMs to serve as dynamic knowledge bases for discovering and integrating relevant background context into online posts. The methodology involves two core stages: context generation and context incorporation, tested in both textual and multimodal domains. To generate context, two approaches are examined: one based on named entities and the other on the full text of posts. To incorporate context, four distinct techniques are tested (described in detail in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.SS4\" title=\"3.4. Experiments &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>). In the multimodal setting, the full-text pipeline is adapted for memes by extracting image text and generating captions and background context all using an LLM. Sentence embeddings are created using SBERT, and are passed to a Multi-Layer Perceptron (MLP) classifier.\nOverall, this modular design ensures the isolation of the effects of different context generation and integration strategies to assess their impact independently of the classification and representation architectures. We make our code and data publicly available.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/c-hsd/\" title=\"\">https://anonymous.4open.science/r/c-hsd/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "full",
                    "llm",
                    "context",
                    "incorporation",
                    "classification",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first is <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite> textual <span class=\"ltx_text ltx_font_italic\">Latent Hatred</span> dataset, focused on identifying <span class=\"ltx_text ltx_font_italic\">implicit</span> hate speech. The authors establish a theoretical framework to identify the &#8220;diverse manifestations&#8221; of implicit hatred, including a six-way multi-class taxonomical classification of implicit hate speech varieties. These include: incitement to violence, inferiority language, irony, stereotyping, threatening, and white grievance.\nThis dataset consists of 21,480 tweets from known hate group accounts and is comprised of roughly 62% non-hateful posts, with 33% labelled as implicit hate speech and 5% explicit. The implicit multi-class labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T4\" title=\"Table 4 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>. We randomly split the data into training and test sets in an 80/20 ratio while maintaining class balance.</p>\n\n",
                "matched_terms": [
                    "violence",
                    "classification",
                    "elsherief",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our second source is <cite class=\"ltx_cite ltx_citemacro_citet\">Fersini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib12\" title=\"\">2022</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Multimedia Automatic Misogyny Identification</span> (MAMI) dataset, used to detect misogynous memes. This dataset is comprised of 10,995 distinct memes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Five duplicate memes were discovered in MAMI during preprocessing and subsequently removed.</span></span></span> scraped from social media platforms and meme-creation websites.\nThe high level annotation is a simple binary distinction between misogynous<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>A meme is defined as misogynous if it &#8220;conceptually describes an offensive, sexist or hateful scene [&#8230;] having as target a woman or a group of women&#8221;.</span></span></span> and non-misogynous memes.\nIn the second level annotation, misogynous memes are labelled with one or more subtypes of misogyny: shaming, stereotyping, objectification, and violence. These labels are distributed as in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T5\" title=\"Table 5 &#8227; F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS6\" title=\"F. Dataset Statistics &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, with example memes and their labels given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.\nWe again split the data in an 80/20 ratio.</p>\n\n",
                "matched_terms": [
                    "violence",
                    "shaming",
                    "mami",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-Context:</span> The first configuration uses only the raw post, simply creating SBERT embeddings from each post in the dataset and feeding them directly into the input of the MLP classification model.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "classification",
                    "zerocontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">REL:</span> The second baseline replicates <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> entity-linking strategy, augmenting post text with Wikipedia-derived summaries based on named entities. Posts are processed using REL <cite class=\"ltx_cite ltx_citemacro_cite\">van Hulst et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib40\" title=\"\">2020</a>)</cite>, with identified entities mapped to corresponding Wikipedia articles. The first two sentences of these articles are then appended to the original post. The combination is encoded using SBERT and given to the classifier, leaving posts without (discovered) named entities unchanged.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "rel",
                    "text",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ConceptNet:</span> The third baseline augments posts using ConceptNet&#8217;s Numberbatch embeddings <cite class=\"ltx_cite ltx_citemacro_cite\">Speer et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib37\" title=\"\">2016</a>)</cite>, replicating the methodology outlined by <cite class=\"ltx_cite ltx_citemacro_citet\">ElSherief et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib11\" title=\"\">2021</a>)</cite>. Entities are extracted using n-gram matching before being mapped to ConceptNet embeddings. These vectors are then averaged and normalised, forming a single contextual representation which is concatenated with the SBERT embedding of the original text, resulting in a 1,068-dimensional input vector.</p>\n\n",
                "matched_terms": [
                    "elsherief",
                    "conceptnet",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Prediction:</span> The final baseline system forgoes embeddings and training entirely, instead using the generative LLM directly as a classifier.\nThis setup aims to determine whether, if given access to an LLM like <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>, such a model is better utilised for generating context or directly for prediction. The LLM is prompted solely with each post and the potential labels and asked to predict binary, multi-class and multi-label classes.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>Full prompt texts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS5\" title=\"E. LLM Prompts for Context Generation and Incorporation &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "llm",
                    "context",
                    "multilabel",
                    "prediction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are designed to systematically evaluate how different forms of LLM-generated context, based on either extracted named entities or the full post text, impact HSD performance across textual and multimodal scenarios.</p>\n\n",
                "matched_terms": [
                    "full",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first branch of experiments simulates the common paradigm of entity-linked knowledge retrieval. Here, we prompt the LLM to generate context using only the extracted named entities.\nOur NER model identified 36,244 entities, spanning 14,358 of the 21,480 posts in the Latent Hatred dataset, giving an average of 1.69 entities per post, with 67% of posts having at least one entity.\nThese entities are given to the LLM in a prompt requesting concise background context about each one.\nThe second branch of experiments removes the reliance on NER, instead prompting the LLM with the full text of each post to request background information.</p>\n\n",
                "matched_terms": [
                    "full",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once context has been generated, we test four strategies to incorporate it into the model input, dubbed i) <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>, ii) <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>, iii) <span class=\"ltx_text ltx_font_italic\">Context-Embed</span>, and iv) <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "enhance",
                    "llm",
                    "context",
                    "append",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Append &amp; Embed</span> is the approach to integrating context most frequently used in previous studies: appending context directly to the end of the post, following a <span class=\"ltx_text ltx_font_typewriter\">[SEP]</span> token, before creating embeddings from their union. We expect this may bias the embeddings to represent the context more than the post itself, as generated context is generally much longer than the original text (with an average of 17 tokens per post and 78 per contextual summary).</p>\n\n",
                "matched_terms": [
                    "text",
                    "append",
                    "context",
                    "embed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embed &amp; Concat</span> tests the inverse of the previous strategy, vectorising the post and context separately, then concatenating them. This preserves a greater separation between post and context, potentially aiding the classifier in distinguishing their respective contributions.</p>\n\n",
                "matched_terms": [
                    "context",
                    "strategy",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Context-Embed</span> follows <cite class=\"ltx_cite ltx_citemacro_citet\">Bourgeade et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib2\" title=\"\">2024</a>)</cite> methodology, fusing post and context hierarchically at the embedding layer before encoding, leading to a deeper integration of context and post than simple concatenation at the input or embedding level. In this configuration, the context is encoded with SBERT, while the post is tokenized and passed through the word embedding matrix of another SBERT model. The context vector is projected to match the dimensionality of token embeddings and prepended to the post embedding sequence. This expanded sequence is then passed through the remaining layers of the encoder. The resulting representation is then pooled before being passed to the classifier.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Enhance</span> is a novel approach, inspired by a study on the generation of counter-narratives against hate speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Do&#287;an&#231; and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib10\" title=\"\">2023</a>)</cite>. Here, the LLM is provided with a post and its generated context and prompted to combine them, creating a modified context-enhanced post. This tests whether rephrasing the post itself to include more context yields better performance than preserving the original structure and adding context separately.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context",
                    "enhance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the multimodal setting, we adapt our full-text pipeline to classify memes from the MAMI dataset, opting not to implement the named-entity-based context generation method for both practical and conceptual reasons (discussed further in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>).\nEach meme is represented by the combination of two texts, comprised of (1) the text extracted directly from the meme and (2) a description of the image itself. This is a relatively common approach to representing multimodal input, used previously for HSD by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib8\" title=\"\">2020</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Britez and Markov (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib3\" title=\"\">2025</a>)</cite>.\nManual investigation of the extracted meme texts from the original MAMI data revealed some inconsistencies (e.g., transcription errors, unclear spacing, and including watermarks and timestamps), prompting us to re-process the whole dataset.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Greif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib16\" title=\"\">2025</a>)</cite> demonstrate that multimodal LLMs can surpass the accuracy of conventional optical character recognition, leading us to use the <span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span> model to generate these new representations. Additional details on our choice of representation are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS3\" title=\"C. Representation of Multimodal Input &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nContext extraction mirrors the methodology used in the textual setting, though multimodally, prompting the LLM with each meme directly to generate background context.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Examples of LLM-generated context are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Context is incorporated using the same four strategies as before.</p>\n\n",
                "matched_terms": [
                    "mami",
                    "llm",
                    "context",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the results of our experiments across two datasets: Latent Hatred for textual HSD and MAMI for multimodal misogyny detection. We evaluate the impact of LLM-generated context under multiple integration strategies and compare these against established baselines.</p>\n\n",
                "matched_terms": [
                    "mami",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S3.T1\" title=\"Table 1 &#8227; 3.5. Evaluation &#8227; 3. Methodology &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarises macro and positive-class F1 scores for binary HSD and multi-class implicit hate speech classification on the Latent Hatred dataset. Our best-performing configuration was the Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, which outperformed all baselines, achieving macro F1 scores of 0.75 in the binary setting and 0.53 in the multi-class setting.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Full results tables with precision, recall, and per-class scores are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a>.</span></span></span>\nBoth REL and ConceptNet baselines underperformed the zero-context model in the multi-class setting, though the latter does better in binary classification.\nThe REL approach, in particular, lead to a 4 point drop in F1, reaffirming the limitations of static entity-linking approaches given noisy data.\nThe named-entity models underperformed in all configurations except <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>, when compared to their Full-Text counterparts. Here the Context-Embed model performed particularly poorly, likely because incorporating empty context (i.e., zero vectors of length 768) for posts without detected entities dilutes the combined representation.\nOverall, both <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> and <span class=\"ltx_text ltx_font_italic\">LLM Enhance</span> performed worse than the baselines, demonstrating that a deeper incorporation of context (i.e., at the text level or embedding level) is less effective than just keeping them separate.\nPredictions made directly by the LLM were comparable to other baselines on the binary task, but fell behind majorly on multi-class prediction. As such, there is utility in using an LLM in the HSD pipeline to generate context, rather than as a replacement for embedding-based training and classification.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "enhance",
                    "rel",
                    "llm",
                    "context",
                    "prediction",
                    "zerocontext",
                    "incorporation",
                    "classification",
                    "embed",
                    "concat",
                    "dataset",
                    "conceptnet",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S4.T2\" title=\"Table 2 &#8227; 4.2. MAMI: Multimodal HSD &#8227; 4. Results &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, incorporating LLM-generated context by any method improved performance against all embedding-based baselines in the multimodal setting.\nThe Full-Text <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> approach again outperformed other strategies, achieving gains of 6 and 4 F1 points over the zero-context baseline in binary and multi-label settings, respectively.\nIn the multi-label task, we observe slight improvements across the board for all labels, demonstrating that our strategy of incorporating context from an LLM does not majorly impact the distribution of correct predictions, only bringing the minority classes closer to level.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>See Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.T11\" title=\"Table 11 &#8227; MAMI Results &#8227; H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> of Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS8\" title=\"H. Additional Results &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">H</span></a> for per-label results.</span></span></span>\nOnce again, the REL model underperformed the zero-context baseline, likely due to limited named entity coverage in memes. As mentioned, this lack of reliable entities was a major reason not to implement our named-entity-based context generation approach in the multimodal setting. We motivate this choice further and briefly discuss the limitations of multimodal NER in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS2\" title=\"B. NER in a Multimodal Setting &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>. The ConceptNet approach fared better but remained within 1 F1 point of the baseline in both tasks.\nIn this setting, the LLM Prediction system performs substantially better than before, surpassing the results of all other setups on binary classification, while maintaining lower but still competitive multi-label performance.</p>\n\n",
                "matched_terms": [
                    "perlabel",
                    "rel",
                    "context",
                    "llm",
                    "strategy",
                    "prediction",
                    "zerocontext",
                    "classification",
                    "embed",
                    "concat",
                    "conceptnet",
                    "multilabel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across both datasets, the clearest pattern is that separating post and context at the embedding level (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) consistently outperforms deeper fusion methods, particularly in binary classification. While simpler fusion strategies (e.g., <span class=\"ltx_text ltx_font_italic\">Append &amp; Embed</span>) showed modest gains, overly complex approaches (e.g., <span class=\"ltx_text ltx_font_italic\">Context-Embed</span> in low-context settings) risk performance degradation. Direct LLM prediction performs reasonably in binary tasks but deteriorates sharply in nuanced, multi-class settings, suggesting that generative models alone are still insufficient for fine-grained distinctions without explicit training or advanced prompt engineering. Together, these results indicate that LLM-generated context is most effective when (i) it is drawn from the full input rather than from extracted entities and (ii) it is kept modular and separately weighted rather than fully merged into the text or embeddings.</p>\n\n",
                "matched_terms": [
                    "contextembed",
                    "full",
                    "llm",
                    "context",
                    "prediction",
                    "classification",
                    "append",
                    "embed",
                    "concat",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the influence of incorporating LLM generated context on model behaviour, we compare variance between the predictions of the zero context model and the best-performing <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model, rather than solely against the true labels.</p>\n\n",
                "matched_terms": [
                    "context",
                    "llm",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is critical to analyse instances in which the context-enhanced model predicted a false negative while the zero-context model predicted the true positive, demonstrating the potential pitfalls of incorporating additional context. 158 posts (3.6% of the test set) were misclassified in this way,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Confusion matrices are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS9\" title=\"I. Confusion Matrices &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span> with many such errors occurring when lengthy context is added to already explicitly hateful posts. For instance, short posts such as &#8220;<span class=\"ltx_text ltx_font_italic\">Fight me, communists</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_italic\">Yes kill all Christians</span>&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Generated contextual summaries for all example posts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span> Adding context may bias the model to treat the combined input as more descriptive than hateful. This is a major issue with context-aware HSD, as extra context does not necessarily make the original post more hateful, meaning classifiers still struggle to recognise it as such.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Conversely, 187 non-hateful posts (4.4%) were incorrectly classified by the Embed &amp; Concat system but not by the zero-context one.\nPrompting the LLM to relate generated context to potential hate speech may lead to inventing hateful context where none exists, consequently leading to the over-estimation of non-hateful posts as hateful.\nThe post &#8220;<span class=\"ltx_text ltx_font_italic\">this is what becoming a groyper does to your life people, it is an irreconcilable decision</span>&#8221; is a warning of the damage that falling into right-wing conspiracist groups can do to someone, but is mistakenly classified as hateful once lengthy context about <span class=\"ltx_text ltx_font_italic\">groypers</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Hawley (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib19\" title=\"\">2021</a>)</cite> is added, associating them with hateful ideologies and leading the classifier to over-represent this inferred hate.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "llm",
                    "context",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">68 misogynous memes (3.1%) were classified correctly with no context but incorrectly by the Embed &amp; Concat system. These generally fall under the aforementioned issue of adding irrelevant or incorrect context that obfuscates previously clearer hate.\nIn the example in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F1\" title=\"Figure 1 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the generated context does not recognise Bill Cosby in the image and misses the implication of Drug-Facilitated Sexual Assault <cite class=\"ltx_cite ltx_citemacro_cite\">Butler and Welch (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib4\" title=\"\">2009</a>)</cite>, leading the model to misclassify it as not misogynous.</p>\n\n",
                "matched_terms": [
                    "context",
                    "embed",
                    "concat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inversely, 63 non-misogynous memes (2.9%) were mislabelled by the <span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span> model but not by the zero-context one. These errors tended to arise from two main phenomena. The first is as before, with context introducing ambiguity or inventing false misogynistic connections. The second is exemplified by the meme in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#S5.F2\" title=\"Figure 2 &#8227; 5.2. MAMI &#8227; 5. Error Analysis &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which contains many words often used with a negative connotation in misogynistic posts.\nPhrases like &#8220;modesty&#8221;, &#8220;feminism&#8221;, and &#8220;empowerment&#8221; are frequently used in polarising, sarcastic, or mocking contexts, especially in this dataset, potentially leading the model to view ostensibly supportive posts as insincere.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "context",
                    "embed",
                    "concat",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More broadly, future work in this domain could explore a wider range of prompting strategies, in particular, utilising online-search and reasoning-capable LLMs to generate higher-quality background context.\nFurthermore, hybrid systems that combine generated context with verifiable domain knowledge may help to detect hateful references that even LLMs can overlook and guarantee more factual grounding <cite class=\"ltx_cite ltx_citemacro_cite\">Pandiani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib29\" title=\"\">2024</a>)</cite>. For instance, known extremist references could be identified by linking to curated hate-related knowledge bases, such as <cite class=\"ltx_cite ltx_citemacro_citet\">GPAHE (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#biba.bib3\" title=\"\">2023</a>)</cite> <span class=\"ltx_text ltx_font_italic\">Global Extremist Symbols Database</span> and integrating discovered information into the LLM prompt.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, while direct LLM predictions excel in binary multimodal classification (even beating our novel approaches), they struggle to maintain accuracy on the implicit hate of the Latent Hatred dataset and in fine-grained tasks. This indicates that generative models can capture broad distinctions yet lack the granularity to separate nuanced classes without explicit training. However, this does not diminish our findings: incorporating LLM-generated context consistently improves HSD performance over zero-context and entity-linking baselines, especially in multimodal settings. While our methods do outperform the entity-based approaches of previous work, we emphasise that the objective of this research is not to develop the optimal HSD model, but rather to evaluate the impact of LLM-generated context in a controlled framework. More advanced architectures, such as transformer-based multimodal fusion networks or attention-based classifiers, may better capture subtleties in the data and potentially outperform direct LLM predictions while still benefiting from generated background context.</p>\n\n",
                "matched_terms": [
                    "zerocontext",
                    "llm",
                    "context",
                    "classification",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This research investigated the use of LLMs as dynamic knowledge bases to generate background context aimed at improving the detection of (implicit) hate speech across textual and multimodal domains. By comparing two prompting strategies for context generation and four distinct methods of integrating context into a classification pipeline, we systematically evaluated the impact of contextual enhancement on HSD performance.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Addressing our research questions, our findings indicate, firstly, that LLM-based context generation can outperform static entity-linking-based approaches, as evidenced by consistent improvements across both binary and fine-grained classification tasks in textual and multimodal settings (RQ1).\nWhile named-entity-based strategies showed marginal gains, incorporating context generated via full-text prompting outperformed baselines in the vast majority of settings.\nResults show that both the quality of generated context and the method by which it is incorporated into model inputs are critical to achieving improvements.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, we find that embedding-level concatenation (<span class=\"ltx_text ltx_font_italic\">Embed &amp; Concat</span>) outperformed other incorporation methods, demonstrating the importance of maintaining separation between original content and additional background context (RQ2).\nIntegrating context directly into the original text through use of an LLM (<span class=\"ltx_text ltx_font_italic\">LLM Enhance</span>) also beat baseline scores in the multimodal setting and lead to strong results when incorporating named-entity context in the textual setting, underscoring the future potential of this approach.</p>\n\n",
                "matched_terms": [
                    "enhance",
                    "llm",
                    "context",
                    "incorporation",
                    "embed",
                    "concat",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Lastly, we quantify the impact of these techniques across a variety of experimental setups (RQ3). The incorporation of LLM-generated context led to improvements of up to 3 F1 points on textual input and up to 6 F1 points on multimodal input, demonstrating the strong applicability of this strategy across domains and its particular effectiveness in augmenting multimodal representations.</p>\n\n",
                "matched_terms": [
                    "incorporation",
                    "strategy",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite promising results, this research is subject to limitations that should be considered when interpreting the findings or generalising them to broader settings.\nPrimarily, we relied on a single LLM (<span class=\"ltx_text ltx_font_typewriter\">Gemini 2.0 Flash</span>) and a fixed set of simple, natural-language prompts to generate background context. Alternative models, prompt formulations, or temperature settings could produce substantially different context and thus alter downstream performance.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In regards to this work more specifically, the use of LLMs to generate background context introduces unique risks. Unlike curated knowledge bases, LLM outputs are probabilistic and not easily verifiable, and may introduce misinformation, stereotypes, or defamatory content directly into the classification pipeline. As our error analysis shows, generated background context can shift the interpretation of posts in unintended ways, leading to false positives or false negatives. In a production setting, such behaviour could unfairly penalise individuals or distort moderation outcomes. These risks underscore the need for mechanisms to validate or fact-check generated context, human review for high-impact decisions, and clear disclaimers about the probabilistic nature of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "classification",
                    "llm",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our replication of <cite class=\"ltx_cite ltx_citemacro_citet\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib24\" title=\"\">2022</a>)</cite> methodology was unable to validate their results, instead demonstrating a slight drop in macro F1 score from the zero-context baseline for both binary and multi-class classification.\nOne major issue with Lin&#8217;s approach is REL itself, which struggles to identify entities that are not capitalised in the &#8220;proper&#8221; way &#8212; a substantial issue when working with informal social media data. Lin does not explicitly mention the number of entities discovered, but our reproduction finds that only 9,595 of 21,480 posts (44.67%) were linked to one or more entities.\nManual inspection of a random sample of 100 posts (containing 79 identified tags) found 28 extra named entities missed by REL in Lin&#8217;s approach, along with 18 incorrectly linked named entities.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "zerocontext",
                    "rel",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Named Entity Recognition (NER) on multimodal input, as in the MAMI dataset, is ill-defined. Even if named entities are present, they may appear either directly in the embedded text or visually within the image.\nRunning NER on the extracted texts identified 6,598 entities spanning 3,394 of the 10,995 memes, meaning only 31% of memes have at least one entity with an average of 0.6 entities per meme, significantly lower than in the textual Latent Hatred set.\n&#8220;<span class=\"ltx_text ltx_font_italic\">trump</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">china</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">covid</span>&#8221; appear frequently in these meme texts, demonstrating the time and political context from which they were originally collated.\nRunning NER on the generated image descriptions reveals another 4,478 named entities. However, given that memes generally rely on specific formats, often based on stills from movies or TV shows, the majority of these entities are likely irrelevant to detecting hate speech. Demonstrating this, the most frequently occurring entities found were &#8220;<span class=\"ltx_text ltx_font_italic\">spongebob squarepants</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_italic\">thanos</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_italic\">gordon ramsay</span>&#8221;, showing both the kinds of references used and the high identifiability of these characters to our image captioning model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span>Some examples of memes with irrelevant named entities are given in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#Sx6.SS7\" title=\"G. Example posts, memes, and generated context &#8227; Appendix &#8227; Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</span></span></span>\nIn general, we find a lack of reliable entity anchors and suggest that, while it makes mores sense to run NER on the extracted text, results from this are still poor.\nAdditionally, full-text prompting generally subsumes NER. Since LLMs are already used to generate background context from the image, these texts should inherently cover any entity-relevant content that might otherwise be retrieved through NER.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "mami",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we represent multimodal memes entirely through text. We stress, primarily, that this leads to improved interpretability, as text-based inputs can be directly inspected for error or bias.\nIn contrast, visual embeddings (from e.g., Contrastive Language&#8211;Image Pretraining <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib33\" title=\"\">2021</a>)</cite>), which have been utilised previously in research on this dataset (by e.g., <cite class=\"ltx_cite ltx_citemacro_citet\">Hakimov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.15685v1#bib.bib18\" title=\"\">2022</a>)</cite>) can be opaque, offering limited insight into the basis for a model&#8217;s decisions. Secondarily, this approach reduces in-house computational cost and allows for the reuse of familiar tools without specialised multimodal infrastructure.\nThe inclusion of the image description is, of course, crucial. The meaning of a meme can generally only be derived from the combination of textual and visual modalities, and captions provide semantic grounding that would otherwise be lost by using only the extracted text.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "text"
                ]
            }
        ]
    }
}