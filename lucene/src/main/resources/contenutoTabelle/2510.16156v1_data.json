{
    "S3.T1": {
        "source_file": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning",
        "caption": "TABLE I: Objective evaluation results summary",
        "body": "Scenario\nMetric\nMonolithic\nExplainer-Only\nAsyncVoice\n\n\nMath Solver\nTTFA (s)\n9.4809.480\n4.6184.618\n0.015\n\n\nScore\n96.3696.36\n90.6090.60\n92.20\n\n\nTravel Planner\nTTFA (s)\n26.90726.907\n12.08912.089\n0.015\n\n\nScore\n96.4096.40\n96.7096.70\n91.80\n\n\nDeep Research\nTTFA (s)\n27.18427.184\n13.98013.980\n0.014\n\n\nScore\n81.8081.80\n72.6072.60\n79.50\n\n\nAll Scenarios\nProcess Fidelity\nN/A\nN/A\n4.73",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Scenario</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Metric</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Monolithic</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Explainer-Only</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">AsyncVoice</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Math Solver</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">TTFA (s)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"9.480\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mn>9.480</mn><annotation encoding=\"application/x-tex\">9.480</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"4.618\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mn>4.618</mn><annotation encoding=\"application/x-tex\">4.618</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.015</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Score</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"96.36\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mn>96.36</mn><annotation encoding=\"application/x-tex\">96.36</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"90.60\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mn>90.60</mn><annotation encoding=\"application/x-tex\">90.60</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">92.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Travel Planner</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">TTFA (s)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"26.907\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mn>26.907</mn><annotation encoding=\"application/x-tex\">26.907</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"12.089\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mn>12.089</mn><annotation encoding=\"application/x-tex\">12.089</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.015</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Score</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"96.40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mn>96.40</mn><annotation encoding=\"application/x-tex\">96.40</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"96.70\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mn>96.70</mn><annotation encoding=\"application/x-tex\">96.70</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">91.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Deep Research</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">TTFA (s)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"27.184\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mn>27.184</mn><annotation encoding=\"application/x-tex\">27.184</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"13.980\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><mn>13.980</mn><annotation encoding=\"application/x-tex\">13.980</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.014</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Score</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"81.80\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m11\" intent=\":literal\"><semantics><mn>81.80</mn><annotation encoding=\"application/x-tex\">81.80</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"72.60\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m12\" intent=\":literal\"><semantics><mn>72.60</mn><annotation encoding=\"application/x-tex\">72.60</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">79.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">All Scenarios</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">Process Fidelity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">N/A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">N/A</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.73</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "monolithic",
            "summary",
            "solver",
            "scenario",
            "scenarios",
            "metric",
            "evaluation",
            "deep",
            "objective",
            "results",
            "planner",
            "fidelity",
            "asyncvoice",
            "score",
            "explaineronly",
            "research",
            "travel",
            "process",
            "math",
            "all",
            "ttfa"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16156v1#S3.T1\" title=\"TABLE I &#8227; III-C Results &#8227; III Evaluation &#8227; AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, the AsyncVoice Agent demonstrates fundamental advantages in responsiveness, achieving TTFA of approximately 15ms across all scenarios, a 600-1,800<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> reduction compared to baseline approaches that enables genuine real-time interaction. While baselines occasionally achieve higher reasoning quality scores, the AsyncVoice Agent remains competitive. It is worth noting that for this evaluation, the backend MCP servers were configured for a single-pass reasoning process; enabling their native multi-round capabilities would likely improve reasoning scores further, but our focus is on interaction fidelity, not task-specific score optimization. This suggests that the streaming benefits do not significantly compromise reasoning accuracy. Critically, the high Process Fidelity scores validate that real-time explanations faithfully represent backend reasoning processes, addressing essential requirements for trustworthy human-AI collaboration.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Effective human-AI collaboration on complex reasoning tasks requires that users understand and interact with the model&#8217;s process, not just receive an output. However, the monolithic text from methods like Chain-of-Thought (CoT) prevents this, as current interfaces lack real-time verbalization and robust user barge-in. We present AsyncVoice Agent, a system whose asynchronous architecture decouples a streaming LLM backend from a conversational voice frontend. This design allows narration and inference to run in parallel, empowering users to interrupt, query, and steer the model&#8217;s reasoning process at any time. Objective benchmarks show this approach reduces interaction latency by more than 600<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"m8\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> compared to monolithic baselines while ensuring high fidelity and competitive task accuracy. By enabling a two-way dialogue with a model&#8217;s thought process, AsyncVoice Agent offers a new paradigm for building more effective, steerable, and trustworthy human-AI systems for high-stakes tasks.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>This work was supported by NSF 2112562 and ARO W911NF-23-2-0224.</span></span></span></p>\n\n",
                "matched_terms": [
                    "monolithic",
                    "objective",
                    "process",
                    "fidelity",
                    "asyncvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Consequently, enhancing the interpretability of these reasoning chains has become a significant focus of ongoing research. While raw CoT aims for transparency, its verbosity can be a significant barrier&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16156v1#bib.bib2\" title=\"\">2</a>]</cite>. Some approaches use text-based summarization or structuring techniques to offer post-hoc digestibility&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16156v1#bib.bib5\" title=\"\">5</a>]</cite>, and dialogue systems have been proposed to improve explanations&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16156v1#bib.bib6\" title=\"\">6</a>]</cite>. However, these typically do not operate on the live, streaming internal state of a separate reasoning LLM. More recent work has explored asynchronous AI agents for real-time tool use with voice interaction, focusing on the agent&#8217;s task execution (e.g., booking a flight while conversing)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16156v1#bib.bib7\" title=\"\">7</a>]</cite>. While this addresses the agent&#8217;s external actions, the challenge of providing real-time, interactive explanations of an agent&#8217;s <em class=\"ltx_emph ltx_font_italic\">internal reasoning stream</em>, and allowing for user interruption during this process, remains a significant, unaddressed gap.</p>\n\n",
                "matched_terms": [
                    "process",
                    "research"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fill this gap, this paper presents the architecture and demonstration of <span class=\"ltx_text ltx_font_bold\">AsyncVoice Agent</span>, a system designed to serve as an interactive, real-time voice interface for explaining an LLM&#8217;s ongoing reasoning processes. Rather than verbalizing a static block of text, our system narrates each thought process as it streams from the model. The primary contribution is a system that enables a truly asynchronous dialogue about a live reasoning process, allowing a user to seamlessly interrupt the agent&#8217;s explanation to ask questions or provide feedback. This capability transforms the interaction from a passive user experience into a collaborative dialogue, filling a crucial gap for explainability and trust. This paper details the system&#8217;s architecture and its novel components.</p>\n\n",
                "matched_terms": [
                    "asyncvoice",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The AsyncVoice Agent is a real-time voice interface system designed to provide interactive explanation of an LLM&#8217;s reasoning process. Our implementation builds upon the foundational <span class=\"ltx_text ltx_font_typewriter\">RealtimeVoiceChat</span> project<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/KoljaB/RealtimeVoiceChat\" title=\"\">https://github.com/KoljaB/RealtimeVoiceChat</a></span></span></span>, adapting its core real-time ASR and WebSocket communication layer. Our primary contributions are the design and integration of several novel components that enable the explanation of an external LLM&#8217;s reasoning stream. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16156v1#S2.F1\" title=\"Figure 1 &#8227; II AsyncVoice Agent Architecture &#8227; AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our architecture adds new backend reasoning modules and modifies the core agent pipeline. The complete system comprises three core subsystems: (1) the adapted WebSocket layer for bidirectional audio streaming, (2) our novel modular Model Context Protocol (MCP) servers for specialized reasoning tasks, and (3) a multi-threaded speech processing pipeline featuring Azure TTS integration for low-latency voice interaction.</p>\n\n",
                "matched_terms": [
                    "asyncvoice",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The backend reasoning capabilities are provided by the specialized <span class=\"ltx_text ltx_font_bold\">Backend MCP Servers</span> shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16156v1#S2.F1\" title=\"Figure 1 &#8227; II AsyncVoice Agent Architecture &#8227; AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. These servers are highly modular, allowing different underlying large language models to be configured for specific tasks. For instance, the <span class=\"ltx_text ltx_font_typewriter\">Travel Planner</span> is configured to use a specialized reasoning model for its complex planning capabilities, while the <span class=\"ltx_text ltx_font_typewriter\">Math Solver</span> leverages the precision of GPT-4o <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16156v1#bib.bib8\" title=\"\">8</a>]</cite>. Regardless of the reasoning model used, each server adheres to the standardized MCP. They emit progress updates and final answers using a consistent notification format (<span class=\"ltx_text ltx_font_typewriter\">ctx.notification</span>). The agent&#8217;s <span class=\"ltx_text ltx_font_typewriter\">MCP Client</span> subscribes to this stream and processes updates based on semantic prefixes: <span class=\"ltx_text ltx_font_typewriter\">Thinking:</span> for an intermediate reasoning step, <span class=\"ltx_text ltx_font_typewriter\">Content:</span> for a status update, and <span class=\"ltx_text ltx_font_typewriter\">Answer:</span> for the final response. This protocol standardization allows the AsyncVoice Agent to adapt to any MCP-compliant backend with minimal effort. A final <span class=\"ltx_text ltx_font_typewriter\">COMPLETE</span> signal is used to trigger UI state transitions on the front-end and update the state manager accordingly.</p>\n\n",
                "matched_terms": [
                    "math",
                    "solver",
                    "travel",
                    "planner",
                    "asyncvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To objectively validate the benefits of our proposed architecture, we present a rigorous evaluation framework designed to quantify the performance of the AsyncVoice Agent. To ensure objective and reproducible results, the evaluation is conducted using an automated framework. This approach focuses on quantifiable metrics: responsiveness, reasoning quality, and process fidelity. We benchmark our system against two baselines across three distinct, pre-defined operational scenarios.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "fidelity",
                    "objective",
                    "results",
                    "process",
                    "scenarios",
                    "asyncvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate across three diverse reasoning scenarios using 100 queries for each: Math Solver problems are drawn from the GSM8K benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16156v1#bib.bib9\" title=\"\">9</a>]</cite> and require multi-step arithmetic; for Travel Planner and Deep Research, we used GPT-4o to generate scenarios with explicit constraints, such as budget limits for travel or minimum citation counts for research, that demand multi-round planning and analysis.</p>\n\n",
                "matched_terms": [
                    "math",
                    "research",
                    "deep",
                    "solver",
                    "travel",
                    "planner",
                    "scenarios"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation assesses performance using three core metrics. The first is <span class=\"ltx_text ltx_font_bold\">responsiveness</span>, measured by Time to First Audio (TTFA), which is the latency from user query submission to the start of audible output. The second is <span class=\"ltx_text ltx_font_bold\">reasoning quality</span>, assessed through hybrid frameworks combining automated scoring with GPT-4o assessment. For the Math Solver, this blends exact numerical accuracy (70%) with reasoning methodology evaluation (30%), while for the Travel Planner and Deep Research scenarios, it integrates constraint satisfaction with solution quality assessment. The third metric, <span class=\"ltx_text ltx_font_bold\">process fidelity</span>, is applied exclusively to our AsyncVoice Agent; here, GPT-4o scores the semantic and logical consistency between streamed explanations and the backend reasoning texts on a 1-to-5 scale. To ensure a fair comparison, complete reasoning traces for all systems are included in the evaluation.</p>\n\n",
                "matched_terms": [
                    "math",
                    "all",
                    "evaluation",
                    "fidelity",
                    "metric",
                    "deep",
                    "research",
                    "ttfa",
                    "solver",
                    "travel",
                    "process",
                    "planner",
                    "scenarios",
                    "asyncvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance gap versus monolithic baselines results from our one-way streaming design where explainability-optimized outputs and token limits constrain reasoning depth. We consider this an acceptable trade-off for achieving 600-1800&#215; latency reduction. Our automated evaluation ensures reproducibility (using TTS-generated queries for audio consistency), while the system fully supports natural voice input with sub-100ms interruption handling and configurable explanation styles. Future work will propagate user feedback to upstream MCP servers, enabling human-in-the-loop refinement that could\nsurpass static approaches. Current limitations include TTS prosody and unidirectional reasoning flow; however, these represent engineering challenges rather than fundamental architectural barriers.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "monolithic",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced AsyncVoice Agent, the first system to enable real-time, interruptible dialogue with an LLM&#8217;s live reasoning stream. By decoupling the reasoning backend from the explanation interface through asynchronous architecture, we transform passive CoT consumption into active collaboration. This work establishes a new paradigm for human-AI interaction in complex reasoning tasks, demonstrating that sub-second responsiveness fundamentally changes how users engage with AI reasoning processes. We believe this approach will inspire further research into interactive AI systems where human expertise guides model computation in real-time.</p>\n\n",
                "matched_terms": [
                    "asyncvoice",
                    "research"
                ]
            }
        ]
    }
}