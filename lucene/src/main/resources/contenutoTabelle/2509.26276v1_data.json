{
    "S3.T1": {
        "source_file": "Optimizing Speech Language Models for Acoustic Consistency",
        "caption": "Table 1: SALMON performance (%). The benchmark reports pairwise preference performance for acoustic consistency (left block) and semantic–acoustic alignment (right block). Best results per column are highlighted.",
        "body": "Acoustic Consistency\nSemantic–Acoustic Alignment\n\n\nMethod\n\nSentiment ↑\\uparrow\n\n\nSpeaker ↑\\uparrow\n\n\nGender ↑\\uparrow\n\n\nBg (domain) ↑\\uparrow\n\n\nBg (rand.) ↑\\uparrow\n\n\nRoom ↑\\uparrow\n\n\nSentiment ↑\\uparrow\n\n\nBackground ↑\\uparrow\n\n\n\n\n\nCAST 0.7B (Speech-only)\n\n\\cellcolorcastblue!2081.8\n\n\n\\cellcolorcastblue!2090.8\n\n\n\\cellcolorcastblue!2090.0\n\n\n\\cellcolorcastblue!2080.0\n\n\n\\cellcolorcastblue!2077.5\n\n90.0\n51.0\n56.0\n\n\nCAST 1B (Speech-only)\n\n\\cellcolorcastblue!2081.8\n\n90.0\n\n\\cellcolorcastblue!2090.0\n\n78.0\n68.5\n\n\\cellcolorcastblue!2091.0\n\n48.5\n51.5\n\n\nCAST 1B (Speech+Text)\n73.0\n83.5\n83.5\n75.0\n71.5\n84.5\n54.5\n58.0\n\n\nSpiritLM 7B (Expr.)\n73.5\n81.0\n85.0\n55.0\n64.0\n55.5\n52.0\n\n\\cellcolorcastblue!2059.5\n\n\n\nTwist 7B\n61.5\n71.0\n70.0\n55.0\n60.5\n62.0\n51.5\n54.5\n\n\nLAST 1.3B\n65.0\n64.5\n68.5\n56.0\n61.0\n62.5\n53.5\n53.0\n\n\nFlow-SLM 1B-ext\n65.0\n76.5\n80.0\n70.0\n64.5\n73.5\n\n\\cellcolorcastblue!2057.0\n\n53.0\n\n\nHuman\n97.2\n91.5\n98.6\n83.1\n88.7\n94.4\n93.3\n95.8",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Acoustic Consistency</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Semantic&#8211;Acoustic Alignment</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sentiment</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speaker</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Gender</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Bg (domain)</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Bg (rand.)</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Room</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sentiment</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Background</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CAST 0.7B (Speech-only)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">81.8</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">90.8</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">90.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">80.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">77.5</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">51.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CAST 1B (Speech-only)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">81.8</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">90.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">78.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">91.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">51.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CAST 1B (Speech+Text)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">75.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">84.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SpiritLM 7B (Expr.)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">81.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">85.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">52.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">59.5</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Twist 7B</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">51.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LAST 1.3B</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Flow-SLM 1B-ext</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">57.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Human</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">94.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">93.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">95.8</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speechonly",
            "sentiment",
            "twist",
            "preference",
            "07b",
            "cellcolorcastblue20775",
            "gender",
            "benchmark",
            "↑uparrow",
            "right",
            "cellcolorcastblue20900",
            "salmon",
            "best",
            "background",
            "domain",
            "cellcolorcastblue20910",
            "block",
            "results",
            "acoustic",
            "rand",
            "cellcolorcastblue20908",
            "column",
            "speechtext",
            "expr",
            "method",
            "room",
            "performance",
            "reports",
            "13b",
            "cellcolorcastblue20570",
            "cellcolorcastblue20818",
            "semantic–acoustic",
            "flowslm",
            "alignment",
            "pairwise",
            "cast",
            "cellcolorcastblue20800",
            "highlighted",
            "spiritlm",
            "consistency",
            "1bext",
            "cellcolorcastblue20595",
            "speaker",
            "human",
            "last",
            "left"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate the three CAST variants against prior systems that represent the main approaches in the field. For comparison, we include prior systems that represent the main approaches in the field: Twist (7B speech-only) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib10\" title=\"\">10</a>]</cite>, which initializes from a large text LM and models HuBERT units; LAST (1.3B) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib14\" title=\"\">14</a>]</cite>, which uses LM-guided tokenization to better align speech tokens with text; SpiritLM (7B interleaved) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib5\" title=\"\">5</a>]</cite>, which extends a text LM with speech tokens and shows strong semantic grounding; and Flow-SLM (1B) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib15\" title=\"\">15</a>]</cite>, which emphasizes efficiency through flow-based objectives. These systems differ in tokenization, architecture, and training objectives, and they provide reference points for evaluating the effects of our LM-side interface and training mix. We summarize results across SALMON acoustic and alignment tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib16\" title=\"\">16</a>]</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S3.T1\" title=\"Table 1 &#8227; 3 Method &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), semantic probes (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), and ARCH linear probes <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib17\" title=\"\">17</a>]</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S4.T3\" title=\"Table 3 &#8227; 4 Results &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We study speech language models that incorporate semantic initialization and planning losses to achieve robust and consistent generation. Our approach initializes speech tokens with self-supervised features, applies a light alignment loss, and trains with thinning and auxiliary objectives that target robustness and content planning. We train three models: a 0.7B speech-only model, a 1.0B speech-only model, and a 1.0B interleaved model with both text and speech. Acoustic studies show that the speech-only models achieve the highest consistency across speaker, gender, sentiment, room, and background factors, surpassing larger systems. Interleaving improves lexical and syntactic probes and semantic&#8211;acoustic alignment but reduces consistency. Linear probes show that our initialization biases the model toward content structure while trading off prosody detail. These results show that LM-side design and training mix control the balance between acoustic stability and semantic grounding without changes to the tokenizer or runtime architecture. A demo and model weights are available for exploration.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mortezaro.github.io/speech-cast/\" title=\"\">https://mortezaro.github.io/speech-cast/</a> (demo); <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/KrauthammerLab/cast-0.7b-s2s\" title=\"\">https://huggingface.co/KrauthammerLab/cast-0.7b-s2s</a> (HF model card).</span></span></span></p>\n\n",
                "matched_terms": [
                    "consistency",
                    "room",
                    "speechonly",
                    "sentiment",
                    "background",
                    "speaker",
                    "semantic–acoustic",
                    "results",
                    "acoustic",
                    "07b",
                    "gender",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech language models aim to extend the generalization and transfer abilities of text-based language models to spoken inputs and outputs. These models operate directly on discrete speech units and support tasks from semantic understanding to continuation and generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib3\" title=\"\">3</a>]</cite>. However, robust speech modeling still faces challenges in consistency and invariance. Prior work shows that speech language models struggle with capturing stable speaker identity, background conditions, and prosodic detail across generations, especially under timing variation or noisy inputs. Yet achieving this remains challenging without added complexity.</p>\n\n",
                "matched_terms": [
                    "background",
                    "consistency",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many recent works address speech modeling by exposing codec outputs as discrete tokens in the vocabulary of a decoder-only transformer and by interleaving these tokens with text <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib5\" title=\"\">5</a>]</cite>. In this setting, the model predicts a unified sequence of speech and text symbols step by step. Our CAST approach builds on this by shaping the language model&#8217;s embeddings and training objectives to prioritize acoustic consistency, isolating LM-side contributions without modifying the tokenizer or inference path.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "consistency",
                    "cast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support this interface, we shape the language model&#8217;s embedding and training strategy. Each speech token maps to a literal token with an embedding initialized from a frozen self-supervised encoder. A stop-gradient alignment loss preserves phonetic structure in the learned representation. During training, we apply consistency-based augmentations: thinning the speech stream at variable rates and erasing short spans to encourage invariance to timing and context gaps. Auxiliary losses guide the model to plan coarse structure before predicting fine acoustic detail.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train three models: a 0.7B speech-only model, a 1.0B speech-only model, and a 1.0B interleaved model with both text and speech. We evaluate them in three ways. Acoustic studies test whether the model gives higher likelihood to natural recordings compared to samples with mid-utterance changes in gender, speaker, background, room, or sentiment. Semantic studies probe lexical and grammatical regularities in spoken sequences. Alignment studies test whether acoustic conditions match the spoken text. We also include ablations that isolate initialization, thinning, and auxiliary losses.</p>\n\n",
                "matched_terms": [
                    "room",
                    "speechonly",
                    "sentiment",
                    "background",
                    "speaker",
                    "acoustic",
                    "07b",
                    "gender",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The 0.7B speech-only model achieves top acoustic consistency (e.g., 90.8% speaker in SALMON), surpassing larger baselines, indicating stable acoustic planning is a sequence modeling property. Interleaving text and speech reduces consistency but boosts semantic and alignment metrics, suggesting training mix tunes consistency versus grounding without altering tokenizer or architecture.</p>\n\n",
                "matched_terms": [
                    "salmon",
                    "consistency",
                    "speechonly",
                    "speaker",
                    "acoustic",
                    "07b",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A complementary line of work trains an LM directly on codec (or pseudo-phonetic) units. Early &#8220;textless&#8221; results show that LMs over self-supervised units capture long-range structure without transcripts <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib1\" title=\"\">1</a>]</cite>. AudioLM introduces separate semantic and acoustic token streams to model content and micro-texture <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib9\" title=\"\">9</a>]</cite>, while SoundStorm improves efficiency with parallel (non-autoregressive) synthesis for the acoustic stream <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib2\" title=\"\">2</a>]</cite>. VALL-E frames TTS as conditional generation of codec tokens from text, demonstrating strong zero-shot voice transfer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib3\" title=\"\">3</a>]</cite>. Subsequent work scales speech-only LMs on discrete units to strengthen long-context generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib10\" title=\"\">10</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "acoustic",
                    "speechonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Consistency training.</span>\nWe train the LM with two mechanisms that improve robustness invariance. (i) Multi rate thinning and span erasure. For a sampled rate <math alttext=\"r\\in\\{1,2,3,4\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in\\{1,2,3,4\\}</annotation></semantics></math> we keep indices <math alttext=\"\\{t:t\\bmod r=0\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mi>t</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mi>t</mi><mo>mod</mo><mi>r</mi></mrow><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t:t\\bmod r=0\\}</annotation></semantics></math> and then erase random spans with probability <math alttext=\"p_{\\text{erase}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m3\" intent=\":literal\"><semantics><msub><mi>p</mi><mtext>erase</mtext></msub><annotation encoding=\"application/x-tex\">p_{\\text{erase}}</annotation></semantics></math>, producing a sparser sequence. Labels are matched to surviving positions, teaching prediction under timing jitter and missing context. (ii) Delayed coarse and fine auxiliaries. We cluster centroids into <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m4\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> buckets and assign each audio token a coarse label <math alttext=\"b_{t}\\in\\{1,\\dots,K\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m5\" intent=\":literal\"><semantics><mrow><msub><mi>b</mi><mi>t</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>K</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b_{t}\\in\\{1,\\dots,K\\}</annotation></semantics></math>. We add a coarse loss <math alttext=\"\\mathcal{L}_{\\text{coarse}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>coarse</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{coarse}}</annotation></semantics></math> on <math alttext=\"b_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m7\" intent=\":literal\"><semantics><msub><mi>b</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">b_{t}</annotation></semantics></math> and a next code loss <math alttext=\"\\mathcal{L}_{\\text{next}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>next</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{next}}</annotation></semantics></math> on <math alttext=\"y_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m9\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">y_{t}</annotation></semantics></math>, which biases the model to plan content before predicting fine acoustic detail. These augmentations target invariance to timing jitter and robustness under missing context.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved training.</span>\nBeyond speech-only sequences we train on interleaved text&#8211;audio streams. For paired audio and transcripts we tokenize text with BPE and audio with codec units and merge them in chronological order using lightweight time alignment. The objective remains next token prediction over the mixed vocabulary. Thinning and erasure are applied only to the audio subsequence, encouraging the LM to anchor acoustic detail in lexical content.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "speechonly",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup</span> We train on LibriLight English <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib12\" title=\"\">12</a>]</cite> (57k hours), which consists of unlabeled English audiobook recordings (mainly read speech), curated to be relatively clean. We also add the clean subset of People&#8217;s Speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib13\" title=\"\">13</a>]</cite>, contributing about 20k hours of conversational and broadcast data. Unless noted all experiments use the combined dataset. We train three CAST variants: CAST 0.7B (speech-only), CAST 1.0B (speech-only), and CAST 1.0B (speech+text interleaved).\nAll models use the Gemma 3 1B decoder-only transformer. We hold the transformer body fixed and vary only the text vocabulary size. The Speech-only 1.0B model uses 262k text tokens plus 4096 speech tokens. The Speech-only 0.77B model uses 56k text tokens plus 4096 speech tokens, reducing parameters by about 24 percent without changing attention. We also train an Interleave 1.0B model with the full text vocabulary and mixed speech text spans. In the interleaved regime each sequence alternates between speech and text spans, with the text covering 35&#8211;55 percent of the utterance duration and inserted at random positions. This balances modalities and preserves acoustic continuity. Training uses a constant learning rate of <math alttext=\"3.0\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p6.m1\" intent=\":literal\"><semantics><mrow><mn>3.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.0\\times 10^{-5}</annotation></semantics></math>, an effective batch size of 16 per device, bfloat16 precision, and identical optimizer settings across models.</p>\n\n",
                "matched_terms": [
                    "speechonly",
                    "acoustic",
                    "07b",
                    "speechtext",
                    "cast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Acoustic studies.</span>\nSALMON measures whether the model assigns higher likelihood to a natural recording compared to one where a single acoustic factor (speaker, gender, sentiment, background, or room) changes mid-utterance. For example, gender consistency tests whether a switch from male to female voice inside the same sentence is penalized. The speech-only 0.7B model achieves the strongest stability, scoring 90.8 on speaker consistency and 90.0 on gender consistency, outperforming the 1.0B speech-only model and even larger baselines such as SpiritLM 7B (81.0 speaker, 85.0 gender). This suggests that stability could be more influenced by LM-side training than by parameter count. Interleaving text with speech reduces stability by 5&#8211;9 points across all factors, despite using the same codec and decoding path, which ties the drop to the training mix. Across tasks, gender is the easiest factor, while background and room are the most difficult, reflecting that identity cues are localized while scene cues require longer context.</p>\n\n",
                "matched_terms": [
                    "spiritlm",
                    "salmon",
                    "consistency",
                    "room",
                    "speechonly",
                    "sentiment",
                    "background",
                    "speaker",
                    "acoustic",
                    "07b",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment studies.</span>\nSALMON also tests whether acoustics match the spoken content by pairing a sample where the background or sentiment is consistent with the text against a mismatched version. The interleaved 1.0B model improves sentiment alignment to 54.5 and background alignment to 59.0, compared to 48.5 and 51.5 for the speech-only 1.0B model. Humans score above 93 on both tasks, leaving a gap in joint reasoning over lexical and acoustic context. The trade-off is clear: speech-only favors stability, while interleaving favors alignment.</p>\n\n",
                "matched_terms": [
                    "salmon",
                    "speechonly",
                    "sentiment",
                    "background",
                    "acoustic",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic probes.</span>\nWe measure lexical and syntactic knowledge with sWUGGY and sBLiMP, where the model must prefer a real word over a pseudoword or a grammatical sentence over an ungrammatical variant. Interleaving speech and text yields a large gain in sWUGGY (73.7 vs. 65.6 for the 0.7B speech-only model) and a smaller gain in sBLiMP (58.3 vs. 55.9). Larger models such as Twist 7B reach higher sWUGGY (82.8) but do not improve sBLiMP, showing that scaling helps lexical cues more than syntax. This suggests interleaving lets the LM reuse text priors for speech, strengthening lexical regularities.</p>\n\n",
                "matched_terms": [
                    "twist",
                    "07b",
                    "speechonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Auxiliaries and robustness.</span>\nWe ablate the delayed coarse and next-code auxiliary losses, which encourage the LM to plan content before predicting fine acoustics. Removing them reduces stability by 5&#8211;7 points on speaker and gender for both the 0.7B and 1.0B models (e.g., speaker drops from 90.8 to 83.5 in the 0.7B model). Background and room consistency also decline but more moderately, suggesting that auxiliaries contribute most strongly to identity cues. A single outlier appears for room in the 1.0B model, which we attribute to instability in a single run, since all other factors degrade smoothly.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "room",
                    "background",
                    "speaker",
                    "07b",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Taken together, these studies show that acoustic stability peaks in the speech-only setting, while interleaving improves alignment and semantics at the cost of consistency. The codec, architecture, and decoding path remain fixed, so the operating point is governed by the training mix and LM-side objectives. This makes the trade-off between stability and lexical grounding controllable without architectural changes.\n</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "consistency",
                    "speechonly",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We study speech language models that improve robustness by distilling self-supervised features and guiding prediction with auxiliary planning. We introduce CAST, with self-supervised initialization, light alignment, and robustness objectives. Across diverse evaluations, the 0.7B speech-only model exhibited top acoustic consistency, while interleaving speech and text enhanced semantics and alignment. Notably, our 0.7B model rivals 7B systems, suggesting LM objectives as a scalable path to consistency. Future work can extend this interface to interactive tasks such as spoken dialogue, assistive speech interfaces, and creative generation where the trade-off between acoustic fidelity and lexical accuracy is critical.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "speechonly",
                    "acoustic",
                    "07b",
                    "alignment",
                    "cast"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Optimizing Speech Language Models for Acoustic Consistency",
        "caption": "Table 2: sWUGGY and sBLiMP performance (%). Best results per column are highlighted.",
        "body": "Method\nsWUGGY\nsBLiMP\n\n\n\n\nCAST 0.7B (Speech-only)\n65.6\n55.9\n\n\nCAST 1B (Speech-only)\n67.0\n57.2\n\n\nCAST 1B (Speech+Text)\n73.7\n58.3\n\n\nSpiritLM 7B\n75.5\n58.3\n\n\nTwist 7B\n\n\\cellcolorcastblue!2082.8\n\n56.2\n\n\nLAST 1.3B\n73.6\n55.3\n\n\nFlow-SLM 1B\n73.2\n\n\\cellcolorcastblue!2060.0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">sWUGGY</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">sBLiMP</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">CAST 0.7B (Speech-only)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">CAST 1B (Speech-only)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">67.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">CAST 1B (Speech+Text)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">SpiritLM 7B</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">75.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Twist 7B</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">82.8</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">LAST 1.3B</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Flow-SLM 1B</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">60.0</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sblimp",
            "speechonly",
            "twist",
            "07b",
            "swuggy",
            "best",
            "cellcolorcastblue20828",
            "results",
            "column",
            "speechtext",
            "method",
            "performance",
            "13b",
            "flowslm",
            "cast",
            "spiritlm",
            "highlighted",
            "cellcolorcastblue20600",
            "last"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate the three CAST variants against prior systems that represent the main approaches in the field. For comparison, we include prior systems that represent the main approaches in the field: Twist (7B speech-only) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib10\" title=\"\">10</a>]</cite>, which initializes from a large text LM and models HuBERT units; LAST (1.3B) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib14\" title=\"\">14</a>]</cite>, which uses LM-guided tokenization to better align speech tokens with text; SpiritLM (7B interleaved) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib5\" title=\"\">5</a>]</cite>, which extends a text LM with speech tokens and shows strong semantic grounding; and Flow-SLM (1B) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib15\" title=\"\">15</a>]</cite>, which emphasizes efficiency through flow-based objectives. These systems differ in tokenization, architecture, and training objectives, and they provide reference points for evaluating the effects of our LM-side interface and training mix. We summarize results across SALMON acoustic and alignment tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib16\" title=\"\">16</a>]</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S3.T1\" title=\"Table 1 &#8227; 3 Method &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), semantic probes (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), and ARCH linear probes <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib17\" title=\"\">17</a>]</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S4.T3\" title=\"Table 3 &#8227; 4 Results &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We study speech language models that incorporate semantic initialization and planning losses to achieve robust and consistent generation. Our approach initializes speech tokens with self-supervised features, applies a light alignment loss, and trains with thinning and auxiliary objectives that target robustness and content planning. We train three models: a 0.7B speech-only model, a 1.0B speech-only model, and a 1.0B interleaved model with both text and speech. Acoustic studies show that the speech-only models achieve the highest consistency across speaker, gender, sentiment, room, and background factors, surpassing larger systems. Interleaving improves lexical and syntactic probes and semantic&#8211;acoustic alignment but reduces consistency. Linear probes show that our initialization biases the model toward content structure while trading off prosody detail. These results show that LM-side design and training mix control the balance between acoustic stability and semantic grounding without changes to the tokenizer or runtime architecture. A demo and model weights are available for exploration.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mortezaro.github.io/speech-cast/\" title=\"\">https://mortezaro.github.io/speech-cast/</a> (demo); <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/KrauthammerLab/cast-0.7b-s2s\" title=\"\">https://huggingface.co/KrauthammerLab/cast-0.7b-s2s</a> (HF model card).</span></span></span></p>\n\n",
                "matched_terms": [
                    "results",
                    "07b",
                    "speechonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train three models: a 0.7B speech-only model, a 1.0B speech-only model, and a 1.0B interleaved model with both text and speech. We evaluate them in three ways. Acoustic studies test whether the model gives higher likelihood to natural recordings compared to samples with mid-utterance changes in gender, speaker, background, room, or sentiment. Semantic studies probe lexical and grammatical regularities in spoken sequences. Alignment studies test whether acoustic conditions match the spoken text. We also include ablations that isolate initialization, thinning, and auxiliary losses.</p>\n\n",
                "matched_terms": [
                    "07b",
                    "speechonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The 0.7B speech-only model achieves top acoustic consistency (e.g., 90.8% speaker in SALMON), surpassing larger baselines, indicating stable acoustic planning is a sequence modeling property. Interleaving text and speech reduces consistency but boosts semantic and alignment metrics, suggesting training mix tunes consistency versus grounding without altering tokenizer or architecture.</p>\n\n",
                "matched_terms": [
                    "07b",
                    "speechonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A complementary line of work trains an LM directly on codec (or pseudo-phonetic) units. Early &#8220;textless&#8221; results show that LMs over self-supervised units capture long-range structure without transcripts <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib1\" title=\"\">1</a>]</cite>. AudioLM introduces separate semantic and acoustic token streams to model content and micro-texture <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib9\" title=\"\">9</a>]</cite>, while SoundStorm improves efficiency with parallel (non-autoregressive) synthesis for the acoustic stream <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib2\" title=\"\">2</a>]</cite>. VALL-E frames TTS as conditional generation of codec tokens from text, demonstrating strong zero-shot voice transfer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib3\" title=\"\">3</a>]</cite>. Subsequent work scales speech-only LMs on discrete units to strengthen long-context generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib10\" title=\"\">10</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speechonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup</span> We train on LibriLight English <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib12\" title=\"\">12</a>]</cite> (57k hours), which consists of unlabeled English audiobook recordings (mainly read speech), curated to be relatively clean. We also add the clean subset of People&#8217;s Speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib13\" title=\"\">13</a>]</cite>, contributing about 20k hours of conversational and broadcast data. Unless noted all experiments use the combined dataset. We train three CAST variants: CAST 0.7B (speech-only), CAST 1.0B (speech-only), and CAST 1.0B (speech+text interleaved).\nAll models use the Gemma 3 1B decoder-only transformer. We hold the transformer body fixed and vary only the text vocabulary size. The Speech-only 1.0B model uses 262k text tokens plus 4096 speech tokens. The Speech-only 0.77B model uses 56k text tokens plus 4096 speech tokens, reducing parameters by about 24 percent without changing attention. We also train an Interleave 1.0B model with the full text vocabulary and mixed speech text spans. In the interleaved regime each sequence alternates between speech and text spans, with the text covering 35&#8211;55 percent of the utterance duration and inserted at random positions. This balances modalities and preserves acoustic continuity. Training uses a constant learning rate of <math alttext=\"3.0\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p6.m1\" intent=\":literal\"><semantics><mrow><mn>3.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.0\\times 10^{-5}</annotation></semantics></math>, an effective batch size of 16 per device, bfloat16 precision, and identical optimizer settings across models.</p>\n\n",
                "matched_terms": [
                    "speechtext",
                    "07b",
                    "cast",
                    "speechonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Acoustic studies.</span>\nSALMON measures whether the model assigns higher likelihood to a natural recording compared to one where a single acoustic factor (speaker, gender, sentiment, background, or room) changes mid-utterance. For example, gender consistency tests whether a switch from male to female voice inside the same sentence is penalized. The speech-only 0.7B model achieves the strongest stability, scoring 90.8 on speaker consistency and 90.0 on gender consistency, outperforming the 1.0B speech-only model and even larger baselines such as SpiritLM 7B (81.0 speaker, 85.0 gender). This suggests that stability could be more influenced by LM-side training than by parameter count. Interleaving text with speech reduces stability by 5&#8211;9 points across all factors, despite using the same codec and decoding path, which ties the drop to the training mix. Across tasks, gender is the easiest factor, while background and room are the most difficult, reflecting that identity cues are localized while scene cues require longer context.</p>\n\n",
                "matched_terms": [
                    "spiritlm",
                    "07b",
                    "speechonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic probes.</span>\nWe measure lexical and syntactic knowledge with sWUGGY and sBLiMP, where the model must prefer a real word over a pseudoword or a grammatical sentence over an ungrammatical variant. Interleaving speech and text yields a large gain in sWUGGY (73.7 vs. 65.6 for the 0.7B speech-only model) and a smaller gain in sBLiMP (58.3 vs. 55.9). Larger models such as Twist 7B reach higher sWUGGY (82.8) but do not improve sBLiMP, showing that scaling helps lexical cues more than syntax. This suggests interleaving lets the LM reuse text priors for speech, strengthening lexical regularities.</p>\n\n",
                "matched_terms": [
                    "speechonly",
                    "sblimp",
                    "swuggy",
                    "twist",
                    "07b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We study speech language models that improve robustness by distilling self-supervised features and guiding prediction with auxiliary planning. We introduce CAST, with self-supervised initialization, light alignment, and robustness objectives. Across diverse evaluations, the 0.7B speech-only model exhibited top acoustic consistency, while interleaving speech and text enhanced semantics and alignment. Notably, our 0.7B model rivals 7B systems, suggesting LM objectives as a scalable path to consistency. Future work can extend this interface to interactive tasks such as spoken dialogue, assistive speech interfaces, and creative generation where the trade-off between acoustic fidelity and lexical accuracy is critical.</p>\n\n",
                "matched_terms": [
                    "07b",
                    "cast",
                    "speechonly"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Optimizing Speech Language Models for Acoustic Consistency",
        "caption": "Table 3: Classification accuracy (%) vs. training stage for a variant with semantic-distilled initialization. Best results per column are highlighted.",
        "body": "Audio Events\nSpeech\n\n\nVariant\nStage\nESC-50\nUS8K\nVIVAE\nRAVDESS\nSLURP\nEMOVO\n\n\nRaw\n100%\n26.5\n40.2\n30.0\n33.1\n8.0\n31.2\n\n\nSem-distilled\n10%\n27.9\n40.8\n27.7\n33.5\n7.9\n29.8\n\n\nSem-distilled\n40%\n30.6\n43.1\n26.5\n34.8\n\n\\cellcolorcastblue!208.1\n\n29.3\n\n\nSem-distilled\n100%\n\n\\cellcolorcastblue!2032.6\n\n\n\\cellcolorcastblue!2045.9\n\n27.5\n\n\\cellcolorcastblue!2038.9\n\n\n\\cellcolorcastblue!208.1\n\n29.3",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"/>\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio Events</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Variant</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Stage</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ESC-50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">US8K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">VIVAE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RAVDESS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SLURP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EMOVO</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Raw</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">100%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Sem-distilled</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Sem-distilled</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">40%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">8.1</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Sem-distilled</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">100%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">32.6</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">45.9</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">38.9</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">8.1</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.3</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "esc50",
            "ravdess",
            "slurp",
            "events",
            "speech",
            "us8k",
            "raw",
            "vivae",
            "classification",
            "initialization",
            "cellcolorcastblue2081",
            "stage",
            "best",
            "accuracy",
            "results",
            "cellcolorcastblue20459",
            "column",
            "audio",
            "cellcolorcastblue20389",
            "training",
            "highlighted",
            "semanticdistilled",
            "emovo",
            "cellcolorcastblue20326",
            "semdistilled",
            "variant"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate the three CAST variants against prior systems that represent the main approaches in the field. For comparison, we include prior systems that represent the main approaches in the field: Twist (7B speech-only) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib10\" title=\"\">10</a>]</cite>, which initializes from a large text LM and models HuBERT units; LAST (1.3B) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib14\" title=\"\">14</a>]</cite>, which uses LM-guided tokenization to better align speech tokens with text; SpiritLM (7B interleaved) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib5\" title=\"\">5</a>]</cite>, which extends a text LM with speech tokens and shows strong semantic grounding; and Flow-SLM (1B) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib15\" title=\"\">15</a>]</cite>, which emphasizes efficiency through flow-based objectives. These systems differ in tokenization, architecture, and training objectives, and they provide reference points for evaluating the effects of our LM-side interface and training mix. We summarize results across SALMON acoustic and alignment tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib16\" title=\"\">16</a>]</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S3.T1\" title=\"Table 1 &#8227; 3 Method &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), semantic probes (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), and ARCH linear probes <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib17\" title=\"\">17</a>]</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S4.T3\" title=\"Table 3 &#8227; 4 Results &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We study speech language models that incorporate semantic initialization and planning losses to achieve robust and consistent generation. Our approach initializes speech tokens with self-supervised features, applies a light alignment loss, and trains with thinning and auxiliary objectives that target robustness and content planning. We train three models: a 0.7B speech-only model, a 1.0B speech-only model, and a 1.0B interleaved model with both text and speech. Acoustic studies show that the speech-only models achieve the highest consistency across speaker, gender, sentiment, room, and background factors, surpassing larger systems. Interleaving improves lexical and syntactic probes and semantic&#8211;acoustic alignment but reduces consistency. Linear probes show that our initialization biases the model toward content structure while trading off prosody detail. These results show that LM-side design and training mix control the balance between acoustic stability and semantic grounding without changes to the tokenizer or runtime architecture. A demo and model weights are available for exploration.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mortezaro.github.io/speech-cast/\" title=\"\">https://mortezaro.github.io/speech-cast/</a> (demo); <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/KrauthammerLab/cast-0.7b-s2s\" title=\"\">https://huggingface.co/KrauthammerLab/cast-0.7b-s2s</a> (HF model card).</span></span></span></p>\n\n",
                "matched_terms": [
                    "results",
                    "initialization",
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nspeech language model, interleaved training, alignment, self-supervised initialization</p>\n\n",
                "matched_terms": [
                    "speech",
                    "initialization",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many recent works address speech modeling by exposing codec outputs as discrete tokens in the vocabulary of a decoder-only transformer and by interleaving these tokens with text <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib5\" title=\"\">5</a>]</cite>. In this setting, the model predicts a unified sequence of speech and text symbols step by step. Our CAST approach builds on this by shaping the language model&#8217;s embeddings and training objectives to prioritize acoustic consistency, isolating LM-side contributions without modifying the tokenizer or inference path.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support this interface, we shape the language model&#8217;s embedding and training strategy. Each speech token maps to a literal token with an embedding initialized from a frozen self-supervised encoder. A stop-gradient alignment loss preserves phonetic structure in the learned representation. During training, we apply consistency-based augmentations: thinning the speech stream at variable rates and erasing short spans to encourage invariance to timing and context gaps. Auxiliary losses guide the model to plan coarse structure before predicting fine acoustic detail.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train three models: a 0.7B speech-only model, a 1.0B speech-only model, and a 1.0B interleaved model with both text and speech. We evaluate them in three ways. Acoustic studies test whether the model gives higher likelihood to natural recordings compared to samples with mid-utterance changes in gender, speaker, background, room, or sentiment. Semantic studies probe lexical and grammatical regularities in spoken sequences. Alignment studies test whether acoustic conditions match the spoken text. We also include ablations that isolate initialization, thinning, and auxiliary losses.</p>\n\n",
                "matched_terms": [
                    "initialization",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The 0.7B speech-only model achieves top acoustic consistency (e.g., 90.8% speaker in SALMON), surpassing larger baselines, indicating stable acoustic planning is a sequence modeling property. Interleaving text and speech reduces consistency but boosts semantic and alignment metrics, suggesting training mix tunes consistency versus grounding without altering tokenizer or architecture.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent omni systems extend frozen text LLMs with audio encoders and lightweight adapters to support speech understanding and generation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib11\" title=\"\">11</a>]</cite>. These designs broaden task coverage (ASR, captioning, S2S, instruction following) but introduce additional components and training stages that make it harder to isolate what the LM itself contributes. We keep the tokenizer and architecture fixed, adapting only LM-side. This isolates design effects, simplifies deployment, and reveals the stability&#8211;grounding trade-off.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We study a speech LM that treats a frozen neural audio codec as a black box and performs all adaptation on the language model side. The LM acts as a sequencer over discrete codec indices while the frozen codec serves as a vocoder. This separation keeps inference simple and fast and lets us shape audio token embeddings and training signals for robustness invariance and content first planning. The backbone is a decoder-only transformer originally designed for text. Its tokenizer and embedding matrix are extended with speech tokens, so text and speech share a unified vocabulary and prediction objective. We call this LM-side design and training recipe CAST. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the design.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Consistency training.</span>\nWe train the LM with two mechanisms that improve robustness invariance. (i) Multi rate thinning and span erasure. For a sampled rate <math alttext=\"r\\in\\{1,2,3,4\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in\\{1,2,3,4\\}</annotation></semantics></math> we keep indices <math alttext=\"\\{t:t\\bmod r=0\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mi>t</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mi>t</mi><mo>mod</mo><mi>r</mi></mrow><mo>=</mo><mn>0</mn></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{t:t\\bmod r=0\\}</annotation></semantics></math> and then erase random spans with probability <math alttext=\"p_{\\text{erase}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m3\" intent=\":literal\"><semantics><msub><mi>p</mi><mtext>erase</mtext></msub><annotation encoding=\"application/x-tex\">p_{\\text{erase}}</annotation></semantics></math>, producing a sparser sequence. Labels are matched to surviving positions, teaching prediction under timing jitter and missing context. (ii) Delayed coarse and fine auxiliaries. We cluster centroids into <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m4\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> buckets and assign each audio token a coarse label <math alttext=\"b_{t}\\in\\{1,\\dots,K\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m5\" intent=\":literal\"><semantics><mrow><msub><mi>b</mi><mi>t</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>K</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b_{t}\\in\\{1,\\dots,K\\}</annotation></semantics></math>. We add a coarse loss <math alttext=\"\\mathcal{L}_{\\text{coarse}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>coarse</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{coarse}}</annotation></semantics></math> on <math alttext=\"b_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m7\" intent=\":literal\"><semantics><msub><mi>b</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">b_{t}</annotation></semantics></math> and a next code loss <math alttext=\"\\mathcal{L}_{\\text{next}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>next</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{next}}</annotation></semantics></math> on <math alttext=\"y_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m9\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">y_{t}</annotation></semantics></math>, which biases the model to plan content before predicting fine acoustic detail. These augmentations target invariance to timing jitter and robustness under missing context.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Interleaved training.</span>\nBeyond speech-only sequences we train on interleaved text&#8211;audio streams. For paired audio and transcripts we tokenize text with BPE and audio with codec units and merge them in chronological order using lightweight time alignment. The objective remains next token prediction over the mixed vocabulary. Thinning and erasure are applied only to the audio subsequence, encouraging the LM to anchor acoustic detail in lexical content.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup</span> We train on LibriLight English <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib12\" title=\"\">12</a>]</cite> (57k hours), which consists of unlabeled English audiobook recordings (mainly read speech), curated to be relatively clean. We also add the clean subset of People&#8217;s Speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib13\" title=\"\">13</a>]</cite>, contributing about 20k hours of conversational and broadcast data. Unless noted all experiments use the combined dataset. We train three CAST variants: CAST 0.7B (speech-only), CAST 1.0B (speech-only), and CAST 1.0B (speech+text interleaved).\nAll models use the Gemma 3 1B decoder-only transformer. We hold the transformer body fixed and vary only the text vocabulary size. The Speech-only 1.0B model uses 262k text tokens plus 4096 speech tokens. The Speech-only 0.77B model uses 56k text tokens plus 4096 speech tokens, reducing parameters by about 24 percent without changing attention. We also train an Interleave 1.0B model with the full text vocabulary and mixed speech text spans. In the interleaved regime each sequence alternates between speech and text spans, with the text covering 35&#8211;55 percent of the utterance duration and inserted at random positions. This balances modalities and preserves acoustic continuity. Training uses a constant learning rate of <math alttext=\"3.0\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p6.m1\" intent=\":literal\"><semantics><mrow><mn>3.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.0\\times 10^{-5}</annotation></semantics></math>, an effective batch size of 16 per device, bfloat16 precision, and identical optimizer settings across models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Acoustic studies.</span>\nSALMON measures whether the model assigns higher likelihood to a natural recording compared to one where a single acoustic factor (speaker, gender, sentiment, background, or room) changes mid-utterance. For example, gender consistency tests whether a switch from male to female voice inside the same sentence is penalized. The speech-only 0.7B model achieves the strongest stability, scoring 90.8 on speaker consistency and 90.0 on gender consistency, outperforming the 1.0B speech-only model and even larger baselines such as SpiritLM 7B (81.0 speaker, 85.0 gender). This suggests that stability could be more influenced by LM-side training than by parameter count. Interleaving text with speech reduces stability by 5&#8211;9 points across all factors, despite using the same codec and decoding path, which ties the drop to the training mix. Across tasks, gender is the easiest factor, while background and room are the most difficult, reflecting that identity cues are localized while scene cues require longer context.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic probes.</span>\nWe measure lexical and syntactic knowledge with sWUGGY and sBLiMP, where the model must prefer a real word over a pseudoword or a grammatical sentence over an ungrammatical variant. Interleaving speech and text yields a large gain in sWUGGY (73.7 vs. 65.6 for the 0.7B speech-only model) and a smaller gain in sBLiMP (58.3 vs. 55.9). Larger models such as Twist 7B reach higher sWUGGY (82.8) but do not improve sBLiMP, showing that scaling helps lexical cues more than syntax. This suggests interleaving lets the LM reuse text priors for speech, strengthening lexical regularities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "variant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ARCH linear probes.</span>\nTo study internal representations, we train linear classifiers on frozen LM features from six datasets. ESC-50, US8K, and SLURP probe environmental events and intent (content-leaning), while VIVAE and EMOVO probe vocal imitations and emotion (prosody-leaning), and RAVDESS probes acted emotions (mixed). Variant A, which initializes speech-token embeddings from SSL centroids and applies a light alignment loss, outperforms a randomly initialized baseline. It matches or surpasses the final baseline accuracy on content-leaning tasks as early as 10% of training and reaches +5&#8211;6 points on ESC-50 and US8K and +6 points on RAVDESS by the end. It maintains parity on SLURP, while giving up 2&#8211;3 points on VIVAE and EMOVO. On average, content tasks rise by +16% relative while prosody tasks dip by &#8211;7%, matching the intended bias toward content.</p>\n\n",
                "matched_terms": [
                    "esc50",
                    "ravdess",
                    "slurp",
                    "emovo",
                    "events",
                    "accuracy",
                    "us8k",
                    "vivae",
                    "training",
                    "variant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We study speech language models that improve robustness by distilling self-supervised features and guiding prediction with auxiliary planning. We introduce CAST, with self-supervised initialization, light alignment, and robustness objectives. Across diverse evaluations, the 0.7B speech-only model exhibited top acoustic consistency, while interleaving speech and text enhanced semantics and alignment. Notably, our 0.7B model rivals 7B systems, suggesting LM objectives as a scalable path to consistency. Future work can extend this interface to interactive tasks such as spoken dialogue, assistive speech interfaces, and creative generation where the trade-off between acoustic fidelity and lexical accuracy is critical.</p>\n\n",
                "matched_terms": [
                    "initialization",
                    "speech",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speech tokenizer (<span class=\"ltx_text ltx_font_typewriter\">WavTokenizer-large-unify-40token</span>) was frozen during training. We verified codebook coverage after filtering to avoid degenerate sequences. No fine-tuning or retraining of the codec was performed.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech tokens <span class=\"ltx_text ltx_font_typewriter\">[Sp1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[Sp4096]</span> were appended to the text LM&#8217;s vocabulary. The embedding matrix was resized once before training and padded to the nearest multiple of 8 for efficiency. No merges or special segmentation rules were changed.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "Optimizing Speech Language Models for Acoustic Consistency",
        "caption": "Table 4: Effect of auxiliary losses on SALMON performance (%). Best results per column are highlighted.",
        "body": "Method\nSent\nSpkr\nGen\nBg(In)\nBg(R)\nRoom\n\n\nCAST 0.7B (+Aux)\n\n\\cellcolorcastblue!2081.8\n\n\n\\cellcolorcastblue!2090.8\n\n\n\\cellcolorcastblue!2090.0\n\n\n\\cellcolorcastblue!2080.0\n\n\n\\cellcolorcastblue!2077.5\n\n90.0\n\n\nCAST 1B (+Aux)\n\n\\cellcolorcastblue!2081.8\n\n90.0\n\n\\cellcolorcastblue!2090.0\n\n78.0\n68.5\n\n\\cellcolorcastblue!2091.0\n\n\n\nCAST 0.7B (–Aux)\n75.5\n83.5\n83.0\n76.0\n71.0\n89.5\n\n\nCAST 1B (–Aux)\n75.0\n83.0\n82.0\n75.0\n71.0\n90.0",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Sent</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Spkr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gen</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Bg(In)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Bg(R)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Room</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CAST 0.7B (+Aux)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">81.8</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">90.8</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">90.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">80.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">77.5</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CAST 1B (+Aux)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">81.8</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">90.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">78.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_ERROR undefined\">\\cellcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">castblue!20</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">91.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CAST 0.7B (&#8211;Aux)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">75.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CAST 1B (&#8211;Aux)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">75.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">83.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">82.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">75.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">90.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "losses",
            "–aux",
            "effect",
            "07b",
            "cellcolorcastblue20775",
            "sent",
            "cellcolorcastblue20900",
            "salmon",
            "best",
            "cellcolorcastblue20910",
            "cellcolorcastblue20908",
            "auxiliary",
            "results",
            "column",
            "method",
            "room",
            "performance",
            "cellcolorcastblue20818",
            "bgr",
            "aux",
            "cast",
            "bgin",
            "gen",
            "cellcolorcastblue20800",
            "highlighted",
            "spkr"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We study speech language models that incorporate semantic initialization and planning losses to achieve robust and consistent generation. Our approach initializes speech tokens with self-supervised features, applies a light alignment loss, and trains with thinning and auxiliary objectives that target robustness and content planning. We train three models: a 0.7B speech-only model, a 1.0B speech-only model, and a 1.0B interleaved model with both text and speech. Acoustic studies show that the speech-only models achieve the highest consistency across speaker, gender, sentiment, room, and background factors, surpassing larger systems. Interleaving improves lexical and syntactic probes and semantic&#8211;acoustic alignment but reduces consistency. Linear probes show that our initialization biases the model toward content structure while trading off prosody detail. These results show that LM-side design and training mix control the balance between acoustic stability and semantic grounding without changes to the tokenizer or runtime architecture. A demo and model weights are available for exploration.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mortezaro.github.io/speech-cast/\" title=\"\">https://mortezaro.github.io/speech-cast/</a> (demo); <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/KrauthammerLab/cast-0.7b-s2s\" title=\"\">https://huggingface.co/KrauthammerLab/cast-0.7b-s2s</a> (HF model card).</span></span></span></p>\n\n",
                "matched_terms": [
                    "losses",
                    "room",
                    "auxiliary",
                    "results",
                    "07b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support this interface, we shape the language model&#8217;s embedding and training strategy. Each speech token maps to a literal token with an embedding initialized from a frozen self-supervised encoder. A stop-gradient alignment loss preserves phonetic structure in the learned representation. During training, we apply consistency-based augmentations: thinning the speech stream at variable rates and erasing short spans to encourage invariance to timing and context gaps. Auxiliary losses guide the model to plan coarse structure before predicting fine acoustic detail.</p>\n\n",
                "matched_terms": [
                    "auxiliary",
                    "losses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train three models: a 0.7B speech-only model, a 1.0B speech-only model, and a 1.0B interleaved model with both text and speech. We evaluate them in three ways. Acoustic studies test whether the model gives higher likelihood to natural recordings compared to samples with mid-utterance changes in gender, speaker, background, room, or sentiment. Semantic studies probe lexical and grammatical regularities in spoken sequences. Alignment studies test whether acoustic conditions match the spoken text. We also include ablations that isolate initialization, thinning, and auxiliary losses.</p>\n\n",
                "matched_terms": [
                    "auxiliary",
                    "07b",
                    "losses",
                    "room"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The 0.7B speech-only model achieves top acoustic consistency (e.g., 90.8% speaker in SALMON), surpassing larger baselines, indicating stable acoustic planning is a sequence modeling property. Interleaving text and speech reduces consistency but boosts semantic and alignment metrics, suggesting training mix tunes consistency versus grounding without altering tokenizer or architecture.</p>\n\n",
                "matched_terms": [
                    "salmon",
                    "07b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Setup</span> We train on LibriLight English <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib12\" title=\"\">12</a>]</cite> (57k hours), which consists of unlabeled English audiobook recordings (mainly read speech), curated to be relatively clean. We also add the clean subset of People&#8217;s Speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib13\" title=\"\">13</a>]</cite>, contributing about 20k hours of conversational and broadcast data. Unless noted all experiments use the combined dataset. We train three CAST variants: CAST 0.7B (speech-only), CAST 1.0B (speech-only), and CAST 1.0B (speech+text interleaved).\nAll models use the Gemma 3 1B decoder-only transformer. We hold the transformer body fixed and vary only the text vocabulary size. The Speech-only 1.0B model uses 262k text tokens plus 4096 speech tokens. The Speech-only 0.77B model uses 56k text tokens plus 4096 speech tokens, reducing parameters by about 24 percent without changing attention. We also train an Interleave 1.0B model with the full text vocabulary and mixed speech text spans. In the interleaved regime each sequence alternates between speech and text spans, with the text covering 35&#8211;55 percent of the utterance duration and inserted at random positions. This balances modalities and preserves acoustic continuity. Training uses a constant learning rate of <math alttext=\"3.0\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p6.m1\" intent=\":literal\"><semantics><mrow><mn>3.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.0\\times 10^{-5}</annotation></semantics></math>, an effective batch size of 16 per device, bfloat16 precision, and identical optimizer settings across models.</p>\n\n",
                "matched_terms": [
                    "07b",
                    "cast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the three CAST variants against prior systems that represent the main approaches in the field. For comparison, we include prior systems that represent the main approaches in the field: Twist (7B speech-only) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib10\" title=\"\">10</a>]</cite>, which initializes from a large text LM and models HuBERT units; LAST (1.3B) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib14\" title=\"\">14</a>]</cite>, which uses LM-guided tokenization to better align speech tokens with text; SpiritLM (7B interleaved) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib5\" title=\"\">5</a>]</cite>, which extends a text LM with speech tokens and shows strong semantic grounding; and Flow-SLM (1B) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib15\" title=\"\">15</a>]</cite>, which emphasizes efficiency through flow-based objectives. These systems differ in tokenization, architecture, and training objectives, and they provide reference points for evaluating the effects of our LM-side interface and training mix. We summarize results across SALMON acoustic and alignment tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib16\" title=\"\">16</a>]</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S3.T1\" title=\"Table 1 &#8227; 3 Method &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>), semantic probes (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), and ARCH linear probes <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#bib.bib17\" title=\"\">17</a>]</cite> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26276v1#S4.T3\" title=\"Table 3 &#8227; 4 Results &#8227; Optimizing Speech Language Models for Acoustic Consistency\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "results",
                    "salmon",
                    "cast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Acoustic studies.</span>\nSALMON measures whether the model assigns higher likelihood to a natural recording compared to one where a single acoustic factor (speaker, gender, sentiment, background, or room) changes mid-utterance. For example, gender consistency tests whether a switch from male to female voice inside the same sentence is penalized. The speech-only 0.7B model achieves the strongest stability, scoring 90.8 on speaker consistency and 90.0 on gender consistency, outperforming the 1.0B speech-only model and even larger baselines such as SpiritLM 7B (81.0 speaker, 85.0 gender). This suggests that stability could be more influenced by LM-side training than by parameter count. Interleaving text with speech reduces stability by 5&#8211;9 points across all factors, despite using the same codec and decoding path, which ties the drop to the training mix. Across tasks, gender is the easiest factor, while background and room are the most difficult, reflecting that identity cues are localized while scene cues require longer context.</p>\n\n",
                "matched_terms": [
                    "salmon",
                    "07b",
                    "room"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Auxiliaries and robustness.</span>\nWe ablate the delayed coarse and next-code auxiliary losses, which encourage the LM to plan content before predicting fine acoustics. Removing them reduces stability by 5&#8211;7 points on speaker and gender for both the 0.7B and 1.0B models (e.g., speaker drops from 90.8 to 83.5 in the 0.7B model). Background and room consistency also decline but more moderately, suggesting that auxiliaries contribute most strongly to identity cues. A single outlier appears for room in the 1.0B model, which we attribute to instability in a single run, since all other factors degrade smoothly.</p>\n\n",
                "matched_terms": [
                    "auxiliary",
                    "07b",
                    "losses",
                    "room"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We study speech language models that improve robustness by distilling self-supervised features and guiding prediction with auxiliary planning. We introduce CAST, with self-supervised initialization, light alignment, and robustness objectives. Across diverse evaluations, the 0.7B speech-only model exhibited top acoustic consistency, while interleaving speech and text enhanced semantics and alignment. Notably, our 0.7B model rivals 7B systems, suggesting LM objectives as a scalable path to consistency. Future work can extend this interface to interactive tasks such as spoken dialogue, assistive speech interfaces, and creative generation where the trade-off between acoustic fidelity and lexical accuracy is critical.</p>\n\n",
                "matched_terms": [
                    "auxiliary",
                    "07b",
                    "cast"
                ]
            }
        ]
    }
}