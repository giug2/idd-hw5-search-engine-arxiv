{
    "S4.T4": {
        "caption": "Table 4: ASR scores for three models on a 10% subset of test-clean. “P1” refers to the case where the discrete token sequences immediately before and after the target sequence are used as the prompt, while “P2” uses a sequence that corresponds to the first utterance of the same chapter as the prompt.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">WER</span>&#8201;<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\">MLL</span>&#8201;<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Gemma3-4B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Qwen3-4B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Phi-4-mini</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">P1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">P2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">P1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">P2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">P1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">P2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">HuBERT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">14.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-1.780</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-1.731</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-1.689</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-1.668</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-3.162</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-3.190</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">WavLM</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">3.44</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-1.770</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-1.712</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-1.683</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-1.647</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-3.138</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">-3.139</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">XEUS</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">-1.750</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">-1.677</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">-1.668</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">-1.613</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">-3.135</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">-3.101</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "testclean",
            "while",
            "hubert",
            "wer",
            "xeus",
            "↓downarrow",
            "before",
            "token",
            "gemma34b",
            "corresponds",
            "after",
            "target",
            "discrete",
            "case",
            "used",
            "mll",
            "wavlm",
            "first",
            "immediately",
            "scores",
            "utterance",
            "sequences",
            "asr",
            "where",
            "same",
            "↑uparrow",
            "sequence",
            "“p2”",
            "qwen34b",
            "“p1”",
            "three",
            "prompt",
            "model",
            "phi4mini",
            "subset",
            "refers",
            "uses",
            "chapter"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">ASR performance was measured on the &#8220;test-clean&#8221; set of LibriSpeech, while the MLL was computed on a random 10 % subset of the &#8220;test-clean&#8221; in order to reduce evaluation cost. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S4.T4\" title=\"Table 4 &#8227; 4.2.2 Relationship Between ASR performance and MLL &#8227; 4.2 Results &#8227; 4 Main Experiments &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the results. &#8220;P1&#8221; refers to the case where the discrete token sequences immediately before and after the target sequence are used as the prompt (<math alttext=\"\\pm 1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1</annotation></semantics></math> utterances as the context), while &#8220;P2&#8221; uses a sequence that corresponds to the first utterance of the same chapter as the prompt as mentioned in the previous paragraph. Regardless of which LLM was used or which prompt pattern was applied, the encoder that achieved the lowest WER, i.e., XEUS also yielded the highest MLLs, whereas the encoder with the highest WER, i.e., HuBERT produced the lowest MLLs. This concordance indicates that the MLL is strongly correlated with ASR performance and can serve as a proxy for comparing SSL models without any task-specific fine-tuning.\nIn addition, the MLL found to be robust to variations in prompt patterns. In particular, the results obtained using the prompt from &#8220;P2&#8221; suggests that the MLL remains effective even in scenarios where contextual information is not necessarily available.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Self-Supervised Learning (SSL) has gained traction for its ability to learn rich representations with low labeling costs, applicable across diverse downstream tasks.\nHowever, assessing the downstream-task performance remains challenging due to the cost of extra training and evaluation.\nExisting methods for task-agnostic evaluation also require extra training or hyperparameter tuning. We propose a novel evaluation metric using large language models (LLMs). By inputting discrete token sequences and minimal domain cues derived from SSL models into LLMs, we obtain the mean log-likelihood; these cues guide in-context learning, rendering the score more reliable without extra training or hyperparameter tuning. Experimental results show a correlation between LLM-based scores and automatic speech recognition task. Additionally, our findings reveal that LLMs not only functions as an SSL evaluation tools but also provides inference-time embeddings that are useful for speaker verification task.</p>\n\n",
                "matched_terms": [
                    "models",
                    "token",
                    "sequences",
                    "discrete",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, self-supervised learning (SSL) has emerged as a powerful paradigm for learning high-quality, task-agnostic representations <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx1\" title=\"\">1</a>]</cite>. In the speech domain, SSL encoders such as wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx2\" title=\"\">2</a>]</cite>, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx3\" title=\"\">3</a>]</cite>, WavLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx4\" title=\"\">4</a>]</cite>, BEST-RQ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx5\" title=\"\">5</a>]</cite> and multilingual models like XLS-R <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx6\" title=\"\">6</a>]</cite> and XEUS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx7\" title=\"\">7</a>]</cite>, and so on have pushed the state of the art on a wide range of downstream tasks.\nBy minimizing the reliance on labeled data, SSL has enabled the development of robust models that can be fine-tuned for various downstream tasks such as automatic speech recognition (ASR) and speech language understanding (SLU). However, assessing the downstream-task performance of SSL models remains a significant challenge.\nTraditional benchmarking approaches often require extensive additional training and evaluation on various tasks such as SUPERB benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx10\" title=\"\">10</a>]</cite> and the evaluation framework based on larger-capacity probing heads <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx11\" title=\"\">11</a>]</cite>, which is both time-consuming and resource-intensive.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hubert",
                    "xeus",
                    "asr",
                    "wavlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, several methods have been proposed to estimate the performance of SSL models without additional training, such as correlation-based analysis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx13\" title=\"\">13</a>]</cite> using canonical correlation analysis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx14\" title=\"\">14</a>]</cite> or its variant <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx15\" title=\"\">15</a>]</cite> to compare each layer&#8217;s representation to phonetic units and word meaning, and so on.\nThese approaches, however, depend on precise phoneme- or word-level alignments produced by a separate model-driven forced-alignment system, and also require labeled data.\nReference <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx16\" title=\"\">16</a>]</cite> has shown a high positive correlation between the pre-training loss of an SSL model and its downstream performance; however, that loss information is typically inaccessible unless the model has been trained in-house.\nPhonetic discriminability can also be assessed with the ABX error metric <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx17\" title=\"\">17</a>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx18\" title=\"\">18</a>]</cite>, but this approach demands an evaluation set specifically constructed from triplet items.\nTherefore, there is a growing need for novel, label-free, parameter-free and training-free evaluation methods that can effectively capture the task-generalization potential of SSL models.\nTherefore, there is a growing need for novel, label-free, parameter-free and training-free evaluation methods that can reliably gauge the cross-task transferability of SSL models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the remarkable progress in text-based large language models (LLMs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx20\" title=\"\">20</a>]</cite>, this paper explores the potential of leveraging these models as a new evaluation metric for speech SSL models.\nSince text-based LLMs are trained on a wide spectrum of character sequences, including natural-language text, structured data such as XML and source code, and even mathematical expressions, we hypothesize that, even without explicit training on speech, LLMs may still possess the intrinsic capability to predict a discrete token sequence that compactly encodes the information contained in speech.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sequence",
                    "token",
                    "sequences",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we propose a novel evaluation method that utilizes the log-likelihood of text-based LLMs when provided with discrete token sequences derived from SSL models. This approach requires no additional training or hyperparameter tuning, making it highly efficient and scalable. Our experiments demonstrate that the proposed method correlates with the performance of SSL models on ASR task. Furthermore, we demonstrate that it is possible to perform the speaker verification task using the embedding vectors obtained during LLM inference. This finding suggests that LLMs are not only useful for the ASR evaluation of SSL models, but also carry rich information that can be leveraged for the speaker verification task. This highlights the versatility and generalization capability of the proposed method.</p>\n\n",
                "matched_terms": [
                    "models",
                    "token",
                    "sequences",
                    "discrete",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a novel, label-free, parameter-free and training-free evaluation metric for SSL speech models, validates the utility of text-based LLMs as an ASR evaluation metric.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although highly relevant to our study, the STAB benchmark <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx21\" title=\"\">21</a>]</cite> provides lightweight diagnostic metrics for speech tokenizers, whereas our work is the first to drive an LLM with such discretized tokens and demonstrate that their ASR performance can be ranked using the likelihood alone.</p>\n\n",
                "matched_terms": [
                    "first",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we describe a novel evaluation metric of SSL models using text-based LLMs designed to address the limitations of existing methods, which require additional training or hyper-parameter tuning.\nText-based LLMs, although these have not been trained on speech data, are trained on a wide spectrum of character sequences, including natural-language text, structured data such as XML and source code <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx22\" title=\"\">22</a>]</cite>, and even mathematical expressions <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx23\" title=\"\">23</a>]</cite>. We hypothesize that, even without explicit training on speech, LLMs may still possess the intrinsic capability to predict a discrete token sequence that compactly encodes the information contained in speech.</p>\n\n",
                "matched_terms": [
                    "models",
                    "sequence",
                    "token",
                    "sequences",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Thus, we propose to evaluate the predictability of the discrete token sequence obtained from SSL models through LLM inference by calculating the mean log-likelihood.\nBy comparing these likelihood scores across SSL models, we can assess how easily LLMs can predict the sequence. An SSL model that achieves a higher score can be regarded as producing discrete token sequences that contain less noise and exhibit greater grammatical and syntactic plausibility in natural language than a model with a lower score.\nThe details of calculating our proposed metric are described below.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "sequence",
                    "token",
                    "model",
                    "sequences",
                    "discrete",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\tilde{\\mathbf{X}}=(\\tilde{x}_{1},\\tilde{x}_{2},...,\\tilde{x}_{T}),\\tilde{x}_{t}\\in\\mathbb{R}^{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mover accent=\"true\"><mi>&#119831;</mi><mo>~</mo></mover><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>D</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{X}}=(\\tilde{x}_{1},\\tilde{x}_{2},...,\\tilde{x}_{T}),\\tilde{x}_{t}\\in\\mathbb{R}^{D}</annotation></semantics></math>, <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of frames and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the feature dimension.\nThe latent sequence <math alttext=\"\\tilde{\\mathbf{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119831;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{X}}</annotation></semantics></math> is discretized via <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means clustering.\nEach representation <math alttext=\"\\tilde{x}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\tilde{x}_{t}</annotation></semantics></math> is assigned to its nearest centroid vector, producing a\ndiscrete token sequence</p>\n\n",
                "matched_terms": [
                    "token",
                    "where",
                    "discrete",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, we create an input string to LLMs. Here, we use context utterances before and after target input <math alttext=\"\\tilde{\\mathbf{Z}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m11\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#119833;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{Z}}</annotation></semantics></math> as a prompt.\nThis enables LLMs to learn properties of the discrete sequence by leveraging in-context learning with limited examples.\nExample prompt and input are as follows:</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "before",
                    "prompt",
                    "after",
                    "target",
                    "discrete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tags such as &#8220;<span class=\"ltx_text ltx_font_typewriter\">&lt;prefix&gt;</span>&#8221; and &#8220;<span class=\"ltx_text ltx_font_typewriter\">&lt;suffix&gt;</span>&#8221; is a syntactic marker that indicates whether the accompanying sequence precedes or follows the target. Let <math alttext=\"\\mathbf{S}_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m14\" intent=\":literal\"><semantics><msub><mi>&#119826;</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{S}_{n}</annotation></semantics></math> denote the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m15\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-th input sequence to LLMs, we define the set of all sequences as <math alttext=\"\\textbf{S}=\\{\\textbf{S}_{1},\\textbf{S}_{2},\\ldots,\\textbf{S}_{N}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m16\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\">S</mtext><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mtext class=\"ltx_mathvariant_bold\">S</mtext><mn>1</mn></msub><mo>,</mo><msub><mtext class=\"ltx_mathvariant_bold\">S</mtext><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mtext class=\"ltx_mathvariant_bold\">S</mtext><mi>N</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\textbf{S}=\\{\\textbf{S}_{1},\\textbf{S}_{2},\\ldots,\\textbf{S}_{N}\\}</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m17\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the number of utterances.\n<math alttext=\"\\textbf{S}_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m18\" intent=\":literal\"><semantics><msub><mtext class=\"ltx_mathvariant_bold\">S</mtext><mi>n</mi></msub><annotation encoding=\"application/x-tex\">\\textbf{S}_{n}</annotation></semantics></math> is composed of the prompt <math alttext=\"\\mathbf{S}_{n}^{\\texttt{(pr)}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m19\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mi>n</mi><mtext class=\"ltx_mathvariant_monospace\">(pr)</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}_{n}^{\\texttt{(pr)}}</annotation></semantics></math> and the target input <math alttext=\"\\mathbf{S}_{n}^{\\texttt{(in)}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m20\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mi>n</mi><mtext class=\"ltx_mathvariant_monospace\">(in)</mtext></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}_{n}^{\\texttt{(in)}}</annotation></semantics></math>, i.e., <math alttext=\"\\mathbf{S}_{n}=(\\mathbf{S}_{n}^{\\texttt{(pr)}},\\mathbf{S}_{n}^{\\texttt{(in)}})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m21\" intent=\":literal\"><semantics><mrow><msub><mi>&#119826;</mi><mi>n</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119826;</mi><mi>n</mi><mtext class=\"ltx_mathvariant_monospace\">(pr)</mtext></msubsup><mo>,</mo><msubsup><mi>&#119826;</mi><mi>n</mi><mtext class=\"ltx_mathvariant_monospace\">(in)</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{n}=(\\mathbf{S}_{n}^{\\texttt{(pr)}},\\mathbf{S}_{n}^{\\texttt{(in)}})</annotation></semantics></math>.\nNote that in this study, <math alttext=\"\\mathbf{S}_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m22\" intent=\":literal\"><semantics><msub><mi>&#119826;</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{S}_{n}</annotation></semantics></math> is treated as just a string. This eliminates the need to add a new LLM token corresponding to the discrete token, and no additional LLM training is required.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "token",
                    "prompt",
                    "target",
                    "sequences",
                    "discrete",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The mean log-likelihood (MLL) over the input part of the sequence can be calculated as follows:</p>\n\n",
                "matched_terms": [
                    "mll",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{S}_{n}^{\\texttt{(in)}}=(s^{\\texttt{(in)}}_{1,n},\\,s^{\\texttt{(in)}}_{2,n},\\ldots,s^{\\texttt{(in)}}_{T^{\\prime}_{n},n})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119826;</mi><mi>n</mi><mtext class=\"ltx_mathvariant_monospace\">(in)</mtext></msubsup><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>s</mi><mrow><mn>1</mn><mo>,</mo><mi>n</mi></mrow><mtext class=\"ltx_mathvariant_monospace\">(in)</mtext></msubsup><mo rspace=\"0.337em\">,</mo><msubsup><mi>s</mi><mrow><mn>2</mn><mo>,</mo><mi>n</mi></mrow><mtext class=\"ltx_mathvariant_monospace\">(in)</mtext></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>s</mi><mrow><msubsup><mi>T</mi><mi>n</mi><mo>&#8242;</mo></msubsup><mo>,</mo><mi>n</mi></mrow><mtext class=\"ltx_mathvariant_monospace\">(in)</mtext></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}_{n}^{\\texttt{(in)}}=(s^{\\texttt{(in)}}_{1,n},\\,s^{\\texttt{(in)}}_{2,n},\\ldots,s^{\\texttt{(in)}}_{T^{\\prime}_{n},n})</annotation></semantics></math> and <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m2\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> represents the parameters of LLMs.\nIn particular, we refer to the score before averaging over the entire dataset as the <span class=\"ltx_text ltx_font_italic\">per-utterance MLL</span>, which will be used in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S4\" title=\"4 Main Experiments &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "before",
                    "used",
                    "mll",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The actual evaluation is done by calculating the MLL of our target speech database for each of the different SSL models and comparing the relative difference in value. Since this MLL score is a metric focusing on the way discrete tokens transition, it is inferred to be highly relevant to the ASR task, but specific verification is provided in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S4\" title=\"4 Main Experiments &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "target",
                    "asr",
                    "discrete",
                    "mll"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section investigates whether the proposed evaluation metric behaves as intended in a series of pilot studies. We first examine how the MLL varies as the length of the input sequence increases. We then measure the effect of supplying additional context&#8212;tokens that precede and follow the target sequence&#8212;as part of the prompt. Although LLMs have never been trained on speech data, a consistent increase in the MLL score with longer sequences or richer context would suggest that the LLM captures the grammatical or syntactic regularities present in the input. Finally, we explore the MLL score&#8217;s sensitivity to different prompt patterns.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "prompt",
                    "target",
                    "sequences",
                    "mll",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Gemma3-4B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx24\" title=\"\">24</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Although precisely this 4B model is a multi-modal LLM with a vision encoder, all analyses in this chapter confirmed the same trend for the 1B model, which is trained only on text. Evaluation using other text-based LLMs will be described in the next chapter.</span></span></span> as the LLM for calculating the MLL.\nThe test speech data consisted of 100 utterances randomly selected from the 100hrs subset of LibriSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx25\" title=\"\">25</a>]</cite> (&#8220;train-clean-100&#8221;).\nTimestep-level speech representations as in Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S2.E1\" title=\"In 2.1 Computation of the Proposed Metric &#8227; 2 Proposed Method &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) were extracted at 50 frames per second using HuBERT-Base that had been trained for two iterations on the same 100 hrs subset.\nTo convert HuBERT features into discrete tokens as in Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S2.E2\" title=\"In 2.1 Computation of the Proposed Metric &#8227; 2 Proposed Method &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), we trained a <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means on 10% of the train-clean-100 and used the resulting centroids to quantize every time step. The number of clusters <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> is 500.\nThe resulting token sequences were deduplicated as in Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S2.E3\" title=\"In 2.1 Computation of the Proposed Metric &#8227; 2 Proposed Method &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>) then fed into Gemma3 for the inference, which was carried out with Hugging Face <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx26\" title=\"\">26</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "same",
                    "hubert",
                    "gemma34b",
                    "token",
                    "model",
                    "chapter",
                    "sequences",
                    "discrete",
                    "subset",
                    "used",
                    "mll"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we investigated how the MLL scores varied when the length of the input string was varied. The results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Variation of MLL when input string length is varied &#8227; 3 behavioral analysis &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, where &#8220;Target Token Length&#8221; is the number of characters cut out from the beginning of the input, not the number of tokens.\n&#8220;Max&#8221; is the case where the entire input string was entered, in which case the average string length was around 1400 characters.\nThe results show that the longer the input sequence becomes, the easier the prediction becomes.\nThis suggests that longer input sequences supply richer context information from the past, making next-token prediction progressively easier.\nBased on this, we can infer that the speech data has a high degree of statistic consistency with the LLM.</p>\n\n",
                "matched_terms": [
                    "where",
                    "sequence",
                    "token",
                    "sequences",
                    "case",
                    "mll",
                    "first",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text\" style=\"font-size:80%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:144.8pt;height:49.2pt;vertical-align:-22.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Target Character Length</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MLL</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">500</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">-1.610</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">1000</span>\n<span class=\"ltx_td ltx_align_center\">-1.588</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Max</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">-1.568</span></span>\n</span>\n</span></span>\n</span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "mll",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:80%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:527.9pt;height:97.2pt;vertical-align:-46.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">No.</span>\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Prompt Pattern</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MLL</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Past:</span> [<math alttext=\"X^{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>&#8722;</mo></msup><annotation encoding=\"application/x-tex\">X^{-}</annotation></semantics></math>], <span class=\"ltx_text ltx_font_typewriter\">Future:</span> [<math alttext=\"X^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>+</mo></msup><annotation encoding=\"application/x-tex\">X^{+}</annotation></semantics></math>]</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">-1.580</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">2</span>\n<span class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">Past Utterances:</span> [<math alttext=\"X^{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m7\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>&#8722;</mo></msup><annotation encoding=\"application/x-tex\">X^{-}</annotation></semantics></math>], <span class=\"ltx_text ltx_font_typewriter\">Future Utterances:</span> [<math alttext=\"X^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m8\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>+</mo></msup><annotation encoding=\"application/x-tex\">X^{+}</annotation></semantics></math>]</span>\n<span class=\"ltx_td ltx_align_center\">-1.588</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">3</span>\n<span class=\"ltx_td ltx_align_left\"><code class=\"ltx_verbatim ltx_font_typewriter\">&lt;prefix&gt;</code>[<math alttext=\"X^{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m9\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>&#8722;</mo></msup><annotation encoding=\"application/x-tex\">X^{-}</annotation></semantics></math>]<code class=\"ltx_verbatim ltx_font_typewriter\">&lt;/prefix&gt;</code>, <code class=\"ltx_verbatim ltx_font_typewriter\">&lt;suffix&gt;</code>[<math alttext=\"X^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m10\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>+</mo></msup><annotation encoding=\"application/x-tex\">X^{+}</annotation></semantics></math>]<code class=\"ltx_verbatim ltx_font_typewriter\">&lt;/suffix&gt;</code></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">-1.568</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">4</span>\n<span class=\"ltx_td ltx_align_left\"><code class=\"ltx_verbatim ltx_font_typewriter\">&lt;past_utterances&gt;</code>[<math alttext=\"X^{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m11\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>&#8722;</mo></msup><annotation encoding=\"application/x-tex\">X^{-}</annotation></semantics></math>]<code class=\"ltx_verbatim ltx_font_typewriter\">&lt;/past_utterances&gt;</code>, <code class=\"ltx_verbatim ltx_font_typewriter\">&lt;future_utterances&gt;</code>[<math alttext=\"X^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m12\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>+</mo></msup><annotation encoding=\"application/x-tex\">X^{+}</annotation></semantics></math>]<code class=\"ltx_verbatim ltx_font_typewriter\">&lt;/future_utterances&gt;</code></span>\n<span class=\"ltx_td ltx_align_center\">-1.578</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">5</span>\n<span class=\"ltx_td ltx_align_left\"><code class=\"ltx_verbatim ltx_font_typewriter\">&lt;previous_utterances&gt;</code>[<math alttext=\"X^{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m13\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>&#8722;</mo></msup><annotation encoding=\"application/x-tex\">X^{-}</annotation></semantics></math>]<code class=\"ltx_verbatim ltx_font_typewriter\">&lt;/previous_utterances&gt;</code>, <code class=\"ltx_verbatim ltx_font_typewriter\">&lt;subsequent_utterances&gt;</code>[<math alttext=\"X^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m14\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>+</mo></msup><annotation encoding=\"application/x-tex\">X^{+}</annotation></semantics></math>]<code class=\"ltx_verbatim ltx_font_typewriter\">&lt;/subsequent_utterances&gt;</code></span>\n<span class=\"ltx_td ltx_align_center\">-1.576</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center\">6</span>\n<span class=\"ltx_td ltx_align_left\"><code class=\"ltx_verbatim ltx_font_typewriter\">&lt;past_context&gt;</code>[<math alttext=\"X^{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m15\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>&#8722;</mo></msup><annotation encoding=\"application/x-tex\">X^{-}</annotation></semantics></math>]<code class=\"ltx_verbatim ltx_font_typewriter\">&lt;/past_context&gt;</code>, <code class=\"ltx_verbatim ltx_font_typewriter\">&lt;future_context&gt;</code>[<math alttext=\"X^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m16\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>+</mo></msup><annotation encoding=\"application/x-tex\">X^{+}</annotation></semantics></math>]<code class=\"ltx_verbatim ltx_font_typewriter\">&lt;/future_context&gt;</code></span>\n<span class=\"ltx_td ltx_align_center\">-1.570</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">7</span>\n<span class=\"ltx_td ltx_align_left ltx_border_bb\"><code class=\"ltx_verbatim ltx_font_typewriter\">&lt;prefix_sequences&gt;</code>[<math alttext=\"X^{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m17\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>&#8722;</mo></msup><annotation encoding=\"application/x-tex\">X^{-}</annotation></semantics></math>]<code class=\"ltx_verbatim ltx_font_typewriter\">&lt;/prefix_sequences&gt;</code>, <code class=\"ltx_verbatim ltx_font_typewriter\">&lt;suffix_sequences&gt;</code>[<math alttext=\"X^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m18\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>+</mo></msup><annotation encoding=\"application/x-tex\">X^{+}</annotation></semantics></math>]<code class=\"ltx_verbatim ltx_font_typewriter\">&lt;/suffix_sequences&gt;</code></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">-1.574</span></span>\n</span>\n</span></span>\n</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "mll",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">dev-clean</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">dev-other</span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math>&#8201;(Gemma3-4B, Trans.)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.547</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.487</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math>&#8201;(Qwen3-4B, Trans.)</span>\n<span class=\"ltx_td ltx_align_center\">0.437</span>\n<span class=\"ltx_td ltx_align_center\">0.363</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m3\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math>&#8201;(Phi-4-mini, Trans.)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.497</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.410</span></span>\n</span>\n</span></p>\n\n",
                "matched_terms": [
                    "qwen34b",
                    "gemma34b",
                    "phi4mini"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this analysis, we investigated how the MLL changes when the context size of <math alttext=\"\\mathbf{S^{\\texttt{(pr)}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext class=\"ltx_mathvariant_monospace\">(pr)</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S^{\\texttt{(pr)}}}</annotation></semantics></math>\nis gradually increased. Since LibriSpeech corpus is read speech and provides chapter annotations, we selected the utterances that immediately precede and follow the target utterance within the same chapter, expanding the context size symmetrically (e.g., <math alttext=\"\\pm 1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1</annotation></semantics></math>, <math alttext=\"\\pm 2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2</annotation></semantics></math>, <math alttext=\"\\pm 3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#177;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 3</annotation></semantics></math> utterances). When a target utterance occurred near the beginning or the end of a chapter and the required number of neighboring utterances was unavailable, the missing context slots were filled with the string &#8220;<span class=\"ltx_text ltx_font_typewriter\">N/A</span>&#8221; in the prompt so that the overall prompt format remained constant across all conditions.</p>\n\n",
                "matched_terms": [
                    "same",
                    "prompt",
                    "utterance",
                    "target",
                    "chapter",
                    "mll",
                    "immediately"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S3.F1\" title=\"Figure 1 &#8227; 3.3 Impact of Varying Information Density in In-Context Learning &#8227; 3 behavioral analysis &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Note that due to the memory limit of the GPU environment used in the experiment, the context size was limited to 6.</span></span></span>As we can see, the MLL score increases monotonically as the context size increases. From this, we can interpret that the model is effectively utilizing the information of the added context and that the understanding of the speech domain data is increasing. Therefore, this indicates that the addition of the context allows the LLM to evaluate the naturalness of the sequences more suited to the speech domain of the input data through in-context learning without any additional training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "used",
                    "mll",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate the sensitivity of the proposed metric to the wording and structure of the prompt itself, we systematically varied the prompt template and measured the resulting MLL differences. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S3.T2\" title=\"Table 2 &#8227; 3.2 Variation of MLL when input string length is varied &#8227; 3 behavioral analysis &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, seven distinct templates were prepared. <math alttext=\"X^{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>&#8722;</mo></msup><annotation encoding=\"application/x-tex\">X^{-}</annotation></semantics></math> and <math alttext=\"X^{+}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>+</mo></msup><annotation encoding=\"application/x-tex\">X^{+}</annotation></semantics></math> denote the context sequences in the past and future directions, respectively. The templates differ along several linguistic and formatting dimensions: (i) how the notions of &#8220;past&#8221; and &#8220;future&#8221; context are paraphrased, (ii) whether or not XML-style tags are used to delimit the contextual segments, and (iii) whether explicit cues such as &#8220;utterance&#8221; or &#8220;sequence&#8221; are included to describe the data type.\nFrom the results, comparing No.&#160;2 and No.&#160;4, it is clear that the tagged patterns have higher scores. Also, a comparison of Nos.&#160;4&#8211;7 shows that MLL did not increase even if the template explicitly states that the input is an utterance. This is a natural result since the training data does not include speech.\nThe pattern in No.&#160;3 had the highest score. From this, it can be said that expressions that the model is familiar with at the time of training are more effective in helping the model understand than what kind of sequence the input data is.\n\nHowever, the variation in MLLs is small compared to the results for different context sizes of <math alttext=\"\\mathbf{S^{\\texttt{(pr)}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mtext class=\"ltx_mathvariant_monospace\">(pr)</mtext></msup><annotation encoding=\"application/x-tex\">\\mathbf{S^{\\texttt{(pr)}}}</annotation></semantics></math>. Therefore, it can be said to be robust to such fluctuations in the template pattern.\nIn subsequent experiments, the No.&#160;3 template will be used unless otherwise noted.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "prompt",
                    "model",
                    "utterance",
                    "sequences",
                    "used",
                    "mll",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the above behavioral analyses, it can be said that the LLM attempts to capture the statistical characteristics of the input data to some extent, although the LLM does not recognize the input token sequences as data derived from the speech domain.\nThese analysis consistently indicates that the MLL score improves monotonically with both the length of the input sequence and the size of the prompt context; in other words, longer sequences and richer contextual information are always beneficial within the range we tested. In contrast, the choice of prompt template pattern exerts only a marginal influence and does not materially alter the outcome. Consequently, no model-specific tuning appears to be required: practitioners can simply select the context size in accordance with their computational budget, secure in the knowledge that the procedure does not hinge on any sensitive hyper-parameter.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S4\" title=\"4 Main Experiments &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further demonstrates that the\nproposed metric remains effective even when the target sequence is evaluated without its immediate preceding and succeeding context sequences, provided that a single example sequence is supplied alongside it.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "token",
                    "prompt",
                    "target",
                    "sequences",
                    "mll"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the findings of the previous section&#8212;which confirmed the validity of estimating the MLL score of token sequence from SSL models by means of LLMs&#8212;we now examine how this score correlates with SSL model&#8217;s effectiveness on a downstream task. Since the MLL score is designed to capture the statistical naturalness of token transitions, ASR is chosen as the evaluation task.\nIn addition, we conduct a comparative analysis employing several alternative LLMs and other SSL models, to determine how the choice of these models influences the observed relationship.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "sequence",
                    "token",
                    "asr",
                    "mll"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to Gemma3-4B, Qwen3-4B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx27\" title=\"\">27</a>]</cite> and Phi-4-mini (3.8B) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx28\" title=\"\">28</a>]</cite> were employed to compute the MLL score. Both are trained exclusively on textual data that include natural-language documents, large-scale web corpora, and source code. Qwen3-4B is notable for its coverage of 119 languages and dialects, whereas Phi-4-mini distinguishes itself by incorporating high-quality synthetic data such as mathematical problems and source code into its training corpus.</p>\n\n",
                "matched_terms": [
                    "qwen34b",
                    "gemma34b",
                    "mll",
                    "phi4mini"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Besides HuBERT Base that had been trained for two iterations\non the LibriSpeech 100 hrs subset, two additional HuBERT-style SSL models were examined: WavLM Large and XEUS.\nWavLM performs masked prediction task while simultaneously denoising speech that has been artificially corrupted with environmental noise.\nXEUS extends this strategy by incorporating dereverberation in addition to denoising and is trained on an extremely multilingual dataset that covers 4,057 languages.\n<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> is 500 for all SSL models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "hubert",
                    "xeus",
                    "subset",
                    "wavlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finetuning and evaluation of the ASR task were conducted within the SUPERB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx8\" title=\"\">8</a>]</cite> framework.\nAll hidden layers of each SSL model encoder are combined through a trainable weighted sum rather than relying on the last layer alone, and this composite representation is passed to a bidirectional LSTM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx29\" title=\"\">29</a>]</cite> with two 1024-unit layers trained with CTC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx30\" title=\"\">30</a>]</cite> on character targets. Decoding is carried out by beam search using the official LibriSpeech four-gram language model implemented with KenLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx31\" title=\"\">31</a>]</cite> and the Flashlight <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx32\" title=\"\">32</a>]</cite> toolkit.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first investigated the relationship between per-utterance MLL scores obtained from discrete token-sequence inputs from final layer of HuBERT and those obtained from transcription inputs in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S3.T3\" title=\"Table 3 &#8227; 3.2 Variation of MLL when input string length is varied &#8227; 3 behavioral analysis &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Incidentally, when MLL (averaged over the entire data) is calculated using the discrete token sequences as input and the transcribed text as input, MLL is considerably larger in the former case. For example, when Gemma3-4B was used for LLM and &#8220;dev-clean&#8221; for the data set, the former was -1.615 and the latter was -4.421. This is because LLM is not fine-tuned with any newly added tokens, but simply inputs the token sequence as a string of characters, so the prediction is for a total of 11 different characters (numbers from 0 to 9 and spaces), and the relative vocabulary is much smaller.</span></span></span>.\nAs can be seen, all models exhibit a moderate positive correlation between the two conditions. Although the correlation is lower for the &#8220;dev-other&#8221;, this can be attributed to the degraded speech quality cased by noise and other artifacts, which likely results in noisier token sequences. Therefore, LLM appears to produce the per-utterance MLL with similar relative magnitudes for both discrete tokens and the transcriptions.\nThis implies it captured common grammatical, lexical-frequency, or character-transition patterns between the inputs.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "hubert",
                    "sequence",
                    "gemma34b",
                    "token",
                    "sequences",
                    "discrete",
                    "case",
                    "used",
                    "mll",
                    "first",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analysed the connection between the MLL and word error rate (WER) obtained with three SSL models.\nFor every layer of each encoder, the latent features were first discretised; MLL was then computed on the resulting token sequence and finally averaged across all layers.\nWe varied the LLM among the three models introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S4\" title=\"4 Main Experiments &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>-A.</p>\n\n",
                "matched_terms": [
                    "models",
                    "wer",
                    "sequence",
                    "token",
                    "three",
                    "mll",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the setup that uses the single discrete token sequence immediately preceding the target sequence and the single sequence immediately following it as context, an alternative condition is evaluated in which the initial sequence in the same chapter is selected once and reused as the prompt for every sequence in that chapter. This setting tests whether supplying a data sample&#8212;without an explicit temporal context&#8212;enables LLMs to capture the input characteristics in an in-context manner and to apply the MLL as effectively as in the explicit-context condition.</p>\n\n",
                "matched_terms": [
                    "same",
                    "sequence",
                    "token",
                    "prompt",
                    "target",
                    "chapter",
                    "discrete",
                    "uses",
                    "mll",
                    "immediately"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S4.F2\" title=\"Figure 2 &#8227; 4 Main Experiments &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> plots layer-wise MLL curves for every combination of SSL models and LLMs. In contrast to existing methods using CCA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx13\" title=\"\">13</a>]</cite> that rely on labeled alignments, our approach provides label-free insights into the internal representation of SSL models. As we can see, although previous studies report that higher layers of WavLM contribute most to ASR performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx4\" title=\"\">4</a>]</cite>, MLL does not always peak in the same layers. This indicates that LLMs consider the discrete token sequence to be statistically natural to some degree, whether the sequence is obtained from the lower or higher layers. When comparing LLMs within the same SSL model, Gemma3-4B and Qwen3-4B exhibit almost identical trajectories\n, whereas Phi-4-mini exhibits a little different pattern. This difference may stem from Phi-4-mini&#8217;s training corpus, which emphasizes synthetic data such as mathematical problems and source-code <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx28\" title=\"\">28</a>]</cite>, thereby altering its MLL statistics.</p>\n\n",
                "matched_terms": [
                    "models",
                    "same",
                    "sequence",
                    "qwen34b",
                    "gemma34b",
                    "token",
                    "phi4mini",
                    "model",
                    "discrete",
                    "asr",
                    "wavlm",
                    "mll"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated our method on the speaker-verification task using the VoxCeleb1 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#bib.bibx33\" title=\"\">33</a>]</cite> test partition. Although the original test set contains 4,874 utterances from 40 speakers, we randomly sampled a balanced subset of 20 speakers (10 male, 10 female). For each selected speaker, 10 utterances were chosen at random, yielding 200 utterances in total.\nWithin the 10 utterances of every speaker, one utterance was reserved as a prompt for computing the MLL score. The remaining 9 utterances served as verification trials.\nFor every utterance we extracted a 2,560-dimensional embedding from the last hidden layer through the LLM inference. Each embedding was compressed to 128 dimensions using principal component analysis (PCA) and subsequently <math alttext=\"\\ell_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p2.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">\\ell_{2}</annotation></semantics></math>-normalized.\nAmong the nine verification utterances, one was further selected per speaker as the enrollment utterance; its frame-level embeddings were averaged over time to obtain the speaker embedding.\nThe evaluation set therefore consisted of <math alttext=\"20\\texttt{speakers}\\times 9=180\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mrow><mn>20</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext class=\"ltx_mathvariant_monospace\">speakers</mtext></mrow><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>9</mn></mrow><mo>=</mo><mn>180</mn></mrow><annotation encoding=\"application/x-tex\">20\\texttt{speakers}\\times 9=180</annotation></semantics></math> verification utterances. Performance was reported in terms of the Equal Error Rate (EER), computed on all target and impostor trials generated from the 180 verification utterances.\nBaseline systems are as follows:\n(i) <em class=\"ltx_emph ltx_font_italic\">MFCC</em>, where 80-dimensional frame-level MFCCs are averaged over each utterance to obtain an 80-D speaker embedding; and (ii) <em class=\"ltx_emph ltx_font_italic\">Centroid-pool</em>, in which every discrete token is replaced by its <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS3.p2.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>-means centroid, the centroid sequence is temporal-mean-pooled, and the resulting vector is projected to 128 dimensions via PCA.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "token",
                    "prompt",
                    "utterance",
                    "target",
                    "discrete",
                    "subset",
                    "mll",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.04463v1#S4.F3\" title=\"Figure 3 &#8227; 4.2.2 Relationship Between ASR performance and MLL &#8227; 4.2 Results &#8227; 4 Main Experiments &#8227; Evaluating Self-Supervised Speech Models via Text-based LLMs\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the evaluation results across three different SSL models.\nUsing the LLM embedding achieved a markedly lower EER than both baseline systems, indicating that the LLM embeddings captured speaker-specific characteristics more effectively.\nMoreover, a layer-wise analysis revealed that LLM embeddings exhibit a smaller variance in performance across layers than the baseline methods, indicating greater robustness of the representation.\nThese findings provide new insights into the nature of the LLM embeddings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposed a novel evaluation metric of SSL models using LLMs.\nBy simply feeding LLMs with discrete token sequences together with minimal domain cues, we can compute the mean log-likelihood in a fully label-free manner; these cues steer in-context learning and make the score more reliable, all without any additional training or hyper-parameter tuning.\nExperimental results have shown that a correlation between LLM-based scores and an automatic speech recognition task.\nFurthermore, our additional experiment revealed that LLMs can provide valuable\nfeatures for speaker verification task.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "token",
                    "sequences",
                    "discrete",
                    "scores"
                ]
            }
        ]
    }
}