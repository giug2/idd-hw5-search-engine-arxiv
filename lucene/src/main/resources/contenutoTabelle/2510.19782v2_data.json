{
    "S3.T1": {
        "source_file": "Adapting Multilingual Models to Code-Mixed Tasks via Model Merging",
        "caption": "Table 1. Statistics of all the datasets used in our study.",
        "body": "2210\n\n\nNeu: 389\n\n\nPos: 909\n\n\nNeg: 912",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">2210</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Neu: 389</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Pos: 909</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Neg: 912</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "our",
            "neu",
            "study",
            "all",
            "neg",
            "datasets",
            "pos",
            "used",
            "statistics"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.T1\" title=\"Table 1 &#8227; Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides statistics of all the datasets we have used in our experiments, along with the label wise distributions in all splits. We include all the labeled, unlabeled datasets in the table for ready reference. SST5 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Socher et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib33\" title=\"\">2013</a>)</cite> has 5 classes, and to use in our experiments we convert its 5-class labels into 3 classes by merging &#8220;very positive&#8221; with &#8220;positive&#8221; and &#8220;very negative&#8221; with &#8220;negative&#8221;. Wherever all three splits (train-test-valid) are available we use the same, and in case they are not available we create 70-20-10 split from the available samples. We use F1 as the measure of performance across all the data and training configurations. F1 is suitable as there is some dataset imbalances across code-mixed datasets.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We study model merging as a practical alternative to conventional adaptation strategies for code-mixed NLP. Starting from a multilingual base model, we: (i) perform continued pre-training (CPT) on unlabeled code-mixed text to obtain an adapted checkpoint, (ii) merge checkpoint with the base model, and (iii) fine-tune&#160;(FT) on the downstream task data. We evaluate our approach for sentence classification (sentiment and hate speech) task in English-Hindi (En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our results show that merged models consistently outperform full fine-tuning and CPT<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>FT. We observe gains of 2&#8211;5 points in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> over full fine-tuning and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2 points over CPT<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>FT, indicating that unlabeled data is leveraged more effectively via merging than via CPT alone. Zero-/few-shot prompting with larger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged checkpoints, underscoring limits of in-context learning for code-mixed inputs. We further test cross-pair transfer by training on En&#8211;Hi and evaluating on En&#8211;Ta and En&#8211;Ml: merged checkpoints transfer more strongly than monolingual-English baselines (e.g., TV/TIES variants reaching 0.65&#8211;0.68 <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> vs. 0.61&#8211;0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more reliable substrate for low-resource pairs. We conclude with adaptation recipes matched to common data regimes (labeled only; labeled+unlabeled; transfer-only) and discuss limitations and scaling considerations for broader tasks and larger models.</p>\n\n",
                "matched_terms": [
                    "our",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model merging integrates task- or domain-specific models into a unified composite, aggregating knowledge from all merged sources. In resource-constrained code-mixed settings, leveraging monolingual or other code-mixed resources can further improve performance. While model merging offers a modular mechanism for combining diverse data sources, its effectiveness for code-mixed tasks remains underexplored and merits closer study.</p>\n\n",
                "matched_terms": [
                    "all",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leverage monolingual English data in our adaptation methods and evaluate on the code-mixed task.\nTo compare cross-lingual/cross-dataset transfer we consider two baselines- a) Joint Training&#160;(JointFT): English and English-Hindi task datasets are combined and model is fine-tuned on the combined dataset; b) Sequential Training&#160;(SeqFT): we first train the model on English task dataset, followed by fine-tuning on the English-Hindi code-mixed dataset. Additionally, we look at the following:</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Models:</span> We use multilingual models, as their fine-tuning has consistently shown strong results <cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>; Das et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite>.\nFor our experiments, we use mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib11\" title=\"\">2019</a>)</cite> and XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib9\" title=\"\">2020</a>)</cite>, which are widely used in code-mixed studies, along with Llama 3.2 1B model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B</a></span></span></span>.\nDue to computational constraints, our model adaptation methods are restricted to Llama 1B; however, we conduct inference on larger models including Llama 3.2 3B, Llama 3.1 8B, and Llama 3.3 70B.</p>\n\n",
                "matched_terms": [
                    "used",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span> Our study focuses on English-Hindi (En-Hi) and English-Spanish (En-Es) code-mixed sentiment classification tasks, as they are part of code-mixing benchmarks GLUECoS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>)</cite> and LinCE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib2\" title=\"\">2020</a>)</cite>. For En-Hi we consider two tasks - a) Sentiment Analysis using three datasets &#8212; GLUECoS, Sentimix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Patwa et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib26\" title=\"\">2020</a>)</cite>, and&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Prabhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib28\" title=\"\">2016</a>)</cite>&#8212;all featuring 3-class labels (positive, neutral, negative); b) Hate Speech Classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bohra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib5\" title=\"\">2018</a>)</cite>. For En-Es, we carry out experiments on Sentiment Analysis task, and use two datasets released by <cite class=\"ltx_cite ltx_citemacro_citet\">Patwa et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib26\" title=\"\">2020</a>)</cite> and GLUECoS benchmark. For transfer from monolingual English task dataset, we use English SST5 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Socher et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib33\" title=\"\">2013</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "our",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompts:</span> We used the following prompts for our zero and few shot experiments. For few-shot prompt, we include the examples and the corresponding labels in between the instruction and the input sample. For the k-shot settings, we carry out five runs, each run with different set of randomly sampled k examples from the train dataset, while ensuring that all the labels are well represented in the examples.</p>\n\n",
                "matched_terms": [
                    "all",
                    "used",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hardware &amp; Hyperparameters:</span> We conduct all experiments on a combination of Nvidia 1080 Ti 12 GB GPUs, Nvidia A6000 32 GB GPU, using a single GPU. We use Nvidia A6000 for Llama experiemnts, and 1080Ti for all other experiments. Batch sizes range from 8 to 128, depending on the dataset and approach. Learning rates were selected after a search between [1e-2, 5e-5], with 1e-5 working best for full model training, 1e-4 for LoRA layers. All experiments ran for with early stopping&#160;(stop if model performance doesn&#8217;t change by 0.5 F1 point across 3 evaluations), maximum up to 20 epochs. We use AdamW optimizer with default optimizer parameters for all our experiments. We implemented the methods using PyTorch, Huggingface Transformers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wolf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib38\" title=\"\">2020</a>)</cite>, PEFT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mangrulkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib23\" title=\"\">2022</a>)</cite>, and mergekit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goddard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib13\" title=\"\">2024</a>)</cite> for model merging methods. Upon publication code, data, models, and detailed hyperparameters configurations will be publicly released for reproducibility.</p>\n\n",
                "matched_terms": [
                    "all",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate that model merging methods (TV<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT, TIES<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>CPT in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) consistently outperform standard fine-tuning (Full FT) across all models and datasets, yielding +2&#8211;5 F1 over Full FT and +1&#8211;2 F1 over CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT. This confirms that unlabeled data can be exploited more effectively through model merging than through continued pre-training alone, supporting our hypothesis in <span class=\"ltx_text ltx_font_bold\">RQ1</span>. Between merging strategies, Task Vectors (TV) generally perform better than TIES, though the relative advantage varies by model and task. We also find that the choice of corpus for CPT has only a marginal effect on downstream performance.</p>\n\n",
                "matched_terms": [
                    "all",
                    "datasets",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The relationship between task performance and in-context learning (ICL) for code-mixed tasks remains underexplored. To better understand how model size and prompting methods influence performance, we conduct zero-shot and few-shot prompting experiments using different-sized models from the LLaMA family&#8212;1B-instruct, 3B-instruct, 8B-instruct, and 70B-instruct. For few-shot prompting, we evaluate performance with 5, 10, 15, and 20 examples. Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> contains results of our experiments on LLama 3.2 70B model. We see that even baseline fine-tuning methods perform better than zero/few shot prompting, suggesting that in-context learning performs weakly for codemixing tasks. The highest F1 scores can be seen for the <cite class=\"ltx_cite ltx_citemacro_citet\">Prabhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib28\" title=\"\">2016</a>)</cite> datasete suggesting that the performance may be dependent on the data setting. Upon further analysis, we we hypothesize that this is due to the lack of noise in the dataset compared to the other datasets we use for comparison.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We take the model fine-tuned on the combination of our three En-Hi datasets, using all the different training configurations, and infer on the test set of the En-Ta, En-Ml datasets. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S5.T4\" title=\"Table 4 &#8227; 5.2. Impact of Continued Pre-Training Corpus &#8227; 5. Results &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the transfer of various training strategies to other code-mixed language pairs for Sentiment Analysis task. We present transfer results for all the combinations of base models and training methods. We also include full fine-tuning on the English dataset (FullFT (En)) to ascertain which is better - transfer from monolingual En resources or code-mixed resources from other language pairs.</p>\n\n",
                "matched_terms": [
                    "all",
                    "datasets",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate both XLM-R (encoder) and LLaMA 3.2 1B (decoder) to study transfer properties. Across En-Ta and En-Ml, we observe a consistent pattern: checkpoints obtained through model merging outperform Full FT baselines by 5&#8211;13 F1 points. This improvement holds across models and target language pairs, highlighting that merged checkpoints capture complementary signals from diverse resources. These findings reinforce our answer to <span class=\"ltx_text ltx_font_bold\">RQ2</span>&#8212;that model merging is an effective strategy for integrating heterogeneous data sources, often surpassing sequential or joint fine-tuning. However, the gains remain dependent on model architecture, the specific code-mixed task, and the language pair being targeted, underscoring the importance of careful merging design.</p>\n\n",
                "matched_terms": [
                    "our",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically show the advantages of using model merging as a strategy for code-mixed tasks. We also introduce an innovative perspective on utilizing existing data resources for code-mixed tasks, diverging from traditional domain-adaptation and fine-tuning techniques. In circumstances where datasets are lacking or are available in different languages, our findings can assist researchers in choosing an appropriate method to optimize the available resources.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations</span>\nOur experiments are limited to sentence classification due to the lack of consistent datasets for other tasks in both monolingual and code-mixed languages. Therefore, the generalizability of our findings to other NLP tasks is unclear. Future research should evaluate these methods on a broader range of tasks with comparable datasets across multiple language pairs.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although our study contains models of different architecture, we do not scale it to larger language models (&#191;1B parameter models), because of prohibitive cost of training/fine-tuning a larger model. Analyzing how model merging behaves as model size increase for code-mixed tasks would be an interesting avenue for future work.</p>\n\n",
                "matched_terms": [
                    "our",
                    "study"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Adapting Multilingual Models to Code-Mixed Tasks via Model Merging",
        "caption": "Table 2. Comparison of fine-tuning, continued pretraining, and model merging strategies for multilingual sentiment analysis (En–Hi, En–Es) and hate speech detection (En–Hi, En–Es) across multiple benchmarks (GLUEcos, SentiMix, Prabhu et al., Bohra et al.). Results are reported for XLM-R and LLaMA 2 13B with full fine-tuning, sequential and joint training, transfer variants (TV, TIES), and combinations with continued pretraining (CPT). Zero- and few-shot prompting results with LLaMA 2 70B are also shown for comparison. Main takeaway: Parameter-efficient strategies such as TIES and transfer variants, particularly when combined with CPT, achieve competitive or superior performance to full fine-tuning, highlighting their effectiveness for code-mixed and multilingual tasks under resource constraints.",
        "body": "0.57\n\n\n(0.56 ± 0.009)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">0.57</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">(0.56 &#177; 0.009)</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "continued",
            "detection",
            "strategies",
            "training",
            "transfer",
            "ties",
            "tasks",
            "llama",
            "70b",
            "zero",
            "codemixed",
            "superior",
            "also",
            "constraints",
            "13b",
            "gluecos",
            "finetuning",
            "joint",
            "speech",
            "combined",
            "across",
            "bohra",
            "prabhu",
            "cpt",
            "competitive",
            "analysis",
            "main",
            "highlighting",
            "resource",
            "multilingual",
            "results",
            "effectiveness",
            "hate",
            "benchmarks",
            "model",
            "particularly",
            "their",
            "such",
            "multiple",
            "variants",
            "takeaway",
            "en–es",
            "fewshot",
            "parameterefficient",
            "prompting",
            "performance",
            "combinations",
            "sentiment",
            "reported",
            "sequential",
            "full",
            "xlmr",
            "under",
            "sentimix",
            "comparison",
            "pretraining",
            "when",
            "merging",
            "en–hi",
            "achieve"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> highlight the comparative performance of fine-tuning, CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT, model merging methods across multiple sentiment analysis and hate speech detection tasks, in English-Hindi&#160;(En-Hi) and English-Spanish&#160;(En-Es) Datasets.</p>\n\n",
            "<p class=\"ltx_p\">The results demonstrate that model merging methods (TV<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT, TIES<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>CPT in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) consistently outperform standard fine-tuning (Full FT) across all models and datasets, yielding +2&#8211;5 F1 over Full FT and +1&#8211;2 F1 over CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT. This confirms that unlabeled data can be exploited more effectively through model merging than through continued pre-training alone, supporting our hypothesis in <span class=\"ltx_text ltx_font_bold\">RQ1</span>. Between merging strategies, Task Vectors (TV) generally perform better than TIES, though the relative advantage varies by model and task. We also find that the choice of corpus for CPT has only a marginal effect on downstream performance.</p>\n\n",
            "<p class=\"ltx_p\">Our analysis varies across three key dimensions: (a) use of only labeled data, (b) use of both labeled and unlabeled data, and (c) type of model augmentation techniques. We combine code-mixed data with monolingual task-specific data. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents a comparison of model performance across different scenarios of data availability. These settings reflect practical conditions in real-world code-mixed applications.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Combining En and En-Hi labeled datasets for finetuning</span> Incorporating English-labeled data alongside En-Hi datasets&#160;(Seq FT and Joint FT rows in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) yields no significant improvement over fine-tuning (Full FT) solely on En-Hi data. Both SeqFT (English first, then English-Hindi) and JointFT (simultaneously on combined datasets) fail to enhance performance for code-mixed tasks (See Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.)</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Leveraging monolingual resources with code-mixed resources</span>\nWe combine three elements here: the base model, a vector fine-tuned on English data, and a vector for continued pre-training on code-mixed data (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E3\" title=\"In Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Although we expected the combination of monolingual and code-mixed resources to perform better than just monolingual or just code-mixed, they perform similarly to CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT method, or outperform it by 1 or 2 F1 points (which can be seen in the <span class=\"ltx_text ltx_font_italic\">TV-T_en + CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT </span>and<span class=\"ltx_text ltx_font_italic\"> TIES-T_en + CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT</span> rows of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Extending to a three-way merge does not provide additional gains over a single CPT merge. Overall, compared to FullFT, the combined monolingual and code-mixed approach either matches performance or underperforms slightly (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2 F1 points).</p>\n\n",
            "<p class=\"ltx_p\">The relationship between task performance and in-context learning (ICL) for code-mixed tasks remains underexplored. To better understand how model size and prompting methods influence performance, we conduct zero-shot and few-shot prompting experiments using different-sized models from the LLaMA family&#8212;1B-instruct, 3B-instruct, 8B-instruct, and 70B-instruct. For few-shot prompting, we evaluate performance with 5, 10, 15, and 20 examples. Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> contains results of our experiments on LLama 3.2 70B model. We see that even baseline fine-tuning methods perform better than zero/few shot prompting, suggesting that in-context learning performs weakly for codemixing tasks. The highest F1 scores can be seen for the <cite class=\"ltx_cite ltx_citemacro_citet\">Prabhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib28\" title=\"\">2016</a>)</cite> datasete suggesting that the performance may be dependent on the data setting. Upon further analysis, we we hypothesize that this is due to the lack of noise in the dataset compared to the other datasets we use for comparison.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We study model merging as a practical alternative to conventional adaptation strategies for code-mixed NLP. Starting from a multilingual base model, we: (i) perform continued pre-training (CPT) on unlabeled code-mixed text to obtain an adapted checkpoint, (ii) merge checkpoint with the base model, and (iii) fine-tune&#160;(FT) on the downstream task data. We evaluate our approach for sentence classification (sentiment and hate speech) task in English-Hindi (En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our results show that merged models consistently outperform full fine-tuning and CPT<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>FT. We observe gains of 2&#8211;5 points in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> over full fine-tuning and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2 points over CPT<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>FT, indicating that unlabeled data is leveraged more effectively via merging than via CPT alone. Zero-/few-shot prompting with larger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged checkpoints, underscoring limits of in-context learning for code-mixed inputs. We further test cross-pair transfer by training on En&#8211;Hi and evaluating on En&#8211;Ta and En&#8211;Ml: merged checkpoints transfer more strongly than monolingual-English baselines (e.g., TV/TIES variants reaching 0.65&#8211;0.68 <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> vs. 0.61&#8211;0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more reliable substrate for low-resource pairs. We conclude with adaptation recipes matched to common data regimes (labeled only; labeled+unlabeled; transfer-only) and discuss limitations and scaling considerations for broader tasks and larger models.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "strategies",
                    "transfer",
                    "training",
                    "tasks",
                    "codemixed",
                    "finetuning",
                    "speech",
                    "cpt",
                    "multilingual",
                    "results",
                    "hate",
                    "model",
                    "variants",
                    "prompting",
                    "sentiment",
                    "full",
                    "xlmr",
                    "pretraining",
                    "merging",
                    "en–hi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-mixed text&#8212;where two or more languages appear within a single utterance or speech event&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib15\" title=\"\">1977</a>)</cite>&#8212;poses persistent challenges for natural language processing (NLP) systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Do&#287;ru&#246;z et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib12\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib43\" title=\"\">2023</a>; Winata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib36\" title=\"\">2023</a>)</cite>. A common response has been to fine-tune large multilingual pre-trained models on task-specific datasets for code-mixed inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>; Aguilar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib2\" title=\"\">2020</a>)</cite>. While effective, this approach depends on the availability of high-quality annotations and on general-purpose multilingual models having seen sufficient code-mixed data during pre-training to capture the characteristics of code-mixed language.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "codemixed",
                    "multilingual",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A complementary strategy for adapting language models to new domains or linguistic settings is continued pre-training (CPT) on unlabeled data&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gururangan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib16\" title=\"\">2020</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib22\" title=\"\">2021</a>; Ruder, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib32\" title=\"\">2021</a>)</cite>. Model merging&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib41\" title=\"\">2024</a>)</cite> has emerged as a promising approach for adapting models to new domains, tasks, and languages, while mitigating several limitations of CPT. Although CPT can aid domain adaptation, it can also erode general language understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gogoulou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib14\" title=\"\">2024</a>; Alexandrov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib4\" title=\"\">2024</a>)</cite>. In contrast, recent work shows that model merging can improve task performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wortsman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib39\" title=\"\">2022</a>; Choshen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib8\" title=\"\">2022</a>; Izmailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib18\" title=\"\">2018</a>)</cite>, enhance robustness and out-of-domain generalization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wortsman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib39\" title=\"\">2022</a>; Jin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib19\" title=\"\">2023</a>)</cite>, and enable multitask models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yadav et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib40\" title=\"\">2023</a>; Ilharco et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib17\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "model",
                    "tasks",
                    "cpt",
                    "also",
                    "pretraining",
                    "performance",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We argue that model merging is well-suited to code-mixed settings because it can incorporate code-mixed knowledge while preserving strong monolingual representations. Since code-mixed text contains monolingual spans from its constituent languages (L1, L2) and coexists with purely monolingual utterances, maintaining these capabilities is crucial for downstream performance. By comparison, CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT risks degrading monolingual processing during adaptation. Our hypothesis is that merging a base multilingual model with a checkpoint adapted on a modest code-mixed corpus can outperform traditional strategies by combining strong monolingual representations with newly acquired code-mixing capabilities, yielding a more robust solution for code-mixed text.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "model",
                    "performance",
                    "codemixed",
                    "comparison",
                    "multilingual",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model merging integrates task- or domain-specific models into a unified composite, aggregating knowledge from all merged sources. In resource-constrained code-mixed settings, leveraging monolingual or other code-mixed resources can further improve performance. While model merging offers a modular mechanism for combining diverse data sources, its effectiveness for code-mixed tasks remains underexplored and merits closer study.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tasks",
                    "codemixed",
                    "performance",
                    "merging",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond improving performance for a single code-mixed language pair, a natural question is whether gains from model merging transfer to other language pairs performing the same downstream task. In such cases, the task formulation remains fixed, but the language composition changes, introducing new patterns of mixing, grammar, and vocabulary overlap. These shifts create both challenges and opportunities: the model must adjust to different lexical and syntactic cues, yet shared structures or typological similarities may support better generalization. Understanding how language composition affects transfer is especially important in low-resource settings, where direct annotations for the target pair may be unavailable and cross-pair transfer becomes a viable adaptation strategy.</p>\n\n",
                "matched_terms": [
                    "model",
                    "transfer",
                    "such",
                    "codemixed",
                    "performance",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given this context, we pose two primary research questions: <span class=\"ltx_text ltx_font_bold\">RQ 1:</span> Can model merging serve as an alternative to fine-tuning approaches for code-mixed tasks? <span class=\"ltx_text ltx_font_bold\">RQ 2:</span> Are model merging methods effective for integrating capabilities from multiple data sources that differ in language composition and training supervision?</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "tasks",
                    "multiple",
                    "codemixed",
                    "finetuning",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work examines model merging for code-mixed sentence classification, an approach not previously explored in code-mixed settings. We run experiments on English&#8211;Hindi and English&#8211;Spanish datasets using XLM-R, and Llama 3.2 1B to assess how major multilingual models respond to model merging. Concretely, we merge a continued pre-trained checkpoint with a base model and then fine-tune for code-mixed tasks. We also evaluate model merging under different data availability scenarios to test its ability to incorporate both monolingual and code-mixed resources.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "merging",
                    "model",
                    "tasks",
                    "xlmr",
                    "under",
                    "llama",
                    "codemixed",
                    "also",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce model merging as an adaptation strategy for code-mixed NLP tasks, distinct from conventional fine-tuning and continued pre-training, and observe improved downstream performance across language pairs, models, and tasks.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "model",
                    "across",
                    "tasks",
                    "performance",
                    "codemixed",
                    "pretraining",
                    "finetuning",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For cross-lingual transfer, merged models trained on code-mixed resources outperform those trained on monolingual English data, with the advantage most pronounced for low-resource language pairs, indicating that code-mixed knowledge is a stronger basis for adaptation when target-pair data is scarce.</p>\n\n",
                "matched_terms": [
                    "codemixed",
                    "when",
                    "transfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Modern work on code-mixed NLP primarily adapts <span class=\"ltx_text ltx_font_italic\">multilingual</span> pretrained models (e.g., mBERT, XLM-R)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pires et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib27\" title=\"\">2019</a>; Muller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib24\" title=\"\">2020</a>; Tan and Joty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib35\" title=\"\">2021</a>; Srivastava and Singh, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib34\" title=\"\">2022</a>; Winata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib36\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib37\" title=\"\">2021</a>; Rani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib30\" title=\"\">2024</a>; Priyadharshini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib29\" title=\"\">2022</a>)</cite>. Such models are adapted via task-specific fine-tuning and their efficacy has been shown on benchmarks such as LinCE and GLUECoS, which span token-level and sentence-level tasks across pairs like En&#8211;Hi and En&#8211;Es&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib2\" title=\"\">2020</a>; Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>)</cite>. In parallel, continued pre-training (CPT) on unlabeled code-mixed corpora has emerged as a complementary strategy before downstream fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Das et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite>. While both routes are effective, they rely on labeled and/or unlabeled code-mixed resources that remain scarce or unevenly distributed across language pairs, and they typically optimize a <em class=\"ltx_emph ltx_font_italic\">single</em> model instance rather than composing multiple models.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "benchmarks",
                    "model",
                    "across",
                    "tasks",
                    "their",
                    "xlmr",
                    "cpt",
                    "such",
                    "multiple",
                    "codemixed",
                    "en–es",
                    "pretraining",
                    "gluecos",
                    "finetuning",
                    "multilingual",
                    "en–hi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A less-explored axis concerns leveraging <span class=\"ltx_text ltx_font_italic\">monolingual</span> resources for code-mixed tasks. Prior work has examined translating mixed inputs to constituent languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pant and Dadu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib25\" title=\"\">2020</a>)</cite>, using language-specific models, or building meta-embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Winata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib37\" title=\"\">2021</a>)</cite> and non-Transformer encoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar and Solorio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib3\" title=\"\">2020</a>)</cite>; these can yield gains in select settings but do not directly address the need to <em class=\"ltx_emph ltx_font_italic\">compose</em> capabilities across sources. In contrast, <em class=\"ltx_emph ltx_font_italic\">modular</em> adaptation by <em class=\"ltx_emph ltx_font_italic\">model merging</em>&#8212;combining parameters from models specialized for different data or tasks (e.g., Task Arithmetic and TIES variants)&#8212;offers a data- and compute-efficient alternative that can, in principle, integrate code-mixed competence with strong monolingual representations without access to original training data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "across",
                    "ties",
                    "tasks",
                    "codemixed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model merging is a powerful technique that combines the parameters of different models, creating a composite model with enhanced functionality, all without the need for original training data or heavy computational resources. Methods like Task Arithmetic&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ilharco et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib17\" title=\"\">2023</a>)</cite> and its variants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yadav et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib40\" title=\"\">2023</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib42\" title=\"\">2024</a>)</cite> demonstrate the potential to merge models with diverse capacities to achieve a unified function. This concept can be extended to code-mixed tasks by combining task- or domain-specific modules. Exploring the use of model merging methods using monolingual or unlabeled resources, presents a promising avenue for advancing code-mixed NLP.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "tasks",
                    "variants",
                    "codemixed",
                    "merging",
                    "achieve"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To our knowledge, <span class=\"ltx_text ltx_font_bold\">model merging has not been applied to code-mixed NLP</span> prior to this line of work. Framing code-mixed adaptation as a modular composition problem allows us to ask: when labeled code-mixed data is limited, can we merge a base multilingual model with a CPT-adapted checkpoint (or with monolingual task vectors) and then fine-tune for the target task? This perspective also clarifies cross-lingual transfer: merged checkpoints trained with code-mixed resources tend to transfer more reliably to new code-mixed pairs than those relying solely on monolingual English supervision, highlighting that code-mixed signals are especially valuable under low-resource target conditions.</p>\n\n",
                "matched_terms": [
                    "merging",
                    "model",
                    "transfer",
                    "under",
                    "codemixed",
                    "also",
                    "highlighting",
                    "when",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Notations</span> Let <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> represent the target task. Let <math alttext=\"D^{T}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{L1-L2}</annotation></semantics></math> stand for a code-mixed dataset for task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> in the L1-L2 language pair (e.g., En-Hi), and &#160;(<math alttext=\"D^{T}_{L1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{L1}</annotation></semantics></math>) for a monolingual dataset in L1 for the same task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>. A language model can be adapted to code-mixed data by performing continued pre-training using an unlabeled code-mixed text corpus &#160;(<math alttext=\"D^{CM}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">D^{CM}_{L1-L2}</annotation></semantics></math>). We start with a base pre-trained model &#160;(<math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>), and when fine-tuned on the downstream task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>, we get <math alttext=\"\\theta^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m9\" intent=\":literal\"><semantics><msup><mi>&#952;</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">\\theta^{T}</annotation></semantics></math>. If <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m10\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> undergoes continued pre-training on a code-mixed dataset, it results in the &#160;<math alttext=\"\\theta^{CM}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m11\" intent=\":literal\"><semantics><msubsup><mi>&#952;</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\theta^{CM}_{L1-L2}</annotation></semantics></math> model, which is expected to better handle code-mixed text.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "model",
                    "codemixed",
                    "pretraining",
                    "when",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adapting a language model to a new domain or task has been shown to improve performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gururangan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib16\" title=\"\">2020</a>)</cite>. We adapt a language model to code-mixed corpus, and such treatment can lead to improved downstream task performance. We use existing unlabeled code-mixed corpus for continued pre-training of base model using Masked Language Modeling objective for Encoder-only models, and Causal LM objective for Decoder-only models&#160;(refer details Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4\" title=\"4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> for more details).\nWe take the continued pre-training checkpoints and perform fine-tuning on the downstream task dataset. We refer this method as &#8220;CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT&#8221;. We should note that domain adaptation incurs additional computational cost, which is much higher than the fine-tuning.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "model",
                    "performance",
                    "such",
                    "codemixed",
                    "pretraining",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For model merging we consider two methods - a) Task Arithmetic &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ilharco et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib17\" title=\"\">2023</a>)</cite>; b) TIES (TrIm, Elect, Sign) &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yadav et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib40\" title=\"\">2023</a>)</cite>. Task Arithmetic defines a direction, or Task Vector, in a model&#8217;s weight space that enhances task performance when followed. These vectors, calculated by subtracting pre-trained model weights from fine-tuned ones, guide neural network behavior. Task vectors can be modified and combined using operations like addition or negation to adjust model behavior. TIES is a variation that addresses interference from redundant parameters and disagreements in parameter signs across models.</p>\n\n",
                "matched_terms": [
                    "combined",
                    "model",
                    "ties",
                    "across",
                    "when",
                    "performance",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compute a task vector for continued pre-training of <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> on code-mixed text (<math alttext=\"\\tau_{CPT}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tau_{CPT}</annotation></semantics></math>) (Eq&#160;&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E1\" title=\"In 3.2. Model Merging &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). We add one Task Vector at a time and fine-tune task specific dataset (<math alttext=\"D^{T}_{enhi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{enhi}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "continued",
                    "codemixed",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> denotes the fine-tuning of the model on a downstream task, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, scaling term is determined using held-out validation sets.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In TIES, the resulting model weights are not a simple weighted average of all parameters; instead, they are adjusted based on the magnitude and weight of the task vectors. For further discussion,\nwe refer to Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E1\" title=\"In 3.2. Model Merging &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> as &#8220;TV&#8221;.\nIn case of TIES they are referred to as &#8220;TIES&#8221;.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ties"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leverage monolingual English data in our adaptation methods and evaluate on the code-mixed task.\nTo compare cross-lingual/cross-dataset transfer we consider two baselines- a) Joint Training&#160;(JointFT): English and English-Hindi task datasets are combined and model is fine-tuned on the combined dataset; b) Sequential Training&#160;(SeqFT): we first train the model on English task dataset, followed by fine-tuning on the English-Hindi code-mixed dataset. Additionally, we look at the following:</p>\n\n",
                "matched_terms": [
                    "combined",
                    "sequential",
                    "model",
                    "training",
                    "transfer",
                    "joint",
                    "codemixed",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> denotes fine-tuning on <math alttext=\"D^{T}_{enhi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{enhi}</annotation></semantics></math> and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> is a scaling factor. We also merge <math alttext=\"\\tau^{T}_{en}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>&#964;</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\tau^{T}_{en}</annotation></semantics></math> and <math alttext=\"\\tau_{CPT}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tau_{CPT}</annotation></semantics></math> before fine-tuning:</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "also"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When using TIES, the resulting model weights are adjusted based on parameter magnitudes and signs rather than simple averaging.</p>\n\n",
                "matched_terms": [
                    "when",
                    "model",
                    "ties"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Models:</span> We use multilingual models, as their fine-tuning has consistently shown strong results <cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>; Das et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite>.\nFor our experiments, we use mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib11\" title=\"\">2019</a>)</cite> and XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib9\" title=\"\">2020</a>)</cite>, which are widely used in code-mixed studies, along with Llama 3.2 1B model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B</a></span></span></span>.\nDue to computational constraints, our model adaptation methods are restricted to Llama 1B; however, we conduct inference on larger models including Llama 3.2 3B, Llama 3.1 8B, and Llama 3.3 70B.</p>\n\n",
                "matched_terms": [
                    "model",
                    "their",
                    "xlmr",
                    "llama",
                    "70b",
                    "codemixed",
                    "constraints",
                    "finetuning",
                    "multilingual",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span> Our study focuses on English-Hindi (En-Hi) and English-Spanish (En-Es) code-mixed sentiment classification tasks, as they are part of code-mixing benchmarks GLUECoS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>)</cite> and LinCE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib2\" title=\"\">2020</a>)</cite>. For En-Hi we consider two tasks - a) Sentiment Analysis using three datasets &#8212; GLUECoS, Sentimix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Patwa et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib26\" title=\"\">2020</a>)</cite>, and&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Prabhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib28\" title=\"\">2016</a>)</cite>&#8212;all featuring 3-class labels (positive, neutral, negative); b) Hate Speech Classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bohra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib5\" title=\"\">2018</a>)</cite>. For En-Es, we carry out experiments on Sentiment Analysis task, and use two datasets released by <cite class=\"ltx_cite ltx_citemacro_citet\">Patwa et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib26\" title=\"\">2020</a>)</cite> and GLUECoS benchmark. For transfer from monolingual English task dataset, we use English SST5 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Socher et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib33\" title=\"\">2013</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "hate",
                    "sentiment",
                    "benchmarks",
                    "transfer",
                    "tasks",
                    "bohra",
                    "prabhu",
                    "codemixed",
                    "analysis",
                    "sentimix",
                    "gluecos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate cross lingual to new language pairs, we consider three target datasets - a) English-Tamil&#160;(En-Ta)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chakravarthi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib7\" title=\"\">2020b</a>)</cite>; b) English-Malayalam&#160;(En-Ml)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chakravarthi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib6\" title=\"\">2020a</a>)</cite>; c) English-Spanish&#160;(En-Es) from GLUECoS benchmark. For unlabeled code-mixed corpus, we use the dataset released by &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite> for both English-Hindi and English-Spanish.</p>\n\n",
                "matched_terms": [
                    "codemixed",
                    "gluecos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot Prompting on Llama:</span>\nWe prompt Llama-3.2-1B, Llama-3.2-3B, Llama-3.1-8B, and Llama-3.3-70B to perform the given tasks. We evaluate them using 0, 5, 10, 15 and 20-shot setting.</p>\n\n",
                "matched_terms": [
                    "prompting",
                    "fewshot",
                    "llama",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.T1\" title=\"Table 1 &#8227; Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides statistics of all the datasets we have used in our experiments, along with the label wise distributions in all splits. We include all the labeled, unlabeled datasets in the table for ready reference. SST5 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Socher et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib33\" title=\"\">2013</a>)</cite> has 5 classes, and to use in our experiments we convert its 5-class labels into 3 classes by merging &#8220;very positive&#8221; with &#8220;positive&#8221; and &#8220;very negative&#8221; with &#8220;negative&#8221;. Wherever all three splits (train-test-valid) are available we use the same, and in case they are not available we create 70-20-10 split from the available samples. We use F1 as the measure of performance across all the data and training configurations. F1 is suitable as there is some dataset imbalances across code-mixed datasets.</p>\n\n",
                "matched_terms": [
                    "training",
                    "across",
                    "codemixed",
                    "performance",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompts:</span> We used the following prompts for our zero and few shot experiments. For few-shot prompt, we include the examples and the corresponding labels in between the instruction and the input sample. For the k-shot settings, we carry out five runs, each run with different set of randomly sampled k examples from the train dataset, while ensuring that all the labels are well represented in the examples.</p>\n\n",
                "matched_terms": [
                    "fewshot",
                    "zero"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero shot prompt used for sentiment analysis is as follows:</p>\n\n",
                "matched_terms": [
                    "sentiment",
                    "analysis",
                    "zero"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hardware &amp; Hyperparameters:</span> We conduct all experiments on a combination of Nvidia 1080 Ti 12 GB GPUs, Nvidia A6000 32 GB GPU, using a single GPU. We use Nvidia A6000 for Llama experiemnts, and 1080Ti for all other experiments. Batch sizes range from 8 to 128, depending on the dataset and approach. Learning rates were selected after a search between [1e-2, 5e-5], with 1e-5 working best for full model training, 1e-4 for LoRA layers. All experiments ran for with early stopping&#160;(stop if model performance doesn&#8217;t change by 0.5 F1 point across 3 evaluations), maximum up to 20 epochs. We use AdamW optimizer with default optimizer parameters for all our experiments. We implemented the methods using PyTorch, Huggingface Transformers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wolf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib38\" title=\"\">2020</a>)</cite>, PEFT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mangrulkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib23\" title=\"\">2022</a>)</cite>, and mergekit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goddard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib13\" title=\"\">2024</a>)</cite> for model merging methods. Upon publication code, data, models, and detailed hyperparameters configurations will be publicly released for reproducibility.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "across",
                    "full",
                    "llama",
                    "performance",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let Base Model&#160;(<math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>) denote the base pretrained model. <em class=\"ltx_emph ltx_font_italic\">Full FT</em> fine-tunes Base Model directly on the target code-mixed dataset. <em class=\"ltx_emph ltx_font_italic\">CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT</em> first performs CPT on code-mixed text to obtain <math alttext=\"\\theta_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mtext>CPT</mtext></msub><annotation encoding=\"application/x-tex\">\\theta_{\\text{CPT}}</annotation></semantics></math> and then fine-tunes on the target task. <em class=\"ltx_emph ltx_font_italic\">Seq FT</em> is sequential fine-tuning&#8212;fine-tune on English data, then fine-tune on the code-mixed target. <em class=\"ltx_emph ltx_font_italic\">Joint FT</em> jointly fine-tunes on the union of English and code-mixed labeled data. The remaining blocks use weight merging before the final task fine-tuning. In the <em class=\"ltx_emph ltx_font_italic\">Task-Vector (TV)</em> setting, the merge operator <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> is (scaled) vector addition, <math alttext=\"\\theta\\oplus\\lambda\\tau:=\\theta+\\lambda\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>&#952;</mi><mo>&#8853;</mo><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#964;</mi></mrow></mrow><mo>:=</mo><mrow><mi>&#952;</mi><mo>+</mo><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#964;</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\theta\\oplus\\lambda\\tau:=\\theta+\\lambda\\tau</annotation></semantics></math>, with <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> (or <math alttext=\"\\lambda_{1},\\lambda_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#955;</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda_{1},\\lambda_{2}</annotation></semantics></math> when combining two vectors) tuned on validation data: <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}</annotation></semantics></math></em> adds the CPT vector; <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> adds the English vector; <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><mrow><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><mo rspace=\"0.392em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> adds both. In the <em class=\"ltx_emph ltx_font_italic\">TIES</em> setting, <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> denotes TIES merging (TrIm&#8211;Elect&#8211;Sign), which resolves interference via magnitude/sign rules rather than simple addition; the rows <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}</annotation></semantics></math></em>, <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em>, and <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m14\" intent=\":literal\"><semantics><mrow><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><mo rspace=\"0.392em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> mirror the TV configurations. In all TV/TIES cases, the merged model is subsequently fine-tuned on the target code-mixed dataset, and reported scores correspond to this final model.</p>\n\n",
                "matched_terms": [
                    "reported",
                    "sequential",
                    "model",
                    "ties",
                    "full",
                    "cpt",
                    "when",
                    "joint",
                    "codemixed",
                    "finetuning",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, baseline fine-tuning significantly outperforms zero-shot and few-shot prompting with LLaMA 3.3 70B, highlighting the limitations of in-context learning for code-mixed tasks. Few-shot prompting improves over zero-shot, but gains plateau at higher k-shot levels, suggesting limited scalability. These findings reinforce the necessity of fine-tuning and merging strategies for robust performance in resource-constrained settings. Finally, as we explore in <span class=\"ltx_text ltx_font_bold\">RQ2</span>, model merging is particularly valuable for integrating heterogeneous resources and enabling cross-lingual transfer, often surpassing sequential or joint fine-tuning.</p>\n\n",
                "matched_terms": [
                    "sequential",
                    "strategies",
                    "model",
                    "transfer",
                    "particularly",
                    "tasks",
                    "performance",
                    "llama",
                    "70b",
                    "joint",
                    "codemixed",
                    "fewshot",
                    "highlighting",
                    "prompting",
                    "finetuning",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Varying the corpus used for pre-training might have some impact on the downstream task. To understand such variance, we do continued pre-training using two different unlabeled corpora, and analyze the impact on the downstream tasks.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "such",
                    "pretraining",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the dataset released by &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite> for continued pre-training, along with a synthetic code-mixed corpus generated via the GCM Toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rizvi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib31\" title=\"\">2021</a>)</cite> and filtered using an acceptability filter proposed by &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kodali et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib21\" title=\"\">2024</a>)</cite>. The sizes of the two datasets are kept consistent. As code-mixed datasets often come from social media, we expect significant distributional differences between the task-specific and synthetic datasets. Data sources for continued pre-training can differ, leading to differing downstream task performance.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "codemixed",
                    "performance",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S5.T3\" title=\"Table 3 &#8227; 5.2. Impact of Continued Pre-Training Corpus &#8227; 5. Results &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> compares the downstream task performance when we vary the corpus used for continued pre-training. Performance on downstream tasks is only marginally affected by the dataset chosen for continued pre-training. Classification results vary by roughly 1 F1 point across different sources of unlabeled data and frequently meet or exceed the results of the fine-tuning exclusively on the downstream dataset.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "across",
                    "tasks",
                    "performance",
                    "when",
                    "pretraining",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We take the model fine-tuned on the combination of our three En-Hi datasets, using all the different training configurations, and infer on the test set of the En-Ta, En-Ml datasets. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S5.T4\" title=\"Table 4 &#8227; 5.2. Impact of Continued Pre-Training Corpus &#8227; 5. Results &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the transfer of various training strategies to other code-mixed language pairs for Sentiment Analysis task. We present transfer results for all the combinations of base models and training methods. We also include full fine-tuning on the English dataset (FullFT (En)) to ascertain which is better - transfer from monolingual En resources or code-mixed resources from other language pairs.</p>\n\n",
                "matched_terms": [
                    "sentiment",
                    "strategies",
                    "model",
                    "transfer",
                    "training",
                    "full",
                    "codemixed",
                    "analysis",
                    "also",
                    "finetuning",
                    "results",
                    "combinations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate both XLM-R (encoder) and LLaMA 3.2 1B (decoder) to study transfer properties. Across En-Ta and En-Ml, we observe a consistent pattern: checkpoints obtained through model merging outperform Full FT baselines by 5&#8211;13 F1 points. This improvement holds across models and target language pairs, highlighting that merged checkpoints capture complementary signals from diverse resources. These findings reinforce our answer to <span class=\"ltx_text ltx_font_bold\">RQ2</span>&#8212;that model merging is an effective strategy for integrating heterogeneous data sources, often surpassing sequential or joint fine-tuning. However, the gains remain dependent on model architecture, the specific code-mixed task, and the language pair being targeted, underscoring the importance of careful merging design.</p>\n\n",
                "matched_terms": [
                    "sequential",
                    "model",
                    "transfer",
                    "across",
                    "full",
                    "xlmr",
                    "llama",
                    "joint",
                    "codemixed",
                    "highlighting",
                    "finetuning",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Model merging methods can be used to integrate capabilities from multiple data sources with varying language composition and supervision, often outperforming sequential fine-tuning and joint fine-tuning <span class=\"ltx_text ltx_font_bold\">(RQ 2)</span>. However, the impact of model merging is highly dependent on the model, task, and target language pair; underscoring the need for careful merging strategies.</p>\n\n",
                "matched_terms": [
                    "sequential",
                    "strategies",
                    "model",
                    "multiple",
                    "joint",
                    "finetuning",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically show the advantages of using model merging as a strategy for code-mixed tasks. We also introduce an innovative perspective on utilizing existing data resources for code-mixed tasks, diverging from traditional domain-adaptation and fine-tuning techniques. In circumstances where datasets are lacking or are available in different languages, our findings can assist researchers in choosing an appropriate method to optimize the available resources.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tasks",
                    "codemixed",
                    "also",
                    "finetuning",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">When both labeled and unlabeled data are available</span>, the most effective strategy is to apply model merging method, merging CPT model with the base model, followed by fine-tuning on the target dataset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cpt",
                    "when",
                    "finetuning",
                    "merging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">For smaller labeled datasets</span>, model merging is preferable to simple fine-tuning due to its superior sample efficiency.</p>\n\n",
                "matched_terms": [
                    "merging",
                    "superior",
                    "finetuning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">If only labeled data is available</span>, full model fine-tuning remains the best option.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "model",
                    "full"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In the absence of both labeled and unlabeled data</span>, the most effective approach is to utilize models trained on similar tasks with other code-mixed language resources. These models consistently outperform those transferred from monolingual English resources.</p>\n\n",
                "matched_terms": [
                    "codemixed",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations</span>\nOur experiments are limited to sentence classification due to the lack of consistent datasets for other tasks in both monolingual and code-mixed languages. Therefore, the generalizability of our findings to other NLP tasks is unclear. Future research should evaluate these methods on a broader range of tasks with comparable datasets across multiple language pairs.</p>\n\n",
                "matched_terms": [
                    "codemixed",
                    "multiple",
                    "across",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although our study contains models of different architecture, we do not scale it to larger language models (&#191;1B parameter models), because of prohibitive cost of training/fine-tuning a larger model. Analyzing how model merging behaves as model size increase for code-mixed tasks would be an interesting avenue for future work.</p>\n\n",
                "matched_terms": [
                    "codemixed",
                    "model",
                    "merging",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For model merging methods, there is a small additional computation cost, where the weights of the models are being merged. This could be a limitation while trying to find the optimal hyperparameters. For example, the <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p6.m1\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> values in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E1\" title=\"In 3.2. Model Merging &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E2\" title=\"In Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E3\" title=\"In Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "merging",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to thank the PreCog Lab at IIIT-Hyderabad for their valuable guidance and discussions throughout this work. In particular, we thank Shashwat Singh, Anisha Saha, Hari Shankar, Ishan Kavethekar and Ritwik Mishra for their detailed feedback and thoughtful suggestions during multiple stages of the project.We gratefully acknowledge the support of Microsoft Research India through the MSR India PhD Award 2024. We also thank the anonymous reviewers for their insightful feedback.</p>\n\n",
                "matched_terms": [
                    "their",
                    "also",
                    "multiple"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Adapting Multilingual Models to Code-Mixed Tasks via Model Merging",
        "caption": "Table 3. Variation in downstream task results when different unlabeled code-mix corpora are used for continued pre-training",
        "body": "0.75\n\n\n(0.75 | 0.007)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">0.75</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">(0.75 | 0.007)</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "continued",
            "task",
            "downstream",
            "different",
            "corpora",
            "variation",
            "codemix",
            "unlabeled",
            "pretraining",
            "when",
            "used",
            "results"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S5.T3\" title=\"Table 3 &#8227; 5.2. Impact of Continued Pre-Training Corpus &#8227; 5. Results &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> compares the downstream task performance when we vary the corpus used for continued pre-training. Performance on downstream tasks is only marginally affected by the dataset chosen for continued pre-training. Classification results vary by roughly 1 F1 point across different sources of unlabeled data and frequently meet or exceed the results of the fine-tuning exclusively on the downstream dataset.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We study model merging as a practical alternative to conventional adaptation strategies for code-mixed NLP. Starting from a multilingual base model, we: (i) perform continued pre-training (CPT) on unlabeled code-mixed text to obtain an adapted checkpoint, (ii) merge checkpoint with the base model, and (iii) fine-tune&#160;(FT) on the downstream task data. We evaluate our approach for sentence classification (sentiment and hate speech) task in English-Hindi (En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our results show that merged models consistently outperform full fine-tuning and CPT<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>FT. We observe gains of 2&#8211;5 points in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> over full fine-tuning and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2 points over CPT<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>FT, indicating that unlabeled data is leveraged more effectively via merging than via CPT alone. Zero-/few-shot prompting with larger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged checkpoints, underscoring limits of in-context learning for code-mixed inputs. We further test cross-pair transfer by training on En&#8211;Hi and evaluating on En&#8211;Ta and En&#8211;Ml: merged checkpoints transfer more strongly than monolingual-English baselines (e.g., TV/TIES variants reaching 0.65&#8211;0.68 <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> vs. 0.61&#8211;0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more reliable substrate for low-resource pairs. We conclude with adaptation recipes matched to common data regimes (labeled only; labeled+unlabeled; transfer-only) and discuss limitations and scaling considerations for broader tasks and larger models.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "task",
                    "downstream",
                    "unlabeled",
                    "pretraining",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A complementary strategy for adapting language models to new domains or linguistic settings is continued pre-training (CPT) on unlabeled data&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gururangan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib16\" title=\"\">2020</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib22\" title=\"\">2021</a>; Ruder, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib32\" title=\"\">2021</a>)</cite>. Model merging&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib41\" title=\"\">2024</a>)</cite> has emerged as a promising approach for adapting models to new domains, tasks, and languages, while mitigating several limitations of CPT. Although CPT can aid domain adaptation, it can also erode general language understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gogoulou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib14\" title=\"\">2024</a>; Alexandrov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib4\" title=\"\">2024</a>)</cite>. In contrast, recent work shows that model merging can improve task performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wortsman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib39\" title=\"\">2022</a>; Choshen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib8\" title=\"\">2022</a>; Izmailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib18\" title=\"\">2018</a>)</cite>, enhance robustness and out-of-domain generalization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wortsman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib39\" title=\"\">2022</a>; Jin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib19\" title=\"\">2023</a>)</cite>, and enable multitask models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yadav et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib40\" title=\"\">2023</a>; Ilharco et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib17\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "unlabeled",
                    "task",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond improving performance for a single code-mixed language pair, a natural question is whether gains from model merging transfer to other language pairs performing the same downstream task. In such cases, the task formulation remains fixed, but the language composition changes, introducing new patterns of mixing, grammar, and vocabulary overlap. These shifts create both challenges and opportunities: the model must adjust to different lexical and syntactic cues, yet shared structures or typological similarities may support better generalization. Understanding how language composition affects transfer is especially important in low-resource settings, where direct annotations for the target pair may be unavailable and cross-pair transfer becomes a viable adaptation strategy.</p>\n\n",
                "matched_terms": [
                    "different",
                    "task",
                    "downstream"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work examines model merging for code-mixed sentence classification, an approach not previously explored in code-mixed settings. We run experiments on English&#8211;Hindi and English&#8211;Spanish datasets using XLM-R, and Llama 3.2 1B to assess how major multilingual models respond to model merging. Concretely, we merge a continued pre-trained checkpoint with a base model and then fine-tune for code-mixed tasks. We also evaluate model merging under different data availability scenarios to test its ability to incorporate both monolingual and code-mixed resources.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce model merging as an adaptation strategy for code-mixed NLP tasks, distinct from conventional fine-tuning and continued pre-training, and observe improved downstream performance across language pairs, models, and tasks.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "pretraining",
                    "downstream"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Modern work on code-mixed NLP primarily adapts <span class=\"ltx_text ltx_font_italic\">multilingual</span> pretrained models (e.g., mBERT, XLM-R)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pires et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib27\" title=\"\">2019</a>; Muller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib24\" title=\"\">2020</a>; Tan and Joty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib35\" title=\"\">2021</a>; Srivastava and Singh, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib34\" title=\"\">2022</a>; Winata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib36\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib37\" title=\"\">2021</a>; Rani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib30\" title=\"\">2024</a>; Priyadharshini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib29\" title=\"\">2022</a>)</cite>. Such models are adapted via task-specific fine-tuning and their efficacy has been shown on benchmarks such as LinCE and GLUECoS, which span token-level and sentence-level tasks across pairs like En&#8211;Hi and En&#8211;Es&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib2\" title=\"\">2020</a>; Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>)</cite>. In parallel, continued pre-training (CPT) on unlabeled code-mixed corpora has emerged as a complementary strategy before downstream fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Das et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite>. While both routes are effective, they rely on labeled and/or unlabeled code-mixed resources that remain scarce or unevenly distributed across language pairs, and they typically optimize a <em class=\"ltx_emph ltx_font_italic\">single</em> model instance rather than composing multiple models.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "downstream",
                    "corpora",
                    "unlabeled",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A less-explored axis concerns leveraging <span class=\"ltx_text ltx_font_italic\">monolingual</span> resources for code-mixed tasks. Prior work has examined translating mixed inputs to constituent languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pant and Dadu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib25\" title=\"\">2020</a>)</cite>, using language-specific models, or building meta-embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Winata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib37\" title=\"\">2021</a>)</cite> and non-Transformer encoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar and Solorio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib3\" title=\"\">2020</a>)</cite>; these can yield gains in select settings but do not directly address the need to <em class=\"ltx_emph ltx_font_italic\">compose</em> capabilities across sources. In contrast, <em class=\"ltx_emph ltx_font_italic\">modular</em> adaptation by <em class=\"ltx_emph ltx_font_italic\">model merging</em>&#8212;combining parameters from models specialized for different data or tasks (e.g., Task Arithmetic and TIES variants)&#8212;offers a data- and compute-efficient alternative that can, in principle, integrate code-mixed competence with strong monolingual representations without access to original training data.</p>\n\n",
                "matched_terms": [
                    "different",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model merging is a powerful technique that combines the parameters of different models, creating a composite model with enhanced functionality, all without the need for original training data or heavy computational resources. Methods like Task Arithmetic&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ilharco et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib17\" title=\"\">2023</a>)</cite> and its variants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yadav et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib40\" title=\"\">2023</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib42\" title=\"\">2024</a>)</cite> demonstrate the potential to merge models with diverse capacities to achieve a unified function. This concept can be extended to code-mixed tasks by combining task- or domain-specific modules. Exploring the use of model merging methods using monolingual or unlabeled resources, presents a promising avenue for advancing code-mixed NLP.</p>\n\n",
                "matched_terms": [
                    "unlabeled",
                    "different",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To our knowledge, <span class=\"ltx_text ltx_font_bold\">model merging has not been applied to code-mixed NLP</span> prior to this line of work. Framing code-mixed adaptation as a modular composition problem allows us to ask: when labeled code-mixed data is limited, can we merge a base multilingual model with a CPT-adapted checkpoint (or with monolingual task vectors) and then fine-tune for the target task? This perspective also clarifies cross-lingual transfer: merged checkpoints trained with code-mixed resources tend to transfer more reliably to new code-mixed pairs than those relying solely on monolingual English supervision, highlighting that code-mixed signals are especially valuable under low-resource target conditions.</p>\n\n",
                "matched_terms": [
                    "when",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Notations</span> Let <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> represent the target task. Let <math alttext=\"D^{T}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{L1-L2}</annotation></semantics></math> stand for a code-mixed dataset for task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> in the L1-L2 language pair (e.g., En-Hi), and &#160;(<math alttext=\"D^{T}_{L1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{L1}</annotation></semantics></math>) for a monolingual dataset in L1 for the same task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>. A language model can be adapted to code-mixed data by performing continued pre-training using an unlabeled code-mixed text corpus &#160;(<math alttext=\"D^{CM}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">D^{CM}_{L1-L2}</annotation></semantics></math>). We start with a base pre-trained model &#160;(<math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>), and when fine-tuned on the downstream task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>, we get <math alttext=\"\\theta^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m9\" intent=\":literal\"><semantics><msup><mi>&#952;</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">\\theta^{T}</annotation></semantics></math>. If <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m10\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> undergoes continued pre-training on a code-mixed dataset, it results in the &#160;<math alttext=\"\\theta^{CM}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m11\" intent=\":literal\"><semantics><msubsup><mi>&#952;</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\theta^{CM}_{L1-L2}</annotation></semantics></math> model, which is expected to better handle code-mixed text.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "task",
                    "downstream",
                    "unlabeled",
                    "pretraining",
                    "when",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adapting a language model to a new domain or task has been shown to improve performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gururangan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib16\" title=\"\">2020</a>)</cite>. We adapt a language model to code-mixed corpus, and such treatment can lead to improved downstream task performance. We use existing unlabeled code-mixed corpus for continued pre-training of base model using Masked Language Modeling objective for Encoder-only models, and Causal LM objective for Decoder-only models&#160;(refer details Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4\" title=\"4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> for more details).\nWe take the continued pre-training checkpoints and perform fine-tuning on the downstream task dataset. We refer this method as &#8220;CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT&#8221;. We should note that domain adaptation incurs additional computational cost, which is much higher than the fine-tuning.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "task",
                    "downstream",
                    "unlabeled",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For model merging we consider two methods - a) Task Arithmetic &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ilharco et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib17\" title=\"\">2023</a>)</cite>; b) TIES (TrIm, Elect, Sign) &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yadav et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib40\" title=\"\">2023</a>)</cite>. Task Arithmetic defines a direction, or Task Vector, in a model&#8217;s weight space that enhances task performance when followed. These vectors, calculated by subtracting pre-trained model weights from fine-tuned ones, guide neural network behavior. Task vectors can be modified and combined using operations like addition or negation to adjust model behavior. TIES is a variation that addresses interference from redundant parameters and disagreements in parameter signs across models.</p>\n\n",
                "matched_terms": [
                    "when",
                    "task",
                    "variation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compute a task vector for continued pre-training of <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> on code-mixed text (<math alttext=\"\\tau_{CPT}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tau_{CPT}</annotation></semantics></math>) (Eq&#160;&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E1\" title=\"In 3.2. Model Merging &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). We add one Task Vector at a time and fine-tune task specific dataset (<math alttext=\"D^{T}_{enhi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{enhi}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "continued",
                    "task",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> denotes the fine-tuning of the model on a downstream task, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, scaling term is determined using held-out validation sets.</p>\n\n",
                "matched_terms": [
                    "task",
                    "downstream"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Models:</span> We use multilingual models, as their fine-tuning has consistently shown strong results <cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>; Das et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite>.\nFor our experiments, we use mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib11\" title=\"\">2019</a>)</cite> and XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib9\" title=\"\">2020</a>)</cite>, which are widely used in code-mixed studies, along with Llama 3.2 1B model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B</a></span></span></span>.\nDue to computational constraints, our model adaptation methods are restricted to Llama 1B; however, we conduct inference on larger models including Llama 3.2 3B, Llama 3.1 8B, and Llama 3.3 70B.</p>\n\n",
                "matched_terms": [
                    "used",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.T1\" title=\"Table 1 &#8227; Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides statistics of all the datasets we have used in our experiments, along with the label wise distributions in all splits. We include all the labeled, unlabeled datasets in the table for ready reference. SST5 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Socher et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib33\" title=\"\">2013</a>)</cite> has 5 classes, and to use in our experiments we convert its 5-class labels into 3 classes by merging &#8220;very positive&#8221; with &#8220;positive&#8221; and &#8220;very negative&#8221; with &#8220;negative&#8221;. Wherever all three splits (train-test-valid) are available we use the same, and in case they are not available we create 70-20-10 split from the available samples. We use F1 as the measure of performance across all the data and training configurations. F1 is suitable as there is some dataset imbalances across code-mixed datasets.</p>\n\n",
                "matched_terms": [
                    "unlabeled",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompts:</span> We used the following prompts for our zero and few shot experiments. For few-shot prompt, we include the examples and the corresponding labels in between the instruction and the input sample. For the k-shot settings, we carry out five runs, each run with different set of randomly sampled k examples from the train dataset, while ensuring that all the labels are well represented in the examples.</p>\n\n",
                "matched_terms": [
                    "used",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let Base Model&#160;(<math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>) denote the base pretrained model. <em class=\"ltx_emph ltx_font_italic\">Full FT</em> fine-tunes Base Model directly on the target code-mixed dataset. <em class=\"ltx_emph ltx_font_italic\">CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT</em> first performs CPT on code-mixed text to obtain <math alttext=\"\\theta_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mtext>CPT</mtext></msub><annotation encoding=\"application/x-tex\">\\theta_{\\text{CPT}}</annotation></semantics></math> and then fine-tunes on the target task. <em class=\"ltx_emph ltx_font_italic\">Seq FT</em> is sequential fine-tuning&#8212;fine-tune on English data, then fine-tune on the code-mixed target. <em class=\"ltx_emph ltx_font_italic\">Joint FT</em> jointly fine-tunes on the union of English and code-mixed labeled data. The remaining blocks use weight merging before the final task fine-tuning. In the <em class=\"ltx_emph ltx_font_italic\">Task-Vector (TV)</em> setting, the merge operator <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> is (scaled) vector addition, <math alttext=\"\\theta\\oplus\\lambda\\tau:=\\theta+\\lambda\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>&#952;</mi><mo>&#8853;</mo><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#964;</mi></mrow></mrow><mo>:=</mo><mrow><mi>&#952;</mi><mo>+</mo><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#964;</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\theta\\oplus\\lambda\\tau:=\\theta+\\lambda\\tau</annotation></semantics></math>, with <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> (or <math alttext=\"\\lambda_{1},\\lambda_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#955;</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda_{1},\\lambda_{2}</annotation></semantics></math> when combining two vectors) tuned on validation data: <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}</annotation></semantics></math></em> adds the CPT vector; <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> adds the English vector; <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><mrow><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><mo rspace=\"0.392em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> adds both. In the <em class=\"ltx_emph ltx_font_italic\">TIES</em> setting, <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> denotes TIES merging (TrIm&#8211;Elect&#8211;Sign), which resolves interference via magnitude/sign rules rather than simple addition; the rows <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}</annotation></semantics></math></em>, <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em>, and <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m14\" intent=\":literal\"><semantics><mrow><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><mo rspace=\"0.392em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> mirror the TV configurations. In all TV/TIES cases, the merged model is subsequently fine-tuned on the target code-mixed dataset, and reported scores correspond to this final model.</p>\n\n",
                "matched_terms": [
                    "when",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate that model merging methods (TV<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT, TIES<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>CPT in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) consistently outperform standard fine-tuning (Full FT) across all models and datasets, yielding +2&#8211;5 F1 over Full FT and +1&#8211;2 F1 over CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT. This confirms that unlabeled data can be exploited more effectively through model merging than through continued pre-training alone, supporting our hypothesis in <span class=\"ltx_text ltx_font_bold\">RQ1</span>. Between merging strategies, Task Vectors (TV) generally perform better than TIES, though the relative advantage varies by model and task. We also find that the choice of corpus for CPT has only a marginal effect on downstream performance.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "task",
                    "downstream",
                    "unlabeled",
                    "pretraining",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis varies across three key dimensions: (a) use of only labeled data, (b) use of both labeled and unlabeled data, and (c) type of model augmentation techniques. We combine code-mixed data with monolingual task-specific data. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents a comparison of model performance across different scenarios of data availability. These settings reflect practical conditions in real-world code-mixed applications.</p>\n\n",
                "matched_terms": [
                    "unlabeled",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Leveraging monolingual resources with code-mixed resources</span>\nWe combine three elements here: the base model, a vector fine-tuned on English data, and a vector for continued pre-training on code-mixed data (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E3\" title=\"In Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Although we expected the combination of monolingual and code-mixed resources to perform better than just monolingual or just code-mixed, they perform similarly to CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT method, or outperform it by 1 or 2 F1 points (which can be seen in the <span class=\"ltx_text ltx_font_italic\">TV-T_en + CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT </span>and<span class=\"ltx_text ltx_font_italic\"> TIES-T_en + CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT</span> rows of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Extending to a three-way merge does not provide additional gains over a single CPT merge. Overall, compared to FullFT, the combined monolingual and code-mixed approach either matches performance or underperforms slightly (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2 F1 points).</p>\n\n",
                "matched_terms": [
                    "continued",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Varying the corpus used for pre-training might have some impact on the downstream task. To understand such variance, we do continued pre-training using two different unlabeled corpora, and analyze the impact on the downstream tasks.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "task",
                    "downstream",
                    "different",
                    "corpora",
                    "unlabeled",
                    "pretraining",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the dataset released by &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite> for continued pre-training, along with a synthetic code-mixed corpus generated via the GCM Toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rizvi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib31\" title=\"\">2021</a>)</cite> and filtered using an acceptability filter proposed by &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kodali et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib21\" title=\"\">2024</a>)</cite>. The sizes of the two datasets are kept consistent. As code-mixed datasets often come from social media, we expect significant distributional differences between the task-specific and synthetic datasets. Data sources for continued pre-training can differ, leading to differing downstream task performance.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "task",
                    "pretraining",
                    "downstream"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The relationship between task performance and in-context learning (ICL) for code-mixed tasks remains underexplored. To better understand how model size and prompting methods influence performance, we conduct zero-shot and few-shot prompting experiments using different-sized models from the LLaMA family&#8212;1B-instruct, 3B-instruct, 8B-instruct, and 70B-instruct. For few-shot prompting, we evaluate performance with 5, 10, 15, and 20 examples. Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> contains results of our experiments on LLama 3.2 70B model. We see that even baseline fine-tuning methods perform better than zero/few shot prompting, suggesting that in-context learning performs weakly for codemixing tasks. The highest F1 scores can be seen for the <cite class=\"ltx_cite ltx_citemacro_citet\">Prabhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib28\" title=\"\">2016</a>)</cite> datasete suggesting that the performance may be dependent on the data setting. Upon further analysis, we we hypothesize that this is due to the lack of noise in the dataset compared to the other datasets we use for comparison.</p>\n\n",
                "matched_terms": [
                    "results",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We take the model fine-tuned on the combination of our three En-Hi datasets, using all the different training configurations, and infer on the test set of the En-Ta, En-Ml datasets. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S5.T4\" title=\"Table 4 &#8227; 5.2. Impact of Continued Pre-Training Corpus &#8227; 5. Results &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the transfer of various training strategies to other code-mixed language pairs for Sentiment Analysis task. We present transfer results for all the combinations of base models and training methods. We also include full fine-tuning on the English dataset (FullFT (En)) to ascertain which is better - transfer from monolingual En resources or code-mixed resources from other language pairs.</p>\n\n",
                "matched_terms": [
                    "different",
                    "task",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Model merging methods can be used to integrate capabilities from multiple data sources with varying language composition and supervision, often outperforming sequential fine-tuning and joint fine-tuning <span class=\"ltx_text ltx_font_bold\">(RQ 2)</span>. However, the impact of model merging is highly dependent on the model, task, and target language pair; underscoring the need for careful merging strategies.</p>\n\n",
                "matched_terms": [
                    "used",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">When both labeled and unlabeled data are available</span>, the most effective strategy is to apply model merging method, merging CPT model with the base model, followed by fine-tuning on the target dataset.</p>\n\n",
                "matched_terms": [
                    "when",
                    "unlabeled"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "Adapting Multilingual Models to Code-Mixed Tasks via Model Merging",
        "caption": "Table 4. Performance comparison of different fine-tuning and transfer strategies for sentiment analysis in En–Ta and En–Ml, using XLM-R and LLaMA 3.2 1B models. The table reports results for full fine-tuning (on English and bilingual data), continued pretraining (CPT), transfer variants (TV), and parameter-efficient strategies such as TIES, with and without CPT. Main takeaway: Transfer-based approaches (TV and TIES), especially when combined with CPT, consistently outperform or match full fine-tuning baselines, showing their effectiveness for low-resource code-mixed scenarios while being more resource-efficient.",
        "body": "Method Model\nEn-Ta\nEn-Ml\n\n\nXLM-R\nLLAMA 3.2 1B\nXLM-R\nLLAMA 3.2 1B\n\n\n\n\nFull FT (En)\n0.52\n0.55\n0.49\n0.58\n\n\nFull FT (En-Hi)\n0.61\n0.63\n0.58\n0.63\n\n\nJoint FT (En + En-Hi)\n0.63\n0.65\n0.6\n0.65\n\n\nCPT→FT\n0.61\n0.63\n0.58\n0.63\n\n\nTV-Te​n→T_{en}\\rightarrowFT\n0.65\n0.68\n0.615\n0.669\n\n\nTIES-Te​n→T_{en}\\rightarrowFT\n0.62\n0.67\n0.6\n0.66\n\n\nTV-CPT→FT\n0.59\n0.64\n0.57\n0.64\n\n\nTIES-CPT→FT\n0.57\n0.63\n0.56\n0.63\n\n\nTV-Te​n+C​P​T→T_{en}+CPT\\rightarrowFT\n0.63\n0.68\n0.61\n0.67\n\n\nTIES-Te​n+C​P​T→T_{en}+CPT\\rightarrowFT\n0.6\n0.67\n0.6\n0.66",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">En-Ta</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">En-Ml</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">XLM-R</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">LLAMA 3.2 1B</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">XLM-R</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">LLAMA 3.2 1B</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Full FT (En)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E67C73;\">0.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E67C73;\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E67C73;\">0.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E67C73;\">0.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">Full FT (En-Hi)</span></th>\n<td class=\"ltx_td ltx_align_center\">0.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FBEAE8;\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCF1F0;\">0.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F9E0DE;\">0.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">Joint FT (En + En-Hi)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#ABDDC5;\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E8F6EF;\">0.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#BCE4D1;\">0.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#DEF2E8;\">0.65</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">CPT&#8594;FT</span></th>\n<td class=\"ltx_td ltx_align_center\">0.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FBEAE8;\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCF1F0;\">0.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F9E0DE;\">0.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">TV-<math alttext=\"T_{en}\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow></msub><mo stretchy=\"false\">&#8594;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">T_{en}\\rightarrow</annotation></semantics></math>FT</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#57BB8A;\">0.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#57BB8A;\">0.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#57BB8A;\">0.615</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#5EBE8F;\">0.669</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">TIES-<math alttext=\"T_{en}\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow></msub><mo stretchy=\"false\">&#8594;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">T_{en}\\rightarrow</annotation></semantics></math>FT</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#D5EEE2;\">0.62</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#87CFAC;\">0.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#BCE4D1;\">0.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#9BD7B9;\">0.66</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">TV-CPT&#8594;FT</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F9E1DF;\">0.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDF8F7;\">0.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F9E4E2;\">0.57</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDF4F4;\">0.64</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">TIES-CPT&#8594;FT</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F3C4C0;\">0.57</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FBEAE8;\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F7D7D5;\">0.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F9E0DE;\">0.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">TV-<math alttext=\"T_{en}+CPT\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m3\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>T</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow></msub><mo>+</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><mo stretchy=\"false\">&#8594;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">T_{en}+CPT\\rightarrow</annotation></semantics></math>FT</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#ABDDC5;\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#57BB8A;\">0.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#79C9A2;\">0.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#57BB8A;\">0.67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">TIES-<math alttext=\"T_{en}+CPT\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m4\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>T</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow></msub><mo>+</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><mo stretchy=\"false\">&#8594;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">T_{en}+CPT\\rightarrow</annotation></semantics></math>FT</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#FCF0EF;\">0.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#87CFAC;\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#BCE4D1;\">0.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#9BD7B9;\">0.66</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "continued",
            "enta",
            "strategies",
            "transfer",
            "ties",
            "tieste​nc​p​t→tencptrightarrowft",
            "enml",
            "llama",
            "codemixed",
            "consistently",
            "more",
            "finetuning",
            "joint",
            "tieste​n→tenrightarrowft",
            "tvcpt→ft",
            "combined",
            "en–ta",
            "en–ml",
            "english",
            "cpt",
            "lowresource",
            "resourceefficient",
            "analysis",
            "main",
            "tiescpt→ft",
            "tvte​n→tenrightarrowft",
            "reports",
            "results",
            "tvte​nc​p​t→tencptrightarrowft",
            "effectiveness",
            "enhi",
            "model",
            "match",
            "especially",
            "transferbased",
            "their",
            "such",
            "variants",
            "without",
            "takeaway",
            "scenarios",
            "cpt→ft",
            "parameterefficient",
            "performance",
            "being",
            "outperform",
            "showing",
            "approaches",
            "sentiment",
            "full",
            "xlmr",
            "models",
            "different",
            "method",
            "while",
            "pretraining",
            "data",
            "when",
            "baselines",
            "comparison",
            "bilingual"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We take the model fine-tuned on the combination of our three En-Hi datasets, using all the different training configurations, and infer on the test set of the En-Ta, En-Ml datasets. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S5.T4\" title=\"Table 4 &#8227; 5.2. Impact of Continued Pre-Training Corpus &#8227; 5. Results &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the transfer of various training strategies to other code-mixed language pairs for Sentiment Analysis task. We present transfer results for all the combinations of base models and training methods. We also include full fine-tuning on the English dataset (FullFT (En)) to ascertain which is better - transfer from monolingual En resources or code-mixed resources from other language pairs.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We study model merging as a practical alternative to conventional adaptation strategies for code-mixed NLP. Starting from a multilingual base model, we: (i) perform continued pre-training (CPT) on unlabeled code-mixed text to obtain an adapted checkpoint, (ii) merge checkpoint with the base model, and (iii) fine-tune&#160;(FT) on the downstream task data. We evaluate our approach for sentence classification (sentiment and hate speech) task in English-Hindi (En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our results show that merged models consistently outperform full fine-tuning and CPT<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>FT. We observe gains of 2&#8211;5 points in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> over full fine-tuning and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2 points over CPT<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>FT, indicating that unlabeled data is leveraged more effectively via merging than via CPT alone. Zero-/few-shot prompting with larger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged checkpoints, underscoring limits of in-context learning for code-mixed inputs. We further test cross-pair transfer by training on En&#8211;Hi and evaluating on En&#8211;Ta and En&#8211;Ml: merged checkpoints transfer more strongly than monolingual-English baselines (e.g., TV/TIES variants reaching 0.65&#8211;0.68 <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> vs. 0.61&#8211;0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more reliable substrate for low-resource pairs. We conclude with adaptation recipes matched to common data regimes (labeled only; labeled+unlabeled; transfer-only) and discuss limitations and scaling considerations for broader tasks and larger models.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "strategies",
                    "transfer",
                    "codemixed",
                    "consistently",
                    "more",
                    "finetuning",
                    "en–ta",
                    "en–ml",
                    "cpt",
                    "lowresource",
                    "results",
                    "enhi",
                    "model",
                    "variants",
                    "outperform",
                    "sentiment",
                    "full",
                    "xlmr",
                    "models",
                    "pretraining",
                    "data",
                    "baselines"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-mixed text&#8212;where two or more languages appear within a single utterance or speech event&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib15\" title=\"\">1977</a>)</cite>&#8212;poses persistent challenges for natural language processing (NLP) systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Do&#287;ru&#246;z et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib12\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib43\" title=\"\">2023</a>; Winata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib36\" title=\"\">2023</a>)</cite>. A common response has been to fine-tune large multilingual pre-trained models on task-specific datasets for code-mixed inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>; Aguilar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib2\" title=\"\">2020</a>)</cite>. While effective, this approach depends on the availability of high-quality annotations and on general-purpose multilingual models having seen sufficient code-mixed data during pre-training to capture the characteristics of code-mixed language.</p>\n\n",
                "matched_terms": [
                    "models",
                    "codemixed",
                    "while",
                    "more",
                    "pretraining",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A complementary strategy for adapting language models to new domains or linguistic settings is continued pre-training (CPT) on unlabeled data&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gururangan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib16\" title=\"\">2020</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib22\" title=\"\">2021</a>; Ruder, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib32\" title=\"\">2021</a>)</cite>. Model merging&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib41\" title=\"\">2024</a>)</cite> has emerged as a promising approach for adapting models to new domains, tasks, and languages, while mitigating several limitations of CPT. Although CPT can aid domain adaptation, it can also erode general language understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gogoulou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib14\" title=\"\">2024</a>; Alexandrov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib4\" title=\"\">2024</a>)</cite>. In contrast, recent work shows that model merging can improve task performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wortsman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib39\" title=\"\">2022</a>; Choshen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib8\" title=\"\">2022</a>; Izmailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib18\" title=\"\">2018</a>)</cite>, enhance robustness and out-of-domain generalization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wortsman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib39\" title=\"\">2022</a>; Jin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib19\" title=\"\">2023</a>)</cite>, and enable multitask models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yadav et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib40\" title=\"\">2023</a>; Ilharco et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib17\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "model",
                    "models",
                    "cpt",
                    "while",
                    "pretraining",
                    "data",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We argue that model merging is well-suited to code-mixed settings because it can incorporate code-mixed knowledge while preserving strong monolingual representations. Since code-mixed text contains monolingual spans from its constituent languages (L1, L2) and coexists with purely monolingual utterances, maintaining these capabilities is crucial for downstream performance. By comparison, CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT risks degrading monolingual processing during adaptation. Our hypothesis is that merging a base multilingual model with a checkpoint adapted on a modest code-mixed corpus can outperform traditional strategies by combining strong monolingual representations with newly acquired code-mixing capabilities, yielding a more robust solution for code-mixed text.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "model",
                    "codemixed",
                    "comparison",
                    "while",
                    "more",
                    "performance",
                    "outperform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model merging integrates task- or domain-specific models into a unified composite, aggregating knowledge from all merged sources. In resource-constrained code-mixed settings, leveraging monolingual or other code-mixed resources can further improve performance. While model merging offers a modular mechanism for combining diverse data sources, its effectiveness for code-mixed tasks remains underexplored and merits closer study.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "codemixed",
                    "while",
                    "data",
                    "performance",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond improving performance for a single code-mixed language pair, a natural question is whether gains from model merging transfer to other language pairs performing the same downstream task. In such cases, the task formulation remains fixed, but the language composition changes, introducing new patterns of mixing, grammar, and vocabulary overlap. These shifts create both challenges and opportunities: the model must adjust to different lexical and syntactic cues, yet shared structures or typological similarities may support better generalization. Understanding how language composition affects transfer is especially important in low-resource settings, where direct annotations for the target pair may be unavailable and cross-pair transfer becomes a viable adaptation strategy.</p>\n\n",
                "matched_terms": [
                    "model",
                    "transfer",
                    "especially",
                    "such",
                    "lowresource",
                    "different",
                    "codemixed",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given this context, we pose two primary research questions: <span class=\"ltx_text ltx_font_bold\">RQ 1:</span> Can model merging serve as an alternative to fine-tuning approaches for code-mixed tasks? <span class=\"ltx_text ltx_font_bold\">RQ 2:</span> Are model merging methods effective for integrating capabilities from multiple data sources that differ in language composition and training supervision?</p>\n\n",
                "matched_terms": [
                    "model",
                    "codemixed",
                    "data",
                    "finetuning",
                    "approaches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work examines model merging for code-mixed sentence classification, an approach not previously explored in code-mixed settings. We run experiments on English&#8211;Hindi and English&#8211;Spanish datasets using XLM-R, and Llama 3.2 1B to assess how major multilingual models respond to model merging. Concretely, we merge a continued pre-trained checkpoint with a base model and then fine-tune for code-mixed tasks. We also evaluate model merging under different data availability scenarios to test its ability to incorporate both monolingual and code-mixed resources.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "model",
                    "xlmr",
                    "models",
                    "llama",
                    "different",
                    "codemixed",
                    "scenarios",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce model merging as an adaptation strategy for code-mixed NLP tasks, distinct from conventional fine-tuning and continued pre-training, and observe improved downstream performance across language pairs, models, and tasks.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "model",
                    "performance",
                    "models",
                    "codemixed",
                    "pretraining",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For cross-lingual transfer, merged models trained on code-mixed resources outperform those trained on monolingual English data, with the advantage most pronounced for low-resource language pairs, indicating that code-mixed knowledge is a stronger basis for adaptation when target-pair data is scarce.</p>\n\n",
                "matched_terms": [
                    "transfer",
                    "models",
                    "english",
                    "lowresource",
                    "codemixed",
                    "data",
                    "when",
                    "outperform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Modern work on code-mixed NLP primarily adapts <span class=\"ltx_text ltx_font_italic\">multilingual</span> pretrained models (e.g., mBERT, XLM-R)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pires et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib27\" title=\"\">2019</a>; Muller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib24\" title=\"\">2020</a>; Tan and Joty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib35\" title=\"\">2021</a>; Srivastava and Singh, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib34\" title=\"\">2022</a>; Winata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib36\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib37\" title=\"\">2021</a>; Rani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib30\" title=\"\">2024</a>; Priyadharshini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib29\" title=\"\">2022</a>)</cite>. Such models are adapted via task-specific fine-tuning and their efficacy has been shown on benchmarks such as LinCE and GLUECoS, which span token-level and sentence-level tasks across pairs like En&#8211;Hi and En&#8211;Es&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib2\" title=\"\">2020</a>; Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>)</cite>. In parallel, continued pre-training (CPT) on unlabeled code-mixed corpora has emerged as a complementary strategy before downstream fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Das et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite>. While both routes are effective, they rely on labeled and/or unlabeled code-mixed resources that remain scarce or unevenly distributed across language pairs, and they typically optimize a <em class=\"ltx_emph ltx_font_italic\">single</em> model instance rather than composing multiple models.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "model",
                    "xlmr",
                    "models",
                    "cpt",
                    "such",
                    "codemixed",
                    "while",
                    "pretraining",
                    "finetuning",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A less-explored axis concerns leveraging <span class=\"ltx_text ltx_font_italic\">monolingual</span> resources for code-mixed tasks. Prior work has examined translating mixed inputs to constituent languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pant and Dadu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib25\" title=\"\">2020</a>)</cite>, using language-specific models, or building meta-embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Winata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib37\" title=\"\">2021</a>)</cite> and non-Transformer encoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar and Solorio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib3\" title=\"\">2020</a>)</cite>; these can yield gains in select settings but do not directly address the need to <em class=\"ltx_emph ltx_font_italic\">compose</em> capabilities across sources. In contrast, <em class=\"ltx_emph ltx_font_italic\">modular</em> adaptation by <em class=\"ltx_emph ltx_font_italic\">model merging</em>&#8212;combining parameters from models specialized for different data or tasks (e.g., Task Arithmetic and TIES variants)&#8212;offers a data- and compute-efficient alternative that can, in principle, integrate code-mixed competence with strong monolingual representations without access to original training data.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ties",
                    "models",
                    "different",
                    "without",
                    "codemixed",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model merging is a powerful technique that combines the parameters of different models, creating a composite model with enhanced functionality, all without the need for original training data or heavy computational resources. Methods like Task Arithmetic&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ilharco et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib17\" title=\"\">2023</a>)</cite> and its variants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yadav et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib40\" title=\"\">2023</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib42\" title=\"\">2024</a>)</cite> demonstrate the potential to merge models with diverse capacities to achieve a unified function. This concept can be extended to code-mixed tasks by combining task- or domain-specific modules. Exploring the use of model merging methods using monolingual or unlabeled resources, presents a promising avenue for advancing code-mixed NLP.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "different",
                    "variants",
                    "without",
                    "codemixed",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To our knowledge, <span class=\"ltx_text ltx_font_bold\">model merging has not been applied to code-mixed NLP</span> prior to this line of work. Framing code-mixed adaptation as a modular composition problem allows us to ask: when labeled code-mixed data is limited, can we merge a base multilingual model with a CPT-adapted checkpoint (or with monolingual task vectors) and then fine-tune for the target task? This perspective also clarifies cross-lingual transfer: merged checkpoints trained with code-mixed resources tend to transfer more reliably to new code-mixed pairs than those relying solely on monolingual English supervision, highlighting that code-mixed signals are especially valuable under low-resource target conditions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "transfer",
                    "especially",
                    "english",
                    "lowresource",
                    "codemixed",
                    "more",
                    "data",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Notations</span> Let <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> represent the target task. Let <math alttext=\"D^{T}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{L1-L2}</annotation></semantics></math> stand for a code-mixed dataset for task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> in the L1-L2 language pair (e.g., En-Hi), and &#160;(<math alttext=\"D^{T}_{L1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{L1}</annotation></semantics></math>) for a monolingual dataset in L1 for the same task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>. A language model can be adapted to code-mixed data by performing continued pre-training using an unlabeled code-mixed text corpus &#160;(<math alttext=\"D^{CM}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">D^{CM}_{L1-L2}</annotation></semantics></math>). We start with a base pre-trained model &#160;(<math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>), and when fine-tuned on the downstream task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>, we get <math alttext=\"\\theta^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m9\" intent=\":literal\"><semantics><msup><mi>&#952;</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">\\theta^{T}</annotation></semantics></math>. If <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m10\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> undergoes continued pre-training on a code-mixed dataset, it results in the &#160;<math alttext=\"\\theta^{CM}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m11\" intent=\":literal\"><semantics><msubsup><mi>&#952;</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\theta^{CM}_{L1-L2}</annotation></semantics></math> model, which is expected to better handle code-mixed text.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "enhi",
                    "model",
                    "codemixed",
                    "pretraining",
                    "data",
                    "when",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines</span>: To analyze various trade-offs and ensure a fair evaluation, we use full-model fine-tuning (FullFT) as our primary baseline.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "baselines"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adapting a language model to a new domain or task has been shown to improve performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gururangan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib16\" title=\"\">2020</a>)</cite>. We adapt a language model to code-mixed corpus, and such treatment can lead to improved downstream task performance. We use existing unlabeled code-mixed corpus for continued pre-training of base model using Masked Language Modeling objective for Encoder-only models, and Causal LM objective for Decoder-only models&#160;(refer details Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4\" title=\"4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> for more details).\nWe take the continued pre-training checkpoints and perform fine-tuning on the downstream task dataset. We refer this method as &#8220;CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT&#8221;. We should note that domain adaptation incurs additional computational cost, which is much higher than the fine-tuning.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "model",
                    "performance",
                    "models",
                    "such",
                    "codemixed",
                    "method",
                    "more",
                    "pretraining",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For model merging we consider two methods - a) Task Arithmetic &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ilharco et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib17\" title=\"\">2023</a>)</cite>; b) TIES (TrIm, Elect, Sign) &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yadav et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib40\" title=\"\">2023</a>)</cite>. Task Arithmetic defines a direction, or Task Vector, in a model&#8217;s weight space that enhances task performance when followed. These vectors, calculated by subtracting pre-trained model weights from fine-tuned ones, guide neural network behavior. Task vectors can be modified and combined using operations like addition or negation to adjust model behavior. TIES is a variation that addresses interference from redundant parameters and disagreements in parameter signs across models.</p>\n\n",
                "matched_terms": [
                    "combined",
                    "model",
                    "ties",
                    "models",
                    "when",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compute a task vector for continued pre-training of <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> on code-mixed text (<math alttext=\"\\tau_{CPT}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tau_{CPT}</annotation></semantics></math>) (Eq&#160;&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E1\" title=\"In 3.2. Model Merging &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). We add one Task Vector at a time and fine-tune task specific dataset (<math alttext=\"D^{T}_{enhi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{enhi}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "continued",
                    "codemixed",
                    "pretraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> denotes the fine-tuning of the model on a downstream task, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, scaling term is determined using held-out validation sets.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In TIES, the resulting model weights are not a simple weighted average of all parameters; instead, they are adjusted based on the magnitude and weight of the task vectors. For further discussion,\nwe refer to Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E1\" title=\"In 3.2. Model Merging &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> as &#8220;TV&#8221;.\nIn case of TIES they are referred to as &#8220;TIES&#8221;.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ties"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leverage monolingual English data in our adaptation methods and evaluate on the code-mixed task.\nTo compare cross-lingual/cross-dataset transfer we consider two baselines- a) Joint Training&#160;(JointFT): English and English-Hindi task datasets are combined and model is fine-tuned on the combined dataset; b) Sequential Training&#160;(SeqFT): we first train the model on English task dataset, followed by fine-tuning on the English-Hindi code-mixed dataset. Additionally, we look at the following:</p>\n\n",
                "matched_terms": [
                    "combined",
                    "model",
                    "transfer",
                    "baselines",
                    "english",
                    "codemixed",
                    "data",
                    "joint",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When using TIES, the resulting model weights are adjusted based on parameter magnitudes and signs rather than simple averaging.</p>\n\n",
                "matched_terms": [
                    "when",
                    "model",
                    "ties"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Models:</span> We use multilingual models, as their fine-tuning has consistently shown strong results <cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>; Das et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite>.\nFor our experiments, we use mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib11\" title=\"\">2019</a>)</cite> and XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib9\" title=\"\">2020</a>)</cite>, which are widely used in code-mixed studies, along with Llama 3.2 1B model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B</a></span></span></span>.\nDue to computational constraints, our model adaptation methods are restricted to Llama 1B; however, we conduct inference on larger models including Llama 3.2 3B, Llama 3.1 8B, and Llama 3.3 70B.</p>\n\n",
                "matched_terms": [
                    "model",
                    "their",
                    "models",
                    "xlmr",
                    "llama",
                    "codemixed",
                    "consistently",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span> Our study focuses on English-Hindi (En-Hi) and English-Spanish (En-Es) code-mixed sentiment classification tasks, as they are part of code-mixing benchmarks GLUECoS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>)</cite> and LinCE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib2\" title=\"\">2020</a>)</cite>. For En-Hi we consider two tasks - a) Sentiment Analysis using three datasets &#8212; GLUECoS, Sentimix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Patwa et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib26\" title=\"\">2020</a>)</cite>, and&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Prabhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib28\" title=\"\">2016</a>)</cite>&#8212;all featuring 3-class labels (positive, neutral, negative); b) Hate Speech Classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bohra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib5\" title=\"\">2018</a>)</cite>. For En-Es, we carry out experiments on Sentiment Analysis task, and use two datasets released by <cite class=\"ltx_cite ltx_citemacro_citet\">Patwa et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib26\" title=\"\">2020</a>)</cite> and GLUECoS benchmark. For transfer from monolingual English task dataset, we use English SST5 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Socher et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib33\" title=\"\">2013</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "sentiment",
                    "enhi",
                    "transfer",
                    "english",
                    "codemixed",
                    "analysis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate cross lingual to new language pairs, we consider three target datasets - a) English-Tamil&#160;(En-Ta)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chakravarthi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib7\" title=\"\">2020b</a>)</cite>; b) English-Malayalam&#160;(En-Ml)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chakravarthi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib6\" title=\"\">2020a</a>)</cite>; c) English-Spanish&#160;(En-Es) from GLUECoS benchmark. For unlabeled code-mixed corpus, we use the dataset released by &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite> for both English-Hindi and English-Spanish.</p>\n\n",
                "matched_terms": [
                    "enta",
                    "enml",
                    "codemixed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.T1\" title=\"Table 1 &#8227; Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides statistics of all the datasets we have used in our experiments, along with the label wise distributions in all splits. We include all the labeled, unlabeled datasets in the table for ready reference. SST5 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Socher et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib33\" title=\"\">2013</a>)</cite> has 5 classes, and to use in our experiments we convert its 5-class labels into 3 classes by merging &#8220;very positive&#8221; with &#8220;positive&#8221; and &#8220;very negative&#8221; with &#8220;negative&#8221;. Wherever all three splits (train-test-valid) are available we use the same, and in case they are not available we create 70-20-10 split from the available samples. We use F1 as the measure of performance across all the data and training configurations. F1 is suitable as there is some dataset imbalances across code-mixed datasets.</p>\n\n",
                "matched_terms": [
                    "codemixed",
                    "performance",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompts:</span> We used the following prompts for our zero and few shot experiments. For few-shot prompt, we include the examples and the corresponding labels in between the instruction and the input sample. For the k-shot settings, we carry out five runs, each run with different set of randomly sampled k examples from the train dataset, while ensuring that all the labels are well represented in the examples.</p>\n\n",
                "matched_terms": [
                    "different",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero shot prompt used for sentiment analysis is as follows:</p>\n\n",
                "matched_terms": [
                    "sentiment",
                    "analysis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hardware &amp; Hyperparameters:</span> We conduct all experiments on a combination of Nvidia 1080 Ti 12 GB GPUs, Nvidia A6000 32 GB GPU, using a single GPU. We use Nvidia A6000 for Llama experiemnts, and 1080Ti for all other experiments. Batch sizes range from 8 to 128, depending on the dataset and approach. Learning rates were selected after a search between [1e-2, 5e-5], with 1e-5 working best for full model training, 1e-4 for LoRA layers. All experiments ran for with early stopping&#160;(stop if model performance doesn&#8217;t change by 0.5 F1 point across 3 evaluations), maximum up to 20 epochs. We use AdamW optimizer with default optimizer parameters for all our experiments. We implemented the methods using PyTorch, Huggingface Transformers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wolf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib38\" title=\"\">2020</a>)</cite>, PEFT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mangrulkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib23\" title=\"\">2022</a>)</cite>, and mergekit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goddard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib13\" title=\"\">2024</a>)</cite> for model merging methods. Upon publication code, data, models, and detailed hyperparameters configurations will be publicly released for reproducibility.</p>\n\n",
                "matched_terms": [
                    "model",
                    "full",
                    "models",
                    "llama",
                    "data",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> highlight the comparative performance of fine-tuning, CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT, model merging methods across multiple sentiment analysis and hate speech detection tasks, in English-Hindi&#160;(En-Hi) and English-Spanish&#160;(En-Es) Datasets.</p>\n\n",
                "matched_terms": [
                    "sentiment",
                    "enhi",
                    "model",
                    "performance",
                    "analysis",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let Base Model&#160;(<math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>) denote the base pretrained model. <em class=\"ltx_emph ltx_font_italic\">Full FT</em> fine-tunes Base Model directly on the target code-mixed dataset. <em class=\"ltx_emph ltx_font_italic\">CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT</em> first performs CPT on code-mixed text to obtain <math alttext=\"\\theta_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mtext>CPT</mtext></msub><annotation encoding=\"application/x-tex\">\\theta_{\\text{CPT}}</annotation></semantics></math> and then fine-tunes on the target task. <em class=\"ltx_emph ltx_font_italic\">Seq FT</em> is sequential fine-tuning&#8212;fine-tune on English data, then fine-tune on the code-mixed target. <em class=\"ltx_emph ltx_font_italic\">Joint FT</em> jointly fine-tunes on the union of English and code-mixed labeled data. The remaining blocks use weight merging before the final task fine-tuning. In the <em class=\"ltx_emph ltx_font_italic\">Task-Vector (TV)</em> setting, the merge operator <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> is (scaled) vector addition, <math alttext=\"\\theta\\oplus\\lambda\\tau:=\\theta+\\lambda\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>&#952;</mi><mo>&#8853;</mo><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#964;</mi></mrow></mrow><mo>:=</mo><mrow><mi>&#952;</mi><mo>+</mo><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#964;</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\theta\\oplus\\lambda\\tau:=\\theta+\\lambda\\tau</annotation></semantics></math>, with <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> (or <math alttext=\"\\lambda_{1},\\lambda_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#955;</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda_{1},\\lambda_{2}</annotation></semantics></math> when combining two vectors) tuned on validation data: <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}</annotation></semantics></math></em> adds the CPT vector; <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> adds the English vector; <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><mrow><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><mo rspace=\"0.392em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> adds both. In the <em class=\"ltx_emph ltx_font_italic\">TIES</em> setting, <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> denotes TIES merging (TrIm&#8211;Elect&#8211;Sign), which resolves interference via magnitude/sign rules rather than simple addition; the rows <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}</annotation></semantics></math></em>, <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em>, and <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m14\" intent=\":literal\"><semantics><mrow><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><mo rspace=\"0.392em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> mirror the TV configurations. In all TV/TIES cases, the merged model is subsequently fine-tuned on the target code-mixed dataset, and reported scores correspond to this final model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ties",
                    "full",
                    "english",
                    "cpt",
                    "when",
                    "joint",
                    "codemixed",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate that model merging methods (TV<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT, TIES<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>CPT in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) consistently outperform standard fine-tuning (Full FT) across all models and datasets, yielding +2&#8211;5 F1 over Full FT and +1&#8211;2 F1 over CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT. This confirms that unlabeled data can be exploited more effectively through model merging than through continued pre-training alone, supporting our hypothesis in <span class=\"ltx_text ltx_font_bold\">RQ1</span>. Between merging strategies, Task Vectors (TV) generally perform better than TIES, though the relative advantage varies by model and task. We also find that the choice of corpus for CPT has only a marginal effect on downstream performance.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "strategies",
                    "model",
                    "ties",
                    "full",
                    "performance",
                    "models",
                    "cpt",
                    "outperform",
                    "consistently",
                    "more",
                    "pretraining",
                    "data",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, baseline fine-tuning significantly outperforms zero-shot and few-shot prompting with LLaMA 3.3 70B, highlighting the limitations of in-context learning for code-mixed tasks. Few-shot prompting improves over zero-shot, but gains plateau at higher k-shot levels, suggesting limited scalability. These findings reinforce the necessity of fine-tuning and merging strategies for robust performance in resource-constrained settings. Finally, as we explore in <span class=\"ltx_text ltx_font_bold\">RQ2</span>, model merging is particularly valuable for integrating heterogeneous resources and enabling cross-lingual transfer, often surpassing sequential or joint fine-tuning.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "model",
                    "transfer",
                    "performance",
                    "llama",
                    "joint",
                    "codemixed",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis varies across three key dimensions: (a) use of only labeled data, (b) use of both labeled and unlabeled data, and (c) type of model augmentation techniques. We combine code-mixed data with monolingual task-specific data. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents a comparison of model performance across different scenarios of data availability. These settings reflect practical conditions in real-world code-mixed applications.</p>\n\n",
                "matched_terms": [
                    "model",
                    "different",
                    "codemixed",
                    "analysis",
                    "scenarios",
                    "data",
                    "performance",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Combining En and En-Hi labeled datasets for finetuning</span> Incorporating English-labeled data alongside En-Hi datasets&#160;(Seq FT and Joint FT rows in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) yields no significant improvement over fine-tuning (Full FT) solely on En-Hi data. Both SeqFT (English first, then English-Hindi) and JointFT (simultaneously on combined datasets) fail to enhance performance for code-mixed tasks (See Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.)</p>\n\n",
                "matched_terms": [
                    "combined",
                    "enhi",
                    "full",
                    "performance",
                    "english",
                    "codemixed",
                    "data",
                    "joint",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Leveraging monolingual resources with code-mixed resources</span>\nWe combine three elements here: the base model, a vector fine-tuned on English data, and a vector for continued pre-training on code-mixed data (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E3\" title=\"In Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Although we expected the combination of monolingual and code-mixed resources to perform better than just monolingual or just code-mixed, they perform similarly to CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT method, or outperform it by 1 or 2 F1 points (which can be seen in the <span class=\"ltx_text ltx_font_italic\">TV-T_en + CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT </span>and<span class=\"ltx_text ltx_font_italic\"> TIES-T_en + CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT</span> rows of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Extending to a three-way merge does not provide additional gains over a single CPT merge. Overall, compared to FullFT, the combined monolingual and code-mixed approach either matches performance or underperforms slightly (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2 F1 points).</p>\n\n",
                "matched_terms": [
                    "continued",
                    "combined",
                    "model",
                    "english",
                    "cpt",
                    "codemixed",
                    "method",
                    "pretraining",
                    "data",
                    "performance",
                    "outperform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Varying the corpus used for pre-training might have some impact on the downstream task. To understand such variance, we do continued pre-training using two different unlabeled corpora, and analyze the impact on the downstream tasks.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "such",
                    "pretraining",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the dataset released by &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite> for continued pre-training, along with a synthetic code-mixed corpus generated via the GCM Toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rizvi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib31\" title=\"\">2021</a>)</cite> and filtered using an acceptability filter proposed by &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kodali et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib21\" title=\"\">2024</a>)</cite>. The sizes of the two datasets are kept consistent. As code-mixed datasets often come from social media, we expect significant distributional differences between the task-specific and synthetic datasets. Data sources for continued pre-training can differ, leading to differing downstream task performance.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "codemixed",
                    "pretraining",
                    "data",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S5.T3\" title=\"Table 3 &#8227; 5.2. Impact of Continued Pre-Training Corpus &#8227; 5. Results &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> compares the downstream task performance when we vary the corpus used for continued pre-training. Performance on downstream tasks is only marginally affected by the dataset chosen for continued pre-training. Classification results vary by roughly 1 F1 point across different sources of unlabeled data and frequently meet or exceed the results of the fine-tuning exclusively on the downstream dataset.</p>\n\n",
                "matched_terms": [
                    "continued",
                    "performance",
                    "different",
                    "when",
                    "pretraining",
                    "data",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The relationship between task performance and in-context learning (ICL) for code-mixed tasks remains underexplored. To better understand how model size and prompting methods influence performance, we conduct zero-shot and few-shot prompting experiments using different-sized models from the LLaMA family&#8212;1B-instruct, 3B-instruct, 8B-instruct, and 70B-instruct. For few-shot prompting, we evaluate performance with 5, 10, 15, and 20 examples. Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> contains results of our experiments on LLama 3.2 70B model. We see that even baseline fine-tuning methods perform better than zero/few shot prompting, suggesting that in-context learning performs weakly for codemixing tasks. The highest F1 scores can be seen for the <cite class=\"ltx_cite ltx_citemacro_citet\">Prabhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib28\" title=\"\">2016</a>)</cite> datasete suggesting that the performance may be dependent on the data setting. Upon further analysis, we we hypothesize that this is due to the lack of noise in the dataset compared to the other datasets we use for comparison.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance",
                    "models",
                    "llama",
                    "codemixed",
                    "analysis",
                    "comparison",
                    "data",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate both XLM-R (encoder) and LLaMA 3.2 1B (decoder) to study transfer properties. Across En-Ta and En-Ml, we observe a consistent pattern: checkpoints obtained through model merging outperform Full FT baselines by 5&#8211;13 F1 points. This improvement holds across models and target language pairs, highlighting that merged checkpoints capture complementary signals from diverse resources. These findings reinforce our answer to <span class=\"ltx_text ltx_font_bold\">RQ2</span>&#8212;that model merging is an effective strategy for integrating heterogeneous data sources, often surpassing sequential or joint fine-tuning. However, the gains remain dependent on model architecture, the specific code-mixed task, and the language pair being targeted, underscoring the importance of careful merging design.</p>\n\n",
                "matched_terms": [
                    "enta",
                    "model",
                    "transfer",
                    "being",
                    "full",
                    "enml",
                    "xlmr",
                    "models",
                    "llama",
                    "joint",
                    "codemixed",
                    "data",
                    "finetuning",
                    "baselines",
                    "outperform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Model merging methods can be used to integrate capabilities from multiple data sources with varying language composition and supervision, often outperforming sequential fine-tuning and joint fine-tuning <span class=\"ltx_text ltx_font_bold\">(RQ 2)</span>. However, the impact of model merging is highly dependent on the model, task, and target language pair; underscoring the need for careful merging strategies.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "model",
                    "joint",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically show the advantages of using model merging as a strategy for code-mixed tasks. We also introduce an innovative perspective on utilizing existing data resources for code-mixed tasks, diverging from traditional domain-adaptation and fine-tuning techniques. In circumstances where datasets are lacking or are available in different languages, our findings can assist researchers in choosing an appropriate method to optimize the available resources.</p>\n\n",
                "matched_terms": [
                    "model",
                    "different",
                    "codemixed",
                    "method",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With respect to various data resource availability scenarios, following are the key recommendations of this study:</p>\n\n",
                "matched_terms": [
                    "data",
                    "scenarios"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">When both labeled and unlabeled data are available</span>, the most effective strategy is to apply model merging method, merging CPT model with the base model, followed by fine-tuning on the target dataset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cpt",
                    "when",
                    "method",
                    "data",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">For smaller labeled datasets</span>, model merging is preferable to simple fine-tuning due to its superior sample efficiency.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">If only labeled data is available</span>, full model fine-tuning remains the best option.</p>\n\n",
                "matched_terms": [
                    "data",
                    "finetuning",
                    "model",
                    "full"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In the absence of both labeled and unlabeled data</span>, the most effective approach is to utilize models trained on similar tasks with other code-mixed language resources. These models consistently outperform those transferred from monolingual English resources.</p>\n\n",
                "matched_terms": [
                    "models",
                    "english",
                    "codemixed",
                    "consistently",
                    "data",
                    "outperform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although our study contains models of different architecture, we do not scale it to larger language models (&#191;1B parameter models), because of prohibitive cost of training/fine-tuning a larger model. Analyzing how model merging behaves as model size increase for code-mixed tasks would be an interesting avenue for future work.</p>\n\n",
                "matched_terms": [
                    "models",
                    "codemixed",
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For model merging methods, there is a small additional computation cost, where the weights of the models are being merged. This could be a limitation while trying to find the optimal hyperparameters. For example, the <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p6.m1\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> values in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E1\" title=\"In 3.2. Model Merging &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E2\" title=\"In Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E3\" title=\"In Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "being",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another limitation lies in the data settings we consider. In our paper, we explore data settings based on the availability of both labeled and unlabeled data in a code-mixed scenario. However, we do not consider the case where unlabeled data is available, but no task-specific labeled data is present. This limitation is illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S6.T5\" title=\"Table 5 &#8227; 6. Discussion &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "codemixed",
                    "data"
                ]
            }
        ]
    },
    "S6.T5": {
        "source_file": "Adapting Multilingual Models to Code-Mixed Tasks via Model Merging",
        "caption": "Table 5. Our experiments provide a comprehensive analysis of model adaptation strategies under different resource constraints, covering all cases except the scenario where unlabeled data is available, but task-specific labeled data is absent (denoted by empty cell in the table).",
        "body": "Cross-Lingual\n\n\n Transfer",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Cross-Lingual</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">&#160;Transfer</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "cases",
            "strategies",
            "transfer",
            "available",
            "constraints",
            "scenario",
            "denoted",
            "our",
            "adaptation",
            "all",
            "where",
            "analysis",
            "resource",
            "unlabeled",
            "crosslingual",
            "absent",
            "provide",
            "cell",
            "model",
            "except",
            "covering",
            "taskspecific",
            "under",
            "experiments",
            "different",
            "empty",
            "labeled",
            "data",
            "comprehensive"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Another limitation lies in the data settings we consider. In our paper, we explore data settings based on the availability of both labeled and unlabeled data in a code-mixed scenario. However, we do not consider the case where unlabeled data is available, but no task-specific labeled data is present. This limitation is illustrated in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S6.T5\" title=\"Table 5 &#8227; 6. Discussion &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We study model merging as a practical alternative to conventional adaptation strategies for code-mixed NLP. Starting from a multilingual base model, we: (i) perform continued pre-training (CPT) on unlabeled code-mixed text to obtain an adapted checkpoint, (ii) merge checkpoint with the base model, and (iii) fine-tune&#160;(FT) on the downstream task data. We evaluate our approach for sentence classification (sentiment and hate speech) task in English-Hindi (En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our results show that merged models consistently outperform full fine-tuning and CPT<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>FT. We observe gains of 2&#8211;5 points in <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> over full fine-tuning and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2 points over CPT<math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math>FT, indicating that unlabeled data is leveraged more effectively via merging than via CPT alone. Zero-/few-shot prompting with larger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged checkpoints, underscoring limits of in-context learning for code-mixed inputs. We further test cross-pair transfer by training on En&#8211;Hi and evaluating on En&#8211;Ta and En&#8211;Ml: merged checkpoints transfer more strongly than monolingual-English baselines (e.g., TV/TIES variants reaching 0.65&#8211;0.68 <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"m5\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">F_{1}</annotation></semantics></math> vs. 0.61&#8211;0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more reliable substrate for low-resource pairs. We conclude with adaptation recipes matched to common data regimes (labeled only; labeled+unlabeled; transfer-only) and discuss limitations and scaling considerations for broader tasks and larger models.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "adaptation",
                    "transfer",
                    "model",
                    "unlabeled",
                    "labeled",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Code-mixed text&#8212;where two or more languages appear within a single utterance or speech event&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib15\" title=\"\">1977</a>)</cite>&#8212;poses persistent challenges for natural language processing (NLP) systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Do&#287;ru&#246;z et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib12\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib43\" title=\"\">2023</a>; Winata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib36\" title=\"\">2023</a>)</cite>. A common response has been to fine-tune large multilingual pre-trained models on task-specific datasets for code-mixed inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>; Aguilar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib2\" title=\"\">2020</a>)</cite>. While effective, this approach depends on the availability of high-quality annotations and on general-purpose multilingual models having seen sufficient code-mixed data during pre-training to capture the characteristics of code-mixed language.</p>\n\n",
                "matched_terms": [
                    "taskspecific",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A complementary strategy for adapting language models to new domains or linguistic settings is continued pre-training (CPT) on unlabeled data&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gururangan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib16\" title=\"\">2020</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib22\" title=\"\">2021</a>; Ruder, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib32\" title=\"\">2021</a>)</cite>. Model merging&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib41\" title=\"\">2024</a>)</cite> has emerged as a promising approach for adapting models to new domains, tasks, and languages, while mitigating several limitations of CPT. Although CPT can aid domain adaptation, it can also erode general language understanding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gogoulou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib14\" title=\"\">2024</a>; Alexandrov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib4\" title=\"\">2024</a>)</cite>. In contrast, recent work shows that model merging can improve task performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wortsman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib39\" title=\"\">2022</a>; Choshen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib8\" title=\"\">2022</a>; Izmailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib18\" title=\"\">2018</a>)</cite>, enhance robustness and out-of-domain generalization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wortsman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib39\" title=\"\">2022</a>; Jin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib19\" title=\"\">2023</a>)</cite>, and enable multitask models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yadav et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib40\" title=\"\">2023</a>; Ilharco et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib17\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "unlabeled",
                    "adaptation",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We argue that model merging is well-suited to code-mixed settings because it can incorporate code-mixed knowledge while preserving strong monolingual representations. Since code-mixed text contains monolingual spans from its constituent languages (L1, L2) and coexists with purely monolingual utterances, maintaining these capabilities is crucial for downstream performance. By comparison, CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT risks degrading monolingual processing during adaptation. Our hypothesis is that merging a base multilingual model with a checkpoint adapted on a modest code-mixed corpus can outperform traditional strategies by combining strong monolingual representations with newly acquired code-mixing capabilities, yielding a more robust solution for code-mixed text.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "model",
                    "adaptation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model merging integrates task- or domain-specific models into a unified composite, aggregating knowledge from all merged sources. In resource-constrained code-mixed settings, leveraging monolingual or other code-mixed resources can further improve performance. While model merging offers a modular mechanism for combining diverse data sources, its effectiveness for code-mixed tasks remains underexplored and merits closer study.</p>\n\n",
                "matched_terms": [
                    "data",
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond improving performance for a single code-mixed language pair, a natural question is whether gains from model merging transfer to other language pairs performing the same downstream task. In such cases, the task formulation remains fixed, but the language composition changes, introducing new patterns of mixing, grammar, and vocabulary overlap. These shifts create both challenges and opportunities: the model must adjust to different lexical and syntactic cues, yet shared structures or typological similarities may support better generalization. Understanding how language composition affects transfer is especially important in low-resource settings, where direct annotations for the target pair may be unavailable and cross-pair transfer becomes a viable adaptation strategy.</p>\n\n",
                "matched_terms": [
                    "cases",
                    "model",
                    "adaptation",
                    "transfer",
                    "different",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given this context, we pose two primary research questions: <span class=\"ltx_text ltx_font_bold\">RQ 1:</span> Can model merging serve as an alternative to fine-tuning approaches for code-mixed tasks? <span class=\"ltx_text ltx_font_bold\">RQ 2:</span> Are model merging methods effective for integrating capabilities from multiple data sources that differ in language composition and training supervision?</p>\n\n",
                "matched_terms": [
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work examines model merging for code-mixed sentence classification, an approach not previously explored in code-mixed settings. We run experiments on English&#8211;Hindi and English&#8211;Spanish datasets using XLM-R, and Llama 3.2 1B to assess how major multilingual models respond to model merging. Concretely, we merge a continued pre-trained checkpoint with a base model and then fine-tune for code-mixed tasks. We also evaluate model merging under different data availability scenarios to test its ability to incorporate both monolingual and code-mixed resources.</p>\n\n",
                "matched_terms": [
                    "model",
                    "under",
                    "experiments",
                    "different",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce model merging as an adaptation strategy for code-mixed NLP tasks, distinct from conventional fine-tuning and continued pre-training, and observe improved downstream performance across language pairs, models, and tasks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "adaptation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For cross-lingual transfer, merged models trained on code-mixed resources outperform those trained on monolingual English data, with the advantage most pronounced for low-resource language pairs, indicating that code-mixed knowledge is a stronger basis for adaptation when target-pair data is scarce.</p>\n\n",
                "matched_terms": [
                    "data",
                    "crosslingual",
                    "adaptation",
                    "transfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Modern work on code-mixed NLP primarily adapts <span class=\"ltx_text ltx_font_italic\">multilingual</span> pretrained models (e.g., mBERT, XLM-R)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pires et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib27\" title=\"\">2019</a>; Muller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib24\" title=\"\">2020</a>; Tan and Joty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib35\" title=\"\">2021</a>; Srivastava and Singh, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib34\" title=\"\">2022</a>; Winata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib36\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib37\" title=\"\">2021</a>; Rani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib30\" title=\"\">2024</a>; Priyadharshini et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib29\" title=\"\">2022</a>)</cite>. Such models are adapted via task-specific fine-tuning and their efficacy has been shown on benchmarks such as LinCE and GLUECoS, which span token-level and sentence-level tasks across pairs like En&#8211;Hi and En&#8211;Es&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib2\" title=\"\">2020</a>; Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>)</cite>. In parallel, continued pre-training (CPT) on unlabeled code-mixed corpora has emerged as a complementary strategy before downstream fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Das et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite>. While both routes are effective, they rely on labeled and/or unlabeled code-mixed resources that remain scarce or unevenly distributed across language pairs, and they typically optimize a <em class=\"ltx_emph ltx_font_italic\">single</em> model instance rather than composing multiple models.</p>\n\n",
                "matched_terms": [
                    "taskspecific",
                    "unlabeled",
                    "model",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A less-explored axis concerns leveraging <span class=\"ltx_text ltx_font_italic\">monolingual</span> resources for code-mixed tasks. Prior work has examined translating mixed inputs to constituent languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pant and Dadu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib25\" title=\"\">2020</a>)</cite>, using language-specific models, or building meta-embeddings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Winata et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib37\" title=\"\">2021</a>)</cite> and non-Transformer encoders&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar and Solorio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib3\" title=\"\">2020</a>)</cite>; these can yield gains in select settings but do not directly address the need to <em class=\"ltx_emph ltx_font_italic\">compose</em> capabilities across sources. In contrast, <em class=\"ltx_emph ltx_font_italic\">modular</em> adaptation by <em class=\"ltx_emph ltx_font_italic\">model merging</em>&#8212;combining parameters from models specialized for different data or tasks (e.g., Task Arithmetic and TIES variants)&#8212;offers a data- and compute-efficient alternative that can, in principle, integrate code-mixed competence with strong monolingual representations without access to original training data.</p>\n\n",
                "matched_terms": [
                    "different",
                    "data",
                    "model",
                    "adaptation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model merging is a powerful technique that combines the parameters of different models, creating a composite model with enhanced functionality, all without the need for original training data or heavy computational resources. Methods like Task Arithmetic&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ilharco et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib17\" title=\"\">2023</a>)</cite> and its variants&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yadav et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib40\" title=\"\">2023</a>; Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib42\" title=\"\">2024</a>)</cite> demonstrate the potential to merge models with diverse capacities to achieve a unified function. This concept can be extended to code-mixed tasks by combining task- or domain-specific modules. Exploring the use of model merging methods using monolingual or unlabeled resources, presents a promising avenue for advancing code-mixed NLP.</p>\n\n",
                "matched_terms": [
                    "model",
                    "all",
                    "different",
                    "unlabeled",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To our knowledge, <span class=\"ltx_text ltx_font_bold\">model merging has not been applied to code-mixed NLP</span> prior to this line of work. Framing code-mixed adaptation as a modular composition problem allows us to ask: when labeled code-mixed data is limited, can we merge a base multilingual model with a CPT-adapted checkpoint (or with monolingual task vectors) and then fine-tune for the target task? This perspective also clarifies cross-lingual transfer: merged checkpoints trained with code-mixed resources tend to transfer more reliably to new code-mixed pairs than those relying solely on monolingual English supervision, highlighting that code-mixed signals are especially valuable under low-resource target conditions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "adaptation",
                    "transfer",
                    "under",
                    "labeled",
                    "data",
                    "crosslingual",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Notations</span> Let <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> represent the target task. Let <math alttext=\"D^{T}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{L1-L2}</annotation></semantics></math> stand for a code-mixed dataset for task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> in the L1-L2 language pair (e.g., En-Hi), and &#160;(<math alttext=\"D^{T}_{L1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">D^{T}_{L1}</annotation></semantics></math>) for a monolingual dataset in L1 for the same task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>. A language model can be adapted to code-mixed data by performing continued pre-training using an unlabeled code-mixed text corpus &#160;(<math alttext=\"D^{CM}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>D</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">D^{CM}_{L1-L2}</annotation></semantics></math>). We start with a base pre-trained model &#160;(<math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>), and when fine-tuned on the downstream task <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>, we get <math alttext=\"\\theta^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m9\" intent=\":literal\"><semantics><msup><mi>&#952;</mi><mi>T</mi></msup><annotation encoding=\"application/x-tex\">\\theta^{T}</annotation></semantics></math>. If <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m10\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> undergoes continued pre-training on a code-mixed dataset, it results in the &#160;<math alttext=\"\\theta^{CM}_{L1-L2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m11\" intent=\":literal\"><semantics><msubsup><mi>&#952;</mi><mrow><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>1</mn></mrow><mo>&#8722;</mo><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn></mrow></mrow><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\theta^{CM}_{L1-L2}</annotation></semantics></math> model, which is expected to better handle code-mixed text.</p>\n\n",
                "matched_terms": [
                    "data",
                    "unlabeled",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adapting a language model to a new domain or task has been shown to improve performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gururangan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib16\" title=\"\">2020</a>)</cite>. We adapt a language model to code-mixed corpus, and such treatment can lead to improved downstream task performance. We use existing unlabeled code-mixed corpus for continued pre-training of base model using Masked Language Modeling objective for Encoder-only models, and Causal LM objective for Decoder-only models&#160;(refer details Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4\" title=\"4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> for more details).\nWe take the continued pre-training checkpoints and perform fine-tuning on the downstream task dataset. We refer this method as &#8220;CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT&#8221;. We should note that domain adaptation incurs additional computational cost, which is much higher than the fine-tuning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "unlabeled",
                    "adaptation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> denotes the fine-tuning of the model on a downstream task, and <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math>, scaling term is determined using held-out validation sets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In TIES, the resulting model weights are not a simple weighted average of all parameters; instead, they are adjusted based on the magnitude and weight of the task vectors. For further discussion,\nwe refer to Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E1\" title=\"In 3.2. Model Merging &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> as &#8220;TV&#8221;.\nIn case of TIES they are referred to as &#8220;TIES&#8221;.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leverage monolingual English data in our adaptation methods and evaluate on the code-mixed task.\nTo compare cross-lingual/cross-dataset transfer we consider two baselines- a) Joint Training&#160;(JointFT): English and English-Hindi task datasets are combined and model is fine-tuned on the combined dataset; b) Sequential Training&#160;(SeqFT): we first train the model on English task dataset, followed by fine-tuning on the English-Hindi code-mixed dataset. Additionally, we look at the following:</p>\n\n",
                "matched_terms": [
                    "model",
                    "adaptation",
                    "transfer",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Models:</span> We use multilingual models, as their fine-tuning has consistently shown strong results <cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>; Das et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite>.\nFor our experiments, we use mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib11\" title=\"\">2019</a>)</cite> and XLM-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib9\" title=\"\">2020</a>)</cite>, which are widely used in code-mixed studies, along with Llama 3.2 1B model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B\" title=\"\">https://huggingface.co/meta-llama/Llama-3.2-1B</a></span></span></span>.\nDue to computational constraints, our model adaptation methods are restricted to Llama 1B; however, we conduct inference on larger models including Llama 3.2 3B, Llama 3.1 8B, and Llama 3.3 70B.</p>\n\n",
                "matched_terms": [
                    "model",
                    "adaptation",
                    "experiments",
                    "constraints",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span> Our study focuses on English-Hindi (En-Hi) and English-Spanish (En-Es) code-mixed sentiment classification tasks, as they are part of code-mixing benchmarks GLUECoS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Khanuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib20\" title=\"\">2020</a>)</cite> and LinCE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Aguilar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib2\" title=\"\">2020</a>)</cite>. For En-Hi we consider two tasks - a) Sentiment Analysis using three datasets &#8212; GLUECoS, Sentimix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Patwa et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib26\" title=\"\">2020</a>)</cite>, and&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Prabhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib28\" title=\"\">2016</a>)</cite>&#8212;all featuring 3-class labels (positive, neutral, negative); b) Hate Speech Classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bohra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib5\" title=\"\">2018</a>)</cite>. For En-Es, we carry out experiments on Sentiment Analysis task, and use two datasets released by <cite class=\"ltx_cite ltx_citemacro_citet\">Patwa et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib26\" title=\"\">2020</a>)</cite> and GLUECoS benchmark. For transfer from monolingual English task dataset, we use English SST5 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Socher et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib33\" title=\"\">2013</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "experiments",
                    "transfer",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets:</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.T1\" title=\"Table 1 &#8227; Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides statistics of all the datasets we have used in our experiments, along with the label wise distributions in all splits. We include all the labeled, unlabeled datasets in the table for ready reference. SST5 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Socher et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib33\" title=\"\">2013</a>)</cite> has 5 classes, and to use in our experiments we convert its 5-class labels into 3 classes by merging &#8220;very positive&#8221; with &#8220;positive&#8221; and &#8220;very negative&#8221; with &#8220;negative&#8221;. Wherever all three splits (train-test-valid) are available we use the same, and in case they are not available we create 70-20-10 split from the available samples. We use F1 as the measure of performance across all the data and training configurations. F1 is suitable as there is some dataset imbalances across code-mixed datasets.</p>\n\n",
                "matched_terms": [
                    "all",
                    "experiments",
                    "available",
                    "unlabeled",
                    "labeled",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompts:</span> We used the following prompts for our zero and few shot experiments. For few-shot prompt, we include the examples and the corresponding labels in between the instruction and the input sample. For the k-shot settings, we carry out five runs, each run with different set of randomly sampled k examples from the train dataset, while ensuring that all the labels are well represented in the examples.</p>\n\n",
                "matched_terms": [
                    "all",
                    "experiments",
                    "different",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hardware &amp; Hyperparameters:</span> We conduct all experiments on a combination of Nvidia 1080 Ti 12 GB GPUs, Nvidia A6000 32 GB GPU, using a single GPU. We use Nvidia A6000 for Llama experiemnts, and 1080Ti for all other experiments. Batch sizes range from 8 to 128, depending on the dataset and approach. Learning rates were selected after a search between [1e-2, 5e-5], with 1e-5 working best for full model training, 1e-4 for LoRA layers. All experiments ran for with early stopping&#160;(stop if model performance doesn&#8217;t change by 0.5 F1 point across 3 evaluations), maximum up to 20 epochs. We use AdamW optimizer with default optimizer parameters for all our experiments. We implemented the methods using PyTorch, Huggingface Transformers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wolf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib38\" title=\"\">2020</a>)</cite>, PEFT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mangrulkar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib23\" title=\"\">2022</a>)</cite>, and mergekit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goddard et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib13\" title=\"\">2024</a>)</cite> for model merging methods. Upon publication code, data, models, and detailed hyperparameters configurations will be publicly released for reproducibility.</p>\n\n",
                "matched_terms": [
                    "model",
                    "all",
                    "experiments",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> highlight the comparative performance of fine-tuning, CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT, model merging methods across multiple sentiment analysis and hate speech detection tasks, in English-Hindi&#160;(En-Hi) and English-Spanish&#160;(En-Es) Datasets.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let Base Model&#160;(<math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math>) denote the base pretrained model. <em class=\"ltx_emph ltx_font_italic\">Full FT</em> fine-tunes Base Model directly on the target code-mixed dataset. <em class=\"ltx_emph ltx_font_italic\">CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT</em> first performs CPT on code-mixed text to obtain <math alttext=\"\\theta_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mtext>CPT</mtext></msub><annotation encoding=\"application/x-tex\">\\theta_{\\text{CPT}}</annotation></semantics></math> and then fine-tunes on the target task. <em class=\"ltx_emph ltx_font_italic\">Seq FT</em> is sequential fine-tuning&#8212;fine-tune on English data, then fine-tune on the code-mixed target. <em class=\"ltx_emph ltx_font_italic\">Joint FT</em> jointly fine-tunes on the union of English and code-mixed labeled data. The remaining blocks use weight merging before the final task fine-tuning. In the <em class=\"ltx_emph ltx_font_italic\">Task-Vector (TV)</em> setting, the merge operator <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> is (scaled) vector addition, <math alttext=\"\\theta\\oplus\\lambda\\tau:=\\theta+\\lambda\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>&#952;</mi><mo>&#8853;</mo><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#964;</mi></mrow></mrow><mo>:=</mo><mrow><mi>&#952;</mi><mo>+</mo><mrow><mi>&#955;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#964;</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\theta\\oplus\\lambda\\tau:=\\theta+\\lambda\\tau</annotation></semantics></math>, with <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> (or <math alttext=\"\\lambda_{1},\\lambda_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#955;</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda_{1},\\lambda_{2}</annotation></semantics></math> when combining two vectors) tuned on validation data: <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}</annotation></semantics></math></em> adds the CPT vector; <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> adds the English vector; <em class=\"ltx_emph ltx_font_italic\">TV: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><mrow><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><mo rspace=\"0.392em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> adds both. In the <em class=\"ltx_emph ltx_font_italic\">TIES</em> setting, <math alttext=\"\\oplus\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><mo>&#8853;</mo><annotation encoding=\"application/x-tex\">\\oplus</annotation></semantics></math> denotes TIES merging (TrIm&#8211;Elect&#8211;Sign), which resolves interference via magnitude/sign rules rather than simple addition; the rows <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}</annotation></semantics></math></em>, <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em>, and <em class=\"ltx_emph ltx_font_italic\">TIES: Base Model <math alttext=\"\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p1.m14\" intent=\":literal\"><semantics><mrow><mrow><mo rspace=\"0em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">CPT</em></mtext></msub></mrow><mo rspace=\"0.392em\">&#8853;</mo><msub><mi>&#964;</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">en</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\oplus\\,\\tau_{\\text{CPT}}\\oplus\\,\\tau_{\\text{en}}</annotation></semantics></math></em> mirror the TV configurations. In all TV/TIES cases, the merged model is subsequently fine-tuned on the target code-mixed dataset, and reported scores correspond to this final model.</p>\n\n",
                "matched_terms": [
                    "cases",
                    "model",
                    "all",
                    "labeled",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results demonstrate that model merging methods (TV<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT, TIES<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>CPT in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) consistently outperform standard fine-tuning (Full FT) across all models and datasets, yielding +2&#8211;5 F1 over Full FT and +1&#8211;2 F1 over CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS0.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT. This confirms that unlabeled data can be exploited more effectively through model merging than through continued pre-training alone, supporting our hypothesis in <span class=\"ltx_text ltx_font_bold\">RQ1</span>. Between merging strategies, Task Vectors (TV) generally perform better than TIES, though the relative advantage varies by model and task. We also find that the choice of corpus for CPT has only a marginal effect on downstream performance.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "model",
                    "all",
                    "unlabeled",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, baseline fine-tuning significantly outperforms zero-shot and few-shot prompting with LLaMA 3.3 70B, highlighting the limitations of in-context learning for code-mixed tasks. Few-shot prompting improves over zero-shot, but gains plateau at higher k-shot levels, suggesting limited scalability. These findings reinforce the necessity of fine-tuning and merging strategies for robust performance in resource-constrained settings. Finally, as we explore in <span class=\"ltx_text ltx_font_bold\">RQ2</span>, model merging is particularly valuable for integrating heterogeneous resources and enabling cross-lingual transfer, often surpassing sequential or joint fine-tuning.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "model",
                    "crosslingual",
                    "transfer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis varies across three key dimensions: (a) use of only labeled data, (b) use of both labeled and unlabeled data, and (c) type of model augmentation techniques. We combine code-mixed data with monolingual task-specific data. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents a comparison of model performance across different scenarios of data availability. These settings reflect practical conditions in real-world code-mixed applications.</p>\n\n",
                "matched_terms": [
                    "model",
                    "taskspecific",
                    "different",
                    "analysis",
                    "unlabeled",
                    "labeled",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Combining En and En-Hi labeled datasets for finetuning</span> Incorporating English-labeled data alongside En-Hi datasets&#160;(Seq FT and Joint FT rows in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) yields no significant improvement over fine-tuning (Full FT) solely on En-Hi data. Both SeqFT (English first, then English-Hindi) and JointFT (simultaneously on combined datasets) fail to enhance performance for code-mixed tasks (See Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.)</p>\n\n",
                "matched_terms": [
                    "data",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Leveraging monolingual resources with code-mixed resources</span>\nWe combine three elements here: the base model, a vector fine-tuned on English data, and a vector for continued pre-training on code-mixed data (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E3\" title=\"In Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Although we expected the combination of monolingual and code-mixed resources to perform better than just monolingual or just code-mixed, they perform similarly to CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT method, or outperform it by 1 or 2 F1 points (which can be seen in the <span class=\"ltx_text ltx_font_italic\">TV-T_en + CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT </span>and<span class=\"ltx_text ltx_font_italic\"> TIES-T_en + CPT<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>FT</span> rows of Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Extending to a three-way merge does not provide additional gains over a single CPT merge. Overall, compared to FullFT, the combined monolingual and code-mixed approach either matches performance or underperforms slightly (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2 F1 points).</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Varying the corpus used for pre-training might have some impact on the downstream task. To understand such variance, we do continued pre-training using two different unlabeled corpora, and analyze the impact on the downstream tasks.</p>\n\n",
                "matched_terms": [
                    "unlabeled",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the dataset released by &#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Das et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib10\" title=\"\">2023</a>)</cite> for continued pre-training, along with a synthetic code-mixed corpus generated via the GCM Toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rizvi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib31\" title=\"\">2021</a>)</cite> and filtered using an acceptability filter proposed by &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kodali et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib21\" title=\"\">2024</a>)</cite>. The sizes of the two datasets are kept consistent. As code-mixed datasets often come from social media, we expect significant distributional differences between the task-specific and synthetic datasets. Data sources for continued pre-training can differ, leading to differing downstream task performance.</p>\n\n",
                "matched_terms": [
                    "taskspecific",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S5.T3\" title=\"Table 3 &#8227; 5.2. Impact of Continued Pre-Training Corpus &#8227; 5. Results &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> compares the downstream task performance when we vary the corpus used for continued pre-training. Performance on downstream tasks is only marginally affected by the dataset chosen for continued pre-training. Classification results vary by roughly 1 F1 point across different sources of unlabeled data and frequently meet or exceed the results of the fine-tuning exclusively on the downstream dataset.</p>\n\n",
                "matched_terms": [
                    "data",
                    "unlabeled",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The relationship between task performance and in-context learning (ICL) for code-mixed tasks remains underexplored. To better understand how model size and prompting methods influence performance, we conduct zero-shot and few-shot prompting experiments using different-sized models from the LLaMA family&#8212;1B-instruct, 3B-instruct, 8B-instruct, and 70B-instruct. For few-shot prompting, we evaluate performance with 5, 10, 15, and 20 examples. Table &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S4.T2\" title=\"Table 2 &#8227; 4. Experimental Setup &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> contains results of our experiments on LLama 3.2 70B model. We see that even baseline fine-tuning methods perform better than zero/few shot prompting, suggesting that in-context learning performs weakly for codemixing tasks. The highest F1 scores can be seen for the <cite class=\"ltx_cite ltx_citemacro_citet\">Prabhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#bib.bib28\" title=\"\">2016</a>)</cite> datasete suggesting that the performance may be dependent on the data setting. Upon further analysis, we we hypothesize that this is due to the lack of noise in the dataset compared to the other datasets we use for comparison.</p>\n\n",
                "matched_terms": [
                    "model",
                    "experiments",
                    "analysis",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We take the model fine-tuned on the combination of our three En-Hi datasets, using all the different training configurations, and infer on the test set of the En-Ta, En-Ml datasets. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S5.T4\" title=\"Table 4 &#8227; 5.2. Impact of Continued Pre-Training Corpus &#8227; 5. Results &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the transfer of various training strategies to other code-mixed language pairs for Sentiment Analysis task. We present transfer results for all the combinations of base models and training methods. We also include full fine-tuning on the English dataset (FullFT (En)) to ascertain which is better - transfer from monolingual En resources or code-mixed resources from other language pairs.</p>\n\n",
                "matched_terms": [
                    "strategies",
                    "model",
                    "transfer",
                    "all",
                    "different",
                    "analysis",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate both XLM-R (encoder) and LLaMA 3.2 1B (decoder) to study transfer properties. Across En-Ta and En-Ml, we observe a consistent pattern: checkpoints obtained through model merging outperform Full FT baselines by 5&#8211;13 F1 points. This improvement holds across models and target language pairs, highlighting that merged checkpoints capture complementary signals from diverse resources. These findings reinforce our answer to <span class=\"ltx_text ltx_font_bold\">RQ2</span>&#8212;that model merging is an effective strategy for integrating heterogeneous data sources, often surpassing sequential or joint fine-tuning. However, the gains remain dependent on model architecture, the specific code-mixed task, and the language pair being targeted, underscoring the importance of careful merging design.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "transfer",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, Model merging methods can be used to integrate capabilities from multiple data sources with varying language composition and supervision, often outperforming sequential fine-tuning and joint fine-tuning <span class=\"ltx_text ltx_font_bold\">(RQ 2)</span>. However, the impact of model merging is highly dependent on the model, task, and target language pair; underscoring the need for careful merging strategies.</p>\n\n",
                "matched_terms": [
                    "data",
                    "strategies",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically show the advantages of using model merging as a strategy for code-mixed tasks. We also introduce an innovative perspective on utilizing existing data resources for code-mixed tasks, diverging from traditional domain-adaptation and fine-tuning techniques. In circumstances where datasets are lacking or are available in different languages, our findings can assist researchers in choosing an appropriate method to optimize the available resources.</p>\n\n",
                "matched_terms": [
                    "model",
                    "different",
                    "available",
                    "where",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With respect to various data resource availability scenarios, following are the key recommendations of this study:</p>\n\n",
                "matched_terms": [
                    "data",
                    "resource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">When both labeled and unlabeled data are available</span>, the most effective strategy is to apply model merging method, merging CPT model with the base model, followed by fine-tuning on the target dataset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "available",
                    "unlabeled",
                    "labeled",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">For smaller labeled datasets</span>, model merging is preferable to simple fine-tuning due to its superior sample efficiency.</p>\n\n",
                "matched_terms": [
                    "model",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">If only labeled data is available</span>, full model fine-tuning remains the best option.</p>\n\n",
                "matched_terms": [
                    "available",
                    "model",
                    "data",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In the absence of both labeled and unlabeled data</span>, the most effective approach is to utilize models trained on similar tasks with other code-mixed language resources. These models consistently outperform those transferred from monolingual English resources.</p>\n\n",
                "matched_terms": [
                    "data",
                    "unlabeled",
                    "labeled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations</span>\nOur experiments are limited to sentence classification due to the lack of consistent datasets for other tasks in both monolingual and code-mixed languages. Therefore, the generalizability of our findings to other NLP tasks is unclear. Future research should evaluate these methods on a broader range of tasks with comparable datasets across multiple language pairs.</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although our study contains models of different architecture, we do not scale it to larger language models (&#191;1B parameter models), because of prohibitive cost of training/fine-tuning a larger model. Analyzing how model merging behaves as model size increase for code-mixed tasks would be an interesting avenue for future work.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For model merging methods, there is a small additional computation cost, where the weights of the models are being merged. This could be a limitation while trying to find the optimal hyperparameters. For example, the <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p6.m1\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> values in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E1\" title=\"In 3.2. Model Merging &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E2\" title=\"In Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19782v2#S3.E3\" title=\"In Model Merging. &#8227; 3.3. Leveraging labeled monolingual data &#8227; 3. Methodology &#8227; Adapting Multilingual Models to Code-Mixed Tasks via Model Merging\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "where"
                ]
            }
        ]
    }
}