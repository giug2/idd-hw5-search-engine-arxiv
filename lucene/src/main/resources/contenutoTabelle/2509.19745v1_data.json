{
    "S3.T1": {
        "source_file": "PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs",
        "caption": "Table 1: Benchmark datasets for ASR and S2TT. “xx→en” means non-English source to English target; “en→xx” means English source to non-English target.",
        "body": "Task\nTest Data\nDomain\n\n\nLanguages\n\n\nMetric\n\n\nASR\nLibriSpeech (test-clean/other)\nread\n\n\nen\n\n\nWER\n\n\nWenetspeech (test-net/meeting)\nYouTube/Podcast\n\n\nzh\n\n\nCER\n\n\nFleurs\nWikipedia (read)\n\n\nzh, en, ja, ko, yue,\n\n\n\n\n\n\n\n\n\nde, fr, ru, es, it\n\n\nCER/WER\n\n\n\nCommon Voice 15\ncrowdsourcing\n\n\nzh, en, ja, ko, yue,\n\n\n\n\n\n\n\n\n\n\nde, fr, ru, es, it\n\n\nCER/WER\n\n\nS2TT\nCoVoST2\ncrowdsourcing\n\n\nxx→en: zh, ja, de, fr, es, it, ru\n\n\n\n\n\n\n\n\n\n\nen→xx: zh, ja, de, sv, id, ar\n\n\nBLEU",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Test Data</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Domain</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Languages</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Metric</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">ASR</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">LibriSpeech (test-clean/other)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">read</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">en</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Wenetspeech (test-net/meeting)</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">YouTube/Podcast</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">zh</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">CER</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Fleurs</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Wikipedia (read)</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">zh, en, ja, ko, yue,</span></span>\n</span>\n</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">de, fr, ru, es, it</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">CER/WER</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Common Voice 15</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">crowdsourcing</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">zh, en, ja, ko, yue,</span></span>\n</span>\n</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">de, fr, ru, es, it</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">CER/WER</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">S2TT</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">CoVoST2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">crowdsourcing</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">xx&#8594;en: zh, ja, de, fr, es, it, ru</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:91.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">en&#8594;xx: zh, ja, de, sv, id, ar</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">BLEU</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wer",
            "read",
            "datasets",
            "data",
            "wenetspeech",
            "yue",
            "benchmark",
            "s2tt",
            "metric",
            "common",
            "crowdsourcing",
            "source",
            "“en→xx”",
            "asr",
            "domain",
            "covost2",
            "languages",
            "xx→en",
            "testnetmeeting",
            "nonenglish",
            "voice",
            "means",
            "fleurs",
            "“xx→en”",
            "youtubepodcast",
            "cerwer",
            "testcleanother",
            "en→xx",
            "test",
            "wikipedia",
            "english",
            "librispeech",
            "target",
            "task",
            "bleu",
            "cer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For model capability evaluation, the ASR task includes four types of test sets: Librispeech (human-read audiobooks)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CommonVoice 15 (crowdsourced speech)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Fleurs (human-read Wikipedia)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Wenetspeech (audio from YouTube and podcasts)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. CER (Character Error Rate)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used for zh, ja, ko, yue, while WER (Word Error Rate)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used for other languages, all combined with Whisper&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> normalizer post-processing.\nThe evaluation of translation tasks uses primarily CoVoST2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is based on Common Voice, using BLEU scores for the evaluation, with character-based tokenization for Chinese and Japanese, and the 13a tokenizer for other languages.\nDetails are elaborated in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Data and Settings &#8227; 3 Data and Training Setting &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large language models (LLMs) have expanded from text to speech, giving rise to Speech Large Models (SLMs) that support recognition, translation, and synthesis. A key challenge is aligning speech and text representations, which becomes harder in multilingual settings. Existing methods often freeze LLM parameters and train encoders on multilingual data, but this forces cross-language convergence and limits performance. We introduce Progressive Alignment Representation Training (PART), a multi-stage and multi-task framework that separates within-language from cross-language alignment. During cross-language training, LLM parameters are dynamically activated, and text-based tasks are later introduced to enhance multilingual understanding. Experiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART surpasses conventional approaches, with analysis confirming its ability to balance language-specific distinctions and cross-language generalization. These results demonstrate PART&#8217;s effectiveness and generality for multilingual speech modality alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "fleurs",
                    "covost2",
                    "data",
                    "wenetspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The mainstream architecture of current SLMs usually consists of a pre-trained speech encoder connected to an LLM through an adapter&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Within this framework, a central challenge is how to effectively align speech representations with the textual representations of the LLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This need becomes more critical and difficult in fine-grained multilingual scenarios.\nIn practice, the conventional approach often treats multilingual tasks as monolingual ones, mixing non-crosslingual tasks like automatic speech recognition (ASR) with crosslingual tasks like speech to text translation (S2TT) during training.\nA common approach is to keep the LLM parameters frozen and train the speech encoder on multilingual speech data so that it aligns with the LLM input layer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, when applied to multilingual tasks, this strategy may force audio representations of different languages to converge and place an excessive burden on the speech encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As a result, such methods remain at the modality-level alignment, lacking deep exploration of multilingual speech-text alignment and facing performance bottlenecks in multilingual speech tasks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "asr",
                    "data",
                    "languages",
                    "s2tt",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Compared with traditional training methods, our multi-stage and multi-task alignment approach achieves better performance on CommonVoice 15, Fleurs, and Wenetspeech (ASR tasks), as well as CoVoST2 (S2TT task). In addition, analytical experiments show that activating the LLM in cross-language tasks effectively leverages its multilingual modeling strength, while introducing text-based tasks for fine-tuning in the final stage significantly improves multilingual ability. Overall, the results confirm the effectiveness and generality of our method for multilingual speech modality alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "fleurs",
                    "asr",
                    "covost2",
                    "task",
                    "wenetspeech",
                    "s2tt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We design a task-dependent activation strategy that freezes the LLM in in-language ASR tasks while activating it in multilingual tasks (e.g., S2TT), allowing the audio encoder and LLM to play to their respective strengths.</span>\n</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We further introduce a final text-based fine-tuning stage that enhances multilingual ability, leading to improved performance across ASR and S2TT tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We formulate the multilingual speech-to-text task as follows: given a speech input </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the goal is to generate its corresponding textual output </span>\n  <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">y</mi>\n      <annotation encoding=\"application/x-tex\">y</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This work focuses on two distinct settings: multilingual ASR and S2TT. The language of </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is referred to as the speech in source language. For multilingual ASR, which is a monolingual task, the output </span>\n  <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">y</mi>\n      <annotation encoding=\"application/x-tex\">y</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a transcription in the same language as </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained using the monolingual dataset </span>\n  <math alttext=\"\\mathcal{D}_{\\text{mono}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n        <mtext mathsize=\"0.900em\">mono</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{mono}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In contrast, speech translation is a cross-lingual task where </span>\n  <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">y</mi>\n      <annotation encoding=\"application/x-tex\">y</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is produced in a target language different from the source language, leveraging the cross-lingual dataset </span>\n  <math alttext=\"\\mathcal{D}_{\\text{cross}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n        <mtext mathsize=\"0.900em\">cross</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{cross}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "target",
                    "asr",
                    "task",
                    "s2tt",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we provide a detailed description of the training data sources, test datasets, and evaluation metrics in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Data and Settings &#8227; 3 Data and Training Setting &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The model configuration and training hyperparameters are specified in </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Model Specifications &#8227; 3 Data and Training Setting &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "test",
                    "datasets",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The training data includes two tasks, ASR (Automatic Speech Recognition) and S2TT (Speech-to-Text Translation).\nFor ASR, there is a total of 810k hours of commercial purchases data covering 10 languages: Chinese (zh), English (en), Japanese (ja), Korean (ko), Cantonese (yue), German (de), French (fr), Russian (ru), Spanish (es), and Italian (it).\nThe S2TT task data encompasses 434k hours, covering language pairs such as zh-en, ja-en, de-en, fr-en, es-en, it-en, ru-en, as well as en-zh, en-ja, en-de, en-sv, en-id, and en-ar.\nThe main sources of S2TT data are: 1) open-source datasets like CoVoST&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TED-LIUM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MuST-C&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; 2) constructing S2TT data by translating ASR transcripts into target languages and The rest comes from commercial purchases of 48k hours.</span>\n</p>\n\n",
                "matched_terms": [
                    "english",
                    "target",
                    "asr",
                    "task",
                    "yue",
                    "data",
                    "datasets",
                    "languages",
                    "s2tt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">For Multilingual Automatic Speech Recognition:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The experimental results are shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 Main Result &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On LibriSpeech, Wenetspeech, Fleurs, and Common Voice 15, PART consistently outperforms the two-stage baseline and remains competitive with larger SLMs. For example, on Fleurs it reduces the average WER from 6.35 to 3.73 (</span>\n  <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo>\n      <annotation encoding=\"application/x-tex\">\\downarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 41%), and on Common Voice 15 from 9.18 to 6.29 (</span>\n  <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo>\n      <annotation encoding=\"application/x-tex\">\\downarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 32%). These results show that the progressive alignment strategy preserves language-specific features while the task-dependent activation mechanism strengthens cross-language robustness, leading to clear advantages in multilingual ASR.</span>\n</p>\n\n",
                "matched_terms": [
                    "voice",
                    "fleurs",
                    "wer",
                    "librispeech",
                    "asr",
                    "wenetspeech",
                    "common"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">For Multilingual Speech-to-Text Translation:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The experimental results are shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.1 Main Result &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On CoVoST2 across both xx2en and en2xx directions, PART also demonstrates significant advantages. PART-8B achieves the highest BLEU scores on most language pairs, with particularly large gains in low-resource directions such as en&#8594;sv, en&#8594;id, and en&#8594;ar, where it surpasses the two-stage baseline by 3&#8211;8 BLEU. This improvement stems from the third-stage introduction of text-based tasks, which, building on modality alignment, further unlocks the LLM&#8217;s cross-lingual generation ability, enabling the model to better capture semantics and produce fluent translations in complex multilingual scenarios. At the same time, compared with Whisper-large-v2 and MinMo, PART maintains stable advantages in high-resource languages such as German and French, showing that the method not only addresses cross-lingual alignment challenges but also achieves general improvements in multilingual generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "bleu",
                    "covost2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of our progressive training strategy, we conducted experiments using mixed ASR and S2TT data under two configurations: two-stage training and three-stage training. The optimization settings and methodologies for both setups were consistent with those described in the Method section. We evaluated ASR performance using Word Error Rate (WER) on the FLEURS test set and S2TT performance using BLEU score on the CoVoST2 test set. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.2 Analysis of Each Stage in Progressive Training &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the model trained with three stages significantly outperformed the two-stage model on both tasks, achieving lower WER and higher BLEU scores. These results underscore the superiority of our progressively fine-grained training strategy.</span>\n</p>\n\n",
                "matched_terms": [
                    "fleurs",
                    "wer",
                    "asr",
                    "covost2",
                    "data",
                    "bleu",
                    "s2tt",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In addition, we verified that our proposed approach of aligning the model first and then fine-tuning it on cross-lingual tasks contributes to better convergence. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.2 Analysis of Each Stage in Progressive Training &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the three-stage training method uses both ASR and S2TT tasks across all three stages, while PART (Progressive Alignment then Tuning) trains the first two stages only on ASR data and the final stage on both ASR and S2TT tasks. PART achieves lower WER and higher BLEU scores on nearly all subsets. These results confirm that performing alignment before cross-lingual fine-tuning offers a more effective training paradigm for multilingual tasks, as it reduces the language model&#8217;s confusion over language-specific speech characteristics.</span>\n</p>\n\n",
                "matched_terms": [
                    "wer",
                    "asr",
                    "data",
                    "bleu",
                    "s2tt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To isolate and evaluate the impact of progressively unfreezing the speech encoder on both ASR and S2TT tasks, we conducted a controlled ablation study. Both the baseline (&#8221;full&#8221;) and our progressive method (&#8221;last8-full&#8221;) were identically trained on the combined dataset </span>\n  <math alttext=\"\\mathcal{D}{\\text{mimo}}+\\mathcal{D}{\\text{cross}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mtext mathsize=\"0.900em\">mimo</mtext>\n        </mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mtext mathsize=\"0.900em\">cross</mtext>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}{\\text{mimo}}+\\mathcal{D}{\\text{cross}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> following a two-stage protocol: first fine-tuning the adapter modules, followed by joint fine-tuning of the speech encoder and adapter. The sole difference lies in the second stage; the &#8221;full&#8221; baseline unfreezes and fine-tunes the entire speech encoder at once, while our &#8221;last8-full&#8221; strategy progressively unfreezes it (last 8 layers first, then the full encoder). As evidenced in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.3 Ablation experiments &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our progressive approach yields a consistent improvement, reducing average WER on FLEURS by 0.1 and increasing average BLEU on CoVoST2 by 0.3. These gains confirm that a gradual unfreezing strategy, within this framework, more effectively facilitates alignment learning compared to full-encoder fine-tuning.</span>\n</p>\n\n",
                "matched_terms": [
                    "fleurs",
                    "wer",
                    "asr",
                    "covost2",
                    "bleu",
                    "s2tt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This work proposes PART, a staged, task-dependent training paradigm that decouples intra-language alignment from cross-lingual alignment. The staged approach preserves language specificity while fully exploiting the LLM&#8217;s multilingual modeling capacity, avoiding over-convergence of speech representations in multilingual settings. Large-scale evaluations show that PART achieves significant gains on ASR and S2TT. Mechanistic analyses and ablations validate the method&#8217;s effectiveness and its marked improvements in robustness to Language Identification (LID) prompts. Overall, PART provides a general and transferable paradigm for multilingual speech&#8211;text alignment, and offers valuable insights for future optimization of multilingual speech-language models.</span>\n</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "asr"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs",
        "caption": "Table 2: ASR main results (WER, %). Bold numbers indicate the best results in each column. “–” means the model does not support the corresponding language or dataset.",
        "body": "Models\nParams\nLibrispeech\nWenetspeech\nFleurs\nCommon Voice 15\n\n\n\n\nclean\nother\nmeeting\nnet\nzh\nen\nja\nko\nyue\nde\nfr\nru\nes\nit\nzh\nen\nja\nko\nyue\nde\nfr\nru\nes\nit\n\n\nSALMON\n14B\n2.1\n4.9\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n\n\nMinMo\n8B\n1.7\n3.9\n6.8\n7.4\n3.0\n3.8\n3.8\n2.9\n4.3\n5.2\n5.5\n6.2\n3.4\n3.5\n6.3\n7.9\n13.4\n6.6\n6.4\n6.6\n8.5\n7.0\n5.0\n6.1\n\n\nQwen2-Audio\n8B\n1.6\n3.6\n8.1\n9.5\n7.5\n5.1\n10.4\n10.6\n4.1\n10.5\n9.4\n23.2\n7.3\n6.7\n6.9\n8.6\n13.5\n17.5\n5.9\n7.6\n9.6\n16.8\n5.7\n6.8\n\n\nQwen2.5-Omni\n8B\n1.8\n3.4\n5.9\n7.7\n3.0\n4.1\n–\n–\n–\n–\n–\n–\n–\n–\n5.2\n7.6\n–\n–\n–\n–\n7.5\n–\n–\n–\n\n\nPART\n2B\n2.0\n4.2\n7.9\n7.2\n4.2\n5.0\n3.6\n3.1\n4.3\n5.6\n6.5\n6.9\n3.8\n4.2\n7.0\n9.6\n10.2\n5.6\n5.8\n6.4\n9.3\n7.6\n6.8\n5.8\n\n\nPART\n8B\n1.7\n3.8\n7.5\n6.8\n3.9\n4.0\n3.1\n2.5\n3.7\n4.4\n4.7\n5.2\n2.8\n3.0\n6.4\n8.5\n9.7\n4.9\n5.5\n5.1\n7.5\n5.6\n5.1\n4.6",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Params</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Librispeech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Wenetspeech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"10\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Fleurs</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"10\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Common Voice 15</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">clean</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">other</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">meeting</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">net</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">zh</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">en</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ja</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ko</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">yue</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">de</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">fr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ru</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">es</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">it</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">zh</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">en</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ja</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ko</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">yue</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">de</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">fr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ru</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">es</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">it</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">SALMON</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">14B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">MinMo</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-Audio</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">PART</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">PART</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">9.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "main",
            "wer",
            "yue",
            "wenetspeech",
            "qwen25omni",
            "minmo",
            "qwen2audio",
            "common",
            "meeting",
            "salmon",
            "clean",
            "net",
            "best",
            "asr",
            "results",
            "numbers",
            "column",
            "each",
            "bold",
            "“–”",
            "part",
            "voice",
            "means",
            "fleurs",
            "params",
            "dataset",
            "14b",
            "language",
            "indicate",
            "librispeech",
            "models",
            "does",
            "corresponding",
            "other",
            "support",
            "model",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">For Multilingual Automatic Speech Recognition:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The experimental results are shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 Main Result &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On LibriSpeech, Wenetspeech, Fleurs, and Common Voice 15, PART consistently outperforms the two-stage baseline and remains competitive with larger SLMs. For example, on Fleurs it reduces the average WER from 6.35 to 3.73 (</span>\n  <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo>\n      <annotation encoding=\"application/x-tex\">\\downarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 41%), and on Common Voice 15 from 9.18 to 6.29 (</span>\n  <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo>\n      <annotation encoding=\"application/x-tex\">\\downarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 32%). These results show that the progressive alignment strategy preserves language-specific features while the task-dependent activation mechanism strengthens cross-language robustness, leading to clear advantages in multilingual ASR.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large language models (LLMs) have expanded from text to speech, giving rise to Speech Large Models (SLMs) that support recognition, translation, and synthesis. A key challenge is aligning speech and text representations, which becomes harder in multilingual settings. Existing methods often freeze LLM parameters and train encoders on multilingual data, but this forces cross-language convergence and limits performance. We introduce Progressive Alignment Representation Training (PART), a multi-stage and multi-task framework that separates within-language from cross-language alignment. During cross-language training, LLM parameters are dynamically activated, and text-based tasks are later introduced to enhance multilingual understanding. Experiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART surpasses conventional approaches, with analysis confirming its ability to balance language-specific distinctions and cross-language generalization. These results demonstrate PART&#8217;s effectiveness and generality for multilingual speech modality alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "part",
                    "language",
                    "fleurs",
                    "models",
                    "wenetspeech",
                    "results",
                    "support"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nMultilingual Speech Processing, Speech-Text Alignment, Large Language Models</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the research of large language models (LLMs), the scope has expanded from text to other modalities. Among them, Speech Large Models (SLMs) that use speech input and output have been widely studied and applied, showing impressive performance in tasks such as speech recognition&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, translation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and speech synthesis&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The mainstream architecture of current SLMs usually consists of a pre-trained speech encoder connected to an LLM through an adapter&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Within this framework, a central challenge is how to effectively align speech representations with the textual representations of the LLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This need becomes more critical and difficult in fine-grained multilingual scenarios.\nIn practice, the conventional approach often treats multilingual tasks as monolingual ones, mixing non-crosslingual tasks like automatic speech recognition (ASR) with crosslingual tasks like speech to text translation (S2TT) during training.\nA common approach is to keep the LLM parameters frozen and train the speech encoder on multilingual speech data so that it aligns with the LLM input layer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, when applied to multilingual tasks, this strategy may force audio representations of different languages to converge and place an excessive burden on the speech encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As a result, such methods remain at the modality-level alignment, lacking deep exploration of multilingual speech-text alignment and facing performance bottlenecks in multilingual speech tasks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "common",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose a </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">P</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">rogressive </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">lignment </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">epresentation </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">raining approach (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PART</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">)\nfor Multilingual SLMs. In this approach, the training is divided into stages to separate within-language alignment from cross-language alignment, preventing excessive convergence of audio representations across languages. For cross-language tasks, LLM parameters are dynamically activated, allowing the speech encoder to focus on semantic mapping within each language while the LLM leverages its multilingual modeling strength. In the final stage, text-based tasks are introduced to fine-tune the LLM, further improving its ability in multilingual instruction understanding and generation. Overall, our method enables a collaborative division of labor between the speech encoder and the LLM, preserving language-specific distinctions while enhancing cross-language generalization.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "part",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Compared with traditional training methods, our multi-stage and multi-task alignment approach achieves better performance on CommonVoice 15, Fleurs, and Wenetspeech (ASR tasks), as well as CoVoST2 (S2TT task). In addition, analytical experiments show that activating the LLM in cross-language tasks effectively leverages its multilingual modeling strength, while introducing text-based tasks for fine-tuning in the final stage significantly improves multilingual ability. Overall, the results confirm the effectiveness and generality of our method for multilingual speech modality alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "asr",
                    "wenetspeech",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We formulate the multilingual speech-to-text task as follows: given a speech input </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the goal is to generate its corresponding textual output </span>\n  <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">y</mi>\n      <annotation encoding=\"application/x-tex\">y</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This work focuses on two distinct settings: multilingual ASR and S2TT. The language of </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is referred to as the speech in source language. For multilingual ASR, which is a monolingual task, the output </span>\n  <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">y</mi>\n      <annotation encoding=\"application/x-tex\">y</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a transcription in the same language as </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained using the monolingual dataset </span>\n  <math alttext=\"\\mathcal{D}_{\\text{mono}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n        <mtext mathsize=\"0.900em\">mono</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{mono}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In contrast, speech translation is a cross-lingual task where </span>\n  <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">y</mi>\n      <annotation encoding=\"application/x-tex\">y</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is produced in a target language different from the source language, leveraging the cross-lingual dataset </span>\n  <math alttext=\"\\mathcal{D}_{\\text{cross}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n        <mtext mathsize=\"0.900em\">cross</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{cross}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "asr",
                    "corresponding",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we first extract log-mel spectrogram features </span>\n  <math alttext=\"\\mathbf{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119820;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the input speech signal </span>\n  <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119831;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using a feature extractor. These features are then fed into a pre-trained multilingual speech encoder (parameterized by </span>\n  <math alttext=\"\\theta_{\\text{se}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n        <mtext mathsize=\"0.900em\">se</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\theta_{\\text{se}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) to obtain linguistic representations </span>\n  <math alttext=\"\\mathbf{X}_{\\text{ling}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">ling</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{ling}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Subsequently, a lightweight adaptor module, randomly initialized with parameters </span>\n  <math alttext=\"\\theta_{\\text{adaptor}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n        <mtext mathsize=\"0.900em\">adaptor</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\theta_{\\text{adaptor}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and not pretrained, projects </span>\n  <math alttext=\"\\mathbf{X}_{\\text{ling}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">ling</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{ling}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into the embedding space of a large language model (LLM, parameterized by </span>\n  <math alttext=\"\\theta_{\\text{llm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n        <mtext mathsize=\"0.900em\">llm</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\theta_{\\text{llm}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), resulting in the aligned feature representation </span>\n  <math alttext=\"\\mathbf{X}_{\\text{align}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">align</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{align}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This adapted representation is then concatenated with the embedding of the instruction token </span>\n  <math alttext=\"\\mathbf{X}_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mi mathsize=\"0.900em\">I</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{I}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the combined input is passed to the multilingual LLM. Finally, the LLM generates the corresponding text. Method details are shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2 Method &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "corresponding",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Following the alignment achieved in the first two stages, speech features and textual representations of the corresponding language are now aligned in the semantic space. We then introduce cross-lingual tasks to better leverage the multilingual capabilities of the LLM. However, due to inherent discrepancies in length between speech and text, as well as the rich diversity in speech (such as variations in speaking rate), the granularity of speech representations cannot be strictly matched to that of text. Therefore, in the third stage, as Eq.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S2.E4\" style=\"font-size:90%;\" title=\"In 2.3.3 Stage 3: Joint Optimization with LLM-Adaptive &#8227; 2.3 Progressive Training for Multilingual SLMs &#8227; 2 Method &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we unfreeze the LLM and perform joint optimization of the speech encoder, adaptor, and LLM together, enhancing the model&#8217;s robustness to such variations.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The training data includes two tasks, ASR (Automatic Speech Recognition) and S2TT (Speech-to-Text Translation).\nFor ASR, there is a total of 810k hours of commercial purchases data covering 10 languages: Chinese (zh), English (en), Japanese (ja), Korean (ko), Cantonese (yue), German (de), French (fr), Russian (ru), Spanish (es), and Italian (it).\nThe S2TT task data encompasses 434k hours, covering language pairs such as zh-en, ja-en, de-en, fr-en, es-en, it-en, ru-en, as well as en-zh, en-ja, en-de, en-sv, en-id, and en-ar.\nThe main sources of S2TT data are: 1) open-source datasets like CoVoST&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TED-LIUM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MuST-C&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; 2) constructing S2TT data by translating ASR transcripts into target languages and The rest comes from commercial purchases of 48k hours.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "asr",
                    "yue",
                    "main"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For model capability evaluation, the ASR task includes four types of test sets: Librispeech (human-read audiobooks)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CommonVoice 15 (crowdsourced speech)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Fleurs (human-read Wikipedia)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Wenetspeech (audio from YouTube and podcasts)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. CER (Character Error Rate)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used for zh, ja, ko, yue, while WER (Word Error Rate)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used for other languages, all combined with Whisper&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> normalizer post-processing.\nThe evaluation of translation tasks uses primarily CoVoST2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is based on Common Voice, using BLEU scores for the evaluation, with character-based tokenization for Chinese and Japanese, and the 13a tokenizer for other languages.\nDetails are elaborated in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Data and Settings &#8227; 3 Data and Training Setting &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "voice",
                    "fleurs",
                    "wer",
                    "librispeech",
                    "asr",
                    "yue",
                    "wenetspeech",
                    "other",
                    "common",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The speech audio encoder is initialized with the SenseVoice-large encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, approximately 700M in size.\nThe LLM is based on Qwen2.5&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enhanced with multilingual continuing pre-training in advance.\nThe Adapter consists of two transformer layers and one CNN layer.\nWe experiment with two sizes of the Qwen2.5 model, 1.5B and 7B parameters, corresponding to PART-2B and PART-8B. Training is performed using 256 NVIDIA A800 GPUs for three epochs, and inference is performed using greedy decoding.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">For Multilingual Speech-to-Text Translation:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The experimental results are shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.1 Main Result &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On CoVoST2 across both xx2en and en2xx directions, PART also demonstrates significant advantages. PART-8B achieves the highest BLEU scores on most language pairs, with particularly large gains in low-resource directions such as en&#8594;sv, en&#8594;id, and en&#8594;ar, where it surpasses the two-stage baseline by 3&#8211;8 BLEU. This improvement stems from the third-stage introduction of text-based tasks, which, building on modality alignment, further unlocks the LLM&#8217;s cross-lingual generation ability, enabling the model to better capture semantics and produce fluent translations in complex multilingual scenarios. At the same time, compared with Whisper-large-v2 and MinMo, PART maintains stable advantages in high-resource languages such as German and French, showing that the method not only addresses cross-lingual alignment challenges but also achieves general improvements in multilingual generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "part",
                    "language",
                    "results",
                    "minmo",
                    "model",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of our progressive training strategy, we conducted experiments using mixed ASR and S2TT data under two configurations: two-stage training and three-stage training. The optimization settings and methodologies for both setups were consistent with those described in the Method section. We evaluated ASR performance using Word Error Rate (WER) on the FLEURS test set and S2TT performance using BLEU score on the CoVoST2 test set. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.2 Analysis of Each Stage in Progressive Training &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the model trained with three stages significantly outperformed the two-stage model on both tasks, achieving lower WER and higher BLEU scores. These results underscore the superiority of our progressively fine-grained training strategy.</span>\n</p>\n\n",
                "matched_terms": [
                    "fleurs",
                    "wer",
                    "asr",
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In addition, we verified that our proposed approach of aligning the model first and then fine-tuning it on cross-lingual tasks contributes to better convergence. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.2 Analysis of Each Stage in Progressive Training &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the three-stage training method uses both ASR and S2TT tasks across all three stages, while PART (Progressive Alignment then Tuning) trains the first two stages only on ASR data and the final stage on both ASR and S2TT tasks. PART achieves lower WER and higher BLEU scores on nearly all subsets. These results confirm that performing alignment before cross-lingual fine-tuning offers a more effective training paradigm for multilingual tasks, as it reduces the language model&#8217;s confusion over language-specific speech characteristics.</span>\n</p>\n\n",
                "matched_terms": [
                    "part",
                    "language",
                    "wer",
                    "asr",
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To isolate and evaluate the impact of progressively unfreezing the speech encoder on both ASR and S2TT tasks, we conducted a controlled ablation study. Both the baseline (&#8221;full&#8221;) and our progressive method (&#8221;last8-full&#8221;) were identically trained on the combined dataset </span>\n  <math alttext=\"\\mathcal{D}{\\text{mimo}}+\\mathcal{D}{\\text{cross}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mtext mathsize=\"0.900em\">mimo</mtext>\n        </mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mtext mathsize=\"0.900em\">cross</mtext>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}{\\text{mimo}}+\\mathcal{D}{\\text{cross}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> following a two-stage protocol: first fine-tuning the adapter modules, followed by joint fine-tuning of the speech encoder and adapter. The sole difference lies in the second stage; the &#8221;full&#8221; baseline unfreezes and fine-tunes the entire speech encoder at once, while our &#8221;last8-full&#8221; strategy progressively unfreezes it (last 8 layers first, then the full encoder). As evidenced in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.3 Ablation experiments &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our progressive approach yields a consistent improvement, reducing average WER on FLEURS by 0.1 and increasing average BLEU on CoVoST2 by 0.3. These gains confirm that a gradual unfreezing strategy, within this framework, more effectively facilitates alignment learning compared to full-encoder fine-tuning.</span>\n</p>\n\n",
                "matched_terms": [
                    "wer",
                    "asr",
                    "dataset",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This work proposes PART, a staged, task-dependent training paradigm that decouples intra-language alignment from cross-lingual alignment. The staged approach preserves language specificity while fully exploiting the LLM&#8217;s multilingual modeling capacity, avoiding over-convergence of speech representations in multilingual settings. Large-scale evaluations show that PART achieves significant gains on ASR and S2TT. Mechanistic analyses and ablations validate the method&#8217;s effectiveness and its marked improvements in robustness to Language Identification (LID) prompts. Overall, PART provides a general and transferable paradigm for multilingual speech&#8211;text alignment, and offers valuable insights for future optimization of multilingual speech-language models.</span>\n</p>\n\n",
                "matched_terms": [
                    "part",
                    "models",
                    "asr",
                    "language"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs",
        "caption": "Table 3: S2TT main results (BLEU). Bold numbers indicate the best results in each column. “–” means the model does not support the corresponding language.",
        "body": "Model\nParams\nxx2en\nen2xx\n\n\n\n\nzh\nja\nde\nfr\nes\nit\nru\nzh\nja\nde\nsv\nid\nar\n\n\nWhisper-large-v2\n1.6B\n18.0\n26.1\n36.3\n36.4\n40.1\n30.9\n–\n–\n–\n–\n–\n–\n–\n\n\nSpeech-LLaMA\n7B\n12.3\n19.9\n27.1\n25.2\n27.9\n25.9\n36.8\n–\n–\n–\n–\n–\n–\n\n\nSALMON\n14B\n–\n–\n–\n–\n–\n–\n–\n33.1\n22.7\n18.6\n–\n–\n–\n\n\nMinMo\n8B\n26.0\n28.9\n39.9\n41.3\n43.3\n40.6\n48.6\n46.7\n35.1\n–\n–\n–\n–\n\n\nQwen2-Audio\n8B\n24.4\n20.7\n35.2\n38.5\n40.0\n36.3\n–\n45.2\n28.8\n29.9\n–\n–\n–\n\n\nQwen2.5-Omni\n8B\n29.4\n–\n37.7\n–\n–\n–\n–\n41.4\n–\n30.2\n–\n–\n–\n\n\nLLaST\n2B\n19.2\n24.2\n36.8\n41.2\n43.2\n39.3\n–\n–\n–\n–\n–\n–\n–\n\n\nPART\n2B\n23.2\n26.8\n37.9\n39.2\n40.9\n37.3\n46.5\n42.4\n43.8\n30.8\n15.7\n32.3\n16.6\n\n\nPART\n8B\n27.0\n30.0\n40.8\n42.3\n42.7\n39.7\n50.9\n46.8\n47.2\n35.0\n25.8\n37.5\n22.4",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Params</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"7\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">xx2en</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">en2xx</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">zh</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ja</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">de</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">fr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">es</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">it</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ru</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">zh</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ja</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">de</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">sv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">id</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ar</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Whisper-large-v2</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.6B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech-LLaMA</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">SALMON</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">14B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">MinMo</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">43.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">40.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.6</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-Audio</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.0</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.9</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">29.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.7</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLaST</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2B</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.3</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">PART</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">PART</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">30.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">40.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">42.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">50.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">46.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">47.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">35.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">25.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">37.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">22.4</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "en2xx",
            "main",
            "llast",
            "xx2en",
            "qwen25omni",
            "whisperlargev2",
            "minmo",
            "qwen2audio",
            "s2tt",
            "salmon",
            "best",
            "results",
            "numbers",
            "column",
            "each",
            "bold",
            "“–”",
            "part",
            "means",
            "16b",
            "params",
            "14b",
            "language",
            "indicate",
            "does",
            "bleu",
            "corresponding",
            "speechllama",
            "support",
            "model",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">For Multilingual Speech-to-Text Translation:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The experimental results are shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.1 Main Result &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On CoVoST2 across both xx2en and en2xx directions, PART also demonstrates significant advantages. PART-8B achieves the highest BLEU scores on most language pairs, with particularly large gains in low-resource directions such as en&#8594;sv, en&#8594;id, and en&#8594;ar, where it surpasses the two-stage baseline by 3&#8211;8 BLEU. This improvement stems from the third-stage introduction of text-based tasks, which, building on modality alignment, further unlocks the LLM&#8217;s cross-lingual generation ability, enabling the model to better capture semantics and produce fluent translations in complex multilingual scenarios. At the same time, compared with Whisper-large-v2 and MinMo, PART maintains stable advantages in high-resource languages such as German and French, showing that the method not only addresses cross-lingual alignment challenges but also achieves general improvements in multilingual generation.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large language models (LLMs) have expanded from text to speech, giving rise to Speech Large Models (SLMs) that support recognition, translation, and synthesis. A key challenge is aligning speech and text representations, which becomes harder in multilingual settings. Existing methods often freeze LLM parameters and train encoders on multilingual data, but this forces cross-language convergence and limits performance. We introduce Progressive Alignment Representation Training (PART), a multi-stage and multi-task framework that separates within-language from cross-language alignment. During cross-language training, LLM parameters are dynamically activated, and text-based tasks are later introduced to enhance multilingual understanding. Experiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART surpasses conventional approaches, with analysis confirming its ability to balance language-specific distinctions and cross-language generalization. These results demonstrate PART&#8217;s effectiveness and generality for multilingual speech modality alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "part",
                    "language",
                    "support"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose a </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">P</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">rogressive </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">lignment </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">epresentation </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">raining approach (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PART</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">)\nfor Multilingual SLMs. In this approach, the training is divided into stages to separate within-language alignment from cross-language alignment, preventing excessive convergence of audio representations across languages. For cross-language tasks, LLM parameters are dynamically activated, allowing the speech encoder to focus on semantic mapping within each language while the LLM leverages its multilingual modeling strength. In the final stage, text-based tasks are introduced to fine-tune the LLM, further improving its ability in multilingual instruction understanding and generation. Overall, our method enables a collaborative division of labor between the speech encoder and the LLM, preserving language-specific distinctions while enhancing cross-language generalization.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "part",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Compared with traditional training methods, our multi-stage and multi-task alignment approach achieves better performance on CommonVoice 15, Fleurs, and Wenetspeech (ASR tasks), as well as CoVoST2 (S2TT task). In addition, analytical experiments show that activating the LLM in cross-language tasks effectively leverages its multilingual modeling strength, while introducing text-based tasks for fine-tuning in the final stage significantly improves multilingual ability. Overall, the results confirm the effectiveness and generality of our method for multilingual speech modality alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We formulate the multilingual speech-to-text task as follows: given a speech input </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the goal is to generate its corresponding textual output </span>\n  <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">y</mi>\n      <annotation encoding=\"application/x-tex\">y</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This work focuses on two distinct settings: multilingual ASR and S2TT. The language of </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is referred to as the speech in source language. For multilingual ASR, which is a monolingual task, the output </span>\n  <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">y</mi>\n      <annotation encoding=\"application/x-tex\">y</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a transcription in the same language as </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained using the monolingual dataset </span>\n  <math alttext=\"\\mathcal{D}_{\\text{mono}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n        <mtext mathsize=\"0.900em\">mono</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{mono}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In contrast, speech translation is a cross-lingual task where </span>\n  <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">y</mi>\n      <annotation encoding=\"application/x-tex\">y</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is produced in a target language different from the source language, leveraging the cross-lingual dataset </span>\n  <math alttext=\"\\mathcal{D}_{\\text{cross}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n        <mtext mathsize=\"0.900em\">cross</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{cross}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "language",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we first extract log-mel spectrogram features </span>\n  <math alttext=\"\\mathbf{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119820;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the input speech signal </span>\n  <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119831;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using a feature extractor. These features are then fed into a pre-trained multilingual speech encoder (parameterized by </span>\n  <math alttext=\"\\theta_{\\text{se}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n        <mtext mathsize=\"0.900em\">se</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\theta_{\\text{se}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) to obtain linguistic representations </span>\n  <math alttext=\"\\mathbf{X}_{\\text{ling}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">ling</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{ling}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Subsequently, a lightweight adaptor module, randomly initialized with parameters </span>\n  <math alttext=\"\\theta_{\\text{adaptor}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n        <mtext mathsize=\"0.900em\">adaptor</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\theta_{\\text{adaptor}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and not pretrained, projects </span>\n  <math alttext=\"\\mathbf{X}_{\\text{ling}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">ling</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{ling}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into the embedding space of a large language model (LLM, parameterized by </span>\n  <math alttext=\"\\theta_{\\text{llm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n        <mtext mathsize=\"0.900em\">llm</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\theta_{\\text{llm}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), resulting in the aligned feature representation </span>\n  <math alttext=\"\\mathbf{X}_{\\text{align}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">align</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{align}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This adapted representation is then concatenated with the embedding of the instruction token </span>\n  <math alttext=\"\\mathbf{X}_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mi mathsize=\"0.900em\">I</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{I}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the combined input is passed to the multilingual LLM. Finally, the LLM generates the corresponding text. Method details are shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2 Method &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "corresponding",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Following the alignment achieved in the first two stages, speech features and textual representations of the corresponding language are now aligned in the semantic space. We then introduce cross-lingual tasks to better leverage the multilingual capabilities of the LLM. However, due to inherent discrepancies in length between speech and text, as well as the rich diversity in speech (such as variations in speaking rate), the granularity of speech representations cannot be strictly matched to that of text. Therefore, in the third stage, as Eq.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S2.E4\" style=\"font-size:90%;\" title=\"In 2.3.3 Stage 3: Joint Optimization with LLM-Adaptive &#8227; 2.3 Progressive Training for Multilingual SLMs &#8227; 2 Method &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we unfreeze the LLM and perform joint optimization of the speech encoder, adaptor, and LLM together, enhancing the model&#8217;s robustness to such variations.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The training data includes two tasks, ASR (Automatic Speech Recognition) and S2TT (Speech-to-Text Translation).\nFor ASR, there is a total of 810k hours of commercial purchases data covering 10 languages: Chinese (zh), English (en), Japanese (ja), Korean (ko), Cantonese (yue), German (de), French (fr), Russian (ru), Spanish (es), and Italian (it).\nThe S2TT task data encompasses 434k hours, covering language pairs such as zh-en, ja-en, de-en, fr-en, es-en, it-en, ru-en, as well as en-zh, en-ja, en-de, en-sv, en-id, and en-ar.\nThe main sources of S2TT data are: 1) open-source datasets like CoVoST&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TED-LIUM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MuST-C&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; 2) constructing S2TT data by translating ASR transcripts into target languages and The rest comes from commercial purchases of 48k hours.</span>\n</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "language",
                    "main"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For model capability evaluation, the ASR task includes four types of test sets: Librispeech (human-read audiobooks)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CommonVoice 15 (crowdsourced speech)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Fleurs (human-read Wikipedia)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Wenetspeech (audio from YouTube and podcasts)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. CER (Character Error Rate)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used for zh, ja, ko, yue, while WER (Word Error Rate)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used for other languages, all combined with Whisper&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> normalizer post-processing.\nThe evaluation of translation tasks uses primarily CoVoST2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is based on Common Voice, using BLEU scores for the evaluation, with character-based tokenization for Chinese and Japanese, and the 13a tokenizer for other languages.\nDetails are elaborated in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Data and Settings &#8227; 3 Data and Training Setting &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "bleu",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The speech audio encoder is initialized with the SenseVoice-large encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, approximately 700M in size.\nThe LLM is based on Qwen2.5&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enhanced with multilingual continuing pre-training in advance.\nThe Adapter consists of two transformer layers and one CNN layer.\nWe experiment with two sizes of the Qwen2.5 model, 1.5B and 7B parameters, corresponding to PART-2B and PART-8B. Training is performed using 256 NVIDIA A800 GPUs for three epochs, and inference is performed using greedy decoding.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">For Multilingual Automatic Speech Recognition:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The experimental results are shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 Main Result &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On LibriSpeech, Wenetspeech, Fleurs, and Common Voice 15, PART consistently outperforms the two-stage baseline and remains competitive with larger SLMs. For example, on Fleurs it reduces the average WER from 6.35 to 3.73 (</span>\n  <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo>\n      <annotation encoding=\"application/x-tex\">\\downarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 41%), and on Common Voice 15 from 9.18 to 6.29 (</span>\n  <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo>\n      <annotation encoding=\"application/x-tex\">\\downarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 32%). These results show that the progressive alignment strategy preserves language-specific features while the task-dependent activation mechanism strengthens cross-language robustness, leading to clear advantages in multilingual ASR.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "part"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of our progressive training strategy, we conducted experiments using mixed ASR and S2TT data under two configurations: two-stage training and three-stage training. The optimization settings and methodologies for both setups were consistent with those described in the Method section. We evaluated ASR performance using Word Error Rate (WER) on the FLEURS test set and S2TT performance using BLEU score on the CoVoST2 test set. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.2 Analysis of Each Stage in Progressive Training &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the model trained with three stages significantly outperformed the two-stage model on both tasks, achieving lower WER and higher BLEU scores. These results underscore the superiority of our progressively fine-grained training strategy.</span>\n</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "results",
                    "bleu",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In addition, we verified that our proposed approach of aligning the model first and then fine-tuning it on cross-lingual tasks contributes to better convergence. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.2 Analysis of Each Stage in Progressive Training &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the three-stage training method uses both ASR and S2TT tasks across all three stages, while PART (Progressive Alignment then Tuning) trains the first two stages only on ASR data and the final stage on both ASR and S2TT tasks. PART achieves lower WER and higher BLEU scores on nearly all subsets. These results confirm that performing alignment before cross-lingual fine-tuning offers a more effective training paradigm for multilingual tasks, as it reduces the language model&#8217;s confusion over language-specific speech characteristics.</span>\n</p>\n\n",
                "matched_terms": [
                    "part",
                    "language",
                    "results",
                    "bleu",
                    "s2tt",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To isolate and evaluate the impact of progressively unfreezing the speech encoder on both ASR and S2TT tasks, we conducted a controlled ablation study. Both the baseline (&#8221;full&#8221;) and our progressive method (&#8221;last8-full&#8221;) were identically trained on the combined dataset </span>\n  <math alttext=\"\\mathcal{D}{\\text{mimo}}+\\mathcal{D}{\\text{cross}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mtext mathsize=\"0.900em\">mimo</mtext>\n        </mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mtext mathsize=\"0.900em\">cross</mtext>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}{\\text{mimo}}+\\mathcal{D}{\\text{cross}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> following a two-stage protocol: first fine-tuning the adapter modules, followed by joint fine-tuning of the speech encoder and adapter. The sole difference lies in the second stage; the &#8221;full&#8221; baseline unfreezes and fine-tunes the entire speech encoder at once, while our &#8221;last8-full&#8221; strategy progressively unfreezes it (last 8 layers first, then the full encoder). As evidenced in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.3 Ablation experiments &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our progressive approach yields a consistent improvement, reducing average WER on FLEURS by 0.1 and increasing average BLEU on CoVoST2 by 0.3. These gains confirm that a gradual unfreezing strategy, within this framework, more effectively facilitates alignment learning compared to full-encoder fine-tuning.</span>\n</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This work proposes PART, a staged, task-dependent training paradigm that decouples intra-language alignment from cross-lingual alignment. The staged approach preserves language specificity while fully exploiting the LLM&#8217;s multilingual modeling capacity, avoiding over-convergence of speech representations in multilingual settings. Large-scale evaluations show that PART achieves significant gains on ASR and S2TT. Mechanistic analyses and ablations validate the method&#8217;s effectiveness and its marked improvements in robustness to Language Identification (LID) prompts. Overall, PART provides a general and transferable paradigm for multilingual speech&#8211;text alignment, and offers valuable insights for future optimization of multilingual speech-language models.</span>\n</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "part",
                    "language"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs",
        "caption": "Table 4: Ablation study on Stage-2 fine-tuning strategies, comparing full encoder unfreezing against progressive unfreezing (last8→full). Performance is measured by average WER on 10 languages of FLEURS and average BLEU score on 13 language pairs of CoVoST2.",
        "body": "Finetune Paradigm in Stage 2\nWER(%)\nBLEU\n\n\n\n\nlast8- full\n6.4\n31.7\n\n\nfull\n6.3\n32.0",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Finetune Paradigm in Stage 2</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER(%)</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BLEU</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">last8-&#8197;full</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.4</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">full</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.3</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "stage2",
            "wer",
            "study",
            "finetune",
            "last8→full",
            "pairs",
            "stage",
            "covost2",
            "languages",
            "measured",
            "progressive",
            "encoder",
            "unfreezing",
            "score",
            "ablation",
            "performance",
            "fleurs",
            "paradigm",
            "last8",
            "average",
            "strategies",
            "language",
            "comparing",
            "against",
            "finetuning",
            "bleu",
            "full"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To isolate and evaluate the impact of progressively unfreezing the speech encoder on both ASR and S2TT tasks, we conducted a controlled ablation study. Both the baseline (&#8221;full&#8221;) and our progressive method (&#8221;last8-full&#8221;) were identically trained on the combined dataset </span>\n  <math alttext=\"\\mathcal{D}{\\text{mimo}}+\\mathcal{D}{\\text{cross}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mtext mathsize=\"0.900em\">mimo</mtext>\n        </mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mtext mathsize=\"0.900em\">cross</mtext>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}{\\text{mimo}}+\\mathcal{D}{\\text{cross}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> following a two-stage protocol: first fine-tuning the adapter modules, followed by joint fine-tuning of the speech encoder and adapter. The sole difference lies in the second stage; the &#8221;full&#8221; baseline unfreezes and fine-tunes the entire speech encoder at once, while our &#8221;last8-full&#8221; strategy progressively unfreezes it (last 8 layers first, then the full encoder). As evidenced in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 4.3 Ablation experiments &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our progressive approach yields a consistent improvement, reducing average WER on FLEURS by 0.1 and increasing average BLEU on CoVoST2 by 0.3. These gains confirm that a gradual unfreezing strategy, within this framework, more effectively facilitates alignment learning compared to full-encoder fine-tuning.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large language models (LLMs) have expanded from text to speech, giving rise to Speech Large Models (SLMs) that support recognition, translation, and synthesis. A key challenge is aligning speech and text representations, which becomes harder in multilingual settings. Existing methods often freeze LLM parameters and train encoders on multilingual data, but this forces cross-language convergence and limits performance. We introduce Progressive Alignment Representation Training (PART), a multi-stage and multi-task framework that separates within-language from cross-language alignment. During cross-language training, LLM parameters are dynamically activated, and text-based tasks are later introduced to enhance multilingual understanding. Experiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART surpasses conventional approaches, with analysis confirming its ability to balance language-specific distinctions and cross-language generalization. These results demonstrate PART&#8217;s effectiveness and generality for multilingual speech modality alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "performance",
                    "fleurs",
                    "covost2",
                    "progressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the research of large language models (LLMs), the scope has expanded from text to other modalities. Among them, Speech Large Models (SLMs) that use speech input and output have been widely studied and applied, showing impressive performance in tasks such as speech recognition&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, translation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and speech synthesis&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The mainstream architecture of current SLMs usually consists of a pre-trained speech encoder connected to an LLM through an adapter&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Within this framework, a central challenge is how to effectively align speech representations with the textual representations of the LLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This need becomes more critical and difficult in fine-grained multilingual scenarios.\nIn practice, the conventional approach often treats multilingual tasks as monolingual ones, mixing non-crosslingual tasks like automatic speech recognition (ASR) with crosslingual tasks like speech to text translation (S2TT) during training.\nA common approach is to keep the LLM parameters frozen and train the speech encoder on multilingual speech data so that it aligns with the LLM input layer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, when applied to multilingual tasks, this strategy may force audio representations of different languages to converge and place an excessive burden on the speech encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As a result, such methods remain at the modality-level alignment, lacking deep exploration of multilingual speech-text alignment and facing performance bottlenecks in multilingual speech tasks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "encoder",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose a </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">P</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">rogressive </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">lignment </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">epresentation </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">raining approach (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PART</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">)\nfor Multilingual SLMs. In this approach, the training is divided into stages to separate within-language alignment from cross-language alignment, preventing excessive convergence of audio representations across languages. For cross-language tasks, LLM parameters are dynamically activated, allowing the speech encoder to focus on semantic mapping within each language while the LLM leverages its multilingual modeling strength. In the final stage, text-based tasks are introduced to fine-tune the LLM, further improving its ability in multilingual instruction understanding and generation. Overall, our method enables a collaborative division of labor between the speech encoder and the LLM, preserving language-specific distinctions while enhancing cross-language generalization.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "stage",
                    "progressive",
                    "languages",
                    "finetune",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Compared with traditional training methods, our multi-stage and multi-task alignment approach achieves better performance on CommonVoice 15, Fleurs, and Wenetspeech (ASR tasks), as well as CoVoST2 (S2TT task). In addition, analytical experiments show that activating the LLM in cross-language tasks effectively leverages its multilingual modeling strength, while introducing text-based tasks for fine-tuning in the final stage significantly improves multilingual ability. Overall, the results confirm the effectiveness and generality of our method for multilingual speech modality alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "fleurs",
                    "stage",
                    "finetuning",
                    "covost2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We further introduce a final text-based fine-tuning stage that enhances multilingual ability, leading to improved performance across ASR and S2TT tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "stage",
                    "finetuning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we first extract log-mel spectrogram features </span>\n  <math alttext=\"\\mathbf{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119820;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the input speech signal </span>\n  <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119831;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using a feature extractor. These features are then fed into a pre-trained multilingual speech encoder (parameterized by </span>\n  <math alttext=\"\\theta_{\\text{se}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n        <mtext mathsize=\"0.900em\">se</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\theta_{\\text{se}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) to obtain linguistic representations </span>\n  <math alttext=\"\\mathbf{X}_{\\text{ling}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">ling</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{ling}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Subsequently, a lightweight adaptor module, randomly initialized with parameters </span>\n  <math alttext=\"\\theta_{\\text{adaptor}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n        <mtext mathsize=\"0.900em\">adaptor</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\theta_{\\text{adaptor}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and not pretrained, projects </span>\n  <math alttext=\"\\mathbf{X}_{\\text{ling}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">ling</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{ling}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into the embedding space of a large language model (LLM, parameterized by </span>\n  <math alttext=\"\\theta_{\\text{llm}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n        <mtext mathsize=\"0.900em\">llm</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\theta_{\\text{llm}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), resulting in the aligned feature representation </span>\n  <math alttext=\"\\mathbf{X}_{\\text{align}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mtext mathsize=\"0.900em\">align</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{\\text{align}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This adapted representation is then concatenated with the embedding of the instruction token </span>\n  <math alttext=\"\\mathbf{X}_{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mi mathsize=\"0.900em\">I</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}_{I}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the combined input is passed to the multilingual LLM. Finally, the LLM generates the corresponding text. Method details are shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2 Method &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In contrast to existing methods, this paper contends that multilingual tasks introduce significant challenges for alignment. To address this, a progressive alignment approach is adopted, consisting of three main stages. The first two stages are responsible for gradual cross-modality alignment, while the third stage builds upon this modality-aligned foundation to fine-tune on cross-lingual tasks, thereby better leveraging the multilingual capabilities of the LLM. All three stages share the same underlying optimization objective, which is formally defined by the loss function in Eq.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S2.E1\" style=\"font-size:90%;\" title=\"In 2.3 Progressive Training for Multilingual SLMs &#8227; 2 Method &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "finetune",
                    "progressive",
                    "stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Since both the speech encoder and the LLM are pre-trained on large-scale datasets, in the first stage, as Eq.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S2.E2\" style=\"font-size:90%;\" title=\"In 2.3.1 Stage 1: Adapter-only Within-Language Alignment &#8227; 2.3 Progressive Training for Multilingual SLMs &#8227; 2 Method &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we fine-tune the adaptor using </span>\n  <math alttext=\"\\mathcal{D}_{\\text{mono}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.SSS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n        <mtext mathsize=\"0.900em\">mono</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{mono}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to perform an initial coarse-grained alignment. This provides a robust starting point for the subsequent joint optimization of multiple components.</span>\n</p>\n\n",
                "matched_terms": [
                    "finetune",
                    "encoder",
                    "stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Following the first-stage fine-tuning of the adaptor, its lightweight design leads to insufficient representational capacity. To achieve more precise alignment in the second stage, as Eq.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S2.E3\" style=\"font-size:90%;\" title=\"In 2.3.2 Stage 2: Within-Language Alignment with Progressive Encoder Unfreezing &#8227; 2.3 Progressive Training for Multilingual SLMs &#8227; 2 Method &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we progressively unfreeze the speech encoder and optimize it jointly with the adaptor. More specifically, we employ a two-phase unfreezing strategy: first, the last eight layers of the speech encoder are unfrozen and fine-tuned alongside the adaptor; then, the entire speech encoder is activated for full network optimization, and this progressive activation will be discussed in Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.SS3\" style=\"font-size:90%;\" title=\"4.3 Ablation experiments &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">4.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Notably, this stage continues to use only the monolingual dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "unfreezing",
                    "stage",
                    "progressive",
                    "finetuning",
                    "full",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Following the alignment achieved in the first two stages, speech features and textual representations of the corresponding language are now aligned in the semantic space. We then introduce cross-lingual tasks to better leverage the multilingual capabilities of the LLM. However, due to inherent discrepancies in length between speech and text, as well as the rich diversity in speech (such as variations in speaking rate), the granularity of speech representations cannot be strictly matched to that of text. Therefore, in the third stage, as Eq.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S2.E4\" style=\"font-size:90%;\" title=\"In 2.3.3 Stage 3: Joint Optimization with LLM-Adaptive &#8227; 2.3 Progressive Training for Multilingual SLMs &#8227; 2 Method &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we unfreeze the LLM and perform joint optimization of the speech encoder, adaptor, and LLM together, enhancing the model&#8217;s robustness to such variations.</span>\n</p>\n\n",
                "matched_terms": [
                    "stage",
                    "encoder",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Through the proposed three-stage progressive alignment framework, our approach effectively bridges the gap between speech and text modalities across languages. This structured training strategy enables the model to fully leverage the LLM&#8217;s inherent multilingual capabilities, facilitating robust cross-lingual transfer. As a result, the system achieves enhanced performance in multilingual speech-to-text tasks, offering improved adaptation to variability in speech while maintaining semantic coherence.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "progressive",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The training data includes two tasks, ASR (Automatic Speech Recognition) and S2TT (Speech-to-Text Translation).\nFor ASR, there is a total of 810k hours of commercial purchases data covering 10 languages: Chinese (zh), English (en), Japanese (ja), Korean (ko), Cantonese (yue), German (de), French (fr), Russian (ru), Spanish (es), and Italian (it).\nThe S2TT task data encompasses 434k hours, covering language pairs such as zh-en, ja-en, de-en, fr-en, es-en, it-en, ru-en, as well as en-zh, en-ja, en-de, en-sv, en-id, and en-ar.\nThe main sources of S2TT data are: 1) open-source datasets like CoVoST&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TED-LIUM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and MuST-C&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; 2) constructing S2TT data by translating ASR transcripts into target languages and The rest comes from commercial purchases of 48k hours.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "language",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For model capability evaluation, the ASR task includes four types of test sets: Librispeech (human-read audiobooks)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CommonVoice 15 (crowdsourced speech)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Fleurs (human-read Wikipedia)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Wenetspeech (audio from YouTube and podcasts)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. CER (Character Error Rate)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used for zh, ja, ko, yue, while WER (Word Error Rate)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used for other languages, all combined with Whisper&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> normalizer post-processing.\nThe evaluation of translation tasks uses primarily CoVoST2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is based on Common Voice, using BLEU scores for the evaluation, with character-based tokenization for Chinese and Japanese, and the 13a tokenizer for other languages.\nDetails are elaborated in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Data and Settings &#8227; 3 Data and Training Setting &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "fleurs",
                    "wer",
                    "covost2",
                    "languages",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we conduct a comprehensive set of experiments to evaluate the proposed method. In Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.SS1\" style=\"font-size:90%;\" title=\"4.1 Main Result &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we compare our approach with several state-of-the-art methods. Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.SS2\" style=\"font-size:90%;\" title=\"4.2 Analysis of Each Stage in Progressive Training &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> provides a detailed analysis of the contribution of our two novel designs. Finally, in Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.SS3\" style=\"font-size:90%;\" title=\"4.3 Ablation experiments &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">4.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we perform ablation studies to better understand the role of progressively unfreezing strategy in stage 2.</span>\n</p>\n\n",
                "matched_terms": [
                    "stage",
                    "unfreezing",
                    "ablation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">For Multilingual Automatic Speech Recognition:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The experimental results are shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 Main Result &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On LibriSpeech, Wenetspeech, Fleurs, and Common Voice 15, PART consistently outperforms the two-stage baseline and remains competitive with larger SLMs. For example, on Fleurs it reduces the average WER from 6.35 to 3.73 (</span>\n  <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo>\n      <annotation encoding=\"application/x-tex\">\\downarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 41%), and on Common Voice 15 from 9.18 to 6.29 (</span>\n  <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo>\n      <annotation encoding=\"application/x-tex\">\\downarrow</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 32%). These results show that the progressive alignment strategy preserves language-specific features while the task-dependent activation mechanism strengthens cross-language robustness, leading to clear advantages in multilingual ASR.</span>\n</p>\n\n",
                "matched_terms": [
                    "wer",
                    "average",
                    "progressive",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">For Multilingual Speech-to-Text Translation:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> The experimental results are shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.1 Main Result &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. On CoVoST2 across both xx2en and en2xx directions, PART also demonstrates significant advantages. PART-8B achieves the highest BLEU scores on most language pairs, with particularly large gains in low-resource directions such as en&#8594;sv, en&#8594;id, and en&#8594;ar, where it surpasses the two-stage baseline by 3&#8211;8 BLEU. This improvement stems from the third-stage introduction of text-based tasks, which, building on modality alignment, further unlocks the LLM&#8217;s cross-lingual generation ability, enabling the model to better capture semantics and produce fluent translations in complex multilingual scenarios. At the same time, compared with Whisper-large-v2 and MinMo, PART maintains stable advantages in high-resource languages such as German and French, showing that the method not only addresses cross-lingual alignment challenges but also achieves general improvements in multilingual generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "pairs",
                    "covost2",
                    "languages",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of our progressive training strategy, we conducted experiments using mixed ASR and S2TT data under two configurations: two-stage training and three-stage training. The optimization settings and methodologies for both setups were consistent with those described in the Method section. We evaluated ASR performance using Word Error Rate (WER) on the FLEURS test set and S2TT performance using BLEU score on the CoVoST2 test set. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.2 Analysis of Each Stage in Progressive Training &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the model trained with three stages significantly outperformed the two-stage model on both tasks, achieving lower WER and higher BLEU scores. These results underscore the superiority of our progressively fine-grained training strategy.</span>\n</p>\n\n",
                "matched_terms": [
                    "score",
                    "performance",
                    "fleurs",
                    "wer",
                    "covost2",
                    "bleu",
                    "progressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In addition, we verified that our proposed approach of aligning the model first and then fine-tuning it on cross-lingual tasks contributes to better convergence. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19745v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.2 Analysis of Each Stage in Progressive Training &#8227; 4 Experiment Result &#8227; PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the three-stage training method uses both ASR and S2TT tasks across all three stages, while PART (Progressive Alignment then Tuning) trains the first two stages only on ASR data and the final stage on both ASR and S2TT tasks. PART achieves lower WER and higher BLEU scores on nearly all subsets. These results confirm that performing alignment before cross-lingual fine-tuning offers a more effective training paradigm for multilingual tasks, as it reduces the language model&#8217;s confusion over language-specific speech characteristics.</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "wer",
                    "stage",
                    "paradigm",
                    "finetuning",
                    "bleu",
                    "progressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This work proposes PART, a staged, task-dependent training paradigm that decouples intra-language alignment from cross-lingual alignment. The staged approach preserves language specificity while fully exploiting the LLM&#8217;s multilingual modeling capacity, avoiding over-convergence of speech representations in multilingual settings. Large-scale evaluations show that PART achieves significant gains on ASR and S2TT. Mechanistic analyses and ablations validate the method&#8217;s effectiveness and its marked improvements in robustness to Language Identification (LID) prompts. Overall, PART provides a general and transferable paradigm for multilingual speech&#8211;text alignment, and offers valuable insights for future optimization of multilingual speech-language models.</span>\n</p>\n\n",
                "matched_terms": [
                    "paradigm",
                    "language"
                ]
            }
        ]
    }
}