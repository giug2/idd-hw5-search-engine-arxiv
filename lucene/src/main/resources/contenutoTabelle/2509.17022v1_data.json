{
    "S3.T1": {
        "caption": "Table 1: Quantitative results on audio separation. Our LLMs text query approach outperforms visual query baselines, with further improvements after finetuning the model on our designed dataset. ∗ indicates the usage of the fine-tuned models.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Query Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">FD </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">KID </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">SI-SDR </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">SDR </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">Sound-of-Pixels </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:70%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Visual</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">95.32</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">11.16</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-24.48</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-1.66</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">iQuery </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:70%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Visual</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">47.95</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">6.16</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-54.92</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-2.98</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">Ours (VGGSound Label)</span><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:70%;\">&#8727;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">24.04</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.54</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">7.10</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">5.31</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Ours (LLMs-Query)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Text</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">34.12</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">5.47</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-3.66</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.86</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Ours (LLMs-Query)<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8727;</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">25.77</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.50</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.07</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.59</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "type",
            "finetuned",
            "separation",
            "visual",
            "query",
            "baselines",
            "llmsquery",
            "llmsquery∗",
            "↓downarrow",
            "outperforms",
            "our",
            "vggsound",
            "audio",
            "after",
            "sdr",
            "text",
            "improvements",
            "label∗",
            "usage",
            "finetuning",
            "designed",
            "results",
            "further",
            "indicates",
            "llms",
            "iquery",
            "↑uparrow",
            "ours",
            "kid",
            "soundofpixels",
            "model",
            "quantitative",
            "approach",
            "sisdr",
            "method",
            "dataset"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate our proposed OmniSep-LLMs method against the iQuery baseline <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib18\" title=\"\">18</a>]</cite> and the Sound-of-Pixels baseline <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib8\" title=\"\">8</a>]</cite> on our custom dataset. Our experimental results show that iQuery consistently fails to reproduce the correct audio pitch, producing outputs that are consistently lower than the ground truth. In contrast, our text-query pipeline accurately generates the pitch of separated audio sources. Meanwhile, the Sound-of-Pixels method frequently produces audio with significant data loss, which severely degrades both output audio quality and audience listening experience. Also, from the Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> we can see that although the original OmniSep checkpoint substantially outperforms these visual query baselines, it remains limited by its reliance on precise text descriptions and struggles to distinguish between sources from the same category (e.g., differentiating between two instruments).</p>\n\n",
            "<p class=\"ltx_p\">To improve generalization, we fine-tuned the model on our custom dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, finetuning results in substantial gains across all metrics. Qualitatively, the separation is significantly improved, with reduced artifacts and clearer output, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.F3\" title=\"Figure 3 &#8227; 3.1.1 Fr&#233;chet Distance (FD) &#8227; 3.1 Objective Metrics &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Although directly using VGGSound labels is infeasible in real-life applications <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib11\" title=\"\">11</a>]</cite>, the proximity of our results to those obtained with ideal VGGSound labels indicates strong alignment between our LLMs-based visual-to-text pipeline and expert annotations. This demonstrates the effectiveness of our query generation approach. After finetuning, the model achieves more precise separation for both same-category and different-category audio mixtures.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Video and audio inpainting for mixed audio-visual content has become a crucial task in multimedia editing recently. However, precisely removing an object and its corresponding audio from a video without affecting the rest of the scene remains a significant challenge. To address this, we propose VAInpaint, a novel pipeline that first utilizes a segmentation model to generate masks and guide a video inpainting model in removing objects. At the same time, an LLM then analyzes the scene globally, while a region-specific model provides localized descriptions. Both the overall and regional descriptions will be inputted into an LLM, which will refine the content and turn it into text queries for our text-driven audio separation model. Our audio separation model is fine-tuned on a customized dataset comprising segmented MUSIC instrument images and VGGSound backgrounds to enhance its generalization performance. Experiments show that our method achieves performance comparable to current benchmarks in both audio and video inpainting.</p>\n\n",
                "matched_terms": [
                    "vggsound",
                    "finetuned",
                    "audio",
                    "model",
                    "separation",
                    "method",
                    "dataset",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the era of multimodal content creation, the ability to separate and edit mixed audio-visual (AV) elements is a crucial part of video editing. Traditional methods in AV combination content are often limited to the handling method within the speech domain or struggle with tangled audio-visual signals in static scenes <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib1\" title=\"\">1</a>]</cite>. Occasionally, when performing the visual-audio separation task in the outdoor scenes, issues with unwanted leftover audio or visual elements in the output can also lower perceived quality&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib2\" title=\"\">2</a>]</cite>. In this paper, we define a machine learning task as follows: Given a video with multiple objects, such as a scene where people are playing instruments, separate the instrument audio from the mixed soundtrack, and inpaint the video to remove the instrument and performer in a zero-shot manner. This paper addresses the challenge of isolating clean video and audio tracks from mixed content, using advanced segmentation, inpainting, and language-guided techniques.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "method",
                    "separation",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior works have constructed important foundations for the components of audio-visual processing. However, they are usually limited by their focus on visual features or audio features individually, which have some potential alignments and connections to other modalities. In video inpainting, efforts have focused on temporal consistency and efficiency <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib3\" title=\"\">3</a>]</cite>, with recent models like ProPainter <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib4\" title=\"\">4</a>]</cite> advancing transformer-based, mask-guided object removal. For segmentation, foundational models such as SAM2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib5\" title=\"\">5</a>]</cite> enable precise, prompt-driven object isolation. On the audio side, OmniSep, a query-based audio separation model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib6\" title=\"\">6</a>]</cite>, supports omni-modal inputs, including text. Meanwhile, LLM-guided methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib7\" title=\"\">7</a>]</cite> have emerged to align representations using human-like language. AV separation or inpainting pipelines are often built based on these techniques, using segmentation for initial source localization, as seen in The Sound-Of-Pixels <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib8\" title=\"\">8</a>]</cite>, and SAVE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib9\" title=\"\">9</a>]</cite>. However, a key limitation is that these methods often lack robust LLM alignment for high-level reasoning, leading to imprecise segmentation or correspondence and an inability to fully remove object-related sounds. Luckily, a new object-to-text pipeline named Describe Anything <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib10\" title=\"\">10</a>]</cite> provides valuable localized captions, which indicates the potential of the model for enabling precise audio separation and inpainting guidance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "soundofpixels",
                    "audio",
                    "model",
                    "separation",
                    "visual",
                    "indicates",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a result, to address the problem of the critical disconnection between high-precision visual inpainting and semantically accurate audio removal, we introduce VAInpaint, which unifies these elements for end-to-end AV inpainting. We first utilize SAM2 to segment out our needed object, then we generate a corresponding object mask to guide ProPainter in removing undesired visual elements. Next, we utilize an LLM to analyze the extracted frame for overall image understanding, while using Describe Anything to generate region-specific text descriptions using the previously generated masks. Combining the above outputs will lead to an LLM-refined text query for OmniSep as an input. For our training data, we developed a custom dataset generation pipeline: We first use SAM2 to segment instruments and performers from MUSIC videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib8\" title=\"\">8</a>]</cite>, creating masks at any resolution. VGGSound videos are then resized to match the resolution, and we mix the segmented MUSIC elements into VGGSound backgrounds&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib11\" title=\"\">11</a>]</cite>. This results in a dataset of mixed audio-visual samples. To increase the separation effect, we fine-tune our audio separation model using our customized dataset. To conclude, our paper&#8217;s main contributions are:\n\n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "vggsound",
                    "audio",
                    "model",
                    "separation",
                    "visual",
                    "query",
                    "results",
                    "dataset",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An integrated hybrid workflow combining video inpainting, LLMs-based comprehensive and regional Scene Understanding with text, and text-query based audio separation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "separation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">OmniSep performs audio source separation in the spectrogram domain <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib6\" title=\"\">6</a>]</cite>. For a mixture spectrogram <math alttext=\"m\\in\\mathbf{R}^{B\\times 1\\times F\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo>&#8712;</mo><msup><mi>&#119825;</mi><mrow><mi>B</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">m\\in\\mathbf{R}^{B\\times 1\\times F\\times T}</annotation></semantics></math> and <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> source embeddings <math alttext=\"\\{e_{n}\\}_{n=1}^{N}\\in\\mathbf{R}^{B\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>e</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>&#8712;</mo><msup><mi>&#119825;</mi><mrow><mi>B</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\{e_{n}\\}_{n=1}^{N}\\in\\mathbf{R}^{B\\times D}</annotation></semantics></math> from multimodal inputs, the model predicts separation masks <math alttext=\"\\{p_{n}\\}_{n=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{p_{n}\\}_{n=1}^{N}</annotation></semantics></math> via:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "separation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">condensing the difference between two descriptions into an audio-focused query. As we can see from the figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S2.F2\" title=\"Figure 2 &#8227; 2.2 LLMs-Generated Text Queries &#8227; 2 METHODOLOGY &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, this text query guides OmniSep, which is our sound separator, for separation tasks. Our workflow transforms visual cues into auditory prompts, enabling the precise isolation of VGGSound audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib11\" title=\"\">11</a>]</cite>. For the details of the text query generation, we can refer to the content inside Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 LLMs-Generated Text Queries &#8227; 2 METHODOLOGY &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Inside, we describe how we utilize the Describe Anything Model (DAM) and VLM to generate regional descriptions and overall descriptions. Then we feed the descriptions into our used LLMs and generate a text query as requested.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "vggsound",
                    "audio",
                    "model",
                    "separation",
                    "visual",
                    "query",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune the OmniSep model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib6\" title=\"\">6</a>]</cite> on our VAInpaint dataset using a supervised learning method. The model processes mixture spectrograms <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> and multimodal embeddings <math alttext=\"\\{e_{n}\\}_{n=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>e</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{e_{n}\\}_{n=1}^{N}</annotation></semantics></math> (extracted using ImageBind <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib12\" title=\"\">12</a>]</cite>) to predict separation masks <math alttext=\"\\{p_{n}\\}_{n=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{p_{n}\\}_{n=1}^{N}</annotation></semantics></math> through the architecture described in Section 2.1.</p>\n\n",
                "matched_terms": [
                    "model",
                    "separation",
                    "method",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our model using the following objective metrics:</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SI-SDR evaluates separation quality while remaining invariant to amplitude scaling <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib17\" title=\"\">17</a>]</cite>:</p>\n\n",
                "matched_terms": [
                    "sisdr",
                    "separation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{s}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{g}</annotation></semantics></math> is the separated signal and <math alttext=\"\\mathbf{s}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{t}</annotation></semantics></math> is the target reference. Higher SI-SDR indicates superior separation performance.</p>\n\n",
                "matched_terms": [
                    "sisdr",
                    "separation",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our ablation study assesses the image-to-text capabilities of various large language models on our custom dataset, utilizing our audio separation pipeline. We assess models on image understanding, regional description, and query refinement through our pipeline to determine their effectiveness in generating user-wanted text queries. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Ablation Study &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> indicates that ChatGPT-5 and Grok-4 outperform other models in these tasks. While DeepSeek-VL2-Small is behind these leaders <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib19\" title=\"\">19</a>]</cite>, it demonstrates stronger reasoning ability than Gemini-2.5-Flash and Qwen3-Max-Preview. Crucially, our fine-tuned model consistently surpasses the original OmniSep checkpoint. The above results confirm that our LLM-driven approach significantly enhances audio separation performance, acting as an important component of our video-audio inpainting pipeline.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned",
                    "audio",
                    "model",
                    "separation",
                    "query",
                    "approach",
                    "results",
                    "indicates",
                    "dataset",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inside the content in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.F3\" title=\"Figure 3 &#8227; 3.1.1 Fr&#233;chet Distance (FD) &#8227; 3.1 Objective Metrics &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, Object 1 stands for the masked region; in the case of our qualitative results, Object 1 stands for the people and their instruments. Meanwhile, Object 2 represents the remaining elements within the overall scene. Based on the qualitative results from Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.F3\" title=\"Figure 3 &#8227; 3.1.1 Fr&#233;chet Distance (FD) &#8227; 3.1 Objective Metrics &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our fine-tuned model&#8217;s predicted audio spectrum closely matches the ground truth audio spectrum, demonstrating clearer separation with fewer artifacts and more alignment compared to the baseline (LLMs-Query method tested on the original checkpoint) and the iQuery prediction method. The qualitative result visually confirms the superior performance of our pipeline in audio separation.</p>\n\n",
                "matched_terms": [
                    "iquery",
                    "finetuned",
                    "audio",
                    "separation",
                    "llmsquery",
                    "results",
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present VAInpaint, a novel audio-visual inpainting pipeline integrating segmentation (SAM2), video inpainting (ProPainter), LLMs, and a query-based audio separation model (OmniSep). Our key innovation is an LLM-driven &#8221;textual subtraction&#8221; method that generates precise separation queries by contrasting global and regional image descriptions. Supported by a custom MUSIC-VGGSound dataset, our fine-tuned model demonstrates competitive performance against current benchmarks. Ablation studies confirm the superiority of high-performance LLMs and significant gains from the fine-tuning process on our custom dataset. Although the complexity of our pipeline presents challenges, this work still advances the field of automated audio-visual editing. Our future work will focus on dynamic multi-object scenes and extend the content of text queries for better Video-Audio inpainting control.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "finetuning",
                    "finetuned",
                    "audio",
                    "model",
                    "separation",
                    "method",
                    "dataset",
                    "text",
                    "our"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Objective metrics comparing audio separation performance, evaluated with synthetically generated text queries. ∗ indicates the usage of a fine-tuned model.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">FD </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">KLD </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">SI-SDR </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">SDR </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">ChatGPT5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">General</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">39.96</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.69</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-0.29</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3.49</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ChatGPT5<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">&#8727;</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">General</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">19.02</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.91</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">6.74</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">5.29</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Gemini-2.5-Flash</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Multimodal</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">84.92</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3.62</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-10.81</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3.62</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">Gemini-2.5-Flash</span><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:70%;\">&#8727;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Multimodal</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">35.98</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3.09</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.41</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.66</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Grok4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">General</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">44.62</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.71</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.63</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">Grok4</span><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:70%;\">&#8727;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">General</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">25.56</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.01</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">6.01</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">5.04</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Qwen3-Max-Preview</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Multimodal</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">74.48</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.37</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-11.64</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3.08</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">Qwen3-Max-Preview</span><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:70%;\">&#8727;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Multimodal</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">35.83</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3.25</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.66</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">DeepSeek-VL2-Small</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">45.71</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3.78</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-4.05</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.52</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">DeepSeek-VL2-Small</span><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:70%;\">&#8727;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">30.34</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.19</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.23</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.37</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "queries",
            "evaluated",
            "type",
            "finetuned",
            "qwen3maxpreview",
            "separation",
            "↓downarrow",
            "general",
            "kld",
            "qwen3maxpreview∗",
            "grok4",
            "audio",
            "objective",
            "sdr",
            "gemini25flash∗",
            "text",
            "deepseekvl2small",
            "performance",
            "comparing",
            "usage",
            "metrics",
            "deepseekvl2small∗",
            "indicates",
            "reasoning",
            "multimodal",
            "↑uparrow",
            "chatgpt5",
            "synthetically",
            "chatgpt5∗",
            "model",
            "generated",
            "sisdr",
            "grok4∗",
            "gemini25flash"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our ablation study assesses the image-to-text capabilities of various large language models on our custom dataset, utilizing our audio separation pipeline. We assess models on image understanding, regional description, and query refinement through our pipeline to determine their effectiveness in generating user-wanted text queries. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Ablation Study &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> indicates that ChatGPT-5 and Grok-4 outperform other models in these tasks. While DeepSeek-VL2-Small is behind these leaders <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib19\" title=\"\">19</a>]</cite>, it demonstrates stronger reasoning ability than Gemini-2.5-Flash and Qwen3-Max-Preview. Crucially, our fine-tuned model consistently surpasses the original OmniSep checkpoint. The above results confirm that our LLM-driven approach significantly enhances audio separation performance, acting as an important component of our video-audio inpainting pipeline.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Video and audio inpainting for mixed audio-visual content has become a crucial task in multimedia editing recently. However, precisely removing an object and its corresponding audio from a video without affecting the rest of the scene remains a significant challenge. To address this, we propose VAInpaint, a novel pipeline that first utilizes a segmentation model to generate masks and guide a video inpainting model in removing objects. At the same time, an LLM then analyzes the scene globally, while a region-specific model provides localized descriptions. Both the overall and regional descriptions will be inputted into an LLM, which will refine the content and turn it into text queries for our text-driven audio separation model. Our audio separation model is fine-tuned on a customized dataset comprising segmented MUSIC instrument images and VGGSound backgrounds to enhance its generalization performance. Experiments show that our method achieves performance comparable to current benchmarks in both audio and video inpainting.</p>\n\n",
                "matched_terms": [
                    "queries",
                    "finetuned",
                    "audio",
                    "model",
                    "separation",
                    "text",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the era of multimodal content creation, the ability to separate and edit mixed audio-visual (AV) elements is a crucial part of video editing. Traditional methods in AV combination content are often limited to the handling method within the speech domain or struggle with tangled audio-visual signals in static scenes <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib1\" title=\"\">1</a>]</cite>. Occasionally, when performing the visual-audio separation task in the outdoor scenes, issues with unwanted leftover audio or visual elements in the output can also lower perceived quality&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib2\" title=\"\">2</a>]</cite>. In this paper, we define a machine learning task as follows: Given a video with multiple objects, such as a scene where people are playing instruments, separate the instrument audio from the mixed soundtrack, and inpaint the video to remove the instrument and performer in a zero-shot manner. This paper addresses the challenge of isolating clean video and audio tracks from mixed content, using advanced segmentation, inpainting, and language-guided techniques.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "audio",
                    "separation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior works have constructed important foundations for the components of audio-visual processing. However, they are usually limited by their focus on visual features or audio features individually, which have some potential alignments and connections to other modalities. In video inpainting, efforts have focused on temporal consistency and efficiency <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib3\" title=\"\">3</a>]</cite>, with recent models like ProPainter <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib4\" title=\"\">4</a>]</cite> advancing transformer-based, mask-guided object removal. For segmentation, foundational models such as SAM2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib5\" title=\"\">5</a>]</cite> enable precise, prompt-driven object isolation. On the audio side, OmniSep, a query-based audio separation model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib6\" title=\"\">6</a>]</cite>, supports omni-modal inputs, including text. Meanwhile, LLM-guided methods <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib7\" title=\"\">7</a>]</cite> have emerged to align representations using human-like language. AV separation or inpainting pipelines are often built based on these techniques, using segmentation for initial source localization, as seen in The Sound-Of-Pixels <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib8\" title=\"\">8</a>]</cite>, and SAVE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib9\" title=\"\">9</a>]</cite>. However, a key limitation is that these methods often lack robust LLM alignment for high-level reasoning, leading to imprecise segmentation or correspondence and an inability to fully remove object-related sounds. Luckily, a new object-to-text pipeline named Describe Anything <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib10\" title=\"\">10</a>]</cite> provides valuable localized captions, which indicates the potential of the model for enabling precise audio separation and inpainting guidance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "separation",
                    "indicates",
                    "reasoning",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a result, to address the problem of the critical disconnection between high-precision visual inpainting and semantically accurate audio removal, we introduce VAInpaint, which unifies these elements for end-to-end AV inpainting. We first utilize SAM2 to segment out our needed object, then we generate a corresponding object mask to guide ProPainter in removing undesired visual elements. Next, we utilize an LLM to analyze the extracted frame for overall image understanding, while using Describe Anything to generate region-specific text descriptions using the previously generated masks. Combining the above outputs will lead to an LLM-refined text query for OmniSep as an input. For our training data, we developed a custom dataset generation pipeline: We first use SAM2 to segment instruments and performers from MUSIC videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib8\" title=\"\">8</a>]</cite>, creating masks at any resolution. VGGSound videos are then resized to match the resolution, and we mix the segmented MUSIC elements into VGGSound backgrounds&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib11\" title=\"\">11</a>]</cite>. This results in a dataset of mixed audio-visual samples. To increase the separation effect, we fine-tune our audio separation model using our customized dataset. To conclude, our paper&#8217;s main contributions are:\n\n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "separation",
                    "generated",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An integrated hybrid workflow combining video inpainting, LLMs-based comprehensive and regional Scene Understanding with text, and text-query based audio separation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text",
                    "separation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">OmniSep performs audio source separation in the spectrogram domain <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib6\" title=\"\">6</a>]</cite>. For a mixture spectrogram <math alttext=\"m\\in\\mathbf{R}^{B\\times 1\\times F\\times T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo>&#8712;</mo><msup><mi>&#119825;</mi><mrow><mi>B</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">m\\in\\mathbf{R}^{B\\times 1\\times F\\times T}</annotation></semantics></math> and <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> source embeddings <math alttext=\"\\{e_{n}\\}_{n=1}^{N}\\in\\mathbf{R}^{B\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>e</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo>&#8712;</mo><msup><mi>&#119825;</mi><mrow><mi>B</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\{e_{n}\\}_{n=1}^{N}\\in\\mathbf{R}^{B\\times D}</annotation></semantics></math> from multimodal inputs, the model predicts separation masks <math alttext=\"\\{p_{n}\\}_{n=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{p_{n}\\}_{n=1}^{N}</annotation></semantics></math> via:</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "audio",
                    "model",
                    "separation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">condensing the difference between two descriptions into an audio-focused query. As we can see from the figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S2.F2\" title=\"Figure 2 &#8227; 2.2 LLMs-Generated Text Queries &#8227; 2 METHODOLOGY &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, this text query guides OmniSep, which is our sound separator, for separation tasks. Our workflow transforms visual cues into auditory prompts, enabling the precise isolation of VGGSound audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib11\" title=\"\">11</a>]</cite>. For the details of the text query generation, we can refer to the content inside Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 LLMs-Generated Text Queries &#8227; 2 METHODOLOGY &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Inside, we describe how we utilize the Describe Anything Model (DAM) and VLM to generate regional descriptions and overall descriptions. Then we feed the descriptions into our used LLMs and generate a text query as requested.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio",
                    "text",
                    "separation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune the OmniSep model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib6\" title=\"\">6</a>]</cite> on our VAInpaint dataset using a supervised learning method. The model processes mixture spectrograms <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> and multimodal embeddings <math alttext=\"\\{e_{n}\\}_{n=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>e</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{e_{n}\\}_{n=1}^{N}</annotation></semantics></math> (extracted using ImageBind <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib12\" title=\"\">12</a>]</cite>) to predict separation masks <math alttext=\"\\{p_{n}\\}_{n=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding=\"application/x-tex\">\\{p_{n}\\}_{n=1}^{N}</annotation></semantics></math> through the architecture described in Section 2.1.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "model",
                    "separation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our model using the following objective metrics:</p>\n\n",
                "matched_terms": [
                    "objective",
                    "metrics",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"(\\mu_{g},\\Sigma_{g})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#956;</mi><mi>g</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">&#931;</mi><mi>g</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mu_{g},\\Sigma_{g})</annotation></semantics></math> and <math alttext=\"(\\mu_{t},\\Sigma_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#956;</mi><mi>t</mi></msub><mo>,</mo><msub><mi mathvariant=\"normal\">&#931;</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mu_{t},\\Sigma_{t})</annotation></semantics></math> represent the mean and covariance of embeddings from generated and target features, respectively. Lower FD indicates better distribution alignment.</p>\n\n",
                "matched_terms": [
                    "indicates",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">KLD measures the distributional discrepancy between classifier outputs for separated and target audio using the binary formulation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib16\" title=\"\">16</a>]</cite>:</p>\n\n",
                "matched_terms": [
                    "kld",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math> represents target class probabilities and <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p1.m2\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math> represents generated audio probabilities from sigmoid-activated outputs. Lower KLD values indicate better distribution matching.</p>\n\n",
                "matched_terms": [
                    "kld",
                    "audio",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SI-SDR evaluates separation quality while remaining invariant to amplitude scaling <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib17\" title=\"\">17</a>]</cite>:</p>\n\n",
                "matched_terms": [
                    "sisdr",
                    "separation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{s}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{g}</annotation></semantics></math> is the separated signal and <math alttext=\"\\mathbf{s}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{t}</annotation></semantics></math> is the target reference. Higher SI-SDR indicates superior separation performance.</p>\n\n",
                "matched_terms": [
                    "sisdr",
                    "separation",
                    "performance",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our proposed OmniSep-LLMs method against the iQuery baseline <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib18\" title=\"\">18</a>]</cite> and the Sound-of-Pixels baseline <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib8\" title=\"\">8</a>]</cite> on our custom dataset. Our experimental results show that iQuery consistently fails to reproduce the correct audio pitch, producing outputs that are consistently lower than the ground truth. In contrast, our text-query pipeline accurately generates the pitch of separated audio sources. Meanwhile, the Sound-of-Pixels method frequently produces audio with significant data loss, which severely degrades both output audio quality and audience listening experience. Also, from the Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> we can see that although the original OmniSep checkpoint substantially outperforms these visual query baselines, it remains limited by its reliance on precise text descriptions and struggles to distinguish between sources from the same category (e.g., differentiating between two instruments).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve generalization, we fine-tuned the model on our custom dataset. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Experimental Results &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, finetuning results in substantial gains across all metrics. Qualitatively, the separation is significantly improved, with reduced artifacts and clearer output, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.F3\" title=\"Figure 3 &#8227; 3.1.1 Fr&#233;chet Distance (FD) &#8227; 3.1 Objective Metrics &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Although directly using VGGSound labels is infeasible in real-life applications <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#bib.bib11\" title=\"\">11</a>]</cite>, the proximity of our results to those obtained with ideal VGGSound labels indicates strong alignment between our LLMs-based visual-to-text pipeline and expert annotations. This demonstrates the effectiveness of our query generation approach. After finetuning, the model achieves more precise separation for both same-category and different-category audio mixtures.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "audio",
                    "metrics",
                    "separation",
                    "model",
                    "indicates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inside the content in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.F3\" title=\"Figure 3 &#8227; 3.1.1 Fr&#233;chet Distance (FD) &#8227; 3.1 Objective Metrics &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, Object 1 stands for the masked region; in the case of our qualitative results, Object 1 stands for the people and their instruments. Meanwhile, Object 2 represents the remaining elements within the overall scene. Based on the qualitative results from Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17022v1#S3.F3\" title=\"Figure 3 &#8227; 3.1.1 Fr&#233;chet Distance (FD) &#8227; 3.1 Objective Metrics &#8227; 3 EXPERIMENTS &#8227; VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our fine-tuned model&#8217;s predicted audio spectrum closely matches the ground truth audio spectrum, demonstrating clearer separation with fewer artifacts and more alignment compared to the baseline (LLMs-Query method tested on the original checkpoint) and the iQuery prediction method. The qualitative result visually confirms the superior performance of our pipeline in audio separation.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "audio",
                    "separation",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present VAInpaint, a novel audio-visual inpainting pipeline integrating segmentation (SAM2), video inpainting (ProPainter), LLMs, and a query-based audio separation model (OmniSep). Our key innovation is an LLM-driven &#8221;textual subtraction&#8221; method that generates precise separation queries by contrasting global and regional image descriptions. Supported by a custom MUSIC-VGGSound dataset, our fine-tuned model demonstrates competitive performance against current benchmarks. Ablation studies confirm the superiority of high-performance LLMs and significant gains from the fine-tuning process on our custom dataset. Although the complexity of our pipeline presents challenges, this work still advances the field of automated audio-visual editing. Our future work will focus on dynamic multi-object scenes and extend the content of text queries for better Video-Audio inpainting control.</p>\n\n",
                "matched_terms": [
                    "queries",
                    "finetuned",
                    "audio",
                    "model",
                    "separation",
                    "text",
                    "performance"
                ]
            }
        ]
    }
}