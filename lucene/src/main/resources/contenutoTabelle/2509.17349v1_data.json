{
    "S5.T1": {
        "source_file": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation",
        "caption": "Table 1: Accuracy of systems in the short-form regime. Best scores in bold. Underlined scores are considered tied with the best metric.",
        "body": "pp-val\nAL\nLAAL\nDAL\nATD\nAP\nYAAL\nN\n\n\nall system pairs\n\n\nall\n0.66\n0.69\n0.59\n0.56\n0.74\n0.96\n5326\n\n\n\n<<0.05\n0.67\n0.70\n0.59\n0.56\n0.75\n0.98\n5149\n\n\n\n<<0.001\n0.68\n0.70\n0.59\n0.56\n0.76\n0.98\n5048\n\n\n0.001-0.05\n0.40\n0.46\n0.40\n0.43\n0.42\n0.71\n101\n\n\nw/o anomalous policy\n\n\nall\n0.95\n0.97\n0.95\n0.92\n0.85\n0.98\n2100\n\n\n\n<<0.05\n0.96\n0.97\n0.96\n0.92\n0.85\n0.99\n2060\n\n\n\n<<0.001\n0.96\n0.98\n0.97\n0.93\n0.85\n0.99\n2025\n\n\n0.001-0.05\n0.71\n0.74\n0.66\n0.74\n0.66\n0.74\n35",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">\n<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-val</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">AL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">LAAL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">DAL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">ATD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">AP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">YAAL</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\">N</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"8\">all system pairs</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">all</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.96</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">5326</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.05</td>\n<td class=\"ltx_td ltx_align_center\">0.67</td>\n<td class=\"ltx_td ltx_align_center\">0.70</td>\n<td class=\"ltx_td ltx_align_center\">0.59</td>\n<td class=\"ltx_td ltx_align_center\">0.56</td>\n<td class=\"ltx_td ltx_align_center\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.98</span></td>\n<td class=\"ltx_td ltx_align_right\">5149</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.001</td>\n<td class=\"ltx_td ltx_align_center\">0.68</td>\n<td class=\"ltx_td ltx_align_center\">0.70</td>\n<td class=\"ltx_td ltx_align_center\">0.59</td>\n<td class=\"ltx_td ltx_align_center\">0.56</td>\n<td class=\"ltx_td ltx_align_center\">0.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.98</span></td>\n<td class=\"ltx_td ltx_align_right\">5048</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">0.001-0.05</td>\n<td class=\"ltx_td ltx_align_center\">0.40</td>\n<td class=\"ltx_td ltx_align_center\">0.46</td>\n<td class=\"ltx_td ltx_align_center\">0.40</td>\n<td class=\"ltx_td ltx_align_center\">0.43</td>\n<td class=\"ltx_td ltx_align_center\">0.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.71</span></td>\n<td class=\"ltx_td ltx_align_right\">101</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"8\">w/o anomalous policy</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">all</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.98</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">2100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.05</td>\n<td class=\"ltx_td ltx_align_center\">0.96</td>\n<td class=\"ltx_td ltx_align_center\">0.97</td>\n<td class=\"ltx_td ltx_align_center\">0.96</td>\n<td class=\"ltx_td ltx_align_center\">0.92</td>\n<td class=\"ltx_td ltx_align_center\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.99</span></td>\n<td class=\"ltx_td ltx_align_right\">2060</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m5\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.001</td>\n<td class=\"ltx_td ltx_align_center\">0.96</td>\n<td class=\"ltx_td ltx_align_center\">0.98</td>\n<td class=\"ltx_td ltx_align_center\">0.97</td>\n<td class=\"ltx_td ltx_align_center\">0.93</td>\n<td class=\"ltx_td ltx_align_center\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.99</span></td>\n<td class=\"ltx_td ltx_align_right\">2025</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">0.001-0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.74</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">35</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tied",
            "yaal",
            "all",
            "anomalous",
            "atd",
            "system",
            "ppval",
            "regime",
            "pairs",
            "considered",
            "bold",
            "systems",
            "accuracy",
            "policy",
            "underlined",
            "laal",
            "metric",
            "dal",
            "shortform",
            "scores",
            "best"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Moving to the metrics, we observe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>) that they all show positive correlations with the true latency, but each language pair has a slightly different scale, which <em class=\"ltx_emph ltx_font_italic\">motivates the use of accuracy instead of simple correlation</em>.\nTherefore, we further compare latency metrics in terms of accuracy in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. If we consider all system pairs (including systems following the anomalous policy), we see that all metrics significantly underperform YAAL (by more than 22% absolute), which reaches an accuracy of 96%. When we progressively filter out system pairs with similar true latency (decreasing <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> values), the accuracies slightly increase, but the ranking of the metrics remains. If we consider a subset that has a <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value between 0.001-0.05 (i.e., removing pairs with similar true latency that are, however, more difficult to distinguish), we see that YAAL still remains the most accurate by a margin of 25% absolute.\nIf we remove systems with the anomalous policy,\nall metrics gain a significant boost in accuracy (bottom part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). However, the YAAL metric remains the best metric in all subsets based on <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values, achieving 98 and 99% accuracy&#8211;even though it relies on assumptions such as uniform source token durations and monotonic source-to-target alignment.\nBased on these observations, we conclude that\n<span class=\"ltx_text ltx_font_italic\">the automatic YAAL metric is almost as accurate as\ntrue latency</span>. We include more accuracy evaluations by isolating different categories of systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3\" title=\"Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">The results on unrelated systems (bottom half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F10\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>) are generally similar to the observations in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. All metrics show a loss of accuracy of no more than 4% points compared to the results on all systems. The only exception seems to be AP, which loses up to 11% points. The order of the metrics remains the same.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency&#8211;the delay between speech input and the translated output.\nWhile quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented.\nIn this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes.\nWe uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more\naccurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems.</p>\n\n",
                "matched_terms": [
                    "yaal",
                    "metric",
                    "shortform",
                    "pairs",
                    "regime",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) is the task in which the system produces incremental translation concurrently with the speaker&#8217;s speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib27\" title=\"\">2020</a>)</cite>. SimulST models have to balance between quality and latency of the output, which is the time elapsed between when a word is uttered and when its corresponding translation is produced. Although translation quality metrics are extensively studied in offline ST and in the related field of machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Freitag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib11\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib10\" title=\"\">2023</a>; Zouhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib30\" title=\"\">2024</a>)</cite>, there is no study on the reliability of latency metrics.\nThe most common latency\nmetrics in SimulST <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho and Esipova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib8\" title=\"\">2016</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib16\" title=\"\">2019</a>; Cherry and Foster, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib7\" title=\"\">2019</a>; Pol&#225;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>; Kano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib14\" title=\"\">2023</a>)</cite>, even though with different approximations, base their calculation on\nsimplifying assumptions such as uniform word duration, absence of pauses, and strict monotonic alignment between source speech and target translation.\nHowever, despite relying on\nthe same assumptions, these metrics often produce very inconsistent assessments.\nThis inconsistency is clearly illustrated in the results of the IWSLT 2023 Shared Task on Simultaneous Translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, where different metrics produced substantially different rankings (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S1.F1\" title=\"In 1 Introduction &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). Such variability raises serious concerns about the validity of current evaluation protocols and their ability to support meaningful comparisons between systems.\nMoreover, this risk can be further exacerbated when shifting from dealing with already presegmented speech, i.e., <span class=\"ltx_text ltx_font_italic\">short-form</span> SimulST, to unsegmented audio, i.e., <span class=\"ltx_text ltx_font_italic\">long-form</span> SimulST, where information about sentence boundaries is not available, further complicating the evaluation <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "system",
                    "shortform",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present the first comprehensive evaluation of latency metrics for SimulST under several aspects, including diverse systems, language pairs, and short- and long-form regimes. Through an in-depth analysis of systems submitted to recent IWSLT SimulST Shared Tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>; Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, we reveal that existing metrics can lead to misleading conclusions and hinder effective system design. We show that the inconsistent evaluations are not primarily due to the aforementioned assumptions, but rather to a structural bias in how latency is measured&#8211;particularly\nin how segmentation influences SimulST models&#8217; behavior.</p>\n\n",
                "matched_terms": [
                    "system",
                    "systems",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose YAAL (Yet Another Average Lagging), a refined latency metric designed to mitigate the biases present in existing latency metrics.\nOur extensive experiments demonstrate that YAAL yields more reliable latency estimates, consistently aligning better with the actual behavior of SimulST systems.\nFurthermore, we also show that resegmentation, which pairs segment-level predictions with their corresponding reference, is necessary to produce meaningful latency measurements for long-form SimulST. To this end, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a new resegmentation tool, and extend our YAAL to LongYAAL, which deals with audio streams. Compared to the current standard alignment tool used in the speech translation community <cite class=\"ltx_cite ltx_citemacro_citep\">(Matusov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> significantly improves alignment quality, enabling more accurate evaluation in long-form scenarios.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The code for YAAL, its long-form variant LongYAAL, and <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> will be released upon the paper acceptance under Apache 2.0 license.</span></span></span></p>\n\n",
                "matched_terms": [
                    "metric",
                    "systems",
                    "yaal",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The short-form is the most common evaluation regime of SimulST <cite class=\"ltx_cite ltx_citemacro_cite\">Anastasopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>); Agarwal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>); Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, where all recordings of the test set are divided, usually following sentence boundaries, into short segments of a few seconds.\nEach segment consists of source audio <math alttext=\"\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is a small portion of raw audio, i.e., audio chunk, and reference translation <math alttext=\"\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>y</mi><mn>1</mn><mi>R</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>y</mi><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow><mi>R</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]</annotation></semantics></math>. Each\naudio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is incrementally fed to the system,\nwhich concurrently\noutputs a translation token <math alttext=\"y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">y_{j}</annotation></semantics></math> at timestamp <math alttext=\"d_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">d_{j}</annotation></semantics></math>, i.e., total duration of audio chunks up to and including the audio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math>. Under these settings, we describe the latency metrics operating in the short-form regime:</p>\n\n",
                "matched_terms": [
                    "system",
                    "all",
                    "shortform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">modifies AL by introducing\na minimal delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> after each\nstep.\nUnlike AL and LAAL, DAL considers all tokens in the hypothesis,\nwithout\ncutoff after <math alttext=\"i&gt;\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\mathbf{X})</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "dal",
                    "all",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form evaluation regime evaluates SimulST systems more realistically <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>, as it assesses their ability to handle long audio streams, often spanning several minutes.\nSince all metrics were developed for the short-form regime,\nrecent studies <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>\nresorted to resegmentation of translations and delays based on the reference translation <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib20\" title=\"\">2005b</a>)</cite>, and computed the metrics on the segment level. We explain the long-form variant of LAAL <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite> below.</p>\n\n",
                "matched_terms": [
                    "laal",
                    "all",
                    "shortform",
                    "regime",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">extends the LAAL metric to unsegmented audio streams <math alttext=\"\\mathbf{S}=[\\mathbf{X}_{1},...,\\mathbf{X}_{|\\mathbf{S}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119826;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119831;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119831;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}=[\\mathbf{X}_{1},...,\\mathbf{X}_{|\\mathbf{S}|}]</annotation></semantics></math>, paired with a continuous stream of predicted translations <math alttext=\"\\mathbf{Y_{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119826;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{S}}</annotation></semantics></math>. Since reference translations <math alttext=\"\\mathbf{Y^{R}_{1}},...,\\mathbf{Y^{R}_{|\\mathbf{S}|}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119832;</mi><mn>&#120783;</mn><mi>&#119825;</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119832;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow><mi>&#119825;</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}_{1}},...,\\mathbf{Y^{R}_{|\\mathbf{S}|}}</annotation></semantics></math> are only available at segment-level <math alttext=\"\\mathbf{X_{1}},...,\\mathbf{X}_{|\\mathbf{S}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#119831;</mi><mn>&#120783;</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119831;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X_{1}},...,\\mathbf{X}_{|\\mathbf{S}|}</annotation></semantics></math>, prediction <math alttext=\"\\mathbf{Y_{S}}=[\\mathbf{Y_{1}},...,\\mathbf{Y_{|\\mathbf{S}|}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#119832;</mi><mi>&#119826;</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119832;</mi><mn>&#120783;</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119832;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y_{S}}=[\\mathbf{Y_{1}},...,\\mathbf{Y_{|\\mathbf{S}|}}]</annotation></semantics></math> with the corresponding delays is segmented based on reference sentences <math alttext=\"\\mathbf{Y^{R}_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119832;</mi><mi>&#119852;</mi><mi>&#119825;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}_{s}}</annotation></semantics></math> to obtain segment-level predictions. Then, StreamLAAL is computed as:</p>\n\n",
                "matched_terms": [
                    "metric",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Where <math alttext=\"d^{*}_{i}=(i-1)\\cdot|\\mathbf{X_{s}}|/{\\max\\{|\\mathbf{Y_{s}}|,|\\mathbf{Y^{R}_{s}|\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi>d</mi><mi>i</mi><mo>&#8727;</mo></msubsup><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#8901;</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>/</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119832;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msubsup><mi>&#119832;</mi><mi>&#119852;</mi><mi>&#119825;</mi></msubsup><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d^{*}_{i}=(i-1)\\cdot|\\mathbf{X_{s}}|/{\\max\\{|\\mathbf{Y_{s}}|,|\\mathbf{Y^{R}_{s}|\\}}}</annotation></semantics></math>\nIn practice, the LAAL metric is calculated for every speech segment <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math> of the stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> and its corresponding reference <math alttext=\"\\mathbf{Y^{R}_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119832;</mi><mi>&#119852;</mi><mi>&#119825;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}_{s}}</annotation></semantics></math> with the automatically aligned prediction <math alttext=\"\\mathbf{Y_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{s}}</annotation></semantics></math> and then averaged over all the speech segments of the stream <math alttext=\"\\mathbf{X_{1}},...,\\mathbf{X_{|\\mathbf{S}|}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><msub><mi>&#119831;</mi><mn>&#120783;</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119831;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X_{1}},...,\\mathbf{X_{|\\mathbf{S}|}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "all",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, <cite class=\"ltx_cite ltx_citemacro_citet\">Huber et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib12\" title=\"\">2023</a>)</cite> proposed an evaluation with resegmentation provided by the system. The evaluation framework expects that the system outputs the segment&#8217;s start and end timestamps that align with the source. First, most systems, including all the IWSLT systems, do not output this information, and relying on the system&#8217;s self-reported alignment might hinder the reliability. Second, as we empirically show in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, relying on potentially low-quality resegmentation significantly lowers the accuracy of latency evaluation. Finally, different segmentations render the observed latencies incomparable across different systems.</p>\n\n",
                "matched_terms": [
                    "system",
                    "all",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of audio segmentation in short-form evaluations significantly affects translation behavior and latency. In practice, short-form SimulST systems are evaluated in a simulated environment where each segment is processed independently <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib17\" title=\"\">2020</a>)</cite>. When the entire source segment has been consumed by the system, the translation is often still in progress. At that point, the simulator requests the remaining translation, which the model emits without any additional delay.\nThis setup introduces two unrealistic conditions. First, the audio is typically segmented in advance by a human annotator or an automatic model with access to the full audio (<span class=\"ltx_text ltx_font_italic\">Oracle Segmentation</span>). Second, the model is allowed to generate the remaining translation (hereinafter, <span class=\"ltx_text ltx_font_italic\">tail words</span>) instantaneously once the input segment ends. These factors unduly distort short-form evaluations, both by providing high-quality segmentation and eliminating the delay that would occur in a realistic setting, where the system must wait to confirm that the sentence has ended. In a more realistic scenario, a model has to rely on online segmentation (<span class=\"ltx_text ltx_font_italic\">Simultaneous Segmentation</span>) and thus delay the final translation until it is confident that the input sentence is complete, thereby introducing extra latency.\nThis discrepancy is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "system",
                    "shortform",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on these observations, we categorize existing short-form latency metrics (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.SS1\" title=\"2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>) into two main groups, depending on whether they include all translated words or only a subset in their latency computation. The first group&#8211;AP, DAL, and ATD&#8211;includes all translated words in the calculation.\nDAL attempts to mitigate the impact of tail words by adding a minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> after each generated word (also within the same step), thus &#8220;spreading&#8221; the tail beyond the sentence.\nHowever, <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math>\nsimply reflects the average source-to-target length ratio and does not accurately capture the system behavior for tail words in settings without segmentation.\nIf multiple words are emitted as tail words, DAL can significantly overestimate\nlatency.\nIn the edge case of a system that waits for an end-of-segment signal (i.e., an offline system), DAL\nreturns the segment length, failing to capture the system&#8217;s true behavior&#8211;in this case, undefined latency.\nAP assigns a delay of 1 to each tail word\nas the entire recording has to be processed to emit that word, thus, the proportion is 1. Although AP is marginally less sensitive to segmentation than DAL&#8211;since it operates on proportions rather than absolute delays&#8211;it still fails to capture system behavior faithfully for the tail words.\nATD also considers all translated words.\nHowever, unlike DAL, it does not apply corrections for tail word behavior, making it the most sensitive to segmentation artifacts among the three metrics.</p>\n\n",
                "matched_terms": [
                    "system",
                    "dal",
                    "all",
                    "shortform",
                    "atd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second group&#8211;AL and LAAL&#8211;computes latency only for words emitted up to and including the cutoff point <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>, which marks the first word generated after the end of the input segment.\nThis corresponds to the word &#8220;<span class=\"ltx_text ltx_font_italic\">gemeinn&#252;tzige</span>&#8221; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nAs discussed, in the short-form regime with oracle segmentation, the <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>-th and following words are often translated earlier than in a more realistic\nlong-form scenario.\nAs a result, this cutoff introduces a systematic bias in the latency estimate, which may lead to either underestimation or overestimation, depending on the system&#8217;s policy.</p>\n\n",
                "matched_terms": [
                    "policy",
                    "shortform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AP, DAL, ATD, AL, and, more recently, LAAL became established metrics in the short-form evaluation of SimulST. However, as discussed above, including any of the tail words in the latency computation leads to a systematic bias that undermines fair comparisons.\nTo cope with this bias, we propose a new metric derived from the LAAL metric:</p>\n\n",
                "matched_terms": [
                    "laal",
                    "metric",
                    "dal",
                    "shortform",
                    "atd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form regime offers a more realistic evaluation setting by assessing systems on continuous, unsegmented audio streams that better reflect real-world use cases. However, the widely used latency metrics were originally designed for the short-form regime and do not directly extend to this setting.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, metrics such as AL, LAAL, and DAL\nrely on a <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> parameter,\nrepresenting the average target-to-source length ratio. However, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> can vary substantially across different segments within the same audio, leading to inconsistent and unreliable latency estimates <cite class=\"ltx_cite ltx_citemacro_cite\">Iranzo-S&#225;nchez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib13\" title=\"\">2021</a>)</cite>.\nSecond, AP\ntends to converge toward 0 for\nlong recordings,\nas typical speech inputs are significantly longer than the translations,\ni.e., <math alttext=\"|\\mathbf{X}|\\gg|\\mathbf{Y}|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow><mo>&#8811;</mo><mrow><mo stretchy=\"false\">|</mo><mi>&#119832;</mi><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">|\\mathbf{X}|\\gg|\\mathbf{Y}|</annotation></semantics></math>,\nleading <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E1\" title=\"In Average Proportion (AP; Cho and Esipova, 2016) &#8227; 2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> to approach 0.\nFinally, ATD\nassumes that each speech token has a fixed duration and that source and target tokens align monotonically&#8211;assumptions that are overly restrictive and especially unrealistic for long-form speech.</p>\n\n",
                "matched_terms": [
                    "atd",
                    "dal",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, StreamLAAL has introduced resegmenting long inputs into short segments and computing latency on these units. While StreamLAAL provides adaptation of existing metrics to long-form input, it relies on the mWERSegmenter tool <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, which may introduce alignment errors <cite class=\"ltx_cite ltx_citemacro_cite\">Amrhein and Haddow (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib3\" title=\"\">2022</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>, and\ncomputes latency up to the cutoff word <math alttext=\"\\tau(\\mathbf{X_{i}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mi>&#119842;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X_{i}})</annotation></semantics></math> (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E7\" title=\"In Streaming LAAL (StreamLAAL; Papi et al., 2024) &#8227; 2.2 Long-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>), which can lead to the systematic bias (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.SS1\" title=\"3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>)\nTo overcome these limitations, we propose a new resegmentation method and an extension of the YAAL metric for the long-form regime.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "yaal",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also extend YAAL to the long-form regime&#8211;i.e., LongYAAL. Unlike StreamLAAL, LongYAAL includes all words in the latency computation, even those generated beyond the aligned segment boundaries <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math>, i.e., all <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for <math alttext=\"i&gt;\\tau(\\mathbf{X_{s}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\mathbf{X_{s}})</annotation></semantics></math>. However, we exclude the final tail words produced after the end of the full stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>, i.e., <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for <math alttext=\"i&gt;\\tau(\\sum_{s=1}^{|\\mathbf{X}|}|\\mathbf{X_{s}}|)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8721;</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msubsup><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\sum_{s=1}^{|\\mathbf{X}|}|\\mathbf{X_{s}}|)</annotation></semantics></math>. This ensures that we include all words emitted beyond the segment boundaries <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math>, but we do not include the tail words generated at the end of the entire stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>. If the stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> consists of a single segment, LongYAAL coincides with YAAL.</p>\n\n",
                "matched_terms": [
                    "all",
                    "yaal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the main evaluation, we adopt the pairwise comparison approach <cite class=\"ltx_cite ltx_citemacro_citep\">(Mathur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib18\" title=\"\">2020</a>)</cite>.\nRather than evaluating each system independently, we examine the difference between the scores of two systems: <math alttext=\"\\Delta=score(\\text{System A})-score(\\text{System B})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System A</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System B</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=score(\\text{System A})-score(\\text{System B})</annotation></semantics></math>.\nPairwise comparison better reflects the typical use case of latency metrics&#8211;distinguishing between two systems. We also restrict comparisons to system pairs evaluated on the same test set and language pair.</p>\n\n",
                "matched_terms": [
                    "system",
                    "systems",
                    "scores",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Kocmi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib15\" title=\"\">2021</a>)</cite>, we evaluate the accuracy of binary comparisons: given a pair of systems, which one is better according to the true latency ranking (used as silver labels)? The accuracy is defined as the proportion of system pairs for which the relative ranking according to a metric matches that of the true latency:</p>\n\n",
                "matched_terms": [
                    "system",
                    "metric",
                    "pairs",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accuracy considers only the ranking, not the magnitude, of the latency differences, allowing us to aggregate comparisons across languages and test sets with different scales.\nHowever, this accuracy might be affected if two systems have similar latencies. To avoid this issue, we compute the accuracies in multiple subsets\nby removing pairs that are not significantly different according to Mann-Whitney U test on their\ntrue latencies.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We do not assume normal distribution of delays. Each system has different hypotheses, so we cannot use paired tests.</span></span></span> We use bootstrap resampling with <math alttext=\"N=10000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10000</mn></mrow><annotation encoding=\"application/x-tex\">N=10000</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tibshirani and Efron, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib29\" title=\"\">1993</a>)</cite> to estimate confidence intervals and consider all metrics within the 95% confidence interval of the top-performing metric to be statistically tied.</p>\n\n",
                "matched_terms": [
                    "system",
                    "metric",
                    "all",
                    "tied",
                    "pairs",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a pairwise comparison of all short-form systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. An important first observation is that a significant portion of system pairs exhibit no or slightly negative correlations&#8211;points that create almost vertical lines and lines far off the diagonal. These systems share a <em class=\"ltx_emph ltx_font_italic\">anomalous simultaneous policy</em>: The lower the latency of the prefix generated simultaneously, the larger the portion of the sentence translated offline.\nWe assume that the underlying reason for the anomalous policy is that the system is too eager to emit output at the beginning. However, eventually it gets to the &#8220;dead end&#8221; of probable outputs and only emits the tail words at the signaled end of the sentence.\nThis policy, coupled with bias in latency metrics, leads to a severe overestimation of the actual latency of the systems. As we show below, overestimation of actual latency can hinder the reliable detection of systems following this anomalous policy.</p>\n\n",
                "matched_terms": [
                    "system",
                    "policy",
                    "all",
                    "shortform",
                    "anomalous",
                    "pairs",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To detect the anomalous policy, we propose comparing the observed and expected fractions of simultaneously translated words, based on automatic latency. The observed fraction of words translated strictly online across the entire test set <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>, i.e.,</p>\n\n",
                "matched_terms": [
                    "policy",
                    "anomalous"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The expected fraction of words translated online based on the value <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> of an automatic latency metric is <math alttext=\"O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mtext>avg</mtext></msub><mo>&#8722;</mo><mi>L</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>X</mi><mtext>avg</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}</annotation></semantics></math>, where <math alttext=\"X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>X</mi><mtext>avg</mtext></msub><annotation encoding=\"application/x-tex\">X_{\\text{avg}}</annotation></semantics></math> is the average segment length.\nIf the expected online-translated word fraction significantly exceeds the observed one, i.e., <math alttext=\"O_{\\text{e}}\\gg O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8811;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\gg O</annotation></semantics></math>, we conclude that the system follows the anomalous policy. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F4\" title=\"In Detecting Anomalous Policy &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the observed and expected fractions of the words translated online based on the proposed YAAL and LAAL metrics. Most systems follow a vertical line, i.e., <math alttext=\"O_{\\text{e}}\\sim O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8764;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\sim O</annotation></semantics></math>. However, based on YAAL, the red and brown circles<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All affected systems were submitted independently by two different teams in IWSLT 2022 and 2023, showing that the anomalous policy is not so uncommon.</span></span></span> show a strong disagreement between <math alttext=\"O^{\\text{YAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>YAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{YAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m7\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. Based on LAAL (crosses), we observe some disagreement between <math alttext=\"O^{\\text{LAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>LAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{LAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m9\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. However, the trend is not as clear as with YAAL, making the anomalous policy detection difficult. <em class=\"ltx_emph ltx_font_italic\">This shows the importance of accurate evaluation of latency using YAAL.</em></p>\n\n",
                "matched_terms": [
                    "policy",
                    "system",
                    "yaal",
                    "laal",
                    "metric",
                    "anomalous",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3\" title=\"3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and empirically observed in this section, short-form evaluation can significantly distort latency evaluation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T2\" title=\"In Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, we present the average fraction\nof words translated <span class=\"ltx_text ltx_font_italic\">after</span> the end-of-segment signal, i.e., words omitted during evaluation by most latency metrics. A substantial portion of the translations are tail words, starting at 41% in the low-latency (1-2s) and reaching 72% in the high-latency regimes (4-5s).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Systems with higher-latency behavior have policies leading to deferred delays, and these delays in turn are more likely to overflow the source duration.</span></span></span>\n<span class=\"ltx_text ltx_font_italic\">Short-form evaluation, with artificial segment boundaries absent in real-world scenarios and metrics&#8217; problematic handling of tail words, often misrepresents SimulST system behavior.</span>\nThis raises serious concerns about\nits reliability\nand underscores the need for long-form evaluation,\nwhich we analyze in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS2\" title=\"5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "system",
                    "shortform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we evaluate two resegmentation tools: mWERSegmenter and our proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>.\nThe evaluation is done on reconcatenated short-form\noutputs, allowing us to compare with gold segment boundaries.\nAs we can see in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_italic\">the accuracy of\nlatency evaluation is significantly higher with the proposed segmenter</span>.\nWhen filtering out comparable systems by the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value,\naccuracy\nfurther decreases\nwith mWERSegmenter, suggesting that the segmentation is not stable.\nBoth segmentation methods achieve over 99% accuracy, indicating resegmentation does not hinder quality assessment.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the accuracy of StreamLAAL that uses resegmentation and the original short-form metrics evaluated <em class=\"ltx_emph ltx_font_italic\">without resegmentation</em> on long-form systems.\nWe see that the accuracies are low, not exceeding 66% on all systems.\nCompared to StreamLAAL, the best-performing AL metric loses 15 to 16% absolute, and the gap is even wider compared to the proposed LongYAAL, with AL falling short by 29 points.\nThe lower part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\nreports the accuracy of latency metrics in long-form systems <em class=\"ltx_emph ltx_font_italic\">with resegmentation</em>.\nWe see that the resegmentation quality significantly influences the accuracy. StreamLAAL and LongLAAL share the same definition, but differ in the resegmentation&#8211;while StreamLAAL uses the mWERSegmenter, LongLAAL (and all other\n&#8220;Long-&#8221; metrics) uses our <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>. The gap in accuracy is 8 to 10 points in all subsets, showing trends similar to those in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\n<span class=\"ltx_text ltx_font_italic\">These results highlight the critical role of resegmentation in ensuring reliable latency evaluation in the long-form regime.</span> Additional observations are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4\" title=\"Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "all",
                    "shortform",
                    "regime",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows\nthat the proposed LongYAAL has the highest accuracy in all subsets. LongATD and LongDAL show slightly worse results, but not\nstatistically significant. This contrasts with &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, where ATD and DAL perform worse than *AL metrics.\nWe explain this discrepancy by the fact that both metrics include tail words that rarely occur in the long-form regime.\nWe attribute the marginal difference of LongATD compared to LongYAAL to its assumption of 300ms words in the source speech, which is dynamic in LongYAAL in the form of the parameter <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and the difference in LongDAL is caused by the artificial minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E4\" title=\"In Differentiable Average Lagging (DAL; Cherry and Foster, 2019) &#8227; 2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nLongLAAL ties with LongYAAL in most subsets, but seems slightly worse when in easily distinguishable systems (<math alttext=\"p\\text{-val}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>-val</mtext></mrow><annotation encoding=\"application/x-tex\">p\\text{-val}</annotation></semantics></math>&lt;0.001), where the metric loses 2 points in accuracy. LongLAAL, unlike LongYAAL, disregards words generated beyond the reference segment boundaries. The number of words ignored increases with the true latency of the system (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1.SSS0.Px3\" title=\"Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), which is more prevalent in this subset. Similarly, LongAL ignores the tail words in the resegmentation and is also vulnerable to overgeneration <cite class=\"ltx_cite ltx_citemacro_cite\">Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>); Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite>. Finally, AP performs the worst, losing more than 21 points compared to other metrics, which we attribute to the metric&#8217;s sensitivity to variable segment length. <span class=\"ltx_text ltx_font_italic\">These results position LongYAAL as the most reliable metric for assessing latency in long-form SimulST</span>.</p>\n\n",
                "matched_terms": [
                    "system",
                    "metric",
                    "dal",
                    "all",
                    "atd",
                    "regime",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented the first systematic evaluation of latency metrics for SimulST across several aspects, such as diverse systems, language pairs, and operating under short- and long-form speech processing. We have identified current pitfalls in the SimulST evaluation by isolating issues in the most commonly used metrics. To overcome these limitations, we propose YAAL, a new latency metric\nbetter\naligned with the\nshort-form evaluation regime.\nHowever,\nour analysis also reveals inherent shortcomings of short-form evaluation, further reinforcing the adoption of long-form evaluation as a more reliable alternative.\nMoreover, we also demonstrated that resegmentation is necessary to conduct a proper evaluation of systems operating under the long-form regime, and proposed an improved resegmentation tool coupled with the extension of YAAL for these settings&#8211;LongYAAL. The results showed that YAAL and LongYAAL improve over all other metrics in both regimes, establishing the new state-of-the-art metric for SimulST.</p>\n\n",
                "matched_terms": [
                    "yaal",
                    "metric",
                    "all",
                    "shortform",
                    "pairs",
                    "regime",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work introduces new evaluation tools that could influence future benchmarking of SimulST systems. However, there is a risk that over-reliance on specific metrics&#8211;even improved ones like YAAL and LongYAAL&#8211;could lead to overfitting system design to particular evaluation settings. For example, systems might be tuned to perform well under LongYAAL but degrade in real-world conditions that are not fully captured by the metric. Additionally, the use of automatic resegmentation methods may inadvertently introduce subtle biases if misaligned with human interpretation of segment boundaries. We encourage the community to use these tools alongside qualitative analysis and human-in-the-loop evaluations where possible.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "system",
                    "systems",
                    "yaal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We did not train any models as part of this study. However, we used several evaluations that required computation. Most of the experiments were conducted on a standard desktop computer equipped with an Intel i7 processor and 32GB of RAM. For forced alignments with neural models, machine translation alignment, and the COMET translation quality metric, we used\na GPU cluster.\nHowever, these evaluations can be done on a desktop machine with a slightly longer runtime. The proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, YAAL, and LongYAAL can be run efficiently on a CPU.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "yaal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the short-form regime, we use systems submitted to the IWSLT Simultaneous Speech Translation tracks of 2022 and 2023. Specifically, we use the SimulEval evaluation logs of the IWSLT 2022 and 2023 test sets <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, and the logs of the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> test set of the MuST-C data set <cite class=\"ltx_cite ltx_citemacro_citep\">(Cattoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib6\" title=\"\">2021</a>)</cite> that were submitted to IWSLT 2022. For the long-form regime, the logs are sourced from IWSLT 2025. In particular, for English-to-{German, Chinese, Japanese} the evaluation was done on the development set of the ACL 60/60 dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Salesky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib28\" title=\"\">2023</a>)</cite>, and IWSLT 2025 test set. For the Czech-to-English language pair, the evaluation was performed on the IWSLT 2024 development set <cite class=\"ltx_cite ltx_citemacro_cite\">Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite> and the IWSLT 2025 test set. A portion of the IWSLT 2024 development set contained segmented audio that could not be reconstructed into the original unsegmented audio.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T5\" title=\"In Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T7\" title=\"Table 7 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we present the number of systems used in the short- and long-form evaluations. The number of systems available to us was slightly larger, but we excluded all systems where the logs were incomplete (e.g., predictions for all recordings were not present, mismatched order of sources and hypotheses). Furthermore, in the long-form regime, we excluded one team entirely from the evaluation due to faulty logs. These logs contained a different number of predicted words and delays, which means that we could not faithfully determine generation timestamps for each predicted word.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "all",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Same as in the short-form evaluation, we follow the definition of the true latency in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4\" title=\"4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nHowever, there are two differences.\nAfter initial experiments, we observed that the Montreal Forced Aligner used in the short-form regime is not robust for the challenging conditions of the IWSLT 2025 test set, which is based on ACL presentations. The recordings include frequent restarts, repetitions, domain-specific terminology, and non-native speech. Instead, we use the alignment method implemented within WhisperX <cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib5\" title=\"\">2023</a>)</cite> for forced alignment. This tool leverages neural speech encoders that seem to be robust to the above-mentioned challenges. In particular, we used WhisperX&#8217;s default settings, i.e., PyTorch&#8217;s <span class=\"ltx_text ltx_font_typewriter\">WAV2VEC2_ASR_BASE_960H</span> for English and <span class=\"ltx_text ltx_font_typewriter\">comodoro/wav2vec2-xls-r-300m-cs-250</span> for Czech speech forced alignments.\nSecond, we perform resegmentation of the system hypotheses prior to the machine translation alignment with the reference. This step is necessary because the <span class=\"ltx_text ltx_font_typewriter\">awesome-align</span> tool uses the <span class=\"ltx_text ltx_font_typewriter\">bert-base-multilingual-cased</span> model for the alignment, and this model has a maximum input length of 512 tokens, which is much lower than the system hypotheses.</p>\n\n",
                "matched_terms": [
                    "system",
                    "shortform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A natural question arises: <em class=\"ltx_emph ltx_font_italic\">Why rely on automatic latency metrics at all, when true latency offers a closer approximation of user experience?</em> In practice, computing true latency requires several requirements that limit its applicability. High-quality transcripts must be available, which is often not the case&#8211;particularly for low-resource languages or unwritten languages where transcription is infeasible. Moreover, forced alignment tools and reliable word-level translation alignments are typically available only for a small set of high-resource language pairs. Even when such resources exist, computing true latency involves multiple processing steps and is substantially more complex than evaluating standard automatic metrics. Importantly, as we show in our analysis in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5\" title=\"5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, several automatic metrics approximate true latency with high accuracy, making them a practical and effective alternative in most evaluation scenarios.</p>\n\n",
                "matched_terms": [
                    "all",
                    "accuracy",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F6\" title=\"In Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we illustrate the trends after filtering out the systems affected by the anomalous policy (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). Unlike in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we see that all metrics and system pairs show a positive correlation with the true latency. As mentioned in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, language pairs exhibit different scales, making the use of the correlation coefficient more cumbersome and motivating the use of accuracy as described in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4.SS2.SSS0.Px3\" title=\"Accuracy &#8227; 4.2 Evaluation &#8227; 4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "system",
                    "policy",
                    "all",
                    "anomalous",
                    "pairs",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To this end, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F7\" title=\"In Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figures</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F8\" title=\"Figure 8 &#8227; Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we also offer the accuracy of subsets of system pairs based on the absolute difference in the true latency.</p>\n\n",
                "matched_terms": [
                    "system",
                    "accuracy",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We were also interested in the accuracy of latency metrics when comparing related against unrelated systems. In our evaluation, we consider the systems submitted by one team as related.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>To the best of our knowledge, most teams submitted multiple systems that were based on the same system with varying hyperparameters.</span></span></span> We also use only a subset of the systems that were not affected by the anomalous simultaneous policy. The results are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "system",
                    "policy",
                    "anomalous",
                    "best",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Surprisingly, when evaluating related systems, all metrics perform almost perfectly, reaching accuracy between 97% and 100%. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F9\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency. Given a difference of at least <math alttext=\"\\sim 250\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS0.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>250</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 250</annotation></semantics></math> ms, all metrics except AP achieve 100% accuracy, and AP achieves around 99% accuracy.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "all",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F11\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we show pairwise comparisons of systems evaluated in the long-form regime without resegmentation, and in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F12\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, we show the same systems evaluated in the long-form regime, but after resegmentation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F13\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "accuracy",
                    "regime"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation",
        "caption": "Table 2: Average fraction of words generated after the end-of-segment signal under the short-form evaluation regime, averaged across all systems.",
        "body": "Latency regime [s]\n1-2\n2-3\n3-4\n4-5\n\n\n\n\nTail Words [%]\n41\n49\n63\n72",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\">Latency regime [s]</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">1-2</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">2-3</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">3-4</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">4-5</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">Tail Words [%]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">72</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "averaged",
            "endofsegment",
            "across",
            "systems",
            "under",
            "shortform",
            "fraction",
            "evaluation",
            "all",
            "signal",
            "regime",
            "words",
            "average",
            "generated",
            "after",
            "latency",
            "tail"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3\" title=\"3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and empirically observed in this section, short-form evaluation can significantly distort latency evaluation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T2\" title=\"In Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, we present the average fraction\nof words translated <span class=\"ltx_text ltx_font_italic\">after</span> the end-of-segment signal, i.e., words omitted during evaluation by most latency metrics. A substantial portion of the translations are tail words, starting at 41% in the low-latency (1-2s) and reaching 72% in the high-latency regimes (4-5s).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Systems with higher-latency behavior have policies leading to deferred delays, and these delays in turn are more likely to overflow the source duration.</span></span></span>\n<span class=\"ltx_text ltx_font_italic\">Short-form evaluation, with artificial segment boundaries absent in real-world scenarios and metrics&#8217; problematic handling of tail words, often misrepresents SimulST system behavior.</span>\nThis raises serious concerns about\nits reliability\nand underscores the need for long-form evaluation,\nwhich we analyze in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS2\" title=\"5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency&#8211;the delay between speech input and the translated output.\nWhile quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented.\nIn this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes.\nWe uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more\naccurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "shortform",
                    "evaluation",
                    "regime",
                    "latency",
                    "average",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) is the task in which the system produces incremental translation concurrently with the speaker&#8217;s speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib27\" title=\"\">2020</a>)</cite>. SimulST models have to balance between quality and latency of the output, which is the time elapsed between when a word is uttered and when its corresponding translation is produced. Although translation quality metrics are extensively studied in offline ST and in the related field of machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Freitag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib11\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib10\" title=\"\">2023</a>; Zouhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib30\" title=\"\">2024</a>)</cite>, there is no study on the reliability of latency metrics.\nThe most common latency\nmetrics in SimulST <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho and Esipova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib8\" title=\"\">2016</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib16\" title=\"\">2019</a>; Cherry and Foster, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib7\" title=\"\">2019</a>; Pol&#225;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>; Kano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib14\" title=\"\">2023</a>)</cite>, even though with different approximations, base their calculation on\nsimplifying assumptions such as uniform word duration, absence of pauses, and strict monotonic alignment between source speech and target translation.\nHowever, despite relying on\nthe same assumptions, these metrics often produce very inconsistent assessments.\nThis inconsistency is clearly illustrated in the results of the IWSLT 2023 Shared Task on Simultaneous Translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, where different metrics produced substantially different rankings (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S1.F1\" title=\"In 1 Introduction &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). Such variability raises serious concerns about the validity of current evaluation protocols and their ability to support meaningful comparisons between systems.\nMoreover, this risk can be further exacerbated when shifting from dealing with already presegmented speech, i.e., <span class=\"ltx_text ltx_font_italic\">short-form</span> SimulST, to unsegmented audio, i.e., <span class=\"ltx_text ltx_font_italic\">long-form</span> SimulST, where information about sentence boundaries is not available, further complicating the evaluation <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "evaluation",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present the first comprehensive evaluation of latency metrics for SimulST under several aspects, including diverse systems, language pairs, and short- and long-form regimes. Through an in-depth analysis of systems submitted to recent IWSLT SimulST Shared Tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>; Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, we reveal that existing metrics can lead to misleading conclusions and hinder effective system design. We show that the inconsistent evaluations are not primarily due to the aforementioned assumptions, but rather to a structural bias in how latency is measured&#8211;particularly\nin how segmentation influences SimulST models&#8217; behavior.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "under",
                    "evaluation",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose YAAL (Yet Another Average Lagging), a refined latency metric designed to mitigate the biases present in existing latency metrics.\nOur extensive experiments demonstrate that YAAL yields more reliable latency estimates, consistently aligning better with the actual behavior of SimulST systems.\nFurthermore, we also show that resegmentation, which pairs segment-level predictions with their corresponding reference, is necessary to produce meaningful latency measurements for long-form SimulST. To this end, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a new resegmentation tool, and extend our YAAL to LongYAAL, which deals with audio streams. Compared to the current standard alignment tool used in the speech translation community <cite class=\"ltx_cite ltx_citemacro_citep\">(Matusov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> significantly improves alignment quality, enabling more accurate evaluation in long-form scenarios.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The code for YAAL, its long-form variant LongYAAL, and <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> will be released upon the paper acceptance under Apache 2.0 license.</span></span></span></p>\n\n",
                "matched_terms": [
                    "under",
                    "evaluation",
                    "latency",
                    "average",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The short-form is the most common evaluation regime of SimulST <cite class=\"ltx_cite ltx_citemacro_cite\">Anastasopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>); Agarwal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>); Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, where all recordings of the test set are divided, usually following sentence boundaries, into short segments of a few seconds.\nEach segment consists of source audio <math alttext=\"\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is a small portion of raw audio, i.e., audio chunk, and reference translation <math alttext=\"\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>y</mi><mn>1</mn><mi>R</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>y</mi><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow><mi>R</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]</annotation></semantics></math>. Each\naudio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is incrementally fed to the system,\nwhich concurrently\noutputs a translation token <math alttext=\"y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">y_{j}</annotation></semantics></math> at timestamp <math alttext=\"d_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">d_{j}</annotation></semantics></math>, i.e., total duration of audio chunks up to and including the audio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math>. Under these settings, we describe the latency metrics operating in the short-form regime:</p>\n\n",
                "matched_terms": [
                    "all",
                    "under",
                    "evaluation",
                    "shortform",
                    "regime",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">for simultaneous machine translation and modified for speech by <cite class=\"ltx_cite ltx_citemacro_citet\">Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib17\" title=\"\">2020</a>)</cite> defines the latency as the average delay behind an ideal policy:</p>\n\n",
                "matched_terms": [
                    "average",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">modifies AL by introducing\na minimal delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> after each\nstep.\nUnlike AL and LAAL, DAL considers all tokens in the hypothesis,\nwithout\ncutoff after <math alttext=\"i&gt;\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\mathbf{X})</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "all",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form evaluation regime evaluates SimulST systems more realistically <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>, as it assesses their ability to handle long audio streams, often spanning several minutes.\nSince all metrics were developed for the short-form regime,\nrecent studies <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>\nresorted to resegmentation of translations and delays based on the reference translation <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib20\" title=\"\">2005b</a>)</cite>, and computed the metrics on the segment level. We explain the long-form variant of LAAL <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite> below.</p>\n\n",
                "matched_terms": [
                    "all",
                    "shortform",
                    "evaluation",
                    "regime",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Where <math alttext=\"d^{*}_{i}=(i-1)\\cdot|\\mathbf{X_{s}}|/{\\max\\{|\\mathbf{Y_{s}}|,|\\mathbf{Y^{R}_{s}|\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi>d</mi><mi>i</mi><mo>&#8727;</mo></msubsup><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#8901;</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>/</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119832;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msubsup><mi>&#119832;</mi><mi>&#119852;</mi><mi>&#119825;</mi></msubsup><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d^{*}_{i}=(i-1)\\cdot|\\mathbf{X_{s}}|/{\\max\\{|\\mathbf{Y_{s}}|,|\\mathbf{Y^{R}_{s}|\\}}}</annotation></semantics></math>\nIn practice, the LAAL metric is calculated for every speech segment <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math> of the stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> and its corresponding reference <math alttext=\"\\mathbf{Y^{R}_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119832;</mi><mi>&#119852;</mi><mi>&#119825;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}_{s}}</annotation></semantics></math> with the automatically aligned prediction <math alttext=\"\\mathbf{Y_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{s}}</annotation></semantics></math> and then averaged over all the speech segments of the stream <math alttext=\"\\mathbf{X_{1}},...,\\mathbf{X_{|\\mathbf{S}|}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><msub><mi>&#119831;</mi><mn>&#120783;</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119831;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X_{1}},...,\\mathbf{X_{|\\mathbf{S}|}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "averaged"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, <cite class=\"ltx_cite ltx_citemacro_citet\">Huber et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib12\" title=\"\">2023</a>)</cite> proposed an evaluation with resegmentation provided by the system. The evaluation framework expects that the system outputs the segment&#8217;s start and end timestamps that align with the source. First, most systems, including all the IWSLT systems, do not output this information, and relying on the system&#8217;s self-reported alignment might hinder the reliability. Second, as we empirically show in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, relying on potentially low-quality resegmentation significantly lowers the accuracy of latency evaluation. Finally, different segmentations render the observed latencies incomparable across different systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "all",
                    "evaluation",
                    "latency",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of audio segmentation in short-form evaluations significantly affects translation behavior and latency. In practice, short-form SimulST systems are evaluated in a simulated environment where each segment is processed independently <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib17\" title=\"\">2020</a>)</cite>. When the entire source segment has been consumed by the system, the translation is often still in progress. At that point, the simulator requests the remaining translation, which the model emits without any additional delay.\nThis setup introduces two unrealistic conditions. First, the audio is typically segmented in advance by a human annotator or an automatic model with access to the full audio (<span class=\"ltx_text ltx_font_italic\">Oracle Segmentation</span>). Second, the model is allowed to generate the remaining translation (hereinafter, <span class=\"ltx_text ltx_font_italic\">tail words</span>) instantaneously once the input segment ends. These factors unduly distort short-form evaluations, both by providing high-quality segmentation and eliminating the delay that would occur in a realistic setting, where the system must wait to confirm that the sentence has ended. In a more realistic scenario, a model has to rely on online segmentation (<span class=\"ltx_text ltx_font_italic\">Simultaneous Segmentation</span>) and thus delay the final translation until it is confident that the input sentence is complete, thereby introducing extra latency.\nThis discrepancy is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "latency",
                    "words",
                    "systems",
                    "tail"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on these observations, we categorize existing short-form latency metrics (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.SS1\" title=\"2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>) into two main groups, depending on whether they include all translated words or only a subset in their latency computation. The first group&#8211;AP, DAL, and ATD&#8211;includes all translated words in the calculation.\nDAL attempts to mitigate the impact of tail words by adding a minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> after each generated word (also within the same step), thus &#8220;spreading&#8221; the tail beyond the sentence.\nHowever, <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math>\nsimply reflects the average source-to-target length ratio and does not accurately capture the system behavior for tail words in settings without segmentation.\nIf multiple words are emitted as tail words, DAL can significantly overestimate\nlatency.\nIn the edge case of a system that waits for an end-of-segment signal (i.e., an offline system), DAL\nreturns the segment length, failing to capture the system&#8217;s true behavior&#8211;in this case, undefined latency.\nAP assigns a delay of 1 to each tail word\nas the entire recording has to be processed to emit that word, thus, the proportion is 1. Although AP is marginally less sensitive to segmentation than DAL&#8211;since it operates on proportions rather than absolute delays&#8211;it still fails to capture system behavior faithfully for the tail words.\nATD also considers all translated words.\nHowever, unlike DAL, it does not apply corrections for tail word behavior, making it the most sensitive to segmentation artifacts among the three metrics.</p>\n\n",
                "matched_terms": [
                    "after",
                    "all",
                    "shortform",
                    "signal",
                    "latency",
                    "words",
                    "average",
                    "generated",
                    "endofsegment",
                    "tail"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second group&#8211;AL and LAAL&#8211;computes latency only for words emitted up to and including the cutoff point <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>, which marks the first word generated after the end of the input segment.\nThis corresponds to the word &#8220;<span class=\"ltx_text ltx_font_italic\">gemeinn&#252;tzige</span>&#8221; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nAs discussed, in the short-form regime with oracle segmentation, the <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>-th and following words are often translated earlier than in a more realistic\nlong-form scenario.\nAs a result, this cutoff introduces a systematic bias in the latency estimate, which may lead to either underestimation or overestimation, depending on the system&#8217;s policy.</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "regime",
                    "words",
                    "latency",
                    "generated",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AP, DAL, ATD, AL, and, more recently, LAAL became established metrics in the short-form evaluation of SimulST. However, as discussed above, including any of the tail words in the latency computation leads to a systematic bias that undermines fair comparisons.\nTo cope with this bias, we propose a new metric derived from the LAAL metric:</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "evaluation",
                    "latency",
                    "words",
                    "tail"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We refine the LAAL formulation to better isolate the portion of output that is actually produced in simultaneous settings. Specifically, we define a new cutoff point\n<math alttext=\"\\tau_{\\text{YAAL}}(\\mathbf{X})=\\text{max}\\{i|d_{i}&lt;|\\mathbf{X}|\\},\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><msub><mi>&#964;</mi><mtext>YAAL</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mtext>max</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">{</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">|</mo><mrow><msub><mi>d</mi><mi>i</mi></msub><mo>&lt;</mo><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\tau_{\\text{YAAL}}(\\mathbf{X})=\\text{max}\\{i|d_{i}&lt;|\\mathbf{X}|\\},</annotation></semantics></math>\nwhich includes only words generated strictly before the end of the input stream. This corresponds to words up to and including &#8220;<span class=\"ltx_text ltx_font_italic\">eine</span>&#8221; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, thereby avoiding distortion from tail words and yielding a more reliable latency estimate that remains consistent across different segmentation regimes.</p>\n\n",
                "matched_terms": [
                    "across",
                    "latency",
                    "words",
                    "generated",
                    "tail"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form regime offers a more realistic evaluation setting by assessing systems on continuous, unsegmented audio streams that better reflect real-world use cases. However, the widely used latency metrics were originally designed for the short-form regime and do not directly extend to this setting.</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "evaluation",
                    "latency",
                    "regime",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, metrics such as AL, LAAL, and DAL\nrely on a <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> parameter,\nrepresenting the average target-to-source length ratio. However, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> can vary substantially across different segments within the same audio, leading to inconsistent and unreliable latency estimates <cite class=\"ltx_cite ltx_citemacro_cite\">Iranzo-S&#225;nchez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib13\" title=\"\">2021</a>)</cite>.\nSecond, AP\ntends to converge toward 0 for\nlong recordings,\nas typical speech inputs are significantly longer than the translations,\ni.e., <math alttext=\"|\\mathbf{X}|\\gg|\\mathbf{Y}|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow><mo>&#8811;</mo><mrow><mo stretchy=\"false\">|</mo><mi>&#119832;</mi><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">|\\mathbf{X}|\\gg|\\mathbf{Y}|</annotation></semantics></math>,\nleading <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E1\" title=\"In Average Proportion (AP; Cho and Esipova, 2016) &#8227; 2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> to approach 0.\nFinally, ATD\nassumes that each speech token has a fixed duration and that source and target tokens align monotonically&#8211;assumptions that are overly restrictive and especially unrealistic for long-form speech.</p>\n\n",
                "matched_terms": [
                    "average",
                    "latency",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, StreamLAAL has introduced resegmenting long inputs into short segments and computing latency on these units. While StreamLAAL provides adaptation of existing metrics to long-form input, it relies on the mWERSegmenter tool <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, which may introduce alignment errors <cite class=\"ltx_cite ltx_citemacro_cite\">Amrhein and Haddow (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib3\" title=\"\">2022</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>, and\ncomputes latency up to the cutoff word <math alttext=\"\\tau(\\mathbf{X_{i}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mi>&#119842;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X_{i}})</annotation></semantics></math> (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E7\" title=\"In Streaming LAAL (StreamLAAL; Papi et al., 2024) &#8227; 2.2 Long-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>), which can lead to the systematic bias (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.SS1\" title=\"3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>)\nTo overcome these limitations, we propose a new resegmentation method and an extension of the YAAL metric for the long-form regime.</p>\n\n",
                "matched_terms": [
                    "latency",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also extend YAAL to the long-form regime&#8211;i.e., LongYAAL. Unlike StreamLAAL, LongYAAL includes all words in the latency computation, even those generated beyond the aligned segment boundaries <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math>, i.e., all <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for <math alttext=\"i&gt;\\tau(\\mathbf{X_{s}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\mathbf{X_{s}})</annotation></semantics></math>. However, we exclude the final tail words produced after the end of the full stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>, i.e., <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for <math alttext=\"i&gt;\\tau(\\sum_{s=1}^{|\\mathbf{X}|}|\\mathbf{X_{s}}|)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8721;</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msubsup><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\sum_{s=1}^{|\\mathbf{X}|}|\\mathbf{X_{s}}|)</annotation></semantics></math>. This ensures that we include all words emitted beyond the segment boundaries <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math>, but we do not include the tail words generated at the end of the entire stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>. If the stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> consists of a single segment, LongYAAL coincides with YAAL.</p>\n\n",
                "matched_terms": [
                    "all",
                    "latency",
                    "words",
                    "generated",
                    "after",
                    "tail"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable\nfair comparisons across latency metrics, we require a reference latency reflecting the user experience, i.e., how long the user needs to wait for translation.\nSince human evaluation is infeasible at scale,\nwe adopt a carefully designed automatic approximation, which we refer to as <span class=\"ltx_text ltx_font_italic\">true latency</span>.\nThis is grounded in an intuitive and practical definition of latency in speech translation:\n<em class=\"ltx_emph ltx_font_italic\">On average, how long does a user have to wait for a given piece of source information to appear in the translation?</em>\nConcretely, we define true latency as the average delay between each target word and its corresponding source word:</p>\n\n",
                "matched_terms": [
                    "average",
                    "latency",
                    "evaluation",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> is the emission time of the target word <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> and <math alttext=\"d^{src}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>d</mi><mi>i</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">d^{src}_{i}</annotation></semantics></math> is the corresponding source delay. We define the source delay as the time that the speaker finished the last word corresponding to the target word: <math alttext=\"d^{src}_{i}=\\max_{l}{\\{s^{end}_{l}|(y_{i},s_{l})\\in\\mathcal{A}(\\mathbf{Y}\\rightarrow\\mathbf{S})\\}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi>d</mi><mi>i</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msubsup><mo>=</mo><mrow><msub><mi>max</mi><mi>l</mi></msub><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><msubsup><mi>s</mi><mi>l</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msubsup><mo fence=\"false\">|</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119832;</mi><mo stretchy=\"false\">&#8594;</mo><mi>&#119826;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d^{src}_{i}=\\max_{l}{\\{s^{end}_{l}|(y_{i},s_{l})\\in\\mathcal{A}(\\mathbf{Y}\\rightarrow\\mathbf{S})\\}}</annotation></semantics></math>,\nwhere <math alttext=\"s^{end}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>s</mi><mi>l</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">s^{end}_{l}</annotation></semantics></math> is the end timestamp of the source word <math alttext=\"s_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">s_{l}</annotation></semantics></math> and <math alttext=\"\\mathcal{A}(\\mathbf{Y}\\rightarrow\\mathbf{S})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119832;</mi><mo stretchy=\"false\">&#8594;</mo><mi>&#119826;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}(\\mathbf{Y}\\rightarrow\\mathbf{S})</annotation></semantics></math> is the translation alignment between the target and the source.\nAs discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.SS1\" title=\"3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, computing latency over all words&#8211;including tail words&#8211;can introduce systematic bias. To mitigate this, we restrict the true latency calculation to words generated strictly during simultaneous decoding, i.e., before the end-of-source signal. Additionally, we consider only the subset of target words <math alttext=\"\\mathbf{Y^{A}}\\subseteq\\mathbf{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>&#119808;</mi></msup><mo>&#8838;</mo><mi>&#119832;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{A}}\\subseteq\\mathbf{Y}</annotation></semantics></math> that are aligned to at least one source word, thereby avoiding biases introduced by over- or under-generation <cite class=\"ltx_cite ltx_citemacro_cite\">Pol&#225;k et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>); Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>)</cite>. The implementation details are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A2\" title=\"Appendix B True Latency &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "signal",
                    "latency",
                    "words",
                    "generated",
                    "tail"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the main evaluation, we adopt the pairwise comparison approach <cite class=\"ltx_cite ltx_citemacro_citep\">(Mathur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib18\" title=\"\">2020</a>)</cite>.\nRather than evaluating each system independently, we examine the difference between the scores of two systems: <math alttext=\"\\Delta=score(\\text{System A})-score(\\text{System B})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System A</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System B</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=score(\\text{System A})-score(\\text{System B})</annotation></semantics></math>.\nPairwise comparison better reflects the typical use case of latency metrics&#8211;distinguishing between two systems. We also restrict comparisons to system pairs evaluated on the same test set and language pair.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Kocmi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib15\" title=\"\">2021</a>)</cite>, we evaluate the accuracy of binary comparisons: given a pair of systems, which one is better according to the true latency ranking (used as silver labels)? The accuracy is defined as the proportion of system pairs for which the relative ranking according to a metric matches that of the true latency:</p>\n\n",
                "matched_terms": [
                    "systems",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accuracy considers only the ranking, not the magnitude, of the latency differences, allowing us to aggregate comparisons across languages and test sets with different scales.\nHowever, this accuracy might be affected if two systems have similar latencies. To avoid this issue, we compute the accuracies in multiple subsets\nby removing pairs that are not significantly different according to Mann-Whitney U test on their\ntrue latencies.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We do not assume normal distribution of delays. Each system has different hypotheses, so we cannot use paired tests.</span></span></span> We use bootstrap resampling with <math alttext=\"N=10000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10000</mn></mrow><annotation encoding=\"application/x-tex\">N=10000</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tibshirani and Efron, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib29\" title=\"\">1993</a>)</cite> to estimate confidence intervals and consider all metrics within the 95% confidence interval of the top-performing metric to be statistically tied.</p>\n\n",
                "matched_terms": [
                    "latency",
                    "systems",
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a pairwise comparison of all short-form systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. An important first observation is that a significant portion of system pairs exhibit no or slightly negative correlations&#8211;points that create almost vertical lines and lines far off the diagonal. These systems share a <em class=\"ltx_emph ltx_font_italic\">anomalous simultaneous policy</em>: The lower the latency of the prefix generated simultaneously, the larger the portion of the sentence translated offline.\nWe assume that the underlying reason for the anomalous policy is that the system is too eager to emit output at the beginning. However, eventually it gets to the &#8220;dead end&#8221; of probable outputs and only emits the tail words at the signaled end of the sentence.\nThis policy, coupled with bias in latency metrics, leads to a severe overestimation of the actual latency of the systems. As we show below, overestimation of actual latency can hinder the reliable detection of systems following this anomalous policy.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "all",
                    "shortform",
                    "latency",
                    "words",
                    "systems",
                    "tail"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To detect the anomalous policy, we propose comparing the observed and expected fractions of simultaneously translated words, based on automatic latency. The observed fraction of words translated strictly online across the entire test set <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>, i.e.,</p>\n\n",
                "matched_terms": [
                    "latency",
                    "fraction",
                    "across",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The expected fraction of words translated online based on the value <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> of an automatic latency metric is <math alttext=\"O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mtext>avg</mtext></msub><mo>&#8722;</mo><mi>L</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>X</mi><mtext>avg</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}</annotation></semantics></math>, where <math alttext=\"X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>X</mi><mtext>avg</mtext></msub><annotation encoding=\"application/x-tex\">X_{\\text{avg}}</annotation></semantics></math> is the average segment length.\nIf the expected online-translated word fraction significantly exceeds the observed one, i.e., <math alttext=\"O_{\\text{e}}\\gg O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8811;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\gg O</annotation></semantics></math>, we conclude that the system follows the anomalous policy. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F4\" title=\"In Detecting Anomalous Policy &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the observed and expected fractions of the words translated online based on the proposed YAAL and LAAL metrics. Most systems follow a vertical line, i.e., <math alttext=\"O_{\\text{e}}\\sim O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8764;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\sim O</annotation></semantics></math>. However, based on YAAL, the red and brown circles<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All affected systems were submitted independently by two different teams in IWSLT 2022 and 2023, showing that the anomalous policy is not so uncommon.</span></span></span> show a strong disagreement between <math alttext=\"O^{\\text{YAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>YAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{YAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m7\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. Based on LAAL (crosses), we observe some disagreement between <math alttext=\"O^{\\text{LAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>LAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{LAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m9\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. However, the trend is not as clear as with YAAL, making the anomalous policy detection difficult. <em class=\"ltx_emph ltx_font_italic\">This shows the importance of accurate evaluation of latency using YAAL.</em></p>\n\n",
                "matched_terms": [
                    "fraction",
                    "evaluation",
                    "latency",
                    "words",
                    "average",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the metrics, we observe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>) that they all show positive correlations with the true latency, but each language pair has a slightly different scale, which <em class=\"ltx_emph ltx_font_italic\">motivates the use of accuracy instead of simple correlation</em>.\nTherefore, we further compare latency metrics in terms of accuracy in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. If we consider all system pairs (including systems following the anomalous policy), we see that all metrics significantly underperform YAAL (by more than 22% absolute), which reaches an accuracy of 96%. When we progressively filter out system pairs with similar true latency (decreasing <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> values), the accuracies slightly increase, but the ranking of the metrics remains. If we consider a subset that has a <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value between 0.001-0.05 (i.e., removing pairs with similar true latency that are, however, more difficult to distinguish), we see that YAAL still remains the most accurate by a margin of 25% absolute.\nIf we remove systems with the anomalous policy,\nall metrics gain a significant boost in accuracy (bottom part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). However, the YAAL metric remains the best metric in all subsets based on <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values, achieving 98 and 99% accuracy&#8211;even though it relies on assumptions such as uniform source token durations and monotonic source-to-target alignment.\nBased on these observations, we conclude that\n<span class=\"ltx_text ltx_font_italic\">the automatic YAAL metric is almost as accurate as\ntrue latency</span>. We include more accuracy evaluations by isolating different categories of systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3\" title=\"Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "all",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we evaluate two resegmentation tools: mWERSegmenter and our proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>.\nThe evaluation is done on reconcatenated short-form\noutputs, allowing us to compare with gold segment boundaries.\nAs we can see in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_italic\">the accuracy of\nlatency evaluation is significantly higher with the proposed segmenter</span>.\nWhen filtering out comparable systems by the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value,\naccuracy\nfurther decreases\nwith mWERSegmenter, suggesting that the segmentation is not stable.\nBoth segmentation methods achieve over 99% accuracy, indicating resegmentation does not hinder quality assessment.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "evaluation",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the accuracy of StreamLAAL that uses resegmentation and the original short-form metrics evaluated <em class=\"ltx_emph ltx_font_italic\">without resegmentation</em> on long-form systems.\nWe see that the accuracies are low, not exceeding 66% on all systems.\nCompared to StreamLAAL, the best-performing AL metric loses 15 to 16% absolute, and the gap is even wider compared to the proposed LongYAAL, with AL falling short by 29 points.\nThe lower part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\nreports the accuracy of latency metrics in long-form systems <em class=\"ltx_emph ltx_font_italic\">with resegmentation</em>.\nWe see that the resegmentation quality significantly influences the accuracy. StreamLAAL and LongLAAL share the same definition, but differ in the resegmentation&#8211;while StreamLAAL uses the mWERSegmenter, LongLAAL (and all other\n&#8220;Long-&#8221; metrics) uses our <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>. The gap in accuracy is 8 to 10 points in all subsets, showing trends similar to those in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\n<span class=\"ltx_text ltx_font_italic\">These results highlight the critical role of resegmentation in ensuring reliable latency evaluation in the long-form regime.</span> Additional observations are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4\" title=\"Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "shortform",
                    "evaluation",
                    "regime",
                    "latency",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows\nthat the proposed LongYAAL has the highest accuracy in all subsets. LongATD and LongDAL show slightly worse results, but not\nstatistically significant. This contrasts with &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, where ATD and DAL perform worse than *AL metrics.\nWe explain this discrepancy by the fact that both metrics include tail words that rarely occur in the long-form regime.\nWe attribute the marginal difference of LongATD compared to LongYAAL to its assumption of 300ms words in the source speech, which is dynamic in LongYAAL in the form of the parameter <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and the difference in LongDAL is caused by the artificial minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E4\" title=\"In Differentiable Average Lagging (DAL; Cherry and Foster, 2019) &#8227; 2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nLongLAAL ties with LongYAAL in most subsets, but seems slightly worse when in easily distinguishable systems (<math alttext=\"p\\text{-val}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>-val</mtext></mrow><annotation encoding=\"application/x-tex\">p\\text{-val}</annotation></semantics></math>&lt;0.001), where the metric loses 2 points in accuracy. LongLAAL, unlike LongYAAL, disregards words generated beyond the reference segment boundaries. The number of words ignored increases with the true latency of the system (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1.SSS0.Px3\" title=\"Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), which is more prevalent in this subset. Similarly, LongAL ignores the tail words in the resegmentation and is also vulnerable to overgeneration <cite class=\"ltx_cite ltx_citemacro_cite\">Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>); Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite>. Finally, AP performs the worst, losing more than 21 points compared to other metrics, which we attribute to the metric&#8217;s sensitivity to variable segment length. <span class=\"ltx_text ltx_font_italic\">These results position LongYAAL as the most reliable metric for assessing latency in long-form SimulST</span>.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "all",
                    "regime",
                    "words",
                    "latency",
                    "systems",
                    "tail"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented the first systematic evaluation of latency metrics for SimulST across several aspects, such as diverse systems, language pairs, and operating under short- and long-form speech processing. We have identified current pitfalls in the SimulST evaluation by isolating issues in the most commonly used metrics. To overcome these limitations, we propose YAAL, a new latency metric\nbetter\naligned with the\nshort-form evaluation regime.\nHowever,\nour analysis also reveals inherent shortcomings of short-form evaluation, further reinforcing the adoption of long-form evaluation as a more reliable alternative.\nMoreover, we also demonstrated that resegmentation is necessary to conduct a proper evaluation of systems operating under the long-form regime, and proposed an improved resegmentation tool coupled with the extension of YAAL for these settings&#8211;LongYAAL. The results showed that YAAL and LongYAAL improve over all other metrics in both regimes, establishing the new state-of-the-art metric for SimulST.</p>\n\n",
                "matched_terms": [
                    "across",
                    "under",
                    "shortform",
                    "evaluation",
                    "all",
                    "regime",
                    "latency",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our study offers a thorough evaluation of latency metrics for SimulST and introduces improved tools for both short- and long-form regimes, some limitations remain. First, our evaluation depends on reference translations and transcriptions, which may not be available or reliable in low-resource or real-time scenarios. Second, although the proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> improves alignment robustness, word-level alignment is still susceptible to errors in the presence of disfluencies or speech recognition noise. Third, our experimental analysis focuses on systems from the IWSLT Shared Tasks, which may not fully represent the range of techniques or data conditions used in broader academic or industrial settings. Fourth, our analysis focuses on high-resource languages, for which data were available, but the findings should be reconfirmed under low-resource language settings.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "under",
                    "evaluation",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work introduces new evaluation tools that could influence future benchmarking of SimulST systems. However, there is a risk that over-reliance on specific metrics&#8211;even improved ones like YAAL and LongYAAL&#8211;could lead to overfitting system design to particular evaluation settings. For example, systems might be tuned to perform well under LongYAAL but degrade in real-world conditions that are not fully captured by the metric. Additionally, the use of automatic resegmentation methods may inadvertently introduce subtle biases if misaligned with human interpretation of segment boundaries. We encourage the community to use these tools alongside qualitative analysis and human-in-the-loop evaluations where possible.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "under",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the short-form regime, we use systems submitted to the IWSLT Simultaneous Speech Translation tracks of 2022 and 2023. Specifically, we use the SimulEval evaluation logs of the IWSLT 2022 and 2023 test sets <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, and the logs of the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> test set of the MuST-C data set <cite class=\"ltx_cite ltx_citemacro_citep\">(Cattoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib6\" title=\"\">2021</a>)</cite> that were submitted to IWSLT 2022. For the long-form regime, the logs are sourced from IWSLT 2025. In particular, for English-to-{German, Chinese, Japanese} the evaluation was done on the development set of the ACL 60/60 dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Salesky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib28\" title=\"\">2023</a>)</cite>, and IWSLT 2025 test set. For the Czech-to-English language pair, the evaluation was performed on the IWSLT 2024 development set <cite class=\"ltx_cite ltx_citemacro_cite\">Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite> and the IWSLT 2025 test set. A portion of the IWSLT 2024 development set contained segmented audio that could not be reconstructed into the original unsegmented audio.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "evaluation",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T5\" title=\"In Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T7\" title=\"Table 7 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we present the number of systems used in the short- and long-form evaluations. The number of systems available to us was slightly larger, but we excluded all systems where the logs were incomplete (e.g., predictions for all recordings were not present, mismatched order of sources and hypotheses). Furthermore, in the long-form regime, we excluded one team entirely from the evaluation due to faulty logs. These logs contained a different number of predicted words and delays, which means that we could not faithfully determine generation timestamps for each predicted word.</p>\n\n",
                "matched_terms": [
                    "all",
                    "evaluation",
                    "regime",
                    "words",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Same as in the short-form evaluation, we follow the definition of the true latency in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4\" title=\"4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nHowever, there are two differences.\nAfter initial experiments, we observed that the Montreal Forced Aligner used in the short-form regime is not robust for the challenging conditions of the IWSLT 2025 test set, which is based on ACL presentations. The recordings include frequent restarts, repetitions, domain-specific terminology, and non-native speech. Instead, we use the alignment method implemented within WhisperX <cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib5\" title=\"\">2023</a>)</cite> for forced alignment. This tool leverages neural speech encoders that seem to be robust to the above-mentioned challenges. In particular, we used WhisperX&#8217;s default settings, i.e., PyTorch&#8217;s <span class=\"ltx_text ltx_font_typewriter\">WAV2VEC2_ASR_BASE_960H</span> for English and <span class=\"ltx_text ltx_font_typewriter\">comodoro/wav2vec2-xls-r-300m-cs-250</span> for Czech speech forced alignments.\nSecond, we perform resegmentation of the system hypotheses prior to the machine translation alignment with the reference. This step is necessary because the <span class=\"ltx_text ltx_font_typewriter\">awesome-align</span> tool uses the <span class=\"ltx_text ltx_font_typewriter\">bert-base-multilingual-cased</span> model for the alignment, and this model has a maximum input length of 512 tokens, which is much lower than the system hypotheses.</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "evaluation",
                    "latency",
                    "regime",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A natural question arises: <em class=\"ltx_emph ltx_font_italic\">Why rely on automatic latency metrics at all, when true latency offers a closer approximation of user experience?</em> In practice, computing true latency requires several requirements that limit its applicability. High-quality transcripts must be available, which is often not the case&#8211;particularly for low-resource languages or unwritten languages where transcription is infeasible. Moreover, forced alignment tools and reliable word-level translation alignments are typically available only for a small set of high-resource language pairs. Even when such resources exist, computing true latency involves multiple processing steps and is substantially more complex than evaluating standard automatic metrics. Importantly, as we show in our analysis in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5\" title=\"5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, several automatic metrics approximate true latency with high accuracy, making them a practical and effective alternative in most evaluation scenarios.</p>\n\n",
                "matched_terms": [
                    "all",
                    "evaluation",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F6\" title=\"In Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we illustrate the trends after filtering out the systems affected by the anomalous policy (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). Unlike in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we see that all metrics and system pairs show a positive correlation with the true latency. As mentioned in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, language pairs exhibit different scales, making the use of the correlation coefficient more cumbersome and motivating the use of accuracy as described in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4.SS2.SSS0.Px3\" title=\"Accuracy &#8227; 4.2 Evaluation &#8227; 4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "latency",
                    "systems",
                    "all",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We were also interested in the accuracy of latency metrics when comparing related against unrelated systems. In our evaluation, we consider the systems submitted by one team as related.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>To the best of our knowledge, most teams submitted multiple systems that were based on the same system with varying hyperparameters.</span></span></span> We also use only a subset of the systems that were not affected by the anomalous simultaneous policy. The results are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Surprisingly, when evaluating related systems, all metrics perform almost perfectly, reaching accuracy between 97% and 100%. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F9\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency. Given a difference of at least <math alttext=\"\\sim 250\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS0.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>250</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 250</annotation></semantics></math> ms, all metrics except AP achieve 100% accuracy, and AP achieves around 99% accuracy.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "all",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results on unrelated systems (bottom half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F10\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>) are generally similar to the observations in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. All metrics show a loss of accuracy of no more than 4% points compared to the results on all systems. The only exception seems to be AP, which loses up to 11% points. The order of the metrics remains the same.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F11\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we show pairwise comparisons of systems evaluated in the long-form regime without resegmentation, and in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F12\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, we show the same systems evaluated in the long-form regime, but after resegmentation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F13\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency.</p>\n\n",
                "matched_terms": [
                    "latency",
                    "systems",
                    "after",
                    "regime"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation",
        "caption": "Table 3: Accuracy of latency and quality metrics after resegmentation.",
        "body": "Latency (StreamLAAL)\nMT Quality (COMET)\n\n\np-value\nmWERSegmenter\nours\nmWERSegmenter\nours\n\n\n\n\nAll\n86.4\n94.0\n99.3\n99.1\n\n\n\n<<0.05\n86.3\n94.3\n100.0\n100.0\n\n\n\n<<0.001\n86.1\n94.3\n100.0\n100.0\n\n\n0.001-0.05\n92.9\n94.6\n100.0\n100.0",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">Latency (StreamLAAL)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">MT Quality (COMET)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">p-value</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">mWERSegmenter</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">ours</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">mWERSegmenter</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\">ours</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">All</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">86.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">94.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">99.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">99.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.05</th>\n<td class=\"ltx_td ltx_align_center\">86.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">94.3</td>\n<td class=\"ltx_td ltx_align_center\">100.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">100.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.001</th>\n<td class=\"ltx_td ltx_align_center\">86.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">94.3</td>\n<td class=\"ltx_td ltx_align_center\">100.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">100.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">0.001-0.05</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">92.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">94.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">100.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">100.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "quality",
            "all",
            "metrics",
            "ours",
            "pvalue",
            "resegmentation",
            "latency",
            "comet",
            "after",
            "accuracy",
            "streamlaal",
            "mwersegmenter"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we evaluate two resegmentation tools: mWERSegmenter and our proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>.\nThe evaluation is done on reconcatenated short-form\noutputs, allowing us to compare with gold segment boundaries.\nAs we can see in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_italic\">the accuracy of\nlatency evaluation is significantly higher with the proposed segmenter</span>.\nWhen filtering out comparable systems by the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value,\naccuracy\nfurther decreases\nwith mWERSegmenter, suggesting that the segmentation is not stable.\nBoth segmentation methods achieve over 99% accuracy, indicating resegmentation does not hinder quality assessment.</p>\n\n",
            "<p class=\"ltx_p\">The upper part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the accuracy of StreamLAAL that uses resegmentation and the original short-form metrics evaluated <em class=\"ltx_emph ltx_font_italic\">without resegmentation</em> on long-form systems.\nWe see that the accuracies are low, not exceeding 66% on all systems.\nCompared to StreamLAAL, the best-performing AL metric loses 15 to 16% absolute, and the gap is even wider compared to the proposed LongYAAL, with AL falling short by 29 points.\nThe lower part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\nreports the accuracy of latency metrics in long-form systems <em class=\"ltx_emph ltx_font_italic\">with resegmentation</em>.\nWe see that the resegmentation quality significantly influences the accuracy. StreamLAAL and LongLAAL share the same definition, but differ in the resegmentation&#8211;while StreamLAAL uses the mWERSegmenter, LongLAAL (and all other\n&#8220;Long-&#8221; metrics) uses our <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>. The gap in accuracy is 8 to 10 points in all subsets, showing trends similar to those in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\n<span class=\"ltx_text ltx_font_italic\">These results highlight the critical role of resegmentation in ensuring reliable latency evaluation in the long-form regime.</span> Additional observations are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4\" title=\"Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency&#8211;the delay between speech input and the translated output.\nWhile quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented.\nIn this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes.\nWe uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more\naccurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems.</p>\n\n",
                "matched_terms": [
                    "resegmentation",
                    "metrics",
                    "quality",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) is the task in which the system produces incremental translation concurrently with the speaker&#8217;s speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib27\" title=\"\">2020</a>)</cite>. SimulST models have to balance between quality and latency of the output, which is the time elapsed between when a word is uttered and when its corresponding translation is produced. Although translation quality metrics are extensively studied in offline ST and in the related field of machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Freitag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib11\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib10\" title=\"\">2023</a>; Zouhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib30\" title=\"\">2024</a>)</cite>, there is no study on the reliability of latency metrics.\nThe most common latency\nmetrics in SimulST <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho and Esipova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib8\" title=\"\">2016</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib16\" title=\"\">2019</a>; Cherry and Foster, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib7\" title=\"\">2019</a>; Pol&#225;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>; Kano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib14\" title=\"\">2023</a>)</cite>, even though with different approximations, base their calculation on\nsimplifying assumptions such as uniform word duration, absence of pauses, and strict monotonic alignment between source speech and target translation.\nHowever, despite relying on\nthe same assumptions, these metrics often produce very inconsistent assessments.\nThis inconsistency is clearly illustrated in the results of the IWSLT 2023 Shared Task on Simultaneous Translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, where different metrics produced substantially different rankings (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S1.F1\" title=\"In 1 Introduction &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). Such variability raises serious concerns about the validity of current evaluation protocols and their ability to support meaningful comparisons between systems.\nMoreover, this risk can be further exacerbated when shifting from dealing with already presegmented speech, i.e., <span class=\"ltx_text ltx_font_italic\">short-form</span> SimulST, to unsegmented audio, i.e., <span class=\"ltx_text ltx_font_italic\">long-form</span> SimulST, where information about sentence boundaries is not available, further complicating the evaluation <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "quality",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present the first comprehensive evaluation of latency metrics for SimulST under several aspects, including diverse systems, language pairs, and short- and long-form regimes. Through an in-depth analysis of systems submitted to recent IWSLT SimulST Shared Tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>; Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, we reveal that existing metrics can lead to misleading conclusions and hinder effective system design. We show that the inconsistent evaluations are not primarily due to the aforementioned assumptions, but rather to a structural bias in how latency is measured&#8211;particularly\nin how segmentation influences SimulST models&#8217; behavior.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose YAAL (Yet Another Average Lagging), a refined latency metric designed to mitigate the biases present in existing latency metrics.\nOur extensive experiments demonstrate that YAAL yields more reliable latency estimates, consistently aligning better with the actual behavior of SimulST systems.\nFurthermore, we also show that resegmentation, which pairs segment-level predictions with their corresponding reference, is necessary to produce meaningful latency measurements for long-form SimulST. To this end, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a new resegmentation tool, and extend our YAAL to LongYAAL, which deals with audio streams. Compared to the current standard alignment tool used in the speech translation community <cite class=\"ltx_cite ltx_citemacro_citep\">(Matusov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> significantly improves alignment quality, enabling more accurate evaluation in long-form scenarios.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The code for YAAL, its long-form variant LongYAAL, and <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> will be released upon the paper acceptance under Apache 2.0 license.</span></span></span></p>\n\n",
                "matched_terms": [
                    "resegmentation",
                    "metrics",
                    "quality",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The short-form is the most common evaluation regime of SimulST <cite class=\"ltx_cite ltx_citemacro_cite\">Anastasopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>); Agarwal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>); Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, where all recordings of the test set are divided, usually following sentence boundaries, into short segments of a few seconds.\nEach segment consists of source audio <math alttext=\"\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is a small portion of raw audio, i.e., audio chunk, and reference translation <math alttext=\"\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>y</mi><mn>1</mn><mi>R</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>y</mi><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow><mi>R</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]</annotation></semantics></math>. Each\naudio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is incrementally fed to the system,\nwhich concurrently\noutputs a translation token <math alttext=\"y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">y_{j}</annotation></semantics></math> at timestamp <math alttext=\"d_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">d_{j}</annotation></semantics></math>, i.e., total duration of audio chunks up to and including the audio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math>. Under these settings, we describe the latency metrics operating in the short-form regime:</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">modifies AL by introducing\na minimal delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> after each\nstep.\nUnlike AL and LAAL, DAL considers all tokens in the hypothesis,\nwithout\ncutoff after <math alttext=\"i&gt;\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\mathbf{X})</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "all",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form evaluation regime evaluates SimulST systems more realistically <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>, as it assesses their ability to handle long audio streams, often spanning several minutes.\nSince all metrics were developed for the short-form regime,\nrecent studies <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>\nresorted to resegmentation of translations and delays based on the reference translation <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib20\" title=\"\">2005b</a>)</cite>, and computed the metrics on the segment level. We explain the long-form variant of LAAL <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite> below.</p>\n\n",
                "matched_terms": [
                    "resegmentation",
                    "all",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, <cite class=\"ltx_cite ltx_citemacro_citet\">Huber et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib12\" title=\"\">2023</a>)</cite> proposed an evaluation with resegmentation provided by the system. The evaluation framework expects that the system outputs the segment&#8217;s start and end timestamps that align with the source. First, most systems, including all the IWSLT systems, do not output this information, and relying on the system&#8217;s self-reported alignment might hinder the reliability. Second, as we empirically show in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, relying on potentially low-quality resegmentation significantly lowers the accuracy of latency evaluation. Finally, different segmentations render the observed latencies incomparable across different systems.</p>\n\n",
                "matched_terms": [
                    "resegmentation",
                    "all",
                    "accuracy",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on these observations, we categorize existing short-form latency metrics (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.SS1\" title=\"2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>) into two main groups, depending on whether they include all translated words or only a subset in their latency computation. The first group&#8211;AP, DAL, and ATD&#8211;includes all translated words in the calculation.\nDAL attempts to mitigate the impact of tail words by adding a minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> after each generated word (also within the same step), thus &#8220;spreading&#8221; the tail beyond the sentence.\nHowever, <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math>\nsimply reflects the average source-to-target length ratio and does not accurately capture the system behavior for tail words in settings without segmentation.\nIf multiple words are emitted as tail words, DAL can significantly overestimate\nlatency.\nIn the edge case of a system that waits for an end-of-segment signal (i.e., an offline system), DAL\nreturns the segment length, failing to capture the system&#8217;s true behavior&#8211;in this case, undefined latency.\nAP assigns a delay of 1 to each tail word\nas the entire recording has to be processed to emit that word, thus, the proportion is 1. Although AP is marginally less sensitive to segmentation than DAL&#8211;since it operates on proportions rather than absolute delays&#8211;it still fails to capture system behavior faithfully for the tail words.\nATD also considers all translated words.\nHowever, unlike DAL, it does not apply corrections for tail word behavior, making it the most sensitive to segmentation artifacts among the three metrics.</p>\n\n",
                "matched_terms": [
                    "after",
                    "all",
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second group&#8211;AL and LAAL&#8211;computes latency only for words emitted up to and including the cutoff point <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>, which marks the first word generated after the end of the input segment.\nThis corresponds to the word &#8220;<span class=\"ltx_text ltx_font_italic\">gemeinn&#252;tzige</span>&#8221; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nAs discussed, in the short-form regime with oracle segmentation, the <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>-th and following words are often translated earlier than in a more realistic\nlong-form scenario.\nAs a result, this cutoff introduces a systematic bias in the latency estimate, which may lead to either underestimation or overestimation, depending on the system&#8217;s policy.</p>\n\n",
                "matched_terms": [
                    "after",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AP, DAL, ATD, AL, and, more recently, LAAL became established metrics in the short-form evaluation of SimulST. However, as discussed above, including any of the tail words in the latency computation leads to a systematic bias that undermines fair comparisons.\nTo cope with this bias, we propose a new metric derived from the LAAL metric:</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form regime offers a more realistic evaluation setting by assessing systems on continuous, unsegmented audio streams that better reflect real-world use cases. However, the widely used latency metrics were originally designed for the short-form regime and do not directly extend to this setting.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, metrics such as AL, LAAL, and DAL\nrely on a <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> parameter,\nrepresenting the average target-to-source length ratio. However, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> can vary substantially across different segments within the same audio, leading to inconsistent and unreliable latency estimates <cite class=\"ltx_cite ltx_citemacro_cite\">Iranzo-S&#225;nchez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib13\" title=\"\">2021</a>)</cite>.\nSecond, AP\ntends to converge toward 0 for\nlong recordings,\nas typical speech inputs are significantly longer than the translations,\ni.e., <math alttext=\"|\\mathbf{X}|\\gg|\\mathbf{Y}|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow><mo>&#8811;</mo><mrow><mo stretchy=\"false\">|</mo><mi>&#119832;</mi><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">|\\mathbf{X}|\\gg|\\mathbf{Y}|</annotation></semantics></math>,\nleading <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E1\" title=\"In Average Proportion (AP; Cho and Esipova, 2016) &#8227; 2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> to approach 0.\nFinally, ATD\nassumes that each speech token has a fixed duration and that source and target tokens align monotonically&#8211;assumptions that are overly restrictive and especially unrealistic for long-form speech.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, StreamLAAL has introduced resegmenting long inputs into short segments and computing latency on these units. While StreamLAAL provides adaptation of existing metrics to long-form input, it relies on the mWERSegmenter tool <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, which may introduce alignment errors <cite class=\"ltx_cite ltx_citemacro_cite\">Amrhein and Haddow (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib3\" title=\"\">2022</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>, and\ncomputes latency up to the cutoff word <math alttext=\"\\tau(\\mathbf{X_{i}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mi>&#119842;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X_{i}})</annotation></semantics></math> (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E7\" title=\"In Streaming LAAL (StreamLAAL; Papi et al., 2024) &#8227; 2.2 Long-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>), which can lead to the systematic bias (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.SS1\" title=\"3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>)\nTo overcome these limitations, we propose a new resegmentation method and an extension of the YAAL metric for the long-form regime.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "resegmentation",
                    "latency",
                    "streamlaal",
                    "mwersegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a new resegmentation method inspired by <cite class=\"ltx_cite ltx_citemacro_citet\">Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>, employing a softer alignment strategy to more accurately match the translation output with the reference segments.\nWe start by lowercasing and tokenizing both the reference and the system hypotheses. This allows for a more precise alignment around the sentence ends, especially in cases where the reference and the model differ in sentence segmentation. However, we still keep the original texts in memory so as not to interfere with the machine translation quality evaluation that is computed over the resegmented hypotheses. Additionally, we keep the delay together with each token and use it during the alignment process to prevent alignment of tokens to future segments, which leads to spurious negative latencies. For alignment, we maximize the following score:</p>\n\n",
                "matched_terms": [
                    "resegmentation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also extend YAAL to the long-form regime&#8211;i.e., LongYAAL. Unlike StreamLAAL, LongYAAL includes all words in the latency computation, even those generated beyond the aligned segment boundaries <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math>, i.e., all <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for <math alttext=\"i&gt;\\tau(\\mathbf{X_{s}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\mathbf{X_{s}})</annotation></semantics></math>. However, we exclude the final tail words produced after the end of the full stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>, i.e., <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for <math alttext=\"i&gt;\\tau(\\sum_{s=1}^{|\\mathbf{X}|}|\\mathbf{X_{s}}|)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8721;</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msubsup><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\sum_{s=1}^{|\\mathbf{X}|}|\\mathbf{X_{s}}|)</annotation></semantics></math>. This ensures that we include all words emitted beyond the segment boundaries <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math>, but we do not include the tail words generated at the end of the entire stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>. If the stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> consists of a single segment, LongYAAL coincides with YAAL.</p>\n\n",
                "matched_terms": [
                    "after",
                    "all",
                    "streamlaal",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable\nfair comparisons across latency metrics, we require a reference latency reflecting the user experience, i.e., how long the user needs to wait for translation.\nSince human evaluation is infeasible at scale,\nwe adopt a carefully designed automatic approximation, which we refer to as <span class=\"ltx_text ltx_font_italic\">true latency</span>.\nThis is grounded in an intuitive and practical definition of latency in speech translation:\n<em class=\"ltx_emph ltx_font_italic\">On average, how long does a user have to wait for a given piece of source information to appear in the translation?</em>\nConcretely, we define true latency as the average delay between each target word and its corresponding source word:</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> is the emission time of the target word <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> and <math alttext=\"d^{src}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>d</mi><mi>i</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">d^{src}_{i}</annotation></semantics></math> is the corresponding source delay. We define the source delay as the time that the speaker finished the last word corresponding to the target word: <math alttext=\"d^{src}_{i}=\\max_{l}{\\{s^{end}_{l}|(y_{i},s_{l})\\in\\mathcal{A}(\\mathbf{Y}\\rightarrow\\mathbf{S})\\}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi>d</mi><mi>i</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msubsup><mo>=</mo><mrow><msub><mi>max</mi><mi>l</mi></msub><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mrow><msubsup><mi>s</mi><mi>l</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msubsup><mo fence=\"false\">|</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>l</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119832;</mi><mo stretchy=\"false\">&#8594;</mo><mi>&#119826;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d^{src}_{i}=\\max_{l}{\\{s^{end}_{l}|(y_{i},s_{l})\\in\\mathcal{A}(\\mathbf{Y}\\rightarrow\\mathbf{S})\\}}</annotation></semantics></math>,\nwhere <math alttext=\"s^{end}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>s</mi><mi>l</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">s^{end}_{l}</annotation></semantics></math> is the end timestamp of the source word <math alttext=\"s_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">s_{l}</annotation></semantics></math> and <math alttext=\"\\mathcal{A}(\\mathbf{Y}\\rightarrow\\mathbf{S})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119832;</mi><mo stretchy=\"false\">&#8594;</mo><mi>&#119826;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}(\\mathbf{Y}\\rightarrow\\mathbf{S})</annotation></semantics></math> is the translation alignment between the target and the source.\nAs discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.SS1\" title=\"3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, computing latency over all words&#8211;including tail words&#8211;can introduce systematic bias. To mitigate this, we restrict the true latency calculation to words generated strictly during simultaneous decoding, i.e., before the end-of-source signal. Additionally, we consider only the subset of target words <math alttext=\"\\mathbf{Y^{A}}\\subseteq\\mathbf{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>&#119808;</mi></msup><mo>&#8838;</mo><mi>&#119832;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{A}}\\subseteq\\mathbf{Y}</annotation></semantics></math> that are aligned to at least one source word, thereby avoiding biases introduced by over- or under-generation <cite class=\"ltx_cite ltx_citemacro_cite\">Pol&#225;k et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>); Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>)</cite>. The implementation details are provided in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A2\" title=\"Appendix B True Latency &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Kocmi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib15\" title=\"\">2021</a>)</cite>, we evaluate the accuracy of binary comparisons: given a pair of systems, which one is better according to the true latency ranking (used as silver labels)? The accuracy is defined as the proportion of system pairs for which the relative ranking according to a metric matches that of the true latency:</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accuracy considers only the ranking, not the magnitude, of the latency differences, allowing us to aggregate comparisons across languages and test sets with different scales.\nHowever, this accuracy might be affected if two systems have similar latencies. To avoid this issue, we compute the accuracies in multiple subsets\nby removing pairs that are not significantly different according to Mann-Whitney U test on their\ntrue latencies.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We do not assume normal distribution of delays. Each system has different hypotheses, so we cannot use paired tests.</span></span></span> We use bootstrap resampling with <math alttext=\"N=10000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10000</mn></mrow><annotation encoding=\"application/x-tex\">N=10000</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tibshirani and Efron, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib29\" title=\"\">1993</a>)</cite> to estimate confidence intervals and consider all metrics within the 95% confidence interval of the top-performing metric to be statistically tied.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "accuracy",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a pairwise comparison of all short-form systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. An important first observation is that a significant portion of system pairs exhibit no or slightly negative correlations&#8211;points that create almost vertical lines and lines far off the diagonal. These systems share a <em class=\"ltx_emph ltx_font_italic\">anomalous simultaneous policy</em>: The lower the latency of the prefix generated simultaneously, the larger the portion of the sentence translated offline.\nWe assume that the underlying reason for the anomalous policy is that the system is too eager to emit output at the beginning. However, eventually it gets to the &#8220;dead end&#8221; of probable outputs and only emits the tail words at the signaled end of the sentence.\nThis policy, coupled with bias in latency metrics, leads to a severe overestimation of the actual latency of the systems. As we show below, overestimation of actual latency can hinder the reliable detection of systems following this anomalous policy.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The expected fraction of words translated online based on the value <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> of an automatic latency metric is <math alttext=\"O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mtext>avg</mtext></msub><mo>&#8722;</mo><mi>L</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>X</mi><mtext>avg</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}</annotation></semantics></math>, where <math alttext=\"X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>X</mi><mtext>avg</mtext></msub><annotation encoding=\"application/x-tex\">X_{\\text{avg}}</annotation></semantics></math> is the average segment length.\nIf the expected online-translated word fraction significantly exceeds the observed one, i.e., <math alttext=\"O_{\\text{e}}\\gg O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8811;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\gg O</annotation></semantics></math>, we conclude that the system follows the anomalous policy. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F4\" title=\"In Detecting Anomalous Policy &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the observed and expected fractions of the words translated online based on the proposed YAAL and LAAL metrics. Most systems follow a vertical line, i.e., <math alttext=\"O_{\\text{e}}\\sim O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8764;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\sim O</annotation></semantics></math>. However, based on YAAL, the red and brown circles<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All affected systems were submitted independently by two different teams in IWSLT 2022 and 2023, showing that the anomalous policy is not so uncommon.</span></span></span> show a strong disagreement between <math alttext=\"O^{\\text{YAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>YAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{YAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m7\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. Based on LAAL (crosses), we observe some disagreement between <math alttext=\"O^{\\text{LAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>LAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{LAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m9\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. However, the trend is not as clear as with YAAL, making the anomalous policy detection difficult. <em class=\"ltx_emph ltx_font_italic\">This shows the importance of accurate evaluation of latency using YAAL.</em></p>\n\n",
                "matched_terms": [
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the metrics, we observe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>) that they all show positive correlations with the true latency, but each language pair has a slightly different scale, which <em class=\"ltx_emph ltx_font_italic\">motivates the use of accuracy instead of simple correlation</em>.\nTherefore, we further compare latency metrics in terms of accuracy in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. If we consider all system pairs (including systems following the anomalous policy), we see that all metrics significantly underperform YAAL (by more than 22% absolute), which reaches an accuracy of 96%. When we progressively filter out system pairs with similar true latency (decreasing <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> values), the accuracies slightly increase, but the ranking of the metrics remains. If we consider a subset that has a <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value between 0.001-0.05 (i.e., removing pairs with similar true latency that are, however, more difficult to distinguish), we see that YAAL still remains the most accurate by a margin of 25% absolute.\nIf we remove systems with the anomalous policy,\nall metrics gain a significant boost in accuracy (bottom part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). However, the YAAL metric remains the best metric in all subsets based on <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values, achieving 98 and 99% accuracy&#8211;even though it relies on assumptions such as uniform source token durations and monotonic source-to-target alignment.\nBased on these observations, we conclude that\n<span class=\"ltx_text ltx_font_italic\">the automatic YAAL metric is almost as accurate as\ntrue latency</span>. We include more accuracy evaluations by isolating different categories of systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3\" title=\"Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "accuracy",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3\" title=\"3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and empirically observed in this section, short-form evaluation can significantly distort latency evaluation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T2\" title=\"In Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, we present the average fraction\nof words translated <span class=\"ltx_text ltx_font_italic\">after</span> the end-of-segment signal, i.e., words omitted during evaluation by most latency metrics. A substantial portion of the translations are tail words, starting at 41% in the low-latency (1-2s) and reaching 72% in the high-latency regimes (4-5s).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Systems with higher-latency behavior have policies leading to deferred delays, and these delays in turn are more likely to overflow the source duration.</span></span></span>\n<span class=\"ltx_text ltx_font_italic\">Short-form evaluation, with artificial segment boundaries absent in real-world scenarios and metrics&#8217; problematic handling of tail words, often misrepresents SimulST system behavior.</span>\nThis raises serious concerns about\nits reliability\nand underscores the need for long-form evaluation,\nwhich we analyze in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS2\" title=\"5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "after",
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows\nthat the proposed LongYAAL has the highest accuracy in all subsets. LongATD and LongDAL show slightly worse results, but not\nstatistically significant. This contrasts with &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, where ATD and DAL perform worse than *AL metrics.\nWe explain this discrepancy by the fact that both metrics include tail words that rarely occur in the long-form regime.\nWe attribute the marginal difference of LongATD compared to LongYAAL to its assumption of 300ms words in the source speech, which is dynamic in LongYAAL in the form of the parameter <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and the difference in LongDAL is caused by the artificial minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E4\" title=\"In Differentiable Average Lagging (DAL; Cherry and Foster, 2019) &#8227; 2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nLongLAAL ties with LongYAAL in most subsets, but seems slightly worse when in easily distinguishable systems (<math alttext=\"p\\text{-val}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>-val</mtext></mrow><annotation encoding=\"application/x-tex\">p\\text{-val}</annotation></semantics></math>&lt;0.001), where the metric loses 2 points in accuracy. LongLAAL, unlike LongYAAL, disregards words generated beyond the reference segment boundaries. The number of words ignored increases with the true latency of the system (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1.SSS0.Px3\" title=\"Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), which is more prevalent in this subset. Similarly, LongAL ignores the tail words in the resegmentation and is also vulnerable to overgeneration <cite class=\"ltx_cite ltx_citemacro_cite\">Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>); Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite>. Finally, AP performs the worst, losing more than 21 points compared to other metrics, which we attribute to the metric&#8217;s sensitivity to variable segment length. <span class=\"ltx_text ltx_font_italic\">These results position LongYAAL as the most reliable metric for assessing latency in long-form SimulST</span>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "resegmentation",
                    "latency",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented the first systematic evaluation of latency metrics for SimulST across several aspects, such as diverse systems, language pairs, and operating under short- and long-form speech processing. We have identified current pitfalls in the SimulST evaluation by isolating issues in the most commonly used metrics. To overcome these limitations, we propose YAAL, a new latency metric\nbetter\naligned with the\nshort-form evaluation regime.\nHowever,\nour analysis also reveals inherent shortcomings of short-form evaluation, further reinforcing the adoption of long-form evaluation as a more reliable alternative.\nMoreover, we also demonstrated that resegmentation is necessary to conduct a proper evaluation of systems operating under the long-form regime, and proposed an improved resegmentation tool coupled with the extension of YAAL for these settings&#8211;LongYAAL. The results showed that YAAL and LongYAAL improve over all other metrics in both regimes, establishing the new state-of-the-art metric for SimulST.</p>\n\n",
                "matched_terms": [
                    "resegmentation",
                    "all",
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our study offers a thorough evaluation of latency metrics for SimulST and introduces improved tools for both short- and long-form regimes, some limitations remain. First, our evaluation depends on reference translations and transcriptions, which may not be available or reliable in low-resource or real-time scenarios. Second, although the proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> improves alignment robustness, word-level alignment is still susceptible to errors in the presence of disfluencies or speech recognition noise. Third, our experimental analysis focuses on systems from the IWSLT Shared Tasks, which may not fully represent the range of techniques or data conditions used in broader academic or industrial settings. Fourth, our analysis focuses on high-resource languages, for which data were available, but the findings should be reconfirmed under low-resource language settings.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We did not train any models as part of this study. However, we used several evaluations that required computation. Most of the experiments were conducted on a standard desktop computer equipped with an Intel i7 processor and 32GB of RAM. For forced alignments with neural models, machine translation alignment, and the COMET translation quality metric, we used\na GPU cluster.\nHowever, these evaluations can be done on a desktop machine with a slightly longer runtime. The proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, YAAL, and LongYAAL can be run efficiently on a CPU.</p>\n\n",
                "matched_terms": [
                    "comet",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Same as in the short-form evaluation, we follow the definition of the true latency in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4\" title=\"4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nHowever, there are two differences.\nAfter initial experiments, we observed that the Montreal Forced Aligner used in the short-form regime is not robust for the challenging conditions of the IWSLT 2025 test set, which is based on ACL presentations. The recordings include frequent restarts, repetitions, domain-specific terminology, and non-native speech. Instead, we use the alignment method implemented within WhisperX <cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib5\" title=\"\">2023</a>)</cite> for forced alignment. This tool leverages neural speech encoders that seem to be robust to the above-mentioned challenges. In particular, we used WhisperX&#8217;s default settings, i.e., PyTorch&#8217;s <span class=\"ltx_text ltx_font_typewriter\">WAV2VEC2_ASR_BASE_960H</span> for English and <span class=\"ltx_text ltx_font_typewriter\">comodoro/wav2vec2-xls-r-300m-cs-250</span> for Czech speech forced alignments.\nSecond, we perform resegmentation of the system hypotheses prior to the machine translation alignment with the reference. This step is necessary because the <span class=\"ltx_text ltx_font_typewriter\">awesome-align</span> tool uses the <span class=\"ltx_text ltx_font_typewriter\">bert-base-multilingual-cased</span> model for the alignment, and this model has a maximum input length of 512 tokens, which is much lower than the system hypotheses.</p>\n\n",
                "matched_terms": [
                    "resegmentation",
                    "after",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A natural question arises: <em class=\"ltx_emph ltx_font_italic\">Why rely on automatic latency metrics at all, when true latency offers a closer approximation of user experience?</em> In practice, computing true latency requires several requirements that limit its applicability. High-quality transcripts must be available, which is often not the case&#8211;particularly for low-resource languages or unwritten languages where transcription is infeasible. Moreover, forced alignment tools and reliable word-level translation alignments are typically available only for a small set of high-resource language pairs. Even when such resources exist, computing true latency involves multiple processing steps and is substantially more complex than evaluating standard automatic metrics. Importantly, as we show in our analysis in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5\" title=\"5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, several automatic metrics approximate true latency with high accuracy, making them a practical and effective alternative in most evaluation scenarios.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "accuracy",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F6\" title=\"In Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we illustrate the trends after filtering out the systems affected by the anomalous policy (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). Unlike in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we see that all metrics and system pairs show a positive correlation with the true latency. As mentioned in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, language pairs exhibit different scales, making the use of the correlation coefficient more cumbersome and motivating the use of accuracy as described in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4.SS2.SSS0.Px3\" title=\"Accuracy &#8227; 4.2 Evaluation &#8227; 4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "latency",
                    "after",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To this end, in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F7\" title=\"In Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figures</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F8\" title=\"Figure 8 &#8227; Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we also offer the accuracy of subsets of system pairs based on the absolute difference in the true latency.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We were also interested in the accuracy of latency metrics when comparing related against unrelated systems. In our evaluation, we consider the systems submitted by one team as related.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>To the best of our knowledge, most teams submitted multiple systems that were based on the same system with varying hyperparameters.</span></span></span> We also use only a subset of the systems that were not affected by the anomalous simultaneous policy. The results are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "accuracy",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Surprisingly, when evaluating related systems, all metrics perform almost perfectly, reaching accuracy between 97% and 100%. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F9\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency. Given a difference of at least <math alttext=\"\\sim 250\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS0.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>250</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 250</annotation></semantics></math> ms, all metrics except AP achieve 100% accuracy, and AP achieves around 99% accuracy.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "accuracy",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results on unrelated systems (bottom half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F10\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>) are generally similar to the observations in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. All metrics show a loss of accuracy of no more than 4% points compared to the results on all systems. The only exception seems to be AP, which loses up to 11% points. The order of the metrics remains the same.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F11\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we show pairwise comparisons of systems evaluated in the long-form regime without resegmentation, and in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F12\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, we show the same systems evaluated in the long-form regime, but after resegmentation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F13\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency.</p>\n\n",
                "matched_terms": [
                    "resegmentation",
                    "after",
                    "accuracy",
                    "latency"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation",
        "caption": "Table 4: Accuracy of systems in the long-form regime. Best scores in bold. Underlined scores are considered tied with the best metric. All metrics in the bottom half use the proposed SoftSegmenter, except for StreamLAAL that uses the original mWERSegmenter.",
        "body": "longform + unsegmented\n\n\n\npp-val\nStreamLAAL\\begin{subarray}{c}\\text{Stream}\\\\\n\\text{LAAL}\\end{subarray}\nAL\nLAAL\nDAL\nATD\nAP\nYAAL\nN\n\n\nall\n0.82\n0.66\n0.61\n0.57\n0.61\n0.39\n0.61\n594\n\n\n\n<<0.05\n0.85\n0.69\n0.64\n0.59\n0.63\n0.36\n0.64\n523\n\n\n\n<<0.001\n0.87\n0.71\n0.65\n0.60\n0.63\n0.34\n0.65\n461\n\n\n0.001-0.05\n0.63\n0.52\n0.55\n0.48\n0.60\n0.47\n0.55\n62\n\n\nlongform + resegmented\n\n\n\npp-val\nStreamLAAL\\begin{subarray}{c}\\text{Stream}\\\\\n\\text{LAAL}\\end{subarray}\nLongAL\\begin{subarray}{c}\\text{Long}\\\\\n\\text{AL}\\end{subarray}\nLongLAAL\\begin{subarray}{c}\\text{Long}\\\\\n\\text{LAAL}\\end{subarray}\nLongDAL\\begin{subarray}{c}\\text{Long}\\\\\n\\text{DAL}\\end{subarray}\nLongATD\\begin{subarray}{c}\\text{Long}\\\\\n\\text{ATD}\\end{subarray}\nLongAP\\begin{subarray}{c}\\text{Long}\\\\\n\\text{AP}\\end{subarray}\nLongYAAL\\begin{subarray}{c}\\text{Long}\\\\\n\\text{YAAL}\\end{subarray}\nN\n\n\nall\n0.82\n0.92\n0.95\n0.94\n0.93\n0.71\n0.95\n594\n\n\n\n<<0.05\n0.85\n0.94\n0.96\n0.97\n0.97\n0.72\n0.98\n523\n\n\n\n<<0.001\n0.87\n0.95\n0.97\n0.98\n0.99\n0.74\n0.99\n461\n\n\n0.001-0.05\n0.63\n0.85\n0.90\n0.85\n0.82\n0.60\n0.87\n62",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"9\">longform + unsegmented</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-val</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math alttext=\"\\begin{subarray}{c}\\text{Stream}\\\\&#10;\\text{LAAL}\\end{subarray}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mtable rowspacing=\"0pt\"><mtr><mtd><mtext>Stream</mtext></mtd></mtr><mtr><mtd><mtext>LAAL</mtext></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{subarray}{c}\\text{Stream}\\\\\n\\text{LAAL}\\end{subarray}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">LAAL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">DAL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ATD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">YAAL</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">N</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">all</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.61</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">594</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m3\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.85</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.69</span></td>\n<td class=\"ltx_td ltx_align_center\">0.64</td>\n<td class=\"ltx_td ltx_align_center\">0.59</td>\n<td class=\"ltx_td ltx_align_center\">0.63</td>\n<td class=\"ltx_td ltx_align_center\">0.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.64</td>\n<td class=\"ltx_td ltx_align_right\">523</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.001</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.87</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center\">0.65</td>\n<td class=\"ltx_td ltx_align_center\">0.60</td>\n<td class=\"ltx_td ltx_align_center\">0.63</td>\n<td class=\"ltx_td ltx_align_center\">0.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.65</td>\n<td class=\"ltx_td ltx_align_right\">461</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">0.001-0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.63</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.52</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.48</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.60</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.55</span></td>\n<td class=\"ltx_td ltx_align_right\">62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"9\">longform + resegmented</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m5\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-val</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math alttext=\"\\begin{subarray}{c}\\text{Stream}\\\\&#10;\\text{LAAL}\\end{subarray}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m6\" intent=\":literal\"><semantics><mtable rowspacing=\"0pt\"><mtr><mtd><mtext>Stream</mtext></mtd></mtr><mtr><mtd><mtext>LAAL</mtext></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{subarray}{c}\\text{Stream}\\\\\n\\text{LAAL}\\end{subarray}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\begin{subarray}{c}\\text{Long}\\\\&#10;\\text{AL}\\end{subarray}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m7\" intent=\":literal\"><semantics><mtable rowspacing=\"0pt\"><mtr><mtd><mtext>Long</mtext></mtd></mtr><mtr><mtd><mtext>AL</mtext></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{subarray}{c}\\text{Long}\\\\\n\\text{AL}\\end{subarray}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\begin{subarray}{c}\\text{Long}\\\\&#10;\\text{LAAL}\\end{subarray}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m8\" intent=\":literal\"><semantics><mtable rowspacing=\"0pt\"><mtr><mtd><mtext>Long</mtext></mtd></mtr><mtr><mtd><mtext>LAAL</mtext></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{subarray}{c}\\text{Long}\\\\\n\\text{LAAL}\\end{subarray}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\begin{subarray}{c}\\text{Long}\\\\&#10;\\text{DAL}\\end{subarray}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m9\" intent=\":literal\"><semantics><mtable rowspacing=\"0pt\"><mtr><mtd><mtext>Long</mtext></mtd></mtr><mtr><mtd><mtext>DAL</mtext></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{subarray}{c}\\text{Long}\\\\\n\\text{DAL}\\end{subarray}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\begin{subarray}{c}\\text{Long}\\\\&#10;\\text{ATD}\\end{subarray}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m10\" intent=\":literal\"><semantics><mtable rowspacing=\"0pt\"><mtr><mtd><mtext>Long</mtext></mtd></mtr><mtr><mtd><mtext>ATD</mtext></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{subarray}{c}\\text{Long}\\\\\n\\text{ATD}\\end{subarray}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\begin{subarray}{c}\\text{Long}\\\\&#10;\\text{AP}\\end{subarray}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m11\" intent=\":literal\"><semantics><mtable rowspacing=\"0pt\"><mtr><mtd><mtext>Long</mtext></mtd></mtr><mtr><mtd><mtext>AP</mtext></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{subarray}{c}\\text{Long}\\\\\n\\text{AP}\\end{subarray}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math alttext=\"\\begin{subarray}{c}\\text{Long}\\\\&#10;\\text{YAAL}\\end{subarray}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m12\" intent=\":literal\"><semantics><mtable rowspacing=\"0pt\"><mtr><mtd><mtext>Long</mtext></mtd></mtr><mtr><mtd><mtext>YAAL</mtext></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{subarray}{c}\\text{Long}\\\\\n\\text{YAAL}\\end{subarray}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">N</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">all</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.95</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">594</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m13\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.85</td>\n<td class=\"ltx_td ltx_align_center\">0.94</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.97</span></td>\n<td class=\"ltx_td ltx_align_center\">0.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.98</span></td>\n<td class=\"ltx_td ltx_align_right\">523</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m14\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.001</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.87</td>\n<td class=\"ltx_td ltx_align_center\">0.95</td>\n<td class=\"ltx_td ltx_align_center\">0.97</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.99</span></td>\n<td class=\"ltx_td ltx_align_center\">0.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.99</span></td>\n<td class=\"ltx_td ltx_align_right\">461</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">0.001-0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.87</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">62</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "longdalbeginsubarrayctextlong",
            "streamlaalbeginsubarrayctextstream",
            "half",
            "tied",
            "proposed",
            "resegmented",
            "longlaalbeginsubarrayctextlong",
            "longform",
            "longapbeginsubarrayctextlong",
            "yaal",
            "all",
            "metrics",
            "bottom",
            "textdalendsubarray",
            "atd",
            "longyaalbeginsubarrayctextlong",
            "original",
            "textatdendsubarray",
            "streamlaal",
            "mwersegmenter",
            "unsegmented",
            "textlaalendsubarray",
            "ppval",
            "regime",
            "except",
            "longatdbeginsubarrayctextlong",
            "considered",
            "bold",
            "systems",
            "accuracy",
            "textalendsubarray",
            "underlined",
            "longalbeginsubarrayctextlong",
            "laal",
            "metric",
            "dal",
            "scores",
            "softsegmenter",
            "textapendsubarray",
            "best",
            "use",
            "textyaalendsubarray",
            "uses"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The upper part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the accuracy of StreamLAAL that uses resegmentation and the original short-form metrics evaluated <em class=\"ltx_emph ltx_font_italic\">without resegmentation</em> on long-form systems.\nWe see that the accuracies are low, not exceeding 66% on all systems.\nCompared to StreamLAAL, the best-performing AL metric loses 15 to 16% absolute, and the gap is even wider compared to the proposed LongYAAL, with AL falling short by 29 points.\nThe lower part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\nreports the accuracy of latency metrics in long-form systems <em class=\"ltx_emph ltx_font_italic\">with resegmentation</em>.\nWe see that the resegmentation quality significantly influences the accuracy. StreamLAAL and LongLAAL share the same definition, but differ in the resegmentation&#8211;while StreamLAAL uses the mWERSegmenter, LongLAAL (and all other\n&#8220;Long-&#8221; metrics) uses our <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>. The gap in accuracy is 8 to 10 points in all subsets, showing trends similar to those in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\n<span class=\"ltx_text ltx_font_italic\">These results highlight the critical role of resegmentation in ensuring reliable latency evaluation in the long-form regime.</span> Additional observations are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4\" title=\"Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
            "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows\nthat the proposed LongYAAL has the highest accuracy in all subsets. LongATD and LongDAL show slightly worse results, but not\nstatistically significant. This contrasts with &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, where ATD and DAL perform worse than *AL metrics.\nWe explain this discrepancy by the fact that both metrics include tail words that rarely occur in the long-form regime.\nWe attribute the marginal difference of LongATD compared to LongYAAL to its assumption of 300ms words in the source speech, which is dynamic in LongYAAL in the form of the parameter <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and the difference in LongDAL is caused by the artificial minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E4\" title=\"In Differentiable Average Lagging (DAL; Cherry and Foster, 2019) &#8227; 2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nLongLAAL ties with LongYAAL in most subsets, but seems slightly worse when in easily distinguishable systems (<math alttext=\"p\\text{-val}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>-val</mtext></mrow><annotation encoding=\"application/x-tex\">p\\text{-val}</annotation></semantics></math>&lt;0.001), where the metric loses 2 points in accuracy. LongLAAL, unlike LongYAAL, disregards words generated beyond the reference segment boundaries. The number of words ignored increases with the true latency of the system (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1.SSS0.Px3\" title=\"Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), which is more prevalent in this subset. Similarly, LongAL ignores the tail words in the resegmentation and is also vulnerable to overgeneration <cite class=\"ltx_cite ltx_citemacro_cite\">Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>); Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite>. Finally, AP performs the worst, losing more than 21 points compared to other metrics, which we attribute to the metric&#8217;s sensitivity to variable segment length. <span class=\"ltx_text ltx_font_italic\">These results position LongYAAL as the most reliable metric for assessing latency in long-form SimulST</span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency&#8211;the delay between speech input and the translated output.\nWhile quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented.\nIn this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes.\nWe uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more\naccurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems.</p>\n\n",
                "matched_terms": [
                    "unsegmented",
                    "yaal",
                    "metric",
                    "metrics",
                    "softsegmenter",
                    "regime",
                    "systems",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) is the task in which the system produces incremental translation concurrently with the speaker&#8217;s speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib27\" title=\"\">2020</a>)</cite>. SimulST models have to balance between quality and latency of the output, which is the time elapsed between when a word is uttered and when its corresponding translation is produced. Although translation quality metrics are extensively studied in offline ST and in the related field of machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Freitag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib11\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib10\" title=\"\">2023</a>; Zouhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib30\" title=\"\">2024</a>)</cite>, there is no study on the reliability of latency metrics.\nThe most common latency\nmetrics in SimulST <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho and Esipova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib8\" title=\"\">2016</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib16\" title=\"\">2019</a>; Cherry and Foster, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib7\" title=\"\">2019</a>; Pol&#225;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>; Kano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib14\" title=\"\">2023</a>)</cite>, even though with different approximations, base their calculation on\nsimplifying assumptions such as uniform word duration, absence of pauses, and strict monotonic alignment between source speech and target translation.\nHowever, despite relying on\nthe same assumptions, these metrics often produce very inconsistent assessments.\nThis inconsistency is clearly illustrated in the results of the IWSLT 2023 Shared Task on Simultaneous Translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, where different metrics produced substantially different rankings (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S1.F1\" title=\"In 1 Introduction &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). Such variability raises serious concerns about the validity of current evaluation protocols and their ability to support meaningful comparisons between systems.\nMoreover, this risk can be further exacerbated when shifting from dealing with already presegmented speech, i.e., <span class=\"ltx_text ltx_font_italic\">short-form</span> SimulST, to unsegmented audio, i.e., <span class=\"ltx_text ltx_font_italic\">long-form</span> SimulST, where information about sentence boundaries is not available, further complicating the evaluation <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "unsegmented",
                    "systems",
                    "metrics",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present the first comprehensive evaluation of latency metrics for SimulST under several aspects, including diverse systems, language pairs, and short- and long-form regimes. Through an in-depth analysis of systems submitted to recent IWSLT SimulST Shared Tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>; Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, we reveal that existing metrics can lead to misleading conclusions and hinder effective system design. We show that the inconsistent evaluations are not primarily due to the aforementioned assumptions, but rather to a structural bias in how latency is measured&#8211;particularly\nin how segmentation influences SimulST models&#8217; behavior.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "metrics",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose YAAL (Yet Another Average Lagging), a refined latency metric designed to mitigate the biases present in existing latency metrics.\nOur extensive experiments demonstrate that YAAL yields more reliable latency estimates, consistently aligning better with the actual behavior of SimulST systems.\nFurthermore, we also show that resegmentation, which pairs segment-level predictions with their corresponding reference, is necessary to produce meaningful latency measurements for long-form SimulST. To this end, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a new resegmentation tool, and extend our YAAL to LongYAAL, which deals with audio streams. Compared to the current standard alignment tool used in the speech translation community <cite class=\"ltx_cite ltx_citemacro_citep\">(Matusov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> significantly improves alignment quality, enabling more accurate evaluation in long-form scenarios.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The code for YAAL, its long-form variant LongYAAL, and <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> will be released upon the paper acceptance under Apache 2.0 license.</span></span></span></p>\n\n",
                "matched_terms": [
                    "yaal",
                    "metric",
                    "metrics",
                    "softsegmenter",
                    "systems",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The short-form is the most common evaluation regime of SimulST <cite class=\"ltx_cite ltx_citemacro_cite\">Anastasopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>); Agarwal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>); Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, where all recordings of the test set are divided, usually following sentence boundaries, into short segments of a few seconds.\nEach segment consists of source audio <math alttext=\"\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is a small portion of raw audio, i.e., audio chunk, and reference translation <math alttext=\"\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>y</mi><mn>1</mn><mi>R</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>y</mi><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow><mi>R</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]</annotation></semantics></math>. Each\naudio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is incrementally fed to the system,\nwhich concurrently\noutputs a translation token <math alttext=\"y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">y_{j}</annotation></semantics></math> at timestamp <math alttext=\"d_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">d_{j}</annotation></semantics></math>, i.e., total duration of audio chunks up to and including the audio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math>. Under these settings, we describe the latency metrics operating in the short-form regime:</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">is an AL modification that is robust to overgeneration, i.e., when the hypothesis <math alttext=\"\\mathbf{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>&#119832;</mi><annotation encoding=\"application/x-tex\">\\mathbf{Y}</annotation></semantics></math> is much longer than <math alttext=\"\\mathbf{Y^{R}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}}</annotation></semantics></math>, which makes the original AL produce negative delays when <math alttext=\"|\\mathbf{Y}|\\gg|\\mathbf{Y^{R}}|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119832;</mi><mo stretchy=\"false\">|</mo></mrow><mo>&#8811;</mo><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">|\\mathbf{Y}|\\gg|\\mathbf{Y^{R}}|</annotation></semantics></math>.\nTo overcome this problem, which was unduly rewarding overgenerating systems,\n<cite class=\"ltx_cite ltx_citemacro_citet\">Pol&#225;k et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>)</cite> proposed\nthe modification\n<math alttext=\"\\gamma=\\max(|\\mathbf{Y}|,|\\mathbf{Y^{R}}|)/|\\mathbf{X}|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mrow><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">|</mo><mi>&#119832;</mi><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\gamma=\\max(|\\mathbf{Y}|,|\\mathbf{Y^{R}}|)/|\\mathbf{X}|</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "original",
                    "systems",
                    "proposed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">modifies AL by introducing\na minimal delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> after each\nstep.\nUnlike AL and LAAL, DAL considers all tokens in the hypothesis,\nwithout\ncutoff after <math alttext=\"i&gt;\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\mathbf{X})</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "dal",
                    "all",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form evaluation regime evaluates SimulST systems more realistically <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>, as it assesses their ability to handle long audio streams, often spanning several minutes.\nSince all metrics were developed for the short-form regime,\nrecent studies <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>\nresorted to resegmentation of translations and delays based on the reference translation <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib20\" title=\"\">2005b</a>)</cite>, and computed the metrics on the segment level. We explain the long-form variant of LAAL <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite> below.</p>\n\n",
                "matched_terms": [
                    "laal",
                    "all",
                    "metrics",
                    "regime",
                    "systems",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">extends the LAAL metric to unsegmented audio streams <math alttext=\"\\mathbf{S}=[\\mathbf{X}_{1},...,\\mathbf{X}_{|\\mathbf{S}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119826;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119831;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119831;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}=[\\mathbf{X}_{1},...,\\mathbf{X}_{|\\mathbf{S}|}]</annotation></semantics></math>, paired with a continuous stream of predicted translations <math alttext=\"\\mathbf{Y_{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119826;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{S}}</annotation></semantics></math>. Since reference translations <math alttext=\"\\mathbf{Y^{R}_{1}},...,\\mathbf{Y^{R}_{|\\mathbf{S}|}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119832;</mi><mn>&#120783;</mn><mi>&#119825;</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119832;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow><mi>&#119825;</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}_{1}},...,\\mathbf{Y^{R}_{|\\mathbf{S}|}}</annotation></semantics></math> are only available at segment-level <math alttext=\"\\mathbf{X_{1}},...,\\mathbf{X}_{|\\mathbf{S}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#119831;</mi><mn>&#120783;</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119831;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X_{1}},...,\\mathbf{X}_{|\\mathbf{S}|}</annotation></semantics></math>, prediction <math alttext=\"\\mathbf{Y_{S}}=[\\mathbf{Y_{1}},...,\\mathbf{Y_{|\\mathbf{S}|}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#119832;</mi><mi>&#119826;</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119832;</mi><mn>&#120783;</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119832;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y_{S}}=[\\mathbf{Y_{1}},...,\\mathbf{Y_{|\\mathbf{S}|}}]</annotation></semantics></math> with the corresponding delays is segmented based on reference sentences <math alttext=\"\\mathbf{Y^{R}_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119832;</mi><mi>&#119852;</mi><mi>&#119825;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}_{s}}</annotation></semantics></math> to obtain segment-level predictions. Then, StreamLAAL is computed as:</p>\n\n",
                "matched_terms": [
                    "unsegmented",
                    "metric",
                    "streamlaal",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Where <math alttext=\"d^{*}_{i}=(i-1)\\cdot|\\mathbf{X_{s}}|/{\\max\\{|\\mathbf{Y_{s}}|,|\\mathbf{Y^{R}_{s}|\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi>d</mi><mi>i</mi><mo>&#8727;</mo></msubsup><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#8901;</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>/</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119832;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msubsup><mi>&#119832;</mi><mi>&#119852;</mi><mi>&#119825;</mi></msubsup><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d^{*}_{i}=(i-1)\\cdot|\\mathbf{X_{s}}|/{\\max\\{|\\mathbf{Y_{s}}|,|\\mathbf{Y^{R}_{s}|\\}}}</annotation></semantics></math>\nIn practice, the LAAL metric is calculated for every speech segment <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math> of the stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> and its corresponding reference <math alttext=\"\\mathbf{Y^{R}_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119832;</mi><mi>&#119852;</mi><mi>&#119825;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}_{s}}</annotation></semantics></math> with the automatically aligned prediction <math alttext=\"\\mathbf{Y_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{s}}</annotation></semantics></math> and then averaged over all the speech segments of the stream <math alttext=\"\\mathbf{X_{1}},...,\\mathbf{X_{|\\mathbf{S}|}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><msub><mi>&#119831;</mi><mn>&#120783;</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119831;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X_{1}},...,\\mathbf{X_{|\\mathbf{S}|}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "all",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, <cite class=\"ltx_cite ltx_citemacro_citet\">Huber et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib12\" title=\"\">2023</a>)</cite> proposed an evaluation with resegmentation provided by the system. The evaluation framework expects that the system outputs the segment&#8217;s start and end timestamps that align with the source. First, most systems, including all the IWSLT systems, do not output this information, and relying on the system&#8217;s self-reported alignment might hinder the reliability. Second, as we empirically show in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, relying on potentially low-quality resegmentation significantly lowers the accuracy of latency evaluation. Finally, different segmentations render the observed latencies incomparable across different systems.</p>\n\n",
                "matched_terms": [
                    "all",
                    "systems",
                    "proposed",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of audio segmentation in short-form evaluations significantly affects translation behavior and latency. In practice, short-form SimulST systems are evaluated in a simulated environment where each segment is processed independently <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib17\" title=\"\">2020</a>)</cite>. When the entire source segment has been consumed by the system, the translation is often still in progress. At that point, the simulator requests the remaining translation, which the model emits without any additional delay.\nThis setup introduces two unrealistic conditions. First, the audio is typically segmented in advance by a human annotator or an automatic model with access to the full audio (<span class=\"ltx_text ltx_font_italic\">Oracle Segmentation</span>). Second, the model is allowed to generate the remaining translation (hereinafter, <span class=\"ltx_text ltx_font_italic\">tail words</span>) instantaneously once the input segment ends. These factors unduly distort short-form evaluations, both by providing high-quality segmentation and eliminating the delay that would occur in a realistic setting, where the system must wait to confirm that the sentence has ended. In a more realistic scenario, a model has to rely on online segmentation (<span class=\"ltx_text ltx_font_italic\">Simultaneous Segmentation</span>) and thus delay the final translation until it is confident that the input sentence is complete, thereby introducing extra latency.\nThis discrepancy is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on these observations, we categorize existing short-form latency metrics (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.SS1\" title=\"2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>) into two main groups, depending on whether they include all translated words or only a subset in their latency computation. The first group&#8211;AP, DAL, and ATD&#8211;includes all translated words in the calculation.\nDAL attempts to mitigate the impact of tail words by adding a minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> after each generated word (also within the same step), thus &#8220;spreading&#8221; the tail beyond the sentence.\nHowever, <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math>\nsimply reflects the average source-to-target length ratio and does not accurately capture the system behavior for tail words in settings without segmentation.\nIf multiple words are emitted as tail words, DAL can significantly overestimate\nlatency.\nIn the edge case of a system that waits for an end-of-segment signal (i.e., an offline system), DAL\nreturns the segment length, failing to capture the system&#8217;s true behavior&#8211;in this case, undefined latency.\nAP assigns a delay of 1 to each tail word\nas the entire recording has to be processed to emit that word, thus, the proportion is 1. Although AP is marginally less sensitive to segmentation than DAL&#8211;since it operates on proportions rather than absolute delays&#8211;it still fails to capture system behavior faithfully for the tail words.\nATD also considers all translated words.\nHowever, unlike DAL, it does not apply corrections for tail word behavior, making it the most sensitive to segmentation artifacts among the three metrics.</p>\n\n",
                "matched_terms": [
                    "atd",
                    "dal",
                    "all",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second group&#8211;AL and LAAL&#8211;computes latency only for words emitted up to and including the cutoff point <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>, which marks the first word generated after the end of the input segment.\nThis corresponds to the word &#8220;<span class=\"ltx_text ltx_font_italic\">gemeinn&#252;tzige</span>&#8221; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nAs discussed, in the short-form regime with oracle segmentation, the <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>-th and following words are often translated earlier than in a more realistic\nlong-form scenario.\nAs a result, this cutoff introduces a systematic bias in the latency estimate, which may lead to either underestimation or overestimation, depending on the system&#8217;s policy.</p>\n\n",
                "matched_terms": [
                    "longform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AP, DAL, ATD, AL, and, more recently, LAAL became established metrics in the short-form evaluation of SimulST. However, as discussed above, including any of the tail words in the latency computation leads to a systematic bias that undermines fair comparisons.\nTo cope with this bias, we propose a new metric derived from the LAAL metric:</p>\n\n",
                "matched_terms": [
                    "laal",
                    "metric",
                    "dal",
                    "metrics",
                    "atd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form regime offers a more realistic evaluation setting by assessing systems on continuous, unsegmented audio streams that better reflect real-world use cases. However, the widely used latency metrics were originally designed for the short-form regime and do not directly extend to this setting.</p>\n\n",
                "matched_terms": [
                    "unsegmented",
                    "metrics",
                    "use",
                    "regime",
                    "systems",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, metrics such as AL, LAAL, and DAL\nrely on a <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> parameter,\nrepresenting the average target-to-source length ratio. However, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> can vary substantially across different segments within the same audio, leading to inconsistent and unreliable latency estimates <cite class=\"ltx_cite ltx_citemacro_cite\">Iranzo-S&#225;nchez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib13\" title=\"\">2021</a>)</cite>.\nSecond, AP\ntends to converge toward 0 for\nlong recordings,\nas typical speech inputs are significantly longer than the translations,\ni.e., <math alttext=\"|\\mathbf{X}|\\gg|\\mathbf{Y}|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow><mo>&#8811;</mo><mrow><mo stretchy=\"false\">|</mo><mi>&#119832;</mi><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">|\\mathbf{X}|\\gg|\\mathbf{Y}|</annotation></semantics></math>,\nleading <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E1\" title=\"In Average Proportion (AP; Cho and Esipova, 2016) &#8227; 2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> to approach 0.\nFinally, ATD\nassumes that each speech token has a fixed duration and that source and target tokens align monotonically&#8211;assumptions that are overly restrictive and especially unrealistic for long-form speech.</p>\n\n",
                "matched_terms": [
                    "laal",
                    "dal",
                    "metrics",
                    "atd",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, StreamLAAL has introduced resegmenting long inputs into short segments and computing latency on these units. While StreamLAAL provides adaptation of existing metrics to long-form input, it relies on the mWERSegmenter tool <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, which may introduce alignment errors <cite class=\"ltx_cite ltx_citemacro_cite\">Amrhein and Haddow (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib3\" title=\"\">2022</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>, and\ncomputes latency up to the cutoff word <math alttext=\"\\tau(\\mathbf{X_{i}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mi>&#119842;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X_{i}})</annotation></semantics></math> (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E7\" title=\"In Streaming LAAL (StreamLAAL; Papi et al., 2024) &#8227; 2.2 Long-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>), which can lead to the systematic bias (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.SS1\" title=\"3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>)\nTo overcome these limitations, we propose a new resegmentation method and an extension of the YAAL metric for the long-form regime.</p>\n\n",
                "matched_terms": [
                    "yaal",
                    "metric",
                    "metrics",
                    "regime",
                    "longform",
                    "streamlaal",
                    "mwersegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a new resegmentation method inspired by <cite class=\"ltx_cite ltx_citemacro_citet\">Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>, employing a softer alignment strategy to more accurately match the translation output with the reference segments.\nWe start by lowercasing and tokenizing both the reference and the system hypotheses. This allows for a more precise alignment around the sentence ends, especially in cases where the reference and the model differ in sentence segmentation. However, we still keep the original texts in memory so as not to interfere with the machine translation quality evaluation that is computed over the resegmented hypotheses. Additionally, we keep the delay together with each token and use it during the alignment process to prevent alignment of tokens to future segments, which leads to spurious negative latencies. For alignment, we maximize the following score:</p>\n\n",
                "matched_terms": [
                    "resegmented",
                    "original",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also extend YAAL to the long-form regime&#8211;i.e., LongYAAL. Unlike StreamLAAL, LongYAAL includes all words in the latency computation, even those generated beyond the aligned segment boundaries <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math>, i.e., all <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for <math alttext=\"i&gt;\\tau(\\mathbf{X_{s}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\mathbf{X_{s}})</annotation></semantics></math>. However, we exclude the final tail words produced after the end of the full stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>, i.e., <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for <math alttext=\"i&gt;\\tau(\\sum_{s=1}^{|\\mathbf{X}|}|\\mathbf{X_{s}}|)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8721;</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msubsup><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\sum_{s=1}^{|\\mathbf{X}|}|\\mathbf{X_{s}}|)</annotation></semantics></math>. This ensures that we include all words emitted beyond the segment boundaries <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math>, but we do not include the tail words generated at the end of the entire stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>. If the stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> consists of a single segment, LongYAAL coincides with YAAL.</p>\n\n",
                "matched_terms": [
                    "all",
                    "streamlaal",
                    "longform",
                    "yaal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the main evaluation, we adopt the pairwise comparison approach <cite class=\"ltx_cite ltx_citemacro_citep\">(Mathur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib18\" title=\"\">2020</a>)</cite>.\nRather than evaluating each system independently, we examine the difference between the scores of two systems: <math alttext=\"\\Delta=score(\\text{System A})-score(\\text{System B})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System A</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System B</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=score(\\text{System A})-score(\\text{System B})</annotation></semantics></math>.\nPairwise comparison better reflects the typical use case of latency metrics&#8211;distinguishing between two systems. We also restrict comparisons to system pairs evaluated on the same test set and language pair.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "scores",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Kocmi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib15\" title=\"\">2021</a>)</cite>, we evaluate the accuracy of binary comparisons: given a pair of systems, which one is better according to the true latency ranking (used as silver labels)? The accuracy is defined as the proportion of system pairs for which the relative ranking according to a metric matches that of the true latency:</p>\n\n",
                "matched_terms": [
                    "metric",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accuracy considers only the ranking, not the magnitude, of the latency differences, allowing us to aggregate comparisons across languages and test sets with different scales.\nHowever, this accuracy might be affected if two systems have similar latencies. To avoid this issue, we compute the accuracies in multiple subsets\nby removing pairs that are not significantly different according to Mann-Whitney U test on their\ntrue latencies.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We do not assume normal distribution of delays. Each system has different hypotheses, so we cannot use paired tests.</span></span></span> We use bootstrap resampling with <math alttext=\"N=10000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10000</mn></mrow><annotation encoding=\"application/x-tex\">N=10000</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tibshirani and Efron, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib29\" title=\"\">1993</a>)</cite> to estimate confidence intervals and consider all metrics within the 95% confidence interval of the top-performing metric to be statistically tied.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "all",
                    "metrics",
                    "tied",
                    "use",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a pairwise comparison of all short-form systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. An important first observation is that a significant portion of system pairs exhibit no or slightly negative correlations&#8211;points that create almost vertical lines and lines far off the diagonal. These systems share a <em class=\"ltx_emph ltx_font_italic\">anomalous simultaneous policy</em>: The lower the latency of the prefix generated simultaneously, the larger the portion of the sentence translated offline.\nWe assume that the underlying reason for the anomalous policy is that the system is too eager to emit output at the beginning. However, eventually it gets to the &#8220;dead end&#8221; of probable outputs and only emits the tail words at the signaled end of the sentence.\nThis policy, coupled with bias in latency metrics, leads to a severe overestimation of the actual latency of the systems. As we show below, overestimation of actual latency can hinder the reliable detection of systems following this anomalous policy.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "all",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The expected fraction of words translated online based on the value <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> of an automatic latency metric is <math alttext=\"O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mtext>avg</mtext></msub><mo>&#8722;</mo><mi>L</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>X</mi><mtext>avg</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}</annotation></semantics></math>, where <math alttext=\"X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>X</mi><mtext>avg</mtext></msub><annotation encoding=\"application/x-tex\">X_{\\text{avg}}</annotation></semantics></math> is the average segment length.\nIf the expected online-translated word fraction significantly exceeds the observed one, i.e., <math alttext=\"O_{\\text{e}}\\gg O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8811;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\gg O</annotation></semantics></math>, we conclude that the system follows the anomalous policy. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F4\" title=\"In Detecting Anomalous Policy &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the observed and expected fractions of the words translated online based on the proposed YAAL and LAAL metrics. Most systems follow a vertical line, i.e., <math alttext=\"O_{\\text{e}}\\sim O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8764;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\sim O</annotation></semantics></math>. However, based on YAAL, the red and brown circles<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All affected systems were submitted independently by two different teams in IWSLT 2022 and 2023, showing that the anomalous policy is not so uncommon.</span></span></span> show a strong disagreement between <math alttext=\"O^{\\text{YAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>YAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{YAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m7\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. Based on LAAL (crosses), we observe some disagreement between <math alttext=\"O^{\\text{LAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>LAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{LAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m9\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. However, the trend is not as clear as with YAAL, making the anomalous policy detection difficult. <em class=\"ltx_emph ltx_font_italic\">This shows the importance of accurate evaluation of latency using YAAL.</em></p>\n\n",
                "matched_terms": [
                    "yaal",
                    "laal",
                    "metric",
                    "metrics",
                    "proposed",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the metrics, we observe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>) that they all show positive correlations with the true latency, but each language pair has a slightly different scale, which <em class=\"ltx_emph ltx_font_italic\">motivates the use of accuracy instead of simple correlation</em>.\nTherefore, we further compare latency metrics in terms of accuracy in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. If we consider all system pairs (including systems following the anomalous policy), we see that all metrics significantly underperform YAAL (by more than 22% absolute), which reaches an accuracy of 96%. When we progressively filter out system pairs with similar true latency (decreasing <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> values), the accuracies slightly increase, but the ranking of the metrics remains. If we consider a subset that has a <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value between 0.001-0.05 (i.e., removing pairs with similar true latency that are, however, more difficult to distinguish), we see that YAAL still remains the most accurate by a margin of 25% absolute.\nIf we remove systems with the anomalous policy,\nall metrics gain a significant boost in accuracy (bottom part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). However, the YAAL metric remains the best metric in all subsets based on <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values, achieving 98 and 99% accuracy&#8211;even though it relies on assumptions such as uniform source token durations and monotonic source-to-target alignment.\nBased on these observations, we conclude that\n<span class=\"ltx_text ltx_font_italic\">the automatic YAAL metric is almost as accurate as\ntrue latency</span>. We include more accuracy evaluations by isolating different categories of systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3\" title=\"Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "yaal",
                    "metric",
                    "all",
                    "bottom",
                    "metrics",
                    "best",
                    "use",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3\" title=\"3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and empirically observed in this section, short-form evaluation can significantly distort latency evaluation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T2\" title=\"In Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, we present the average fraction\nof words translated <span class=\"ltx_text ltx_font_italic\">after</span> the end-of-segment signal, i.e., words omitted during evaluation by most latency metrics. A substantial portion of the translations are tail words, starting at 41% in the low-latency (1-2s) and reaching 72% in the high-latency regimes (4-5s).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Systems with higher-latency behavior have policies leading to deferred delays, and these delays in turn are more likely to overflow the source duration.</span></span></span>\n<span class=\"ltx_text ltx_font_italic\">Short-form evaluation, with artificial segment boundaries absent in real-world scenarios and metrics&#8217; problematic handling of tail words, often misrepresents SimulST system behavior.</span>\nThis raises serious concerns about\nits reliability\nand underscores the need for long-form evaluation,\nwhich we analyze in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS2\" title=\"5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we evaluate two resegmentation tools: mWERSegmenter and our proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>.\nThe evaluation is done on reconcatenated short-form\noutputs, allowing us to compare with gold segment boundaries.\nAs we can see in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_italic\">the accuracy of\nlatency evaluation is significantly higher with the proposed segmenter</span>.\nWhen filtering out comparable systems by the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value,\naccuracy\nfurther decreases\nwith mWERSegmenter, suggesting that the segmentation is not stable.\nBoth segmentation methods achieve over 99% accuracy, indicating resegmentation does not hinder quality assessment.</p>\n\n",
                "matched_terms": [
                    "softsegmenter",
                    "proposed",
                    "systems",
                    "accuracy",
                    "mwersegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented the first systematic evaluation of latency metrics for SimulST across several aspects, such as diverse systems, language pairs, and operating under short- and long-form speech processing. We have identified current pitfalls in the SimulST evaluation by isolating issues in the most commonly used metrics. To overcome these limitations, we propose YAAL, a new latency metric\nbetter\naligned with the\nshort-form evaluation regime.\nHowever,\nour analysis also reveals inherent shortcomings of short-form evaluation, further reinforcing the adoption of long-form evaluation as a more reliable alternative.\nMoreover, we also demonstrated that resegmentation is necessary to conduct a proper evaluation of systems operating under the long-form regime, and proposed an improved resegmentation tool coupled with the extension of YAAL for these settings&#8211;LongYAAL. The results showed that YAAL and LongYAAL improve over all other metrics in both regimes, establishing the new state-of-the-art metric for SimulST.</p>\n\n",
                "matched_terms": [
                    "yaal",
                    "metric",
                    "all",
                    "metrics",
                    "proposed",
                    "regime",
                    "systems",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our study offers a thorough evaluation of latency metrics for SimulST and introduces improved tools for both short- and long-form regimes, some limitations remain. First, our evaluation depends on reference translations and transcriptions, which may not be available or reliable in low-resource or real-time scenarios. Second, although the proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> improves alignment robustness, word-level alignment is still susceptible to errors in the presence of disfluencies or speech recognition noise. Third, our experimental analysis focuses on systems from the IWSLT Shared Tasks, which may not fully represent the range of techniques or data conditions used in broader academic or industrial settings. Fourth, our analysis focuses on high-resource languages, for which data were available, but the findings should be reconfirmed under low-resource language settings.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "softsegmenter",
                    "proposed",
                    "systems",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work introduces new evaluation tools that could influence future benchmarking of SimulST systems. However, there is a risk that over-reliance on specific metrics&#8211;even improved ones like YAAL and LongYAAL&#8211;could lead to overfitting system design to particular evaluation settings. For example, systems might be tuned to perform well under LongYAAL but degrade in real-world conditions that are not fully captured by the metric. Additionally, the use of automatic resegmentation methods may inadvertently introduce subtle biases if misaligned with human interpretation of segment boundaries. We encourage the community to use these tools alongside qualitative analysis and human-in-the-loop evaluations where possible.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "systems",
                    "yaal",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We did not train any models as part of this study. However, we used several evaluations that required computation. Most of the experiments were conducted on a standard desktop computer equipped with an Intel i7 processor and 32GB of RAM. For forced alignments with neural models, machine translation alignment, and the COMET translation quality metric, we used\na GPU cluster.\nHowever, these evaluations can be done on a desktop machine with a slightly longer runtime. The proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, YAAL, and LongYAAL can be run efficiently on a CPU.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "proposed",
                    "yaal",
                    "softsegmenter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the short-form regime, we use systems submitted to the IWSLT Simultaneous Speech Translation tracks of 2022 and 2023. Specifically, we use the SimulEval evaluation logs of the IWSLT 2022 and 2023 test sets <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, and the logs of the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> test set of the MuST-C data set <cite class=\"ltx_cite ltx_citemacro_citep\">(Cattoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib6\" title=\"\">2021</a>)</cite> that were submitted to IWSLT 2022. For the long-form regime, the logs are sourced from IWSLT 2025. In particular, for English-to-{German, Chinese, Japanese} the evaluation was done on the development set of the ACL 60/60 dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Salesky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib28\" title=\"\">2023</a>)</cite>, and IWSLT 2025 test set. For the Czech-to-English language pair, the evaluation was performed on the IWSLT 2024 development set <cite class=\"ltx_cite ltx_citemacro_cite\">Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite> and the IWSLT 2025 test set. A portion of the IWSLT 2024 development set contained segmented audio that could not be reconstructed into the original unsegmented audio.</p>\n\n",
                "matched_terms": [
                    "unsegmented",
                    "use",
                    "regime",
                    "systems",
                    "original",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T5\" title=\"In Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T7\" title=\"Table 7 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we present the number of systems used in the short- and long-form evaluations. The number of systems available to us was slightly larger, but we excluded all systems where the logs were incomplete (e.g., predictions for all recordings were not present, mismatched order of sources and hypotheses). Furthermore, in the long-form regime, we excluded one team entirely from the evaluation due to faulty logs. These logs contained a different number of predicted words and delays, which means that we could not faithfully determine generation timestamps for each predicted word.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "all",
                    "longform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Same as in the short-form evaluation, we follow the definition of the true latency in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4\" title=\"4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nHowever, there are two differences.\nAfter initial experiments, we observed that the Montreal Forced Aligner used in the short-form regime is not robust for the challenging conditions of the IWSLT 2025 test set, which is based on ACL presentations. The recordings include frequent restarts, repetitions, domain-specific terminology, and non-native speech. Instead, we use the alignment method implemented within WhisperX <cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib5\" title=\"\">2023</a>)</cite> for forced alignment. This tool leverages neural speech encoders that seem to be robust to the above-mentioned challenges. In particular, we used WhisperX&#8217;s default settings, i.e., PyTorch&#8217;s <span class=\"ltx_text ltx_font_typewriter\">WAV2VEC2_ASR_BASE_960H</span> for English and <span class=\"ltx_text ltx_font_typewriter\">comodoro/wav2vec2-xls-r-300m-cs-250</span> for Czech speech forced alignments.\nSecond, we perform resegmentation of the system hypotheses prior to the machine translation alignment with the reference. This step is necessary because the <span class=\"ltx_text ltx_font_typewriter\">awesome-align</span> tool uses the <span class=\"ltx_text ltx_font_typewriter\">bert-base-multilingual-cased</span> model for the alignment, and this model has a maximum input length of 512 tokens, which is much lower than the system hypotheses.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "use",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A natural question arises: <em class=\"ltx_emph ltx_font_italic\">Why rely on automatic latency metrics at all, when true latency offers a closer approximation of user experience?</em> In practice, computing true latency requires several requirements that limit its applicability. High-quality transcripts must be available, which is often not the case&#8211;particularly for low-resource languages or unwritten languages where transcription is infeasible. Moreover, forced alignment tools and reliable word-level translation alignments are typically available only for a small set of high-resource language pairs. Even when such resources exist, computing true latency involves multiple processing steps and is substantially more complex than evaluating standard automatic metrics. Importantly, as we show in our analysis in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5\" title=\"5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, several automatic metrics approximate true latency with high accuracy, making them a practical and effective alternative in most evaluation scenarios.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F6\" title=\"In Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we illustrate the trends after filtering out the systems affected by the anomalous policy (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). Unlike in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we see that all metrics and system pairs show a positive correlation with the true latency. As mentioned in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, language pairs exhibit different scales, making the use of the correlation coefficient more cumbersome and motivating the use of accuracy as described in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4.SS2.SSS0.Px3\" title=\"Accuracy &#8227; 4.2 Evaluation &#8227; 4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "use",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We were also interested in the accuracy of latency metrics when comparing related against unrelated systems. In our evaluation, we consider the systems submitted by one team as related.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>To the best of our knowledge, most teams submitted multiple systems that were based on the same system with varying hyperparameters.</span></span></span> We also use only a subset of the systems that were not affected by the anomalous simultaneous policy. The results are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "best",
                    "use",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Surprisingly, when evaluating related systems, all metrics perform almost perfectly, reaching accuracy between 97% and 100%. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F9\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency. Given a difference of at least <math alttext=\"\\sim 250\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS0.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>250</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 250</annotation></semantics></math> ms, all metrics except AP achieve 100% accuracy, and AP achieves around 99% accuracy.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "except",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results on unrelated systems (bottom half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F10\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>) are generally similar to the observations in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. All metrics show a loss of accuracy of no more than 4% points compared to the results on all systems. The only exception seems to be AP, which loses up to 11% points. The order of the metrics remains the same.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "bottom",
                    "half",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F11\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we show pairwise comparisons of systems evaluated in the long-form regime without resegmentation, and in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F12\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, we show the same systems evaluated in the long-form regime, but after resegmentation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F13\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "longform",
                    "regime",
                    "accuracy"
                ]
            }
        ]
    },
    "A1.T5": {
        "source_file": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation",
        "caption": "Table 5: Overview of the short-form systems in our evaluation.",
        "body": "Language Pair\nDataset\nTeams\nSystems\n\n\n\n\nEN\\rightarrowDE\nIWSLT 22 test set\n5\n68\n\n\nIWSLT 23 test set\n5\n5\n\n\ntst-COMMON\n7\n75\n\n\nEN\\rightarrowJA\nIWSLT 22 test set\n3\n9\n\n\nIWSLT 23 test set\n4\n4\n\n\ntst-COMMON\n3\n14\n\n\nEN\\rightarrowZH\nIWSLT 22 test set\n3\n14\n\n\nIWSLT 23 test set\n3\n3\n\n\ntst-COMMON\n3\n14",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Language Pair</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Teams</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Systems</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>DE</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">IWSLT 22 test set</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IWSLT 23 test set</td>\n<td class=\"ltx_td ltx_align_center\">5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">tst-COMMON</td>\n<td class=\"ltx_td ltx_align_center\">7</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">75</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>JA</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">IWSLT 22 test set</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IWSLT 23 test set</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">tst-COMMON</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"3\">EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ZH</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">IWSLT 22 test set</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IWSLT 23 test set</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">tst-COMMON</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">14</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "language",
            "enrightarrowja",
            "test",
            "enrightarrowzh",
            "shortform",
            "tstcommon",
            "evaluation",
            "pair",
            "dataset",
            "teams",
            "overview",
            "set",
            "systems",
            "iwslt",
            "enrightarrowde",
            "our"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T5\" title=\"In Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T7\" title=\"Table 7 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we present the number of systems used in the short- and long-form evaluations. The number of systems available to us was slightly larger, but we excluded all systems where the logs were incomplete (e.g., predictions for all recordings were not present, mismatched order of sources and hypotheses). Furthermore, in the long-form regime, we excluded one team entirely from the evaluation due to faulty logs. These logs contained a different number of predicted words and delays, which means that we could not faithfully determine generation timestamps for each predicted word.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency&#8211;the delay between speech input and the translated output.\nWhile quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented.\nIn this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes.\nWe uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more\naccurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems.</p>\n\n",
                "matched_terms": [
                    "language",
                    "shortform",
                    "evaluation",
                    "systems",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) is the task in which the system produces incremental translation concurrently with the speaker&#8217;s speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib27\" title=\"\">2020</a>)</cite>. SimulST models have to balance between quality and latency of the output, which is the time elapsed between when a word is uttered and when its corresponding translation is produced. Although translation quality metrics are extensively studied in offline ST and in the related field of machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Freitag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib11\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib10\" title=\"\">2023</a>; Zouhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib30\" title=\"\">2024</a>)</cite>, there is no study on the reliability of latency metrics.\nThe most common latency\nmetrics in SimulST <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho and Esipova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib8\" title=\"\">2016</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib16\" title=\"\">2019</a>; Cherry and Foster, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib7\" title=\"\">2019</a>; Pol&#225;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>; Kano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib14\" title=\"\">2023</a>)</cite>, even though with different approximations, base their calculation on\nsimplifying assumptions such as uniform word duration, absence of pauses, and strict monotonic alignment between source speech and target translation.\nHowever, despite relying on\nthe same assumptions, these metrics often produce very inconsistent assessments.\nThis inconsistency is clearly illustrated in the results of the IWSLT 2023 Shared Task on Simultaneous Translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, where different metrics produced substantially different rankings (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S1.F1\" title=\"In 1 Introduction &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). Such variability raises serious concerns about the validity of current evaluation protocols and their ability to support meaningful comparisons between systems.\nMoreover, this risk can be further exacerbated when shifting from dealing with already presegmented speech, i.e., <span class=\"ltx_text ltx_font_italic\">short-form</span> SimulST, to unsegmented audio, i.e., <span class=\"ltx_text ltx_font_italic\">long-form</span> SimulST, where information about sentence boundaries is not available, further complicating the evaluation <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "evaluation",
                    "iwslt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present the first comprehensive evaluation of latency metrics for SimulST under several aspects, including diverse systems, language pairs, and short- and long-form regimes. Through an in-depth analysis of systems submitted to recent IWSLT SimulST Shared Tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>; Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, we reveal that existing metrics can lead to misleading conclusions and hinder effective system design. We show that the inconsistent evaluations are not primarily due to the aforementioned assumptions, but rather to a structural bias in how latency is measured&#8211;particularly\nin how segmentation influences SimulST models&#8217; behavior.</p>\n\n",
                "matched_terms": [
                    "language",
                    "systems",
                    "evaluation",
                    "iwslt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose YAAL (Yet Another Average Lagging), a refined latency metric designed to mitigate the biases present in existing latency metrics.\nOur extensive experiments demonstrate that YAAL yields more reliable latency estimates, consistently aligning better with the actual behavior of SimulST systems.\nFurthermore, we also show that resegmentation, which pairs segment-level predictions with their corresponding reference, is necessary to produce meaningful latency measurements for long-form SimulST. To this end, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a new resegmentation tool, and extend our YAAL to LongYAAL, which deals with audio streams. Compared to the current standard alignment tool used in the speech translation community <cite class=\"ltx_cite ltx_citemacro_citep\">(Matusov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> significantly improves alignment quality, enabling more accurate evaluation in long-form scenarios.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The code for YAAL, its long-form variant LongYAAL, and <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> will be released upon the paper acceptance under Apache 2.0 license.</span></span></span></p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The short-form is the most common evaluation regime of SimulST <cite class=\"ltx_cite ltx_citemacro_cite\">Anastasopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>); Agarwal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>); Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, where all recordings of the test set are divided, usually following sentence boundaries, into short segments of a few seconds.\nEach segment consists of source audio <math alttext=\"\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is a small portion of raw audio, i.e., audio chunk, and reference translation <math alttext=\"\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>y</mi><mn>1</mn><mi>R</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>y</mi><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow><mi>R</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]</annotation></semantics></math>. Each\naudio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is incrementally fed to the system,\nwhich concurrently\noutputs a translation token <math alttext=\"y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">y_{j}</annotation></semantics></math> at timestamp <math alttext=\"d_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">d_{j}</annotation></semantics></math>, i.e., total duration of audio chunks up to and including the audio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math>. Under these settings, we describe the latency metrics operating in the short-form regime:</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "evaluation",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form evaluation regime evaluates SimulST systems more realistically <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>, as it assesses their ability to handle long audio streams, often spanning several minutes.\nSince all metrics were developed for the short-form regime,\nrecent studies <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>\nresorted to resegmentation of translations and delays based on the reference translation <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib20\" title=\"\">2005b</a>)</cite>, and computed the metrics on the segment level. We explain the long-form variant of LAAL <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite> below.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, <cite class=\"ltx_cite ltx_citemacro_citet\">Huber et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib12\" title=\"\">2023</a>)</cite> proposed an evaluation with resegmentation provided by the system. The evaluation framework expects that the system outputs the segment&#8217;s start and end timestamps that align with the source. First, most systems, including all the IWSLT systems, do not output this information, and relying on the system&#8217;s self-reported alignment might hinder the reliability. Second, as we empirically show in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, relying on potentially low-quality resegmentation significantly lowers the accuracy of latency evaluation. Finally, different segmentations render the observed latencies incomparable across different systems.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "iwslt",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of audio segmentation in short-form evaluations significantly affects translation behavior and latency. In practice, short-form SimulST systems are evaluated in a simulated environment where each segment is processed independently <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib17\" title=\"\">2020</a>)</cite>. When the entire source segment has been consumed by the system, the translation is often still in progress. At that point, the simulator requests the remaining translation, which the model emits without any additional delay.\nThis setup introduces two unrealistic conditions. First, the audio is typically segmented in advance by a human annotator or an automatic model with access to the full audio (<span class=\"ltx_text ltx_font_italic\">Oracle Segmentation</span>). Second, the model is allowed to generate the remaining translation (hereinafter, <span class=\"ltx_text ltx_font_italic\">tail words</span>) instantaneously once the input segment ends. These factors unduly distort short-form evaluations, both by providing high-quality segmentation and eliminating the delay that would occur in a realistic setting, where the system must wait to confirm that the sentence has ended. In a more realistic scenario, a model has to rely on online segmentation (<span class=\"ltx_text ltx_font_italic\">Simultaneous Segmentation</span>) and thus delay the final translation until it is confident that the input sentence is complete, thereby introducing extra latency.\nThis discrepancy is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AP, DAL, ATD, AL, and, more recently, LAAL became established metrics in the short-form evaluation of SimulST. However, as discussed above, including any of the tail words in the latency computation leads to a systematic bias that undermines fair comparisons.\nTo cope with this bias, we propose a new metric derived from the LAAL metric:</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form regime offers a more realistic evaluation setting by assessing systems on continuous, unsegmented audio streams that better reflect real-world use cases. However, the widely used latency metrics were originally designed for the short-form regime and do not directly extend to this setting.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the main evaluation, we adopt the pairwise comparison approach <cite class=\"ltx_cite ltx_citemacro_citep\">(Mathur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib18\" title=\"\">2020</a>)</cite>.\nRather than evaluating each system independently, we examine the difference between the scores of two systems: <math alttext=\"\\Delta=score(\\text{System A})-score(\\text{System B})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System A</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System B</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=score(\\text{System A})-score(\\text{System B})</annotation></semantics></math>.\nPairwise comparison better reflects the typical use case of latency metrics&#8211;distinguishing between two systems. We also restrict comparisons to system pairs evaluated on the same test set and language pair.</p>\n\n",
                "matched_terms": [
                    "language",
                    "test",
                    "pair",
                    "evaluation",
                    "systems",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Kocmi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib15\" title=\"\">2021</a>)</cite>, we evaluate the accuracy of binary comparisons: given a pair of systems, which one is better according to the true latency ranking (used as silver labels)? The accuracy is defined as the proportion of system pairs for which the relative ranking according to a metric matches that of the true latency:</p>\n\n",
                "matched_terms": [
                    "systems",
                    "pair"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accuracy considers only the ranking, not the magnitude, of the latency differences, allowing us to aggregate comparisons across languages and test sets with different scales.\nHowever, this accuracy might be affected if two systems have similar latencies. To avoid this issue, we compute the accuracies in multiple subsets\nby removing pairs that are not significantly different according to Mann-Whitney U test on their\ntrue latencies.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We do not assume normal distribution of delays. Each system has different hypotheses, so we cannot use paired tests.</span></span></span> We use bootstrap resampling with <math alttext=\"N=10000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10000</mn></mrow><annotation encoding=\"application/x-tex\">N=10000</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tibshirani and Efron, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib29\" title=\"\">1993</a>)</cite> to estimate confidence intervals and consider all metrics within the 95% confidence interval of the top-performing metric to be statistically tied.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a pairwise comparison of all short-form systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. An important first observation is that a significant portion of system pairs exhibit no or slightly negative correlations&#8211;points that create almost vertical lines and lines far off the diagonal. These systems share a <em class=\"ltx_emph ltx_font_italic\">anomalous simultaneous policy</em>: The lower the latency of the prefix generated simultaneously, the larger the portion of the sentence translated offline.\nWe assume that the underlying reason for the anomalous policy is that the system is too eager to emit output at the beginning. However, eventually it gets to the &#8220;dead end&#8221; of probable outputs and only emits the tail words at the signaled end of the sentence.\nThis policy, coupled with bias in latency metrics, leads to a severe overestimation of the actual latency of the systems. As we show below, overestimation of actual latency can hinder the reliable detection of systems following this anomalous policy.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To detect the anomalous policy, we propose comparing the observed and expected fractions of simultaneously translated words, based on automatic latency. The observed fraction of words translated strictly online across the entire test set <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>, i.e.,</p>\n\n",
                "matched_terms": [
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The expected fraction of words translated online based on the value <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> of an automatic latency metric is <math alttext=\"O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mtext>avg</mtext></msub><mo>&#8722;</mo><mi>L</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>X</mi><mtext>avg</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}</annotation></semantics></math>, where <math alttext=\"X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>X</mi><mtext>avg</mtext></msub><annotation encoding=\"application/x-tex\">X_{\\text{avg}}</annotation></semantics></math> is the average segment length.\nIf the expected online-translated word fraction significantly exceeds the observed one, i.e., <math alttext=\"O_{\\text{e}}\\gg O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8811;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\gg O</annotation></semantics></math>, we conclude that the system follows the anomalous policy. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F4\" title=\"In Detecting Anomalous Policy &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the observed and expected fractions of the words translated online based on the proposed YAAL and LAAL metrics. Most systems follow a vertical line, i.e., <math alttext=\"O_{\\text{e}}\\sim O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8764;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\sim O</annotation></semantics></math>. However, based on YAAL, the red and brown circles<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All affected systems were submitted independently by two different teams in IWSLT 2022 and 2023, showing that the anomalous policy is not so uncommon.</span></span></span> show a strong disagreement between <math alttext=\"O^{\\text{YAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>YAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{YAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m7\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. Based on LAAL (crosses), we observe some disagreement between <math alttext=\"O^{\\text{LAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>LAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{LAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m9\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. However, the trend is not as clear as with YAAL, making the anomalous policy detection difficult. <em class=\"ltx_emph ltx_font_italic\">This shows the importance of accurate evaluation of latency using YAAL.</em></p>\n\n",
                "matched_terms": [
                    "systems",
                    "iwslt",
                    "evaluation",
                    "teams"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the metrics, we observe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>) that they all show positive correlations with the true latency, but each language pair has a slightly different scale, which <em class=\"ltx_emph ltx_font_italic\">motivates the use of accuracy instead of simple correlation</em>.\nTherefore, we further compare latency metrics in terms of accuracy in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. If we consider all system pairs (including systems following the anomalous policy), we see that all metrics significantly underperform YAAL (by more than 22% absolute), which reaches an accuracy of 96%. When we progressively filter out system pairs with similar true latency (decreasing <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> values), the accuracies slightly increase, but the ranking of the metrics remains. If we consider a subset that has a <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value between 0.001-0.05 (i.e., removing pairs with similar true latency that are, however, more difficult to distinguish), we see that YAAL still remains the most accurate by a margin of 25% absolute.\nIf we remove systems with the anomalous policy,\nall metrics gain a significant boost in accuracy (bottom part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). However, the YAAL metric remains the best metric in all subsets based on <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values, achieving 98 and 99% accuracy&#8211;even though it relies on assumptions such as uniform source token durations and monotonic source-to-target alignment.\nBased on these observations, we conclude that\n<span class=\"ltx_text ltx_font_italic\">the automatic YAAL metric is almost as accurate as\ntrue latency</span>. We include more accuracy evaluations by isolating different categories of systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3\" title=\"Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "systems",
                    "pair"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3\" title=\"3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and empirically observed in this section, short-form evaluation can significantly distort latency evaluation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T2\" title=\"In Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, we present the average fraction\nof words translated <span class=\"ltx_text ltx_font_italic\">after</span> the end-of-segment signal, i.e., words omitted during evaluation by most latency metrics. A substantial portion of the translations are tail words, starting at 41% in the low-latency (1-2s) and reaching 72% in the high-latency regimes (4-5s).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Systems with higher-latency behavior have policies leading to deferred delays, and these delays in turn are more likely to overflow the source duration.</span></span></span>\n<span class=\"ltx_text ltx_font_italic\">Short-form evaluation, with artificial segment boundaries absent in real-world scenarios and metrics&#8217; problematic handling of tail words, often misrepresents SimulST system behavior.</span>\nThis raises serious concerns about\nits reliability\nand underscores the need for long-form evaluation,\nwhich we analyze in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS2\" title=\"5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we evaluate two resegmentation tools: mWERSegmenter and our proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>.\nThe evaluation is done on reconcatenated short-form\noutputs, allowing us to compare with gold segment boundaries.\nAs we can see in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_italic\">the accuracy of\nlatency evaluation is significantly higher with the proposed segmenter</span>.\nWhen filtering out comparable systems by the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value,\naccuracy\nfurther decreases\nwith mWERSegmenter, suggesting that the segmentation is not stable.\nBoth segmentation methods achieve over 99% accuracy, indicating resegmentation does not hinder quality assessment.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the accuracy of StreamLAAL that uses resegmentation and the original short-form metrics evaluated <em class=\"ltx_emph ltx_font_italic\">without resegmentation</em> on long-form systems.\nWe see that the accuracies are low, not exceeding 66% on all systems.\nCompared to StreamLAAL, the best-performing AL metric loses 15 to 16% absolute, and the gap is even wider compared to the proposed LongYAAL, with AL falling short by 29 points.\nThe lower part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\nreports the accuracy of latency metrics in long-form systems <em class=\"ltx_emph ltx_font_italic\">with resegmentation</em>.\nWe see that the resegmentation quality significantly influences the accuracy. StreamLAAL and LongLAAL share the same definition, but differ in the resegmentation&#8211;while StreamLAAL uses the mWERSegmenter, LongLAAL (and all other\n&#8220;Long-&#8221; metrics) uses our <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>. The gap in accuracy is 8 to 10 points in all subsets, showing trends similar to those in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\n<span class=\"ltx_text ltx_font_italic\">These results highlight the critical role of resegmentation in ensuring reliable latency evaluation in the long-form regime.</span> Additional observations are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4\" title=\"Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented the first systematic evaluation of latency metrics for SimulST across several aspects, such as diverse systems, language pairs, and operating under short- and long-form speech processing. We have identified current pitfalls in the SimulST evaluation by isolating issues in the most commonly used metrics. To overcome these limitations, we propose YAAL, a new latency metric\nbetter\naligned with the\nshort-form evaluation regime.\nHowever,\nour analysis also reveals inherent shortcomings of short-form evaluation, further reinforcing the adoption of long-form evaluation as a more reliable alternative.\nMoreover, we also demonstrated that resegmentation is necessary to conduct a proper evaluation of systems operating under the long-form regime, and proposed an improved resegmentation tool coupled with the extension of YAAL for these settings&#8211;LongYAAL. The results showed that YAAL and LongYAAL improve over all other metrics in both regimes, establishing the new state-of-the-art metric for SimulST.</p>\n\n",
                "matched_terms": [
                    "language",
                    "shortform",
                    "evaluation",
                    "systems",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our study offers a thorough evaluation of latency metrics for SimulST and introduces improved tools for both short- and long-form regimes, some limitations remain. First, our evaluation depends on reference translations and transcriptions, which may not be available or reliable in low-resource or real-time scenarios. Second, although the proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> improves alignment robustness, word-level alignment is still susceptible to errors in the presence of disfluencies or speech recognition noise. Third, our experimental analysis focuses on systems from the IWSLT Shared Tasks, which may not fully represent the range of techniques or data conditions used in broader academic or industrial settings. Fourth, our analysis focuses on high-resource languages, for which data were available, but the findings should be reconfirmed under low-resource language settings.</p>\n\n",
                "matched_terms": [
                    "language",
                    "evaluation",
                    "systems",
                    "iwslt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work introduces new evaluation tools that could influence future benchmarking of SimulST systems. However, there is a risk that over-reliance on specific metrics&#8211;even improved ones like YAAL and LongYAAL&#8211;could lead to overfitting system design to particular evaluation settings. For example, systems might be tuned to perform well under LongYAAL but degrade in real-world conditions that are not fully captured by the metric. Additionally, the use of automatic resegmentation methods may inadvertently introduce subtle biases if misaligned with human interpretation of segment boundaries. We encourage the community to use these tools alongside qualitative analysis and human-in-the-loop evaluations where possible.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the short-form regime, we use systems submitted to the IWSLT Simultaneous Speech Translation tracks of 2022 and 2023. Specifically, we use the SimulEval evaluation logs of the IWSLT 2022 and 2023 test sets <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, and the logs of the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> test set of the MuST-C data set <cite class=\"ltx_cite ltx_citemacro_citep\">(Cattoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib6\" title=\"\">2021</a>)</cite> that were submitted to IWSLT 2022. For the long-form regime, the logs are sourced from IWSLT 2025. In particular, for English-to-{German, Chinese, Japanese} the evaluation was done on the development set of the ACL 60/60 dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Salesky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib28\" title=\"\">2023</a>)</cite>, and IWSLT 2025 test set. For the Czech-to-English language pair, the evaluation was performed on the IWSLT 2024 development set <cite class=\"ltx_cite ltx_citemacro_cite\">Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite> and the IWSLT 2025 test set. A portion of the IWSLT 2024 development set contained segmented audio that could not be reconstructed into the original unsegmented audio.</p>\n\n",
                "matched_terms": [
                    "language",
                    "test",
                    "tstcommon",
                    "shortform",
                    "evaluation",
                    "pair",
                    "dataset",
                    "systems",
                    "iwslt",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Same as in the short-form evaluation, we follow the definition of the true latency in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4\" title=\"4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nHowever, there are two differences.\nAfter initial experiments, we observed that the Montreal Forced Aligner used in the short-form regime is not robust for the challenging conditions of the IWSLT 2025 test set, which is based on ACL presentations. The recordings include frequent restarts, repetitions, domain-specific terminology, and non-native speech. Instead, we use the alignment method implemented within WhisperX <cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib5\" title=\"\">2023</a>)</cite> for forced alignment. This tool leverages neural speech encoders that seem to be robust to the above-mentioned challenges. In particular, we used WhisperX&#8217;s default settings, i.e., PyTorch&#8217;s <span class=\"ltx_text ltx_font_typewriter\">WAV2VEC2_ASR_BASE_960H</span> for English and <span class=\"ltx_text ltx_font_typewriter\">comodoro/wav2vec2-xls-r-300m-cs-250</span> for Czech speech forced alignments.\nSecond, we perform resegmentation of the system hypotheses prior to the machine translation alignment with the reference. This step is necessary because the <span class=\"ltx_text ltx_font_typewriter\">awesome-align</span> tool uses the <span class=\"ltx_text ltx_font_typewriter\">bert-base-multilingual-cased</span> model for the alignment, and this model has a maximum input length of 512 tokens, which is much lower than the system hypotheses.</p>\n\n",
                "matched_terms": [
                    "test",
                    "shortform",
                    "evaluation",
                    "iwslt",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A natural question arises: <em class=\"ltx_emph ltx_font_italic\">Why rely on automatic latency metrics at all, when true latency offers a closer approximation of user experience?</em> In practice, computing true latency requires several requirements that limit its applicability. High-quality transcripts must be available, which is often not the case&#8211;particularly for low-resource languages or unwritten languages where transcription is infeasible. Moreover, forced alignment tools and reliable word-level translation alignments are typically available only for a small set of high-resource language pairs. Even when such resources exist, computing true latency involves multiple processing steps and is substantially more complex than evaluating standard automatic metrics. Importantly, as we show in our analysis in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5\" title=\"5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, several automatic metrics approximate true latency with high accuracy, making them a practical and effective alternative in most evaluation scenarios.</p>\n\n",
                "matched_terms": [
                    "language",
                    "evaluation",
                    "set",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F6\" title=\"In Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we illustrate the trends after filtering out the systems affected by the anomalous policy (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). Unlike in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we see that all metrics and system pairs show a positive correlation with the true latency. As mentioned in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, language pairs exhibit different scales, making the use of the correlation coefficient more cumbersome and motivating the use of accuracy as described in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4.SS2.SSS0.Px3\" title=\"Accuracy &#8227; 4.2 Evaluation &#8227; 4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We were also interested in the accuracy of latency metrics when comparing related against unrelated systems. In our evaluation, we consider the systems submitted by one team as related.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>To the best of our knowledge, most teams submitted multiple systems that were based on the same system with varying hyperparameters.</span></span></span> We also use only a subset of the systems that were not affected by the anomalous simultaneous policy. The results are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "our",
                    "evaluation",
                    "teams"
                ]
            }
        ]
    },
    "A1.T6": {
        "source_file": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation",
        "caption": "Table 6: Overview of the short-form systems in our evaluation after filtering out systems with anomalous policy.",
        "body": "Language Pair\nDataset\nTeams\nSystems\n\n\n\n\nEN\\rightarrowDE\nIWSLT 22 test set\n4\n40\n\n\nIWSLT 23 test set\n4\n4\n\n\ntst-COMMON\n6\n47\n\n\nEN\\rightarrowJA\nIWSLT 22 test set\n3\n7\n\n\nIWSLT 23 test set\n4\n4\n\n\ntst-COMMON\n3\n7\n\n\nEN\\rightarrowZH\nIWSLT 22 test set\n3\n14\n\n\nIWSLT 23 test set\n3\n3\n\n\ntst-COMMON\n3\n14",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Language Pair</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Teams</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Systems</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>DE</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">IWSLT 22 test set</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IWSLT 23 test set</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">tst-COMMON</td>\n<td class=\"ltx_td ltx_align_center\">6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">47</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>JA</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">IWSLT 22 test set</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IWSLT 23 test set</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">tst-COMMON</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"3\">EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ZH</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">IWSLT 22 test set</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IWSLT 23 test set</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">tst-COMMON</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">14</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "filtering",
            "overview",
            "iwslt",
            "our",
            "test",
            "anomalous",
            "teams",
            "language",
            "tstcommon",
            "evaluation",
            "out",
            "pair",
            "dataset",
            "systems",
            "set",
            "policy",
            "enrightarrowja",
            "shortform",
            "after",
            "enrightarrowde",
            "enrightarrowzh"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T5\" title=\"In Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T7\" title=\"Table 7 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we present the number of systems used in the short- and long-form evaluations. The number of systems available to us was slightly larger, but we excluded all systems where the logs were incomplete (e.g., predictions for all recordings were not present, mismatched order of sources and hypotheses). Furthermore, in the long-form regime, we excluded one team entirely from the evaluation due to faulty logs. These logs contained a different number of predicted words and delays, which means that we could not faithfully determine generation timestamps for each predicted word.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency&#8211;the delay between speech input and the translated output.\nWhile quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented.\nIn this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes.\nWe uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more\naccurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems.</p>\n\n",
                "matched_terms": [
                    "language",
                    "shortform",
                    "evaluation",
                    "systems",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) is the task in which the system produces incremental translation concurrently with the speaker&#8217;s speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib27\" title=\"\">2020</a>)</cite>. SimulST models have to balance between quality and latency of the output, which is the time elapsed between when a word is uttered and when its corresponding translation is produced. Although translation quality metrics are extensively studied in offline ST and in the related field of machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Freitag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib11\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib10\" title=\"\">2023</a>; Zouhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib30\" title=\"\">2024</a>)</cite>, there is no study on the reliability of latency metrics.\nThe most common latency\nmetrics in SimulST <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho and Esipova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib8\" title=\"\">2016</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib16\" title=\"\">2019</a>; Cherry and Foster, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib7\" title=\"\">2019</a>; Pol&#225;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>; Kano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib14\" title=\"\">2023</a>)</cite>, even though with different approximations, base their calculation on\nsimplifying assumptions such as uniform word duration, absence of pauses, and strict monotonic alignment between source speech and target translation.\nHowever, despite relying on\nthe same assumptions, these metrics often produce very inconsistent assessments.\nThis inconsistency is clearly illustrated in the results of the IWSLT 2023 Shared Task on Simultaneous Translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, where different metrics produced substantially different rankings (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S1.F1\" title=\"In 1 Introduction &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). Such variability raises serious concerns about the validity of current evaluation protocols and their ability to support meaningful comparisons between systems.\nMoreover, this risk can be further exacerbated when shifting from dealing with already presegmented speech, i.e., <span class=\"ltx_text ltx_font_italic\">short-form</span> SimulST, to unsegmented audio, i.e., <span class=\"ltx_text ltx_font_italic\">long-form</span> SimulST, where information about sentence boundaries is not available, further complicating the evaluation <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "iwslt",
                    "evaluation",
                    "shortform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present the first comprehensive evaluation of latency metrics for SimulST under several aspects, including diverse systems, language pairs, and short- and long-form regimes. Through an in-depth analysis of systems submitted to recent IWSLT SimulST Shared Tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>; Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, we reveal that existing metrics can lead to misleading conclusions and hinder effective system design. We show that the inconsistent evaluations are not primarily due to the aforementioned assumptions, but rather to a structural bias in how latency is measured&#8211;particularly\nin how segmentation influences SimulST models&#8217; behavior.</p>\n\n",
                "matched_terms": [
                    "language",
                    "iwslt",
                    "evaluation",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose YAAL (Yet Another Average Lagging), a refined latency metric designed to mitigate the biases present in existing latency metrics.\nOur extensive experiments demonstrate that YAAL yields more reliable latency estimates, consistently aligning better with the actual behavior of SimulST systems.\nFurthermore, we also show that resegmentation, which pairs segment-level predictions with their corresponding reference, is necessary to produce meaningful latency measurements for long-form SimulST. To this end, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a new resegmentation tool, and extend our YAAL to LongYAAL, which deals with audio streams. Compared to the current standard alignment tool used in the speech translation community <cite class=\"ltx_cite ltx_citemacro_citep\">(Matusov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> significantly improves alignment quality, enabling more accurate evaluation in long-form scenarios.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The code for YAAL, its long-form variant LongYAAL, and <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> will be released upon the paper acceptance under Apache 2.0 license.</span></span></span></p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The short-form is the most common evaluation regime of SimulST <cite class=\"ltx_cite ltx_citemacro_cite\">Anastasopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>); Agarwal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>); Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, where all recordings of the test set are divided, usually following sentence boundaries, into short segments of a few seconds.\nEach segment consists of source audio <math alttext=\"\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is a small portion of raw audio, i.e., audio chunk, and reference translation <math alttext=\"\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>y</mi><mn>1</mn><mi>R</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>y</mi><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow><mi>R</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]</annotation></semantics></math>. Each\naudio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is incrementally fed to the system,\nwhich concurrently\noutputs a translation token <math alttext=\"y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">y_{j}</annotation></semantics></math> at timestamp <math alttext=\"d_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">d_{j}</annotation></semantics></math>, i.e., total duration of audio chunks up to and including the audio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math>. Under these settings, we describe the latency metrics operating in the short-form regime:</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "evaluation",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form evaluation regime evaluates SimulST systems more realistically <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>, as it assesses their ability to handle long audio streams, often spanning several minutes.\nSince all metrics were developed for the short-form regime,\nrecent studies <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>\nresorted to resegmentation of translations and delays based on the reference translation <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib20\" title=\"\">2005b</a>)</cite>, and computed the metrics on the segment level. We explain the long-form variant of LAAL <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite> below.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, <cite class=\"ltx_cite ltx_citemacro_citet\">Huber et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib12\" title=\"\">2023</a>)</cite> proposed an evaluation with resegmentation provided by the system. The evaluation framework expects that the system outputs the segment&#8217;s start and end timestamps that align with the source. First, most systems, including all the IWSLT systems, do not output this information, and relying on the system&#8217;s self-reported alignment might hinder the reliability. Second, as we empirically show in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, relying on potentially low-quality resegmentation significantly lowers the accuracy of latency evaluation. Finally, different segmentations render the observed latencies incomparable across different systems.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "iwslt",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of audio segmentation in short-form evaluations significantly affects translation behavior and latency. In practice, short-form SimulST systems are evaluated in a simulated environment where each segment is processed independently <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib17\" title=\"\">2020</a>)</cite>. When the entire source segment has been consumed by the system, the translation is often still in progress. At that point, the simulator requests the remaining translation, which the model emits without any additional delay.\nThis setup introduces two unrealistic conditions. First, the audio is typically segmented in advance by a human annotator or an automatic model with access to the full audio (<span class=\"ltx_text ltx_font_italic\">Oracle Segmentation</span>). Second, the model is allowed to generate the remaining translation (hereinafter, <span class=\"ltx_text ltx_font_italic\">tail words</span>) instantaneously once the input segment ends. These factors unduly distort short-form evaluations, both by providing high-quality segmentation and eliminating the delay that would occur in a realistic setting, where the system must wait to confirm that the sentence has ended. In a more realistic scenario, a model has to rely on online segmentation (<span class=\"ltx_text ltx_font_italic\">Simultaneous Segmentation</span>) and thus delay the final translation until it is confident that the input sentence is complete, thereby introducing extra latency.\nThis discrepancy is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on these observations, we categorize existing short-form latency metrics (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.SS1\" title=\"2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>) into two main groups, depending on whether they include all translated words or only a subset in their latency computation. The first group&#8211;AP, DAL, and ATD&#8211;includes all translated words in the calculation.\nDAL attempts to mitigate the impact of tail words by adding a minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> after each generated word (also within the same step), thus &#8220;spreading&#8221; the tail beyond the sentence.\nHowever, <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math>\nsimply reflects the average source-to-target length ratio and does not accurately capture the system behavior for tail words in settings without segmentation.\nIf multiple words are emitted as tail words, DAL can significantly overestimate\nlatency.\nIn the edge case of a system that waits for an end-of-segment signal (i.e., an offline system), DAL\nreturns the segment length, failing to capture the system&#8217;s true behavior&#8211;in this case, undefined latency.\nAP assigns a delay of 1 to each tail word\nas the entire recording has to be processed to emit that word, thus, the proportion is 1. Although AP is marginally less sensitive to segmentation than DAL&#8211;since it operates on proportions rather than absolute delays&#8211;it still fails to capture system behavior faithfully for the tail words.\nATD also considers all translated words.\nHowever, unlike DAL, it does not apply corrections for tail word behavior, making it the most sensitive to segmentation artifacts among the three metrics.</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second group&#8211;AL and LAAL&#8211;computes latency only for words emitted up to and including the cutoff point <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>, which marks the first word generated after the end of the input segment.\nThis corresponds to the word &#8220;<span class=\"ltx_text ltx_font_italic\">gemeinn&#252;tzige</span>&#8221; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nAs discussed, in the short-form regime with oracle segmentation, the <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>-th and following words are often translated earlier than in a more realistic\nlong-form scenario.\nAs a result, this cutoff introduces a systematic bias in the latency estimate, which may lead to either underestimation or overestimation, depending on the system&#8217;s policy.</p>\n\n",
                "matched_terms": [
                    "policy",
                    "shortform",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AP, DAL, ATD, AL, and, more recently, LAAL became established metrics in the short-form evaluation of SimulST. However, as discussed above, including any of the tail words in the latency computation leads to a systematic bias that undermines fair comparisons.\nTo cope with this bias, we propose a new metric derived from the LAAL metric:</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form regime offers a more realistic evaluation setting by assessing systems on continuous, unsegmented audio streams that better reflect real-world use cases. However, the widely used latency metrics were originally designed for the short-form regime and do not directly extend to this setting.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the main evaluation, we adopt the pairwise comparison approach <cite class=\"ltx_cite ltx_citemacro_citep\">(Mathur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib18\" title=\"\">2020</a>)</cite>.\nRather than evaluating each system independently, we examine the difference between the scores of two systems: <math alttext=\"\\Delta=score(\\text{System A})-score(\\text{System B})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System A</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System B</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=score(\\text{System A})-score(\\text{System B})</annotation></semantics></math>.\nPairwise comparison better reflects the typical use case of latency metrics&#8211;distinguishing between two systems. We also restrict comparisons to system pairs evaluated on the same test set and language pair.</p>\n\n",
                "matched_terms": [
                    "language",
                    "test",
                    "pair",
                    "evaluation",
                    "systems",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Kocmi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib15\" title=\"\">2021</a>)</cite>, we evaluate the accuracy of binary comparisons: given a pair of systems, which one is better according to the true latency ranking (used as silver labels)? The accuracy is defined as the proportion of system pairs for which the relative ranking according to a metric matches that of the true latency:</p>\n\n",
                "matched_terms": [
                    "systems",
                    "pair"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accuracy considers only the ranking, not the magnitude, of the latency differences, allowing us to aggregate comparisons across languages and test sets with different scales.\nHowever, this accuracy might be affected if two systems have similar latencies. To avoid this issue, we compute the accuracies in multiple subsets\nby removing pairs that are not significantly different according to Mann-Whitney U test on their\ntrue latencies.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We do not assume normal distribution of delays. Each system has different hypotheses, so we cannot use paired tests.</span></span></span> We use bootstrap resampling with <math alttext=\"N=10000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10000</mn></mrow><annotation encoding=\"application/x-tex\">N=10000</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tibshirani and Efron, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib29\" title=\"\">1993</a>)</cite> to estimate confidence intervals and consider all metrics within the 95% confidence interval of the top-performing metric to be statistically tied.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a pairwise comparison of all short-form systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. An important first observation is that a significant portion of system pairs exhibit no or slightly negative correlations&#8211;points that create almost vertical lines and lines far off the diagonal. These systems share a <em class=\"ltx_emph ltx_font_italic\">anomalous simultaneous policy</em>: The lower the latency of the prefix generated simultaneously, the larger the portion of the sentence translated offline.\nWe assume that the underlying reason for the anomalous policy is that the system is too eager to emit output at the beginning. However, eventually it gets to the &#8220;dead end&#8221; of probable outputs and only emits the tail words at the signaled end of the sentence.\nThis policy, coupled with bias in latency metrics, leads to a severe overestimation of the actual latency of the systems. As we show below, overestimation of actual latency can hinder the reliable detection of systems following this anomalous policy.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "anomalous",
                    "policy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To detect the anomalous policy, we propose comparing the observed and expected fractions of simultaneously translated words, based on automatic latency. The observed fraction of words translated strictly online across the entire test set <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>, i.e.,</p>\n\n",
                "matched_terms": [
                    "policy",
                    "anomalous",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The expected fraction of words translated online based on the value <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> of an automatic latency metric is <math alttext=\"O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mtext>avg</mtext></msub><mo>&#8722;</mo><mi>L</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>X</mi><mtext>avg</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}</annotation></semantics></math>, where <math alttext=\"X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>X</mi><mtext>avg</mtext></msub><annotation encoding=\"application/x-tex\">X_{\\text{avg}}</annotation></semantics></math> is the average segment length.\nIf the expected online-translated word fraction significantly exceeds the observed one, i.e., <math alttext=\"O_{\\text{e}}\\gg O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8811;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\gg O</annotation></semantics></math>, we conclude that the system follows the anomalous policy. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F4\" title=\"In Detecting Anomalous Policy &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the observed and expected fractions of the words translated online based on the proposed YAAL and LAAL metrics. Most systems follow a vertical line, i.e., <math alttext=\"O_{\\text{e}}\\sim O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8764;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\sim O</annotation></semantics></math>. However, based on YAAL, the red and brown circles<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All affected systems were submitted independently by two different teams in IWSLT 2022 and 2023, showing that the anomalous policy is not so uncommon.</span></span></span> show a strong disagreement between <math alttext=\"O^{\\text{YAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>YAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{YAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m7\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. Based on LAAL (crosses), we observe some disagreement between <math alttext=\"O^{\\text{LAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>LAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{LAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m9\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. However, the trend is not as clear as with YAAL, making the anomalous policy detection difficult. <em class=\"ltx_emph ltx_font_italic\">This shows the importance of accurate evaluation of latency using YAAL.</em></p>\n\n",
                "matched_terms": [
                    "policy",
                    "anomalous",
                    "evaluation",
                    "teams",
                    "systems",
                    "iwslt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the metrics, we observe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>) that they all show positive correlations with the true latency, but each language pair has a slightly different scale, which <em class=\"ltx_emph ltx_font_italic\">motivates the use of accuracy instead of simple correlation</em>.\nTherefore, we further compare latency metrics in terms of accuracy in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. If we consider all system pairs (including systems following the anomalous policy), we see that all metrics significantly underperform YAAL (by more than 22% absolute), which reaches an accuracy of 96%. When we progressively filter out system pairs with similar true latency (decreasing <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> values), the accuracies slightly increase, but the ranking of the metrics remains. If we consider a subset that has a <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value between 0.001-0.05 (i.e., removing pairs with similar true latency that are, however, more difficult to distinguish), we see that YAAL still remains the most accurate by a margin of 25% absolute.\nIf we remove systems with the anomalous policy,\nall metrics gain a significant boost in accuracy (bottom part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). However, the YAAL metric remains the best metric in all subsets based on <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values, achieving 98 and 99% accuracy&#8211;even though it relies on assumptions such as uniform source token durations and monotonic source-to-target alignment.\nBased on these observations, we conclude that\n<span class=\"ltx_text ltx_font_italic\">the automatic YAAL metric is almost as accurate as\ntrue latency</span>. We include more accuracy evaluations by isolating different categories of systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3\" title=\"Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "policy",
                    "out",
                    "anomalous",
                    "pair",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3\" title=\"3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and empirically observed in this section, short-form evaluation can significantly distort latency evaluation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T2\" title=\"In Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, we present the average fraction\nof words translated <span class=\"ltx_text ltx_font_italic\">after</span> the end-of-segment signal, i.e., words omitted during evaluation by most latency metrics. A substantial portion of the translations are tail words, starting at 41% in the low-latency (1-2s) and reaching 72% in the high-latency regimes (4-5s).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Systems with higher-latency behavior have policies leading to deferred delays, and these delays in turn are more likely to overflow the source duration.</span></span></span>\n<span class=\"ltx_text ltx_font_italic\">Short-form evaluation, with artificial segment boundaries absent in real-world scenarios and metrics&#8217; problematic handling of tail words, often misrepresents SimulST system behavior.</span>\nThis raises serious concerns about\nits reliability\nand underscores the need for long-form evaluation,\nwhich we analyze in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS2\" title=\"5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "evaluation",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we evaluate two resegmentation tools: mWERSegmenter and our proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>.\nThe evaluation is done on reconcatenated short-form\noutputs, allowing us to compare with gold segment boundaries.\nAs we can see in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_italic\">the accuracy of\nlatency evaluation is significantly higher with the proposed segmenter</span>.\nWhen filtering out comparable systems by the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value,\naccuracy\nfurther decreases\nwith mWERSegmenter, suggesting that the segmentation is not stable.\nBoth segmentation methods achieve over 99% accuracy, indicating resegmentation does not hinder quality assessment.</p>\n\n",
                "matched_terms": [
                    "filtering",
                    "shortform",
                    "out",
                    "evaluation",
                    "systems",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the accuracy of StreamLAAL that uses resegmentation and the original short-form metrics evaluated <em class=\"ltx_emph ltx_font_italic\">without resegmentation</em> on long-form systems.\nWe see that the accuracies are low, not exceeding 66% on all systems.\nCompared to StreamLAAL, the best-performing AL metric loses 15 to 16% absolute, and the gap is even wider compared to the proposed LongYAAL, with AL falling short by 29 points.\nThe lower part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\nreports the accuracy of latency metrics in long-form systems <em class=\"ltx_emph ltx_font_italic\">with resegmentation</em>.\nWe see that the resegmentation quality significantly influences the accuracy. StreamLAAL and LongLAAL share the same definition, but differ in the resegmentation&#8211;while StreamLAAL uses the mWERSegmenter, LongLAAL (and all other\n&#8220;Long-&#8221; metrics) uses our <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>. The gap in accuracy is 8 to 10 points in all subsets, showing trends similar to those in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\n<span class=\"ltx_text ltx_font_italic\">These results highlight the critical role of resegmentation in ensuring reliable latency evaluation in the long-form regime.</span> Additional observations are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4\" title=\"Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented the first systematic evaluation of latency metrics for SimulST across several aspects, such as diverse systems, language pairs, and operating under short- and long-form speech processing. We have identified current pitfalls in the SimulST evaluation by isolating issues in the most commonly used metrics. To overcome these limitations, we propose YAAL, a new latency metric\nbetter\naligned with the\nshort-form evaluation regime.\nHowever,\nour analysis also reveals inherent shortcomings of short-form evaluation, further reinforcing the adoption of long-form evaluation as a more reliable alternative.\nMoreover, we also demonstrated that resegmentation is necessary to conduct a proper evaluation of systems operating under the long-form regime, and proposed an improved resegmentation tool coupled with the extension of YAAL for these settings&#8211;LongYAAL. The results showed that YAAL and LongYAAL improve over all other metrics in both regimes, establishing the new state-of-the-art metric for SimulST.</p>\n\n",
                "matched_terms": [
                    "language",
                    "shortform",
                    "evaluation",
                    "systems",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our study offers a thorough evaluation of latency metrics for SimulST and introduces improved tools for both short- and long-form regimes, some limitations remain. First, our evaluation depends on reference translations and transcriptions, which may not be available or reliable in low-resource or real-time scenarios. Second, although the proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> improves alignment robustness, word-level alignment is still susceptible to errors in the presence of disfluencies or speech recognition noise. Third, our experimental analysis focuses on systems from the IWSLT Shared Tasks, which may not fully represent the range of techniques or data conditions used in broader academic or industrial settings. Fourth, our analysis focuses on high-resource languages, for which data were available, but the findings should be reconfirmed under low-resource language settings.</p>\n\n",
                "matched_terms": [
                    "language",
                    "evaluation",
                    "systems",
                    "iwslt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work introduces new evaluation tools that could influence future benchmarking of SimulST systems. However, there is a risk that over-reliance on specific metrics&#8211;even improved ones like YAAL and LongYAAL&#8211;could lead to overfitting system design to particular evaluation settings. For example, systems might be tuned to perform well under LongYAAL but degrade in real-world conditions that are not fully captured by the metric. Additionally, the use of automatic resegmentation methods may inadvertently introduce subtle biases if misaligned with human interpretation of segment boundaries. We encourage the community to use these tools alongside qualitative analysis and human-in-the-loop evaluations where possible.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the short-form regime, we use systems submitted to the IWSLT Simultaneous Speech Translation tracks of 2022 and 2023. Specifically, we use the SimulEval evaluation logs of the IWSLT 2022 and 2023 test sets <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, and the logs of the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> test set of the MuST-C data set <cite class=\"ltx_cite ltx_citemacro_citep\">(Cattoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib6\" title=\"\">2021</a>)</cite> that were submitted to IWSLT 2022. For the long-form regime, the logs are sourced from IWSLT 2025. In particular, for English-to-{German, Chinese, Japanese} the evaluation was done on the development set of the ACL 60/60 dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Salesky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib28\" title=\"\">2023</a>)</cite>, and IWSLT 2025 test set. For the Czech-to-English language pair, the evaluation was performed on the IWSLT 2024 development set <cite class=\"ltx_cite ltx_citemacro_cite\">Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite> and the IWSLT 2025 test set. A portion of the IWSLT 2024 development set contained segmented audio that could not be reconstructed into the original unsegmented audio.</p>\n\n",
                "matched_terms": [
                    "language",
                    "test",
                    "tstcommon",
                    "shortform",
                    "evaluation",
                    "pair",
                    "dataset",
                    "systems",
                    "iwslt",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Same as in the short-form evaluation, we follow the definition of the true latency in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4\" title=\"4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nHowever, there are two differences.\nAfter initial experiments, we observed that the Montreal Forced Aligner used in the short-form regime is not robust for the challenging conditions of the IWSLT 2025 test set, which is based on ACL presentations. The recordings include frequent restarts, repetitions, domain-specific terminology, and non-native speech. Instead, we use the alignment method implemented within WhisperX <cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib5\" title=\"\">2023</a>)</cite> for forced alignment. This tool leverages neural speech encoders that seem to be robust to the above-mentioned challenges. In particular, we used WhisperX&#8217;s default settings, i.e., PyTorch&#8217;s <span class=\"ltx_text ltx_font_typewriter\">WAV2VEC2_ASR_BASE_960H</span> for English and <span class=\"ltx_text ltx_font_typewriter\">comodoro/wav2vec2-xls-r-300m-cs-250</span> for Czech speech forced alignments.\nSecond, we perform resegmentation of the system hypotheses prior to the machine translation alignment with the reference. This step is necessary because the <span class=\"ltx_text ltx_font_typewriter\">awesome-align</span> tool uses the <span class=\"ltx_text ltx_font_typewriter\">bert-base-multilingual-cased</span> model for the alignment, and this model has a maximum input length of 512 tokens, which is much lower than the system hypotheses.</p>\n\n",
                "matched_terms": [
                    "test",
                    "after",
                    "shortform",
                    "evaluation",
                    "iwslt",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A natural question arises: <em class=\"ltx_emph ltx_font_italic\">Why rely on automatic latency metrics at all, when true latency offers a closer approximation of user experience?</em> In practice, computing true latency requires several requirements that limit its applicability. High-quality transcripts must be available, which is often not the case&#8211;particularly for low-resource languages or unwritten languages where transcription is infeasible. Moreover, forced alignment tools and reliable word-level translation alignments are typically available only for a small set of high-resource language pairs. Even when such resources exist, computing true latency involves multiple processing steps and is substantially more complex than evaluating standard automatic metrics. Importantly, as we show in our analysis in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5\" title=\"5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, several automatic metrics approximate true latency with high accuracy, making them a practical and effective alternative in most evaluation scenarios.</p>\n\n",
                "matched_terms": [
                    "language",
                    "evaluation",
                    "set",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F6\" title=\"In Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we illustrate the trends after filtering out the systems affected by the anomalous policy (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). Unlike in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we see that all metrics and system pairs show a positive correlation with the true latency. As mentioned in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, language pairs exhibit different scales, making the use of the correlation coefficient more cumbersome and motivating the use of accuracy as described in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4.SS2.SSS0.Px3\" title=\"Accuracy &#8227; 4.2 Evaluation &#8227; 4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "filtering",
                    "language",
                    "policy",
                    "out",
                    "anomalous",
                    "systems",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We were also interested in the accuracy of latency metrics when comparing related against unrelated systems. In our evaluation, we consider the systems submitted by one team as related.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>To the best of our knowledge, most teams submitted multiple systems that were based on the same system with varying hyperparameters.</span></span></span> We also use only a subset of the systems that were not affected by the anomalous simultaneous policy. The results are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "policy",
                    "anomalous",
                    "evaluation",
                    "teams",
                    "systems",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F11\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we show pairwise comparisons of systems evaluated in the long-form regime without resegmentation, and in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F12\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, we show the same systems evaluated in the long-form regime, but after resegmentation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F13\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "after"
                ]
            }
        ]
    },
    "A1.T7": {
        "source_file": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation",
        "caption": "Table 7: Overview of the long-form systems in our evaluation.",
        "body": "Language Pair\nDataset\nTeams\nSystems\n\n\n\n\nEN\\rightarrowDE\nACL 6060 dev set\n6\n20\n\n\nIWSLT 25 test set\n6\n10\n\n\nEN\\rightarrowJA\nACL 6060 dev set\n3\n16\n\n\nIWSLT 25 test set\n2\n3\n\n\nEN\\rightarrowZH\nACL 6060 dev set\n4\n16\n\n\nIWSLT 25 test set\n4\n8\n\n\nCS\\rightarrowEN\nIWSLT 24 dev set\n2\n14\n\n\nIWSLT 25 test set\n2\n4",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Language Pair</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Teams</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Systems</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\">EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>DE</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">ACL 6060 dev set</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IWSLT 25 test set</td>\n<td class=\"ltx_td ltx_align_center\">6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\">EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>JA</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">ACL 6060 dev set</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IWSLT 25 test set</td>\n<td class=\"ltx_td ltx_align_center\">2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\">EN<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>ZH</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">ACL 6060 dev set</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">IWSLT 25 test set</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" rowspan=\"2\">CS<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>EN</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">IWSLT 24 dev set</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\">IWSLT 25 test set</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_b\">4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "language",
            "enrightarrowja",
            "acl",
            "test",
            "enrightarrowzh",
            "csrightarrowen",
            "evaluation",
            "pair",
            "dataset",
            "teams",
            "dev",
            "overview",
            "set",
            "systems",
            "iwslt",
            "longform",
            "enrightarrowde",
            "our"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T5\" title=\"In Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T7\" title=\"Table 7 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we present the number of systems used in the short- and long-form evaluations. The number of systems available to us was slightly larger, but we excluded all systems where the logs were incomplete (e.g., predictions for all recordings were not present, mismatched order of sources and hypotheses). Furthermore, in the long-form regime, we excluded one team entirely from the evaluation due to faulty logs. These logs contained a different number of predicted words and delays, which means that we could not faithfully determine generation timestamps for each predicted word.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency&#8211;the delay between speech input and the translated output.\nWhile quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented.\nIn this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes.\nWe uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more\naccurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems.</p>\n\n",
                "matched_terms": [
                    "language",
                    "evaluation",
                    "systems",
                    "longform",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) is the task in which the system produces incremental translation concurrently with the speaker&#8217;s speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib27\" title=\"\">2020</a>)</cite>. SimulST models have to balance between quality and latency of the output, which is the time elapsed between when a word is uttered and when its corresponding translation is produced. Although translation quality metrics are extensively studied in offline ST and in the related field of machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Freitag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib11\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib10\" title=\"\">2023</a>; Zouhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib30\" title=\"\">2024</a>)</cite>, there is no study on the reliability of latency metrics.\nThe most common latency\nmetrics in SimulST <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho and Esipova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib8\" title=\"\">2016</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib16\" title=\"\">2019</a>; Cherry and Foster, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib7\" title=\"\">2019</a>; Pol&#225;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>; Kano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib14\" title=\"\">2023</a>)</cite>, even though with different approximations, base their calculation on\nsimplifying assumptions such as uniform word duration, absence of pauses, and strict monotonic alignment between source speech and target translation.\nHowever, despite relying on\nthe same assumptions, these metrics often produce very inconsistent assessments.\nThis inconsistency is clearly illustrated in the results of the IWSLT 2023 Shared Task on Simultaneous Translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, where different metrics produced substantially different rankings (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S1.F1\" title=\"In 1 Introduction &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). Such variability raises serious concerns about the validity of current evaluation protocols and their ability to support meaningful comparisons between systems.\nMoreover, this risk can be further exacerbated when shifting from dealing with already presegmented speech, i.e., <span class=\"ltx_text ltx_font_italic\">short-form</span> SimulST, to unsegmented audio, i.e., <span class=\"ltx_text ltx_font_italic\">long-form</span> SimulST, where information about sentence boundaries is not available, further complicating the evaluation <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "iwslt",
                    "evaluation",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present the first comprehensive evaluation of latency metrics for SimulST under several aspects, including diverse systems, language pairs, and short- and long-form regimes. Through an in-depth analysis of systems submitted to recent IWSLT SimulST Shared Tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>; Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, we reveal that existing metrics can lead to misleading conclusions and hinder effective system design. We show that the inconsistent evaluations are not primarily due to the aforementioned assumptions, but rather to a structural bias in how latency is measured&#8211;particularly\nin how segmentation influences SimulST models&#8217; behavior.</p>\n\n",
                "matched_terms": [
                    "language",
                    "evaluation",
                    "systems",
                    "iwslt",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose YAAL (Yet Another Average Lagging), a refined latency metric designed to mitigate the biases present in existing latency metrics.\nOur extensive experiments demonstrate that YAAL yields more reliable latency estimates, consistently aligning better with the actual behavior of SimulST systems.\nFurthermore, we also show that resegmentation, which pairs segment-level predictions with their corresponding reference, is necessary to produce meaningful latency measurements for long-form SimulST. To this end, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a new resegmentation tool, and extend our YAAL to LongYAAL, which deals with audio streams. Compared to the current standard alignment tool used in the speech translation community <cite class=\"ltx_cite ltx_citemacro_citep\">(Matusov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> significantly improves alignment quality, enabling more accurate evaluation in long-form scenarios.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The code for YAAL, its long-form variant LongYAAL, and <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> will be released upon the paper acceptance under Apache 2.0 license.</span></span></span></p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "our",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The short-form is the most common evaluation regime of SimulST <cite class=\"ltx_cite ltx_citemacro_cite\">Anastasopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>); Agarwal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>); Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, where all recordings of the test set are divided, usually following sentence boundaries, into short segments of a few seconds.\nEach segment consists of source audio <math alttext=\"\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is a small portion of raw audio, i.e., audio chunk, and reference translation <math alttext=\"\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>y</mi><mn>1</mn><mi>R</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>y</mi><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow><mi>R</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]</annotation></semantics></math>. Each\naudio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is incrementally fed to the system,\nwhich concurrently\noutputs a translation token <math alttext=\"y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">y_{j}</annotation></semantics></math> at timestamp <math alttext=\"d_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">d_{j}</annotation></semantics></math>, i.e., total duration of audio chunks up to and including the audio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math>. Under these settings, we describe the latency metrics operating in the short-form regime:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form evaluation regime evaluates SimulST systems more realistically <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>, as it assesses their ability to handle long audio streams, often spanning several minutes.\nSince all metrics were developed for the short-form regime,\nrecent studies <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>\nresorted to resegmentation of translations and delays based on the reference translation <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib20\" title=\"\">2005b</a>)</cite>, and computed the metrics on the segment level. We explain the long-form variant of LAAL <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite> below.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, <cite class=\"ltx_cite ltx_citemacro_citet\">Huber et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib12\" title=\"\">2023</a>)</cite> proposed an evaluation with resegmentation provided by the system. The evaluation framework expects that the system outputs the segment&#8217;s start and end timestamps that align with the source. First, most systems, including all the IWSLT systems, do not output this information, and relying on the system&#8217;s self-reported alignment might hinder the reliability. Second, as we empirically show in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, relying on potentially low-quality resegmentation significantly lowers the accuracy of latency evaluation. Finally, different segmentations render the observed latencies incomparable across different systems.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "iwslt",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form regime offers a more realistic evaluation setting by assessing systems on continuous, unsegmented audio streams that better reflect real-world use cases. However, the widely used latency metrics were originally designed for the short-form regime and do not directly extend to this setting.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the main evaluation, we adopt the pairwise comparison approach <cite class=\"ltx_cite ltx_citemacro_citep\">(Mathur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib18\" title=\"\">2020</a>)</cite>.\nRather than evaluating each system independently, we examine the difference between the scores of two systems: <math alttext=\"\\Delta=score(\\text{System A})-score(\\text{System B})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System A</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System B</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=score(\\text{System A})-score(\\text{System B})</annotation></semantics></math>.\nPairwise comparison better reflects the typical use case of latency metrics&#8211;distinguishing between two systems. We also restrict comparisons to system pairs evaluated on the same test set and language pair.</p>\n\n",
                "matched_terms": [
                    "language",
                    "test",
                    "pair",
                    "evaluation",
                    "systems",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Kocmi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib15\" title=\"\">2021</a>)</cite>, we evaluate the accuracy of binary comparisons: given a pair of systems, which one is better according to the true latency ranking (used as silver labels)? The accuracy is defined as the proportion of system pairs for which the relative ranking according to a metric matches that of the true latency:</p>\n\n",
                "matched_terms": [
                    "systems",
                    "pair"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accuracy considers only the ranking, not the magnitude, of the latency differences, allowing us to aggregate comparisons across languages and test sets with different scales.\nHowever, this accuracy might be affected if two systems have similar latencies. To avoid this issue, we compute the accuracies in multiple subsets\nby removing pairs that are not significantly different according to Mann-Whitney U test on their\ntrue latencies.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We do not assume normal distribution of delays. Each system has different hypotheses, so we cannot use paired tests.</span></span></span> We use bootstrap resampling with <math alttext=\"N=10000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10000</mn></mrow><annotation encoding=\"application/x-tex\">N=10000</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tibshirani and Efron, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib29\" title=\"\">1993</a>)</cite> to estimate confidence intervals and consider all metrics within the 95% confidence interval of the top-performing metric to be statistically tied.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To detect the anomalous policy, we propose comparing the observed and expected fractions of simultaneously translated words, based on automatic latency. The observed fraction of words translated strictly online across the entire test set <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>, i.e.,</p>\n\n",
                "matched_terms": [
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The expected fraction of words translated online based on the value <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> of an automatic latency metric is <math alttext=\"O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mtext>avg</mtext></msub><mo>&#8722;</mo><mi>L</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>X</mi><mtext>avg</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}</annotation></semantics></math>, where <math alttext=\"X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>X</mi><mtext>avg</mtext></msub><annotation encoding=\"application/x-tex\">X_{\\text{avg}}</annotation></semantics></math> is the average segment length.\nIf the expected online-translated word fraction significantly exceeds the observed one, i.e., <math alttext=\"O_{\\text{e}}\\gg O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8811;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\gg O</annotation></semantics></math>, we conclude that the system follows the anomalous policy. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F4\" title=\"In Detecting Anomalous Policy &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the observed and expected fractions of the words translated online based on the proposed YAAL and LAAL metrics. Most systems follow a vertical line, i.e., <math alttext=\"O_{\\text{e}}\\sim O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8764;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\sim O</annotation></semantics></math>. However, based on YAAL, the red and brown circles<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All affected systems were submitted independently by two different teams in IWSLT 2022 and 2023, showing that the anomalous policy is not so uncommon.</span></span></span> show a strong disagreement between <math alttext=\"O^{\\text{YAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>YAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{YAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m7\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. Based on LAAL (crosses), we observe some disagreement between <math alttext=\"O^{\\text{LAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>LAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{LAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m9\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. However, the trend is not as clear as with YAAL, making the anomalous policy detection difficult. <em class=\"ltx_emph ltx_font_italic\">This shows the importance of accurate evaluation of latency using YAAL.</em></p>\n\n",
                "matched_terms": [
                    "systems",
                    "iwslt",
                    "evaluation",
                    "teams"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the metrics, we observe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>) that they all show positive correlations with the true latency, but each language pair has a slightly different scale, which <em class=\"ltx_emph ltx_font_italic\">motivates the use of accuracy instead of simple correlation</em>.\nTherefore, we further compare latency metrics in terms of accuracy in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. If we consider all system pairs (including systems following the anomalous policy), we see that all metrics significantly underperform YAAL (by more than 22% absolute), which reaches an accuracy of 96%. When we progressively filter out system pairs with similar true latency (decreasing <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> values), the accuracies slightly increase, but the ranking of the metrics remains. If we consider a subset that has a <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value between 0.001-0.05 (i.e., removing pairs with similar true latency that are, however, more difficult to distinguish), we see that YAAL still remains the most accurate by a margin of 25% absolute.\nIf we remove systems with the anomalous policy,\nall metrics gain a significant boost in accuracy (bottom part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). However, the YAAL metric remains the best metric in all subsets based on <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values, achieving 98 and 99% accuracy&#8211;even though it relies on assumptions such as uniform source token durations and monotonic source-to-target alignment.\nBased on these observations, we conclude that\n<span class=\"ltx_text ltx_font_italic\">the automatic YAAL metric is almost as accurate as\ntrue latency</span>. We include more accuracy evaluations by isolating different categories of systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3\" title=\"Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "systems",
                    "pair"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3\" title=\"3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and empirically observed in this section, short-form evaluation can significantly distort latency evaluation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T2\" title=\"In Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, we present the average fraction\nof words translated <span class=\"ltx_text ltx_font_italic\">after</span> the end-of-segment signal, i.e., words omitted during evaluation by most latency metrics. A substantial portion of the translations are tail words, starting at 41% in the low-latency (1-2s) and reaching 72% in the high-latency regimes (4-5s).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Systems with higher-latency behavior have policies leading to deferred delays, and these delays in turn are more likely to overflow the source duration.</span></span></span>\n<span class=\"ltx_text ltx_font_italic\">Short-form evaluation, with artificial segment boundaries absent in real-world scenarios and metrics&#8217; problematic handling of tail words, often misrepresents SimulST system behavior.</span>\nThis raises serious concerns about\nits reliability\nand underscores the need for long-form evaluation,\nwhich we analyze in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS2\" title=\"5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "longform",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we evaluate two resegmentation tools: mWERSegmenter and our proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>.\nThe evaluation is done on reconcatenated short-form\noutputs, allowing us to compare with gold segment boundaries.\nAs we can see in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_italic\">the accuracy of\nlatency evaluation is significantly higher with the proposed segmenter</span>.\nWhen filtering out comparable systems by the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value,\naccuracy\nfurther decreases\nwith mWERSegmenter, suggesting that the segmentation is not stable.\nBoth segmentation methods achieve over 99% accuracy, indicating resegmentation does not hinder quality assessment.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the accuracy of StreamLAAL that uses resegmentation and the original short-form metrics evaluated <em class=\"ltx_emph ltx_font_italic\">without resegmentation</em> on long-form systems.\nWe see that the accuracies are low, not exceeding 66% on all systems.\nCompared to StreamLAAL, the best-performing AL metric loses 15 to 16% absolute, and the gap is even wider compared to the proposed LongYAAL, with AL falling short by 29 points.\nThe lower part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\nreports the accuracy of latency metrics in long-form systems <em class=\"ltx_emph ltx_font_italic\">with resegmentation</em>.\nWe see that the resegmentation quality significantly influences the accuracy. StreamLAAL and LongLAAL share the same definition, but differ in the resegmentation&#8211;while StreamLAAL uses the mWERSegmenter, LongLAAL (and all other\n&#8220;Long-&#8221; metrics) uses our <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>. The gap in accuracy is 8 to 10 points in all subsets, showing trends similar to those in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\n<span class=\"ltx_text ltx_font_italic\">These results highlight the critical role of resegmentation in ensuring reliable latency evaluation in the long-form regime.</span> Additional observations are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4\" title=\"Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "our",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows\nthat the proposed LongYAAL has the highest accuracy in all subsets. LongATD and LongDAL show slightly worse results, but not\nstatistically significant. This contrasts with &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, where ATD and DAL perform worse than *AL metrics.\nWe explain this discrepancy by the fact that both metrics include tail words that rarely occur in the long-form regime.\nWe attribute the marginal difference of LongATD compared to LongYAAL to its assumption of 300ms words in the source speech, which is dynamic in LongYAAL in the form of the parameter <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and the difference in LongDAL is caused by the artificial minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E4\" title=\"In Differentiable Average Lagging (DAL; Cherry and Foster, 2019) &#8227; 2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nLongLAAL ties with LongYAAL in most subsets, but seems slightly worse when in easily distinguishable systems (<math alttext=\"p\\text{-val}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>-val</mtext></mrow><annotation encoding=\"application/x-tex\">p\\text{-val}</annotation></semantics></math>&lt;0.001), where the metric loses 2 points in accuracy. LongLAAL, unlike LongYAAL, disregards words generated beyond the reference segment boundaries. The number of words ignored increases with the true latency of the system (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1.SSS0.Px3\" title=\"Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), which is more prevalent in this subset. Similarly, LongAL ignores the tail words in the resegmentation and is also vulnerable to overgeneration <cite class=\"ltx_cite ltx_citemacro_cite\">Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>); Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite>. Finally, AP performs the worst, losing more than 21 points compared to other metrics, which we attribute to the metric&#8217;s sensitivity to variable segment length. <span class=\"ltx_text ltx_font_italic\">These results position LongYAAL as the most reliable metric for assessing latency in long-form SimulST</span>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "longform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented the first systematic evaluation of latency metrics for SimulST across several aspects, such as diverse systems, language pairs, and operating under short- and long-form speech processing. We have identified current pitfalls in the SimulST evaluation by isolating issues in the most commonly used metrics. To overcome these limitations, we propose YAAL, a new latency metric\nbetter\naligned with the\nshort-form evaluation regime.\nHowever,\nour analysis also reveals inherent shortcomings of short-form evaluation, further reinforcing the adoption of long-form evaluation as a more reliable alternative.\nMoreover, we also demonstrated that resegmentation is necessary to conduct a proper evaluation of systems operating under the long-form regime, and proposed an improved resegmentation tool coupled with the extension of YAAL for these settings&#8211;LongYAAL. The results showed that YAAL and LongYAAL improve over all other metrics in both regimes, establishing the new state-of-the-art metric for SimulST.</p>\n\n",
                "matched_terms": [
                    "language",
                    "evaluation",
                    "systems",
                    "longform",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our study offers a thorough evaluation of latency metrics for SimulST and introduces improved tools for both short- and long-form regimes, some limitations remain. First, our evaluation depends on reference translations and transcriptions, which may not be available or reliable in low-resource or real-time scenarios. Second, although the proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> improves alignment robustness, word-level alignment is still susceptible to errors in the presence of disfluencies or speech recognition noise. Third, our experimental analysis focuses on systems from the IWSLT Shared Tasks, which may not fully represent the range of techniques or data conditions used in broader academic or industrial settings. Fourth, our analysis focuses on high-resource languages, for which data were available, but the findings should be reconfirmed under low-resource language settings.</p>\n\n",
                "matched_terms": [
                    "language",
                    "evaluation",
                    "systems",
                    "iwslt",
                    "longform",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work introduces new evaluation tools that could influence future benchmarking of SimulST systems. However, there is a risk that over-reliance on specific metrics&#8211;even improved ones like YAAL and LongYAAL&#8211;could lead to overfitting system design to particular evaluation settings. For example, systems might be tuned to perform well under LongYAAL but degrade in real-world conditions that are not fully captured by the metric. Additionally, the use of automatic resegmentation methods may inadvertently introduce subtle biases if misaligned with human interpretation of segment boundaries. We encourage the community to use these tools alongside qualitative analysis and human-in-the-loop evaluations where possible.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "evaluation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the short-form regime, we use systems submitted to the IWSLT Simultaneous Speech Translation tracks of 2022 and 2023. Specifically, we use the SimulEval evaluation logs of the IWSLT 2022 and 2023 test sets <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, and the logs of the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> test set of the MuST-C data set <cite class=\"ltx_cite ltx_citemacro_citep\">(Cattoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib6\" title=\"\">2021</a>)</cite> that were submitted to IWSLT 2022. For the long-form regime, the logs are sourced from IWSLT 2025. In particular, for English-to-{German, Chinese, Japanese} the evaluation was done on the development set of the ACL 60/60 dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Salesky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib28\" title=\"\">2023</a>)</cite>, and IWSLT 2025 test set. For the Czech-to-English language pair, the evaluation was performed on the IWSLT 2024 development set <cite class=\"ltx_cite ltx_citemacro_cite\">Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite> and the IWSLT 2025 test set. A portion of the IWSLT 2024 development set contained segmented audio that could not be reconstructed into the original unsegmented audio.</p>\n\n",
                "matched_terms": [
                    "language",
                    "acl",
                    "test",
                    "evaluation",
                    "pair",
                    "dataset",
                    "systems",
                    "iwslt",
                    "longform",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Same as in the short-form evaluation, we follow the definition of the true latency in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4\" title=\"4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nHowever, there are two differences.\nAfter initial experiments, we observed that the Montreal Forced Aligner used in the short-form regime is not robust for the challenging conditions of the IWSLT 2025 test set, which is based on ACL presentations. The recordings include frequent restarts, repetitions, domain-specific terminology, and non-native speech. Instead, we use the alignment method implemented within WhisperX <cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib5\" title=\"\">2023</a>)</cite> for forced alignment. This tool leverages neural speech encoders that seem to be robust to the above-mentioned challenges. In particular, we used WhisperX&#8217;s default settings, i.e., PyTorch&#8217;s <span class=\"ltx_text ltx_font_typewriter\">WAV2VEC2_ASR_BASE_960H</span> for English and <span class=\"ltx_text ltx_font_typewriter\">comodoro/wav2vec2-xls-r-300m-cs-250</span> for Czech speech forced alignments.\nSecond, we perform resegmentation of the system hypotheses prior to the machine translation alignment with the reference. This step is necessary because the <span class=\"ltx_text ltx_font_typewriter\">awesome-align</span> tool uses the <span class=\"ltx_text ltx_font_typewriter\">bert-base-multilingual-cased</span> model for the alignment, and this model has a maximum input length of 512 tokens, which is much lower than the system hypotheses.</p>\n\n",
                "matched_terms": [
                    "acl",
                    "test",
                    "evaluation",
                    "iwslt",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A natural question arises: <em class=\"ltx_emph ltx_font_italic\">Why rely on automatic latency metrics at all, when true latency offers a closer approximation of user experience?</em> In practice, computing true latency requires several requirements that limit its applicability. High-quality transcripts must be available, which is often not the case&#8211;particularly for low-resource languages or unwritten languages where transcription is infeasible. Moreover, forced alignment tools and reliable word-level translation alignments are typically available only for a small set of high-resource language pairs. Even when such resources exist, computing true latency involves multiple processing steps and is substantially more complex than evaluating standard automatic metrics. Importantly, as we show in our analysis in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5\" title=\"5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, several automatic metrics approximate true latency with high accuracy, making them a practical and effective alternative in most evaluation scenarios.</p>\n\n",
                "matched_terms": [
                    "language",
                    "evaluation",
                    "set",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F6\" title=\"In Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we illustrate the trends after filtering out the systems affected by the anomalous policy (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). Unlike in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we see that all metrics and system pairs show a positive correlation with the true latency. As mentioned in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, language pairs exhibit different scales, making the use of the correlation coefficient more cumbersome and motivating the use of accuracy as described in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4.SS2.SSS0.Px3\" title=\"Accuracy &#8227; 4.2 Evaluation &#8227; 4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We were also interested in the accuracy of latency metrics when comparing related against unrelated systems. In our evaluation, we consider the systems submitted by one team as related.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>To the best of our knowledge, most teams submitted multiple systems that were based on the same system with varying hyperparameters.</span></span></span> We also use only a subset of the systems that were not affected by the anomalous simultaneous policy. The results are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "our",
                    "evaluation",
                    "teams"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F11\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we show pairwise comparisons of systems evaluated in the long-form regime without resegmentation, and in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F12\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, we show the same systems evaluated in the long-form regime, but after resegmentation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F13\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "longform"
                ]
            }
        ]
    },
    "A3.T8": {
        "source_file": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation",
        "caption": "Table 8: Accuracy of systems in the short-form regime when comparing related and unrelated systems. Systems with the anomalous policy were omitted. Best scores in bold. Underlined scores are considered tied with the best metric.",
        "body": "pp-val\nAL\nLAAL\nDAL\nATD\nAP\nYAAL\nN\n\n\nrelated systems\n\n\nall\n0.99\n1.00\n0.99\n0.96\n0.99\n1.00\n897\n\n\n\n<<0.05\n1.00\n1.00\n1.00\n0.97\n0.99\n1.00\n888\n\n\n\n<<0.01\n1.00\n1.00\n1.00\n0.97\n0.99\n1.00\n888\n\n\n\n<<0.001\n1.00\n1.00\n1.00\n0.97\n0.99\n1.00\n881\n\n\n0.001-0.05\n1.00\n1.00\n1.00\n0.57\n1.00\n1.00\n7\n\n\nunrelated systems\n\n\nall\n0.92\n0.95\n0.92\n0.88\n0.74\n0.97\n1203\n\n\n\n<<0.05\n0.93\n0.96\n0.94\n0.89\n0.75\n0.98\n1172\n\n\n\n<<0.01\n0.93\n0.96\n0.94\n0.89\n0.75\n0.98\n1158\n\n\n\n<<0.001\n0.93\n0.96\n0.94\n0.89\n0.75\n0.98\n1144\n\n\n0.001-0.05\n0.64\n0.68\n0.57\n0.79\n0.57\n0.68\n28",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">\n<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-val</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">AL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">LAAL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">DAL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">ATD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">AP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">YAAL</td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\">N</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"8\">related systems</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">all</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.00</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">897</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.05</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">0.97</td>\n<td class=\"ltx_td ltx_align_center\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.00</td>\n<td class=\"ltx_td ltx_align_right\">888</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m3\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">0.97</td>\n<td class=\"ltx_td ltx_align_center\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.00</td>\n<td class=\"ltx_td ltx_align_right\">888</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.001</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">0.97</td>\n<td class=\"ltx_td ltx_align_center\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.00</td>\n<td class=\"ltx_td ltx_align_right\">881</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">0.001-0.05</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center\">0.57</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.00</td>\n<td class=\"ltx_td ltx_align_right\">7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"8\">unrelated systems</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">all</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.97</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1203</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m5\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.05</td>\n<td class=\"ltx_td ltx_align_center\">0.93</td>\n<td class=\"ltx_td ltx_align_center\">0.96</td>\n<td class=\"ltx_td ltx_align_center\">0.94</td>\n<td class=\"ltx_td ltx_align_center\">0.89</td>\n<td class=\"ltx_td ltx_align_center\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.98</span></td>\n<td class=\"ltx_td ltx_align_right\">1172</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m6\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.93</td>\n<td class=\"ltx_td ltx_align_center\">0.96</td>\n<td class=\"ltx_td ltx_align_center\">0.94</td>\n<td class=\"ltx_td ltx_align_center\">0.89</td>\n<td class=\"ltx_td ltx_align_center\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.98</span></td>\n<td class=\"ltx_td ltx_align_right\">1158</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m7\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>0.001</td>\n<td class=\"ltx_td ltx_align_center\">0.93</td>\n<td class=\"ltx_td ltx_align_center\">0.96</td>\n<td class=\"ltx_td ltx_align_center\">0.94</td>\n<td class=\"ltx_td ltx_align_center\">0.89</td>\n<td class=\"ltx_td ltx_align_center\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.98</span></td>\n<td class=\"ltx_td ltx_align_right\">1144</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">0.001-0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.68</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">28</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tied",
            "related",
            "unrelated",
            "yaal",
            "all",
            "anomalous",
            "atd",
            "ppval",
            "regime",
            "considered",
            "bold",
            "systems",
            "accuracy",
            "comparing",
            "policy",
            "underlined",
            "laal",
            "metric",
            "dal",
            "shortform",
            "scores",
            "omitted",
            "best",
            "when"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We were also interested in the accuracy of latency metrics when comparing related against unrelated systems. In our evaluation, we consider the systems submitted by one team as related.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>To the best of our knowledge, most teams submitted multiple systems that were based on the same system with varying hyperparameters.</span></span></span> We also use only a subset of the systems that were not affected by the anomalous simultaneous policy. The results are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">The results on unrelated systems (bottom half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.T8\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">8</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F10\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">10</span></a>) are generally similar to the observations in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. All metrics show a loss of accuracy of no more than 4% points compared to the results on all systems. The only exception seems to be AP, which loses up to 11% points. The order of the metrics remains the same.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency&#8211;the delay between speech input and the translated output.\nWhile quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented.\nIn this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes.\nWe uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more\naccurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems.</p>\n\n",
                "matched_terms": [
                    "yaal",
                    "metric",
                    "shortform",
                    "related",
                    "regime",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Simultaneous speech-to-text translation (SimulST) is the task in which the system produces incremental translation concurrently with the speaker&#8217;s speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib27\" title=\"\">2020</a>)</cite>. SimulST models have to balance between quality and latency of the output, which is the time elapsed between when a word is uttered and when its corresponding translation is produced. Although translation quality metrics are extensively studied in offline ST and in the related field of machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Freitag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib11\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib10\" title=\"\">2023</a>; Zouhar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib30\" title=\"\">2024</a>)</cite>, there is no study on the reliability of latency metrics.\nThe most common latency\nmetrics in SimulST <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho and Esipova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib8\" title=\"\">2016</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib16\" title=\"\">2019</a>; Cherry and Foster, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib7\" title=\"\">2019</a>; Pol&#225;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>; Kano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib14\" title=\"\">2023</a>)</cite>, even though with different approximations, base their calculation on\nsimplifying assumptions such as uniform word duration, absence of pauses, and strict monotonic alignment between source speech and target translation.\nHowever, despite relying on\nthe same assumptions, these metrics often produce very inconsistent assessments.\nThis inconsistency is clearly illustrated in the results of the IWSLT 2023 Shared Task on Simultaneous Translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, where different metrics produced substantially different rankings (see <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S1.F1\" title=\"In 1 Introduction &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). Such variability raises serious concerns about the validity of current evaluation protocols and their ability to support meaningful comparisons between systems.\nMoreover, this risk can be further exacerbated when shifting from dealing with already presegmented speech, i.e., <span class=\"ltx_text ltx_font_italic\">short-form</span> SimulST, to unsegmented audio, i.e., <span class=\"ltx_text ltx_font_italic\">long-form</span> SimulST, where information about sentence boundaries is not available, further complicating the evaluation <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "related",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose YAAL (Yet Another Average Lagging), a refined latency metric designed to mitigate the biases present in existing latency metrics.\nOur extensive experiments demonstrate that YAAL yields more reliable latency estimates, consistently aligning better with the actual behavior of SimulST systems.\nFurthermore, we also show that resegmentation, which pairs segment-level predictions with their corresponding reference, is necessary to produce meaningful latency measurements for long-form SimulST. To this end, we introduce <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, a new resegmentation tool, and extend our YAAL to LongYAAL, which deals with audio streams. Compared to the current standard alignment tool used in the speech translation community <cite class=\"ltx_cite ltx_citemacro_citep\">(Matusov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> significantly improves alignment quality, enabling more accurate evaluation in long-form scenarios.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The code for YAAL, its long-form variant LongYAAL, and <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span> will be released upon the paper acceptance under Apache 2.0 license.</span></span></span></p>\n\n",
                "matched_terms": [
                    "metric",
                    "systems",
                    "yaal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The short-form is the most common evaluation regime of SimulST <cite class=\"ltx_cite ltx_citemacro_cite\">Anastasopoulos et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>); Agarwal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>); Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite>, where all recordings of the test set are divided, usually following sentence boundaries, into short segments of a few seconds.\nEach segment consists of source audio <math alttext=\"\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}=[x_{1},\\dots,x_{|\\mathbf{X}|}]</annotation></semantics></math>, where <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is a small portion of raw audio, i.e., audio chunk, and reference translation <math alttext=\"\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>y</mi><mn>1</mn><mi>R</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>y</mi><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow><mi>R</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}}=[y^{R}_{1},\\dots,y^{R}_{|\\mathbf{Y^{R}}|}]</annotation></semantics></math>. Each\naudio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math> is incrementally fed to the system,\nwhich concurrently\noutputs a translation token <math alttext=\"y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">y_{j}</annotation></semantics></math> at timestamp <math alttext=\"d_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">d_{j}</annotation></semantics></math>, i.e., total duration of audio chunks up to and including the audio chunk <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math>. Under these settings, we describe the latency metrics operating in the short-form regime:</p>\n\n",
                "matched_terms": [
                    "all",
                    "shortform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\tau(\\mathbf{X})=\\text{min}\\{i|d_{i}=|\\mathbf{X}||\\}\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mtext>min</mtext><mrow><mo stretchy=\"false\">{</mo><mi>i</mi><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><msub><mi>d</mi><mi>i</mi></msub><mo rspace=\"0em\">=</mo><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mi>&#119831;</mi><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})=\\text{min}\\{i|d_{i}=|\\mathbf{X}||\\}</annotation></semantics></math> is the index of the hypothesis token when the model reaches the end of the source sentence, also known as the cutoff point. AL considers delays up to and including the one associated with the token at the cutoff point.\nThe <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th delay of the ideal policy is defined as <math alttext=\"d^{*}_{i}=(i-1)/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi>d</mi><mi>i</mi><mo>&#8727;</mo></msubsup><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><mi>&#947;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">d^{*}_{i}=(i-1)/\\gamma</annotation></semantics></math>, where <math alttext=\"\\gamma=|\\mathbf{Y^{R}}|/|\\mathbf{X}|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px2.p3.m4\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mrow><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow><mo>/</mo><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\gamma=|\\mathbf{Y^{R}}|/|\\mathbf{X}|</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "policy",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">is an AL modification that is robust to overgeneration, i.e., when the hypothesis <math alttext=\"\\mathbf{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>&#119832;</mi><annotation encoding=\"application/x-tex\">\\mathbf{Y}</annotation></semantics></math> is much longer than <math alttext=\"\\mathbf{Y^{R}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}}</annotation></semantics></math>, which makes the original AL produce negative delays when <math alttext=\"|\\mathbf{Y}|\\gg|\\mathbf{Y^{R}}|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119832;</mi><mo stretchy=\"false\">|</mo></mrow><mo>&#8811;</mo><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">|\\mathbf{Y}|\\gg|\\mathbf{Y^{R}}|</annotation></semantics></math>.\nTo overcome this problem, which was unduly rewarding overgenerating systems,\n<cite class=\"ltx_cite ltx_citemacro_citet\">Pol&#225;k et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib25\" title=\"\">2022</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib23\" title=\"\">2022</a>)</cite> proposed\nthe modification\n<math alttext=\"\\gamma=\\max(|\\mathbf{Y}|,|\\mathbf{Y^{R}}|)/|\\mathbf{X}|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#947;</mi><mo>=</mo><mrow><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo stretchy=\"false\">|</mo><mi>&#119832;</mi><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msup><mi>&#119832;</mi><mi>&#119825;</mi></msup><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\gamma=\\max(|\\mathbf{Y}|,|\\mathbf{Y^{R}}|)/|\\mathbf{X}|</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">modifies AL by introducing\na minimal delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> after each\nstep.\nUnlike AL and LAAL, DAL considers all tokens in the hypothesis,\nwithout\ncutoff after <math alttext=\"i&gt;\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\mathbf{X})</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "dal",
                    "all",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form evaluation regime evaluates SimulST systems more realistically <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib24\" title=\"\">2025</a>)</cite>, as it assesses their ability to handle long audio streams, often spanning several minutes.\nSince all metrics were developed for the short-form regime,\nrecent studies <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>\nresorted to resegmentation of translations and delays based on the reference translation <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib20\" title=\"\">2005b</a>)</cite>, and computed the metrics on the segment level. We explain the long-form variant of LAAL <cite class=\"ltx_cite ltx_citemacro_cite\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite> below.</p>\n\n",
                "matched_terms": [
                    "laal",
                    "all",
                    "shortform",
                    "regime",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">extends the LAAL metric to unsegmented audio streams <math alttext=\"\\mathbf{S}=[\\mathbf{X}_{1},...,\\mathbf{X}_{|\\mathbf{S}|}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119826;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119831;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119831;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}=[\\mathbf{X}_{1},...,\\mathbf{X}_{|\\mathbf{S}|}]</annotation></semantics></math>, paired with a continuous stream of predicted translations <math alttext=\"\\mathbf{Y_{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119826;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{S}}</annotation></semantics></math>. Since reference translations <math alttext=\"\\mathbf{Y^{R}_{1}},...,\\mathbf{Y^{R}_{|\\mathbf{S}|}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119832;</mi><mn>&#120783;</mn><mi>&#119825;</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119832;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow><mi>&#119825;</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}_{1}},...,\\mathbf{Y^{R}_{|\\mathbf{S}|}}</annotation></semantics></math> are only available at segment-level <math alttext=\"\\mathbf{X_{1}},...,\\mathbf{X}_{|\\mathbf{S}|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#119831;</mi><mn>&#120783;</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119831;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X_{1}},...,\\mathbf{X}_{|\\mathbf{S}|}</annotation></semantics></math>, prediction <math alttext=\"\\mathbf{Y_{S}}=[\\mathbf{Y_{1}},...,\\mathbf{Y_{|\\mathbf{S}|}}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#119832;</mi><mi>&#119826;</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119832;</mi><mn>&#120783;</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119832;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y_{S}}=[\\mathbf{Y_{1}},...,\\mathbf{Y_{|\\mathbf{S}|}}]</annotation></semantics></math> with the corresponding delays is segmented based on reference sentences <math alttext=\"\\mathbf{Y^{R}_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119832;</mi><mi>&#119852;</mi><mi>&#119825;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}_{s}}</annotation></semantics></math> to obtain segment-level predictions. Then, StreamLAAL is computed as:</p>\n\n",
                "matched_terms": [
                    "metric",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Where <math alttext=\"d^{*}_{i}=(i-1)\\cdot|\\mathbf{X_{s}}|/{\\max\\{|\\mathbf{Y_{s}}|,|\\mathbf{Y^{R}_{s}|\\}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><msubsup><mi>d</mi><mi>i</mi><mo>&#8727;</mo></msubsup><mo>=</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#8901;</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo>/</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119832;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msubsup><mi>&#119832;</mi><mi>&#119852;</mi><mi>&#119825;</mi></msubsup><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d^{*}_{i}=(i-1)\\cdot|\\mathbf{X_{s}}|/{\\max\\{|\\mathbf{Y_{s}}|,|\\mathbf{Y^{R}_{s}|\\}}}</annotation></semantics></math>\nIn practice, the LAAL metric is calculated for every speech segment <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math> of the stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> and its corresponding reference <math alttext=\"\\mathbf{Y^{R}_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119832;</mi><mi>&#119852;</mi><mi>&#119825;</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{Y^{R}_{s}}</annotation></semantics></math> with the automatically aligned prediction <math alttext=\"\\mathbf{Y_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{s}}</annotation></semantics></math> and then averaged over all the speech segments of the stream <math alttext=\"\\mathbf{X_{1}},...,\\mathbf{X_{|\\mathbf{S}|}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><mrow><msub><mi>&#119831;</mi><mn>&#120783;</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119831;</mi><mrow><mo stretchy=\"false\">|</mo><mi>&#119826;</mi><mo stretchy=\"false\">|</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X_{1}},...,\\mathbf{X_{|\\mathbf{S}|}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "all",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Alternatively, <cite class=\"ltx_cite ltx_citemacro_citet\">Huber et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib12\" title=\"\">2023</a>)</cite> proposed an evaluation with resegmentation provided by the system. The evaluation framework expects that the system outputs the segment&#8217;s start and end timestamps that align with the source. First, most systems, including all the IWSLT systems, do not output this information, and relying on the system&#8217;s self-reported alignment might hinder the reliability. Second, as we empirically show in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, relying on potentially low-quality resegmentation significantly lowers the accuracy of latency evaluation. Finally, different segmentations render the observed latencies incomparable across different systems.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "all",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The use of audio segmentation in short-form evaluations significantly affects translation behavior and latency. In practice, short-form SimulST systems are evaluated in a simulated environment where each segment is processed independently <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib17\" title=\"\">2020</a>)</cite>. When the entire source segment has been consumed by the system, the translation is often still in progress. At that point, the simulator requests the remaining translation, which the model emits without any additional delay.\nThis setup introduces two unrealistic conditions. First, the audio is typically segmented in advance by a human annotator or an automatic model with access to the full audio (<span class=\"ltx_text ltx_font_italic\">Oracle Segmentation</span>). Second, the model is allowed to generate the remaining translation (hereinafter, <span class=\"ltx_text ltx_font_italic\">tail words</span>) instantaneously once the input segment ends. These factors unduly distort short-form evaluations, both by providing high-quality segmentation and eliminating the delay that would occur in a realistic setting, where the system must wait to confirm that the sentence has ended. In a more realistic scenario, a model has to rely on online segmentation (<span class=\"ltx_text ltx_font_italic\">Simultaneous Segmentation</span>) and thus delay the final translation until it is confident that the input sentence is complete, thereby introducing extra latency.\nThis discrepancy is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on these observations, we categorize existing short-form latency metrics (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.SS1\" title=\"2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>) into two main groups, depending on whether they include all translated words or only a subset in their latency computation. The first group&#8211;AP, DAL, and ATD&#8211;includes all translated words in the calculation.\nDAL attempts to mitigate the impact of tail words by adding a minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> after each generated word (also within the same step), thus &#8220;spreading&#8221; the tail beyond the sentence.\nHowever, <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math>\nsimply reflects the average source-to-target length ratio and does not accurately capture the system behavior for tail words in settings without segmentation.\nIf multiple words are emitted as tail words, DAL can significantly overestimate\nlatency.\nIn the edge case of a system that waits for an end-of-segment signal (i.e., an offline system), DAL\nreturns the segment length, failing to capture the system&#8217;s true behavior&#8211;in this case, undefined latency.\nAP assigns a delay of 1 to each tail word\nas the entire recording has to be processed to emit that word, thus, the proportion is 1. Although AP is marginally less sensitive to segmentation than DAL&#8211;since it operates on proportions rather than absolute delays&#8211;it still fails to capture system behavior faithfully for the tail words.\nATD also considers all translated words.\nHowever, unlike DAL, it does not apply corrections for tail word behavior, making it the most sensitive to segmentation artifacts among the three metrics.</p>\n\n",
                "matched_terms": [
                    "atd",
                    "dal",
                    "all",
                    "shortform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second group&#8211;AL and LAAL&#8211;computes latency only for words emitted up to and including the cutoff point <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>, which marks the first word generated after the end of the input segment.\nThis corresponds to the word &#8220;<span class=\"ltx_text ltx_font_italic\">gemeinn&#252;tzige</span>&#8221; in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.F2\" title=\"In 3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nAs discussed, in the short-form regime with oracle segmentation, the <math alttext=\"\\tau(\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X})</annotation></semantics></math>-th and following words are often translated earlier than in a more realistic\nlong-form scenario.\nAs a result, this cutoff introduces a systematic bias in the latency estimate, which may lead to either underestimation or overestimation, depending on the system&#8217;s policy.</p>\n\n",
                "matched_terms": [
                    "policy",
                    "shortform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AP, DAL, ATD, AL, and, more recently, LAAL became established metrics in the short-form evaluation of SimulST. However, as discussed above, including any of the tail words in the latency computation leads to a systematic bias that undermines fair comparisons.\nTo cope with this bias, we propose a new metric derived from the LAAL metric:</p>\n\n",
                "matched_terms": [
                    "laal",
                    "metric",
                    "dal",
                    "shortform",
                    "atd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The long-form regime offers a more realistic evaluation setting by assessing systems on continuous, unsegmented audio streams that better reflect real-world use cases. However, the widely used latency metrics were originally designed for the short-form regime and do not directly extend to this setting.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, metrics such as AL, LAAL, and DAL\nrely on a <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> parameter,\nrepresenting the average target-to-source length ratio. However, <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math> can vary substantially across different segments within the same audio, leading to inconsistent and unreliable latency estimates <cite class=\"ltx_cite ltx_citemacro_cite\">Iranzo-S&#225;nchez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib13\" title=\"\">2021</a>)</cite>.\nSecond, AP\ntends to converge toward 0 for\nlong recordings,\nas typical speech inputs are significantly longer than the translations,\ni.e., <math alttext=\"|\\mathbf{X}|\\gg|\\mathbf{Y}|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow><mo>&#8811;</mo><mrow><mo stretchy=\"false\">|</mo><mi>&#119832;</mi><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">|\\mathbf{X}|\\gg|\\mathbf{Y}|</annotation></semantics></math>,\nleading <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E1\" title=\"In Average Proportion (AP; Cho and Esipova, 2016) &#8227; 2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a> to approach 0.\nFinally, ATD\nassumes that each speech token has a fixed duration and that source and target tokens align monotonically&#8211;assumptions that are overly restrictive and especially unrealistic for long-form speech.</p>\n\n",
                "matched_terms": [
                    "atd",
                    "dal",
                    "laal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, StreamLAAL has introduced resegmenting long inputs into short segments and computing latency on these units. While StreamLAAL provides adaptation of existing metrics to long-form input, it relies on the mWERSegmenter tool <cite class=\"ltx_cite ltx_citemacro_cite\">Matusov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib19\" title=\"\">2005a</a>)</cite>, which may introduce alignment errors <cite class=\"ltx_cite ltx_citemacro_cite\">Amrhein and Haddow (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib3\" title=\"\">2022</a>); Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>)</cite>, and\ncomputes latency up to the cutoff word <math alttext=\"\\tau(\\mathbf{X_{i}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mi>&#119842;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau(\\mathbf{X_{i}})</annotation></semantics></math> (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E7\" title=\"In Streaming LAAL (StreamLAAL; Papi et al., 2024) &#8227; 2.2 Long-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">7</span></a>), which can lead to the systematic bias (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3.SS1\" title=\"3.1 The Short-Form Regime &#8227; 3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>)\nTo overcome these limitations, we propose a new resegmentation method and an extension of the YAAL metric for the long-form regime.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "yaal",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also extend YAAL to the long-form regime&#8211;i.e., LongYAAL. Unlike StreamLAAL, LongYAAL includes all words in the latency computation, even those generated beyond the aligned segment boundaries <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math>, i.e., all <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for <math alttext=\"i&gt;\\tau(\\mathbf{X_{s}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\mathbf{X_{s}})</annotation></semantics></math>. However, we exclude the final tail words produced after the end of the full stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>, i.e., <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for <math alttext=\"i&gt;\\tau(\\sum_{s=1}^{|\\mathbf{X}|}|\\mathbf{X_{s}}|)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&gt;</mo><mrow><mi>&#964;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8721;</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy=\"false\">|</mo><mi>&#119831;</mi><mo stretchy=\"false\">|</mo></mrow></msubsup><mrow><mo stretchy=\"false\">|</mo><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><mo stretchy=\"false\">|</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">i&gt;\\tau(\\sum_{s=1}^{|\\mathbf{X}|}|\\mathbf{X_{s}}|)</annotation></semantics></math>. This ensures that we include all words emitted beyond the segment boundaries <math alttext=\"\\mathbf{X_{s}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{s}}</annotation></semantics></math>, but we do not include the tail words generated at the end of the entire stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>. If the stream <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math> consists of a single segment, LongYAAL coincides with YAAL.</p>\n\n",
                "matched_terms": [
                    "all",
                    "yaal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the main evaluation, we adopt the pairwise comparison approach <cite class=\"ltx_cite ltx_citemacro_citep\">(Mathur et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib18\" title=\"\">2020</a>)</cite>.\nRather than evaluating each system independently, we examine the difference between the scores of two systems: <math alttext=\"\\Delta=score(\\text{System A})-score(\\text{System B})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo>=</mo><mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System A</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>System B</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta=score(\\text{System A})-score(\\text{System B})</annotation></semantics></math>.\nPairwise comparison better reflects the typical use case of latency metrics&#8211;distinguishing between two systems. We also restrict comparisons to system pairs evaluated on the same test set and language pair.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following <cite class=\"ltx_cite ltx_citemacro_citet\">Kocmi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib15\" title=\"\">2021</a>)</cite>, we evaluate the accuracy of binary comparisons: given a pair of systems, which one is better according to the true latency ranking (used as silver labels)? The accuracy is defined as the proportion of system pairs for which the relative ranking according to a metric matches that of the true latency:</p>\n\n",
                "matched_terms": [
                    "metric",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accuracy considers only the ranking, not the magnitude, of the latency differences, allowing us to aggregate comparisons across languages and test sets with different scales.\nHowever, this accuracy might be affected if two systems have similar latencies. To avoid this issue, we compute the accuracies in multiple subsets\nby removing pairs that are not significantly different according to Mann-Whitney U test on their\ntrue latencies.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We do not assume normal distribution of delays. Each system has different hypotheses, so we cannot use paired tests.</span></span></span> We use bootstrap resampling with <math alttext=\"N=10000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10000</mn></mrow><annotation encoding=\"application/x-tex\">N=10000</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Tibshirani and Efron, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib29\" title=\"\">1993</a>)</cite> to estimate confidence intervals and consider all metrics within the 95% confidence interval of the top-performing metric to be statistically tied.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "all",
                    "tied",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a pairwise comparison of all short-form systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>. An important first observation is that a significant portion of system pairs exhibit no or slightly negative correlations&#8211;points that create almost vertical lines and lines far off the diagonal. These systems share a <em class=\"ltx_emph ltx_font_italic\">anomalous simultaneous policy</em>: The lower the latency of the prefix generated simultaneously, the larger the portion of the sentence translated offline.\nWe assume that the underlying reason for the anomalous policy is that the system is too eager to emit output at the beginning. However, eventually it gets to the &#8220;dead end&#8221; of probable outputs and only emits the tail words at the signaled end of the sentence.\nThis policy, coupled with bias in latency metrics, leads to a severe overestimation of the actual latency of the systems. As we show below, overestimation of actual latency can hinder the reliable detection of systems following this anomalous policy.</p>\n\n",
                "matched_terms": [
                    "policy",
                    "all",
                    "shortform",
                    "anomalous",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To detect the anomalous policy, we propose comparing the observed and expected fractions of simultaneously translated words, based on automatic latency. The observed fraction of words translated strictly online across the entire test set <math alttext=\"\\mathbf{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>&#119826;</mi><annotation encoding=\"application/x-tex\">\\mathbf{S}</annotation></semantics></math>, i.e.,</p>\n\n",
                "matched_terms": [
                    "policy",
                    "anomalous",
                    "comparing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The expected fraction of words translated online based on the value <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> of an automatic latency metric is <math alttext=\"O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>=</mo><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>X</mi><mtext>avg</mtext></msub><mo>&#8722;</mo><mi>L</mi></mrow><mo stretchy=\"false\">)</mo></mrow><mo>/</mo><msub><mi>X</mi><mtext>avg</mtext></msub></mrow></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}=(X_{\\text{avg}}-L)/X_{\\text{avg}}</annotation></semantics></math>, where <math alttext=\"X_{\\text{avg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m3\" intent=\":literal\"><semantics><msub><mi>X</mi><mtext>avg</mtext></msub><annotation encoding=\"application/x-tex\">X_{\\text{avg}}</annotation></semantics></math> is the average segment length.\nIf the expected online-translated word fraction significantly exceeds the observed one, i.e., <math alttext=\"O_{\\text{e}}\\gg O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8811;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\gg O</annotation></semantics></math>, we conclude that the system follows the anomalous policy. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F4\" title=\"In Detecting Anomalous Policy &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>, we plot the observed and expected fractions of the words translated online based on the proposed YAAL and LAAL metrics. Most systems follow a vertical line, i.e., <math alttext=\"O_{\\text{e}}\\sim O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>O</mi><mtext>e</mtext></msub><mo>&#8764;</mo><mi>O</mi></mrow><annotation encoding=\"application/x-tex\">O_{\\text{e}}\\sim O</annotation></semantics></math>. However, based on YAAL, the red and brown circles<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All affected systems were submitted independently by two different teams in IWSLT 2022 and 2023, showing that the anomalous policy is not so uncommon.</span></span></span> show a strong disagreement between <math alttext=\"O^{\\text{YAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>YAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{YAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m7\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. Based on LAAL (crosses), we observe some disagreement between <math alttext=\"O^{\\text{LAAL}}_{\\text{e}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>O</mi><mtext>e</mtext><mtext>LAAL</mtext></msubsup><annotation encoding=\"application/x-tex\">O^{\\text{LAAL}}_{\\text{e}}</annotation></semantics></math> and <math alttext=\"O\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p2.m9\" intent=\":literal\"><semantics><mi>O</mi><annotation encoding=\"application/x-tex\">O</annotation></semantics></math>. However, the trend is not as clear as with YAAL, making the anomalous policy detection difficult. <em class=\"ltx_emph ltx_font_italic\">This shows the importance of accurate evaluation of latency using YAAL.</em></p>\n\n",
                "matched_terms": [
                    "policy",
                    "yaal",
                    "laal",
                    "metric",
                    "anomalous",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the metrics, we observe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>) that they all show positive correlations with the true latency, but each language pair has a slightly different scale, which <em class=\"ltx_emph ltx_font_italic\">motivates the use of accuracy instead of simple correlation</em>.\nTherefore, we further compare latency metrics in terms of accuracy in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. If we consider all system pairs (including systems following the anomalous policy), we see that all metrics significantly underperform YAAL (by more than 22% absolute), which reaches an accuracy of 96%. When we progressively filter out system pairs with similar true latency (decreasing <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> values), the accuracies slightly increase, but the ranking of the metrics remains. If we consider a subset that has a <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value between 0.001-0.05 (i.e., removing pairs with similar true latency that are, however, more difficult to distinguish), we see that YAAL still remains the most accurate by a margin of 25% absolute.\nIf we remove systems with the anomalous policy,\nall metrics gain a significant boost in accuracy (bottom part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T1\" title=\"In Which is the best Short-form Latency Metric? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>). However, the YAAL metric remains the best metric in all subsets based on <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-values, achieving 98 and 99% accuracy&#8211;even though it relies on assumptions such as uniform source token durations and monotonic source-to-target alignment.\nBased on these observations, we conclude that\n<span class=\"ltx_text ltx_font_italic\">the automatic YAAL metric is almost as accurate as\ntrue latency</span>. We include more accuracy evaluations by isolating different categories of systems in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3\" title=\"Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "policy",
                    "yaal",
                    "metric",
                    "all",
                    "anomalous",
                    "best",
                    "systems",
                    "when",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S3\" title=\"3 Overcoming the Pitfalls in SimulST Latency Metrics &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and empirically observed in this section, short-form evaluation can significantly distort latency evaluation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T2\" title=\"In Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, we present the average fraction\nof words translated <span class=\"ltx_text ltx_font_italic\">after</span> the end-of-segment signal, i.e., words omitted during evaluation by most latency metrics. A substantial portion of the translations are tail words, starting at 41% in the low-latency (1-2s) and reaching 72% in the high-latency regimes (4-5s).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Systems with higher-latency behavior have policies leading to deferred delays, and these delays in turn are more likely to overflow the source duration.</span></span></span>\n<span class=\"ltx_text ltx_font_italic\">Short-form evaluation, with artificial segment boundaries absent in real-world scenarios and metrics&#8217; problematic handling of tail words, often misrepresents SimulST system behavior.</span>\nThis raises serious concerns about\nits reliability\nand underscores the need for long-form evaluation,\nwhich we analyze in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS2\" title=\"5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "omitted"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we evaluate two resegmentation tools: mWERSegmenter and our proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>.\nThe evaluation is done on reconcatenated short-form\noutputs, allowing us to compare with gold segment boundaries.\nAs we can see in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, <span class=\"ltx_text ltx_font_italic\">the accuracy of\nlatency evaluation is significantly higher with the proposed segmenter</span>.\nWhen filtering out comparable systems by the <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>-value,\naccuracy\nfurther decreases\nwith mWERSegmenter, suggesting that the segmentation is not stable.\nBoth segmentation methods achieve over 99% accuracy, indicating resegmentation does not hinder quality assessment.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "accuracy",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The upper part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the accuracy of StreamLAAL that uses resegmentation and the original short-form metrics evaluated <em class=\"ltx_emph ltx_font_italic\">without resegmentation</em> on long-form systems.\nWe see that the accuracies are low, not exceeding 66% on all systems.\nCompared to StreamLAAL, the best-performing AL metric loses 15 to 16% absolute, and the gap is even wider compared to the proposed LongYAAL, with AL falling short by 29 points.\nThe lower part of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>\nreports the accuracy of latency metrics in long-form systems <em class=\"ltx_emph ltx_font_italic\">with resegmentation</em>.\nWe see that the resegmentation quality significantly influences the accuracy. StreamLAAL and LongLAAL share the same definition, but differ in the resegmentation&#8211;while StreamLAAL uses the mWERSegmenter, LongLAAL (and all other\n&#8220;Long-&#8221; metrics) uses our <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>. The gap in accuracy is 8 to 10 points in all subsets, showing trends similar to those in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T3\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>.\n<span class=\"ltx_text ltx_font_italic\">These results highlight the critical role of resegmentation in ensuring reliable latency evaluation in the long-form regime.</span> Additional observations are in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4\" title=\"Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>&#732;<span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "all",
                    "shortform",
                    "regime",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.T4\" title=\"In Which Resegmentation is Better? &#8227; 5.2 Long-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows\nthat the proposed LongYAAL has the highest accuracy in all subsets. LongATD and LongDAL show slightly worse results, but not\nstatistically significant. This contrasts with &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, where ATD and DAL perform worse than *AL metrics.\nWe explain this discrepancy by the fact that both metrics include tail words that rarely occur in the long-form regime.\nWe attribute the marginal difference of LongATD compared to LongYAAL to its assumption of 300ms words in the source speech, which is dynamic in LongYAAL in the form of the parameter <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>&#947;</mi><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math>, and the difference in LongDAL is caused by the artificial minimum delay of <math alttext=\"1/\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>/</mo><mi>&#947;</mi></mrow><annotation encoding=\"application/x-tex\">1/\\gamma</annotation></semantics></math> in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S2.E4\" title=\"In Differentiable Average Lagging (DAL; Cherry and Foster, 2019) &#8227; 2.1 Short-Form SimulST Latency Metrics &#8227; 2 Background &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Equation</span>&#732;<span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nLongLAAL ties with LongYAAL in most subsets, but seems slightly worse when in easily distinguishable systems (<math alttext=\"p\\text{-val}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>-val</mtext></mrow><annotation encoding=\"application/x-tex\">p\\text{-val}</annotation></semantics></math>&lt;0.001), where the metric loses 2 points in accuracy. LongLAAL, unlike LongYAAL, disregards words generated beyond the reference segment boundaries. The number of words ignored increases with the true latency of the system (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1.SSS0.Px3\" title=\"Should we use the Short-Form Regime? &#8227; 5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>), which is more prevalent in this subset. Similarly, LongAL ignores the tail words in the resegmentation and is also vulnerable to overgeneration <cite class=\"ltx_cite ltx_citemacro_cite\">Pol&#225;k and Bojar (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib26\" title=\"\">2024</a>); Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib22\" title=\"\">2024</a>)</cite>. Finally, AP performs the worst, losing more than 21 points compared to other metrics, which we attribute to the metric&#8217;s sensitivity to variable segment length. <span class=\"ltx_text ltx_font_italic\">These results position LongYAAL as the most reliable metric for assessing latency in long-form SimulST</span>.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "dal",
                    "all",
                    "atd",
                    "regime",
                    "systems",
                    "when",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we presented the first systematic evaluation of latency metrics for SimulST across several aspects, such as diverse systems, language pairs, and operating under short- and long-form speech processing. We have identified current pitfalls in the SimulST evaluation by isolating issues in the most commonly used metrics. To overcome these limitations, we propose YAAL, a new latency metric\nbetter\naligned with the\nshort-form evaluation regime.\nHowever,\nour analysis also reveals inherent shortcomings of short-form evaluation, further reinforcing the adoption of long-form evaluation as a more reliable alternative.\nMoreover, we also demonstrated that resegmentation is necessary to conduct a proper evaluation of systems operating under the long-form regime, and proposed an improved resegmentation tool coupled with the extension of YAAL for these settings&#8211;LongYAAL. The results showed that YAAL and LongYAAL improve over all other metrics in both regimes, establishing the new state-of-the-art metric for SimulST.</p>\n\n",
                "matched_terms": [
                    "yaal",
                    "metric",
                    "all",
                    "shortform",
                    "regime",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work introduces new evaluation tools that could influence future benchmarking of SimulST systems. However, there is a risk that over-reliance on specific metrics&#8211;even improved ones like YAAL and LongYAAL&#8211;could lead to overfitting system design to particular evaluation settings. For example, systems might be tuned to perform well under LongYAAL but degrade in real-world conditions that are not fully captured by the metric. Additionally, the use of automatic resegmentation methods may inadvertently introduce subtle biases if misaligned with human interpretation of segment boundaries. We encourage the community to use these tools alongside qualitative analysis and human-in-the-loop evaluations where possible.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "systems",
                    "yaal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We did not train any models as part of this study. However, we used several evaluations that required computation. Most of the experiments were conducted on a standard desktop computer equipped with an Intel i7 processor and 32GB of RAM. For forced alignments with neural models, machine translation alignment, and the COMET translation quality metric, we used\na GPU cluster.\nHowever, these evaluations can be done on a desktop machine with a slightly longer runtime. The proposed <span class=\"ltx_text ltx_font_smallcaps\">SoftSegmenter</span>, YAAL, and LongYAAL can be run efficiently on a CPU.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "yaal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the short-form regime, we use systems submitted to the IWSLT Simultaneous Speech Translation tracks of 2022 and 2023. Specifically, we use the SimulEval evaluation logs of the IWSLT 2022 and 2023 test sets <cite class=\"ltx_cite ltx_citemacro_citep\">(Anastasopoulos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib4\" title=\"\">2022</a>; Agarwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib1\" title=\"\">2023</a>)</cite>, and the logs of the <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> test set of the MuST-C data set <cite class=\"ltx_cite ltx_citemacro_citep\">(Cattoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib6\" title=\"\">2021</a>)</cite> that were submitted to IWSLT 2022. For the long-form regime, the logs are sourced from IWSLT 2025. In particular, for English-to-{German, Chinese, Japanese} the evaluation was done on the development set of the ACL 60/60 dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Salesky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib28\" title=\"\">2023</a>)</cite>, and IWSLT 2025 test set. For the Czech-to-English language pair, the evaluation was performed on the IWSLT 2024 development set <cite class=\"ltx_cite ltx_citemacro_cite\">Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib2\" title=\"\">2024</a>)</cite> and the IWSLT 2025 test set. A portion of the IWSLT 2024 development set contained segmented audio that could not be reconstructed into the original unsegmented audio.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "shortform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T5\" title=\"In Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#732;<span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T6\" title=\"Table 6 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A1.T7\" title=\"Table 7 &#8227; Appendix A Evaluated Systems &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we present the number of systems used in the short- and long-form evaluations. The number of systems available to us was slightly larger, but we excluded all systems where the logs were incomplete (e.g., predictions for all recordings were not present, mismatched order of sources and hypotheses). Furthermore, in the long-form regime, we excluded one team entirely from the evaluation due to faulty logs. These logs contained a different number of predicted words and delays, which means that we could not faithfully determine generation timestamps for each predicted word.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "all",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Same as in the short-form evaluation, we follow the definition of the true latency in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4\" title=\"4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nHowever, there are two differences.\nAfter initial experiments, we observed that the Montreal Forced Aligner used in the short-form regime is not robust for the challenging conditions of the IWSLT 2025 test set, which is based on ACL presentations. The recordings include frequent restarts, repetitions, domain-specific terminology, and non-native speech. Instead, we use the alignment method implemented within WhisperX <cite class=\"ltx_cite ltx_citemacro_cite\">Bain et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#bib.bib5\" title=\"\">2023</a>)</cite> for forced alignment. This tool leverages neural speech encoders that seem to be robust to the above-mentioned challenges. In particular, we used WhisperX&#8217;s default settings, i.e., PyTorch&#8217;s <span class=\"ltx_text ltx_font_typewriter\">WAV2VEC2_ASR_BASE_960H</span> for English and <span class=\"ltx_text ltx_font_typewriter\">comodoro/wav2vec2-xls-r-300m-cs-250</span> for Czech speech forced alignments.\nSecond, we perform resegmentation of the system hypotheses prior to the machine translation alignment with the reference. This step is necessary because the <span class=\"ltx_text ltx_font_typewriter\">awesome-align</span> tool uses the <span class=\"ltx_text ltx_font_typewriter\">bert-base-multilingual-cased</span> model for the alignment, and this model has a maximum input length of 512 tokens, which is much lower than the system hypotheses.</p>\n\n",
                "matched_terms": [
                    "shortform",
                    "regime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A natural question arises: <em class=\"ltx_emph ltx_font_italic\">Why rely on automatic latency metrics at all, when true latency offers a closer approximation of user experience?</em> In practice, computing true latency requires several requirements that limit its applicability. High-quality transcripts must be available, which is often not the case&#8211;particularly for low-resource languages or unwritten languages where transcription is infeasible. Moreover, forced alignment tools and reliable word-level translation alignments are typically available only for a small set of high-resource language pairs. Even when such resources exist, computing true latency involves multiple processing steps and is substantially more complex than evaluating standard automatic metrics. Importantly, as we show in our analysis in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5\" title=\"5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, several automatic metrics approximate true latency with high accuracy, making them a practical and effective alternative in most evaluation scenarios.</p>\n\n",
                "matched_terms": [
                    "all",
                    "accuracy",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F6\" title=\"In Additional Analysis &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">6</span></a>, we illustrate the trends after filtering out the systems affected by the anomalous policy (see &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>). Unlike in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.F3\" title=\"In 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a>, we see that all metrics and system pairs show a positive correlation with the true latency. As mentioned in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S5.SS1\" title=\"5.1 Short-Form Evaluation &#8227; 5 Results &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, language pairs exhibit different scales, making the use of the correlation coefficient more cumbersome and motivating the use of accuracy as described in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#S4.SS2.SSS0.Px3\" title=\"Accuracy &#8227; 4.2 Evaluation &#8227; 4 Experimental Settings &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "policy",
                    "all",
                    "anomalous",
                    "systems",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Surprisingly, when evaluating related systems, all metrics perform almost perfectly, reaching accuracy between 97% and 100%. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A3.F9\" title=\"In Comparing Related vs. Unrelated Systems &#8227; Appendix C Short-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">9</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency. Given a difference of at least <math alttext=\"\\sim 250\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS0.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>250</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 250</annotation></semantics></math> ms, all metrics except AP achieve 100% accuracy, and AP achieves around 99% accuracy.</p>\n\n",
                "matched_terms": [
                    "all",
                    "related",
                    "systems",
                    "when",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F11\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">11</span></a>, we show pairwise comparisons of systems evaluated in the long-form regime without resegmentation, and in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F12\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">12</span></a>, we show the same systems evaluated in the long-form regime, but after resegmentation. In <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17349v1#A4.F13\" title=\"In Appendix D Long-Form Evaluation &#8227; Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">13</span></a>, we report the accuracy of subsets based on the minimal difference in the true latency.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "accuracy",
                    "regime"
                ]
            }
        ]
    }
}