{
    "S4.T1": {
        "caption": "Table 1: Average scores for each task across the three SEA languages. We show the scores judged by humans and by Gemini-2.5-flash. The highest scores for each task are highlighted in bold.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Task</th>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Human-as-a-judge</span></td>\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_tt\" colspan=\"5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-judge</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">task</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">SeaLLMs-Audio</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">Qwen2-Audio</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">Qwen2.5-Omni</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">MERa-LiON</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">MERa-LiON-2</span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">SeaLLMs-Audio</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">Qwen2-Audio</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">Qwen2.5-Omni</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">MERa-LiON</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">MERa-LiON-2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">AA</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">2.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">2.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">2.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">2.4</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">AQA</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.4</span></span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.7</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">ASR</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">4.2</span></span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">4.4</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">CS</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">4</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">MED</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.6</span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.6</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">S2TT_EX</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.7</span></span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.9</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">S2TT_XE</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.3</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">SER</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.2</span></span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.5</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">SKI</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.8</span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.7</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">SQA</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">4.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">4.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">4.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">4.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">4.3</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">SS</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">4.1</span></span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">4.4</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">fact</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.8</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.6</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">life</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.9</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.3</span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.7</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">math</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">3.6</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.7</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.3</span>\n</span>\n</td>\n<td class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">3.5</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">safety</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">2.5</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\"><span class=\"ltx_text ltx_font_bold\">2.7</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">1.6</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:27.6pt;\">2.4</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "seallmsaudio",
            "life",
            "med",
            "each",
            "s2ttxe",
            "qwen2audio",
            "aqa",
            "humanasajudge",
            "judged",
            "average",
            "show",
            "fact",
            "math",
            "sea",
            "scores",
            "across",
            "bold",
            "highlighted",
            "asr",
            "safety",
            "humans",
            "qwen25omni",
            "languages",
            "llmasajudge",
            "task",
            "ser",
            "s2ttex",
            "three",
            "sqa",
            "meralion2",
            "meralion",
            "highest",
            "ski",
            "gemini25flash"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In addition to assessing average performance across languages, we further examine model outcomes by task. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.T1\" title=\"Table 1 &#8227; How does SeaLLMs-Audio perform on each task? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents average scores for each model across all evaluated tasks. Since the English subset excludes the S2TT task, averages for that setting are computed only over id, th, and vi. MERaLiON-2 consistently achieves the strongest results in audio comprehension tasks&#8212;specifically ASR, S2TT, and SER&#8212;which we attribute to its substantially larger and more diverse training corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib8\" title=\"\">2025</a>)</cite>. Conversely, SeaLLMs-Audio attains state-of-the-art performance in selected categories, including <span class=\"ltx_text ltx_font_italic\">fact, life, MED</span>, and <span class=\"ltx_text ltx_font_italic\">math</span>. We ascribe SeaLLMs-Audio&#8217;s advantages to the extensive scope and heterogeneity of its training data, encompassing both varied task types and multimodal input formats.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">SeaLLMs-Audio</span>, the first large audio-language model (LALM) tailored for multiple Southeast Asian (SEA) languages&#8212;Indonesian (id), Thai (th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across diverse audio-centric tasks, spanning fine-grained audio understanding and voice-based interaction. Its key features include: 1) Multilingual: the model primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English, and Chinese; 2) Multimodal: the model accepts flexible input modalities, including audio only, text only, as well as audio with text; 3) Multi-task: the model supports a wide range of tasks, including audio analysis tasks such as Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation, Speech Emotion Recognition, Speech Question Answering, and Speech Summarization. It also enables voice-based dialogue, including answering factual, mathematical, and general knowledge queries. As a significant step towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to benefit both the regional research community and industry. To automate LALM evaluation for Southeast Asia, we introduce <span class=\"ltx_text ltx_font_bold\">SeaBench-Audio</span>, a benchmark spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves competitive performance compared with other LALMs on SEA languages.&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>SeaLLMs-Audio is publicly available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/DAMO-NLP-SG/SeaLLMs-Audio\" title=\"\">https://github.com/DAMO-NLP-SG/SeaLLMs-Audio</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "seallmsaudio",
                    "show",
                    "sea"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the absence of LALMs tailored for Southeast Asian languages, progress is further constrained by the lack of comprehensive and rigorous evaluation frameworks. Existing benchmarks, such as SeaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib17\" title=\"\">2024</a>)</cite>, SeaExam, and SeaBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib13\" title=\"\">2025</a>)</cite>, focus primarily on textual evaluation within SEA contexts. Meanwhile, audio-related benchmarks remain limited to specific tasks like automatic speech recognition (ASR) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib18\" title=\"\">2025</a>)</cite>, without providing a holistic assessment of audio understanding and voice-based interaction. This lack of broad, multimodal benchmarks continue to impede the advancement of audio-language modeling in SEA languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "sea",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge above gaps, we introduce SeaLLMs-Audio (Southeast Asian Large Language Models with audio capabilities), a Large Audio-Language Model designed specifically for Southeast Asia. SeaLLMs-Audio is trained using data from a comprehensive curation pipeline that aggregates, organizes, and synthesizes multimodal resources across SEA languages, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The curated dataset spans diverse tasks, including automatic speech recognition (ASR), audio captioning (AC), speech-to-text translation (S2TT), speech summarization (SS), audio question answering (AQA), and multimodal reasoning.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "aqa",
                    "seallmsaudio",
                    "asr",
                    "sea"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, to facilitate standardized evaluation, we present SeaBench-Audio, a manually curated benchmark for assessing LALMs in Southeast Asian languages. SeaBench-Audio encompasses multiple open-ended task categories that reflect real-world, multimodal language understanding scenarios. To facilitate consistent and scalable evaluation, we adopt an LLM-as-a-judge framework with task-specific prompt templates, achieving high agreement with human annotations. Experimental results on SeaBench-Audio demonstrate that SeaLLMs-Audio delivers robust and competitive performance across a wide range of audio-language tasks.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "seallmsaudio",
                    "llmasajudge",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This training dataset for SeaLLMs-Audio contains 1.58M conversations for multiple tasks, including 7% multi-turn dialogues that better reflect real-world interactive scenarios. The tasks can be roughly classified as the following categories: automatic speech recognition (ASR), audio captioning (AC), speech-to-text translation (S2TT), question answering (QA), speech summarization (SS), audio question answering (AQA), chat, math, and factoid QA (fact) and other tasks (mixed).</p>\n\n",
                "matched_terms": [
                    "aqa",
                    "seallmsaudio",
                    "fact",
                    "asr",
                    "math"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training dataset was curated from multiple data sources, including public datasets and private data. Public datasets include: GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib2\" title=\"\">2021</a>)</cite>, GigaSpeech2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib21\" title=\"\">2025</a>)</cite>, Common Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib1\" title=\"\">2020</a>)</cite>, AudioCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib11\" title=\"\">2019</a>)</cite>, VoiceAssistant-400 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib19\" title=\"\">2024</a>)</cite>, YODAS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib12\" title=\"\">2024</a>)</cite>, and Multitask National Speech Corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib9\" title=\"\">2024</a>)</cite>.\nAs these datasets span multiple sources with disparate formats (e.g., different audio encodings, annotation schemas, and text structures), they cannot be directly used for end-to-end training. We therefore perform comprehensive preprocessing to unify the data.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the overall data curation pipeline with some examples.\nThe following describes the construction process for each task.</p>\n\n",
                "matched_terms": [
                    "task",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR datasets such as GigaSpeech, we normalize transcripts to improve readability. For example, we transform <span class=\"ltx_text ltx_font_typewriter\">\"AND LOOK AT THE PERCENTAGE OF REPORTS &lt;PERIOD&gt;\"</span> into <span class=\"ltx_text ltx_font_typewriter\">\"And look at the percentage of reports.\"</span> For GigaSpeech2, which includes Thai, Indonesian, and Vietnamese while its text does not contain punctuations, we employ a selected LLM to restore punctuations and spacing, producing more reader-friendly text for each language. As LLMs may introduce errors, we discard samples whose outputs are inconsistent with the original transcripts.</p>\n\n",
                "matched_terms": [
                    "each",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the absence of open-source S2TT datasets for SEA languages, we construct such data by leveraging ASR corpora in different languages.\nMore specifically, since each unit of the ASR data comprises the same-language speech audio plus its text transcription, we utilize this text to create translations into multiple targeted languages. This results in data pairs of speech audio in one language plus their translated text in another language.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "sea",
                    "each",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the absence of standard audio benchmarks for evaluating audio LLMs in SEA languages, we manually create a benchmark called SeaBench-Audio. It comprises 14 tasks: 1) Tasks with both audio and text inputs:\nAutomatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Speech Summarization (SS), Speech Question Answering (SQA), Customer Service (CS), Safety, Audio Cationing (AC), Audio Question Answering (AQA), Speaker Identifiers (SKI), and Speech Emotion Recognition (SER); 2) Tasks with only audio inputs: Life, Medical (MED), Math, and Fact.\nThe task descriptions and annotation criteria are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1.T3\" title=\"Table 3 &#8227; Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1\" title=\"Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "aqa",
                    "life",
                    "task",
                    "ser",
                    "sqa",
                    "med",
                    "fact",
                    "asr",
                    "safety",
                    "math",
                    "sea",
                    "ski"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An overview of the datasets is provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S3.F4\" title=\"Figure 4 &#8227; 3 SeaBench-Audio &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a). For each language, we engage a professional native linguist to annotate 10 questions per task.\nOne exception is S2TT task, which requires the translation between two languages.\nFor id/th/vi, we construct two versions&#8212;native audio to English text and English audio to native text&#8212;each with 10 questions. For English, we omit this task to prevent redundancy. Consequently, there are 150 questions for each SEA language and 130 for English, yielding a total of 580 questions. For every question, a linguist supplies a reference to facilitate scoring. The benchmark underwent multiple rounds of careful review to ensure quality.</p>\n\n",
                "matched_terms": [
                    "task",
                    "languages",
                    "sea",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, qualified native speakers rated each response on a scale of 1 to 5, with 5 representing the highest quality. However, human evaluations are expensive and time-consuming. In order to facilitate automatic evaluation, we employ an LLM-as-a-judge framework. We choose Gemini-2.5-flash (Gemini) <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib5\" title=\"\">2025</a>)</cite>, due to its capabilities of audio understanding and good balance of cost and performance. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S3.F4\" title=\"Figure 4 &#8227; 3 SeaBench-Audio &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(b), the procedures to evaluate an LALM are: 1) Generate responses for each instance with the LALM; 2) Construct an evaluation prompt with the text instruction (optional), reference answer, response, rubrics, and the template; 3) Prompt Gemini with the audio and evaluation prompt; 4) Extract the score from the final response. The prompt template for LLM-as-a-judge is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1.F8\" title=\"Figure 8 &#8227; Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1\" title=\"Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. For each task, we additionally engage linguists to develop a task-specific evaluation rubric for the responses on a scale of 1 to 5.\nWe hypothesize that tasks exhibit distinct characteristics, and a dedicated rubric more accurately captures the nuances of each task.</p>\n\n",
                "matched_terms": [
                    "llmasajudge",
                    "task",
                    "highest",
                    "each",
                    "gemini25flash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This is a concurrent work with SeaLLMs-Audio. Compared with MERaLion, it supports more languages, including English, Chinese, Indonesian, Thai, and Vietnamese; thus, it has a similar motivation to SeaLLMs-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib8\" title=\"\">2025</a>)</cite>. Like MERaLiON, we add a text instruction <span class=\"ltx_text ltx_font_italic\">\"Please follow the instruction in the speech.\"</span> for tasks without text input.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "seallmsaudio",
                    "meralion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F5\" title=\"Figure 5 &#8227; 4.1 Main Results &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the human evaluations. Evaluators assessed both overall performance and language quality, the latter referring to the correctness of language usage in responses. Language quality was rated on a 1-5 scale, where 5 indicated entirely correct language devoid of code-switching. We can see that SeaLLMs-Audio achieves the best language quality for the three SEA languages.\nThe LLM-as-a-judge evaluation result is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F6\" title=\"Figure 6 &#8227; 4.1 Main Results &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nFrom these results, SeaLLMs-Audio attains the strongest performance on id/th/vi, irrespective of whether evaluation is conducted by human annotators or Gemini. Additional observations include: (1) MERaLiON-2 surpasses MERaLiON, which is expected given that MERaLiON-2 is the newer iteration; and (2) Qwen-Omni outperforms Qwen2-Audio across the three languages, consistent with its more recent release. This alignment with prior expectations supports the validity of the LLM-as-a-judge framework.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "languages",
                    "across",
                    "seallmsaudio",
                    "llmasajudge",
                    "three",
                    "meralion2",
                    "meralion",
                    "sea"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that the scores by human judgments and LLM-as-a-judge evaluations are not perfectly aligned. To evaluate their correlation, we calculate their Pearson correlation coefficient. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F7\" title=\"Figure 7 &#8227; How is LLM-as-a-judge consistent with human judges? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, LLM-as-a-judge and human judges have an average correlation coefficient of 0.8, which shows high correlation between the scores by humans and by the LLM judge.\nWe also calculate the agreement between human judges and LLM judges when comparing the responses from two models. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.T2\" title=\"Table 2 &#8227; How is LLM-as-a-judge consistent with human judges? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, they have an average agreement of 69% with tie and 93% without tie, which is even higher than the result in MT-bench <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib24\" title=\"\">2023</a>)</cite>. Such high agreement between humans and the LLM judge shows the reliability of SeaBench-Audio.</p>\n\n",
                "matched_terms": [
                    "humans",
                    "average",
                    "llmasajudge",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduce a large audio-language model specifically designed for Southeast Asian languages, named SeaLLMs-Audio. Trained on an extensive multilingual audio corpus, SeaLLMs-Audio exhibits robust audio understanding and generation capabilities across Indonesian, Thai, and Vietnamese. To systematically assess LALMs within this region, we construct the SeaBench-Audio benchmark, encompassing multiple-choice and open-ended questions spanning 14 distinct tasks. Experimental outcomes highlight the strong performance of SeaLLMs-Audio on the proposed benchmark. We anticipate that SeaLLMs-Audio and SeaBench-Audio will promote further research on LALMs for Southeast Asia and stimulate broader efforts toward supporting low-resource languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "seallmsaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limitations in manpower and computational resources, we confined SeaLLMs-Audio and SeaBench-Audio to three selected Southeast Asian languages. Nevertheless, the proposed methodology can be easily extended to a broader range of languages. Although SeaLLMs-Audio demonstrated strong performance, instances of language mixing still exist, a behavior commonly observed in other LALMs. We anticipate that this issue can be mitigated through reinforcement learning, which we identify as a promising direction for future work.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "three",
                    "seallmsaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to express our special thanks to our professional, native linguists, Tantong Champaiboon, Nguyen Ngoc Yen Nhi and Tara Devina Putri, who helped build, evaluate, and fact-check our SeaBench-Audio dataset as well as evaluating our models across different aspects. We sincerely appreciate the valuable suggestions from Hao Zhang (DAMO Academy, Alibaba Group) on improving SeaLLMs-Audio.</p>\n\n",
                "matched_terms": [
                    "across",
                    "seallmsaudio"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Agreement between human judges and the LLM judge. We convert the single-answer grading to pairwise comparison results for calculating the agreement. \"w/ tie\" includes tie scores and non-tie scores. \"w/o tie\" includes only non-tie scores. \"R=\" indicates the agreement between two random judges.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Setup</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">id</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">th</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">vi</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Avg</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">w/ tie (R=33%)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">71%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">70%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">w/o tie (R=50%)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">92%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">95%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">92%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">94%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">93%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "convert",
            "setup",
            "agreement",
            "r50",
            "nontie",
            "comparison",
            "two",
            "tie",
            "pairwise",
            "llm",
            "r33",
            "between",
            "judge",
            "scores",
            "avg",
            "includes",
            "calculating",
            "results",
            "indicates",
            "only",
            "random",
            "grading",
            "singleanswer",
            "human",
            "judges"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We observed that the scores by human judgments and LLM-as-a-judge evaluations are not perfectly aligned. To evaluate their correlation, we calculate their Pearson correlation coefficient. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F7\" title=\"Figure 7 &#8227; How is LLM-as-a-judge consistent with human judges? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, LLM-as-a-judge and human judges have an average correlation coefficient of 0.8, which shows high correlation between the scores by humans and by the LLM judge.\nWe also calculate the agreement between human judges and LLM judges when comparing the responses from two models. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.T2\" title=\"Table 2 &#8227; How is LLM-as-a-judge consistent with human judges? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, they have an average agreement of 69% with tie and 93% without tie, which is even higher than the result in MT-bench <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib24\" title=\"\">2023</a>)</cite>. Such high agreement between humans and the LLM judge shows the reliability of SeaBench-Audio.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large audio-language models (LALMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib4\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib3\" title=\"\">2024</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib9\" title=\"\">2024</a>; Pipatanakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib15\" title=\"\">2024</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib10\" title=\"\">2024</a>)</cite> have shown impressive capabilities in understanding the rich information contained in audio signals. However, most existing LALMs support only one or two languages, most typically English, leaving multilingual and low-resource regions under-represented.</p>\n\n",
                "matched_terms": [
                    "only",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, to facilitate standardized evaluation, we present SeaBench-Audio, a manually curated benchmark for assessing LALMs in Southeast Asian languages. SeaBench-Audio encompasses multiple open-ended task categories that reflect real-world, multimodal language understanding scenarios. To facilitate consistent and scalable evaluation, we adopt an LLM-as-a-judge framework with task-specific prompt templates, achieving high agreement with human annotations. Experimental results on SeaBench-Audio demonstrate that SeaLLMs-Audio delivers robust and competitive performance across a wide range of audio-language tasks.</p>\n\n",
                "matched_terms": [
                    "agreement",
                    "human",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR datasets such as GigaSpeech, we normalize transcripts to improve readability. For example, we transform <span class=\"ltx_text ltx_font_typewriter\">\"AND LOOK AT THE PERCENTAGE OF REPORTS &lt;PERIOD&gt;\"</span> into <span class=\"ltx_text ltx_font_typewriter\">\"And look at the percentage of reports.\"</span> For GigaSpeech2, which includes Thai, Indonesian, and Vietnamese while its text does not contain punctuations, we employ a selected LLM to restore punctuations and spacing, producing more reader-friendly text for each language. As LLMs may introduce errors, we discard samples whose outputs are inconsistent with the original transcripts.</p>\n\n",
                "matched_terms": [
                    "includes",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to create voice chat data, we make use of existing text conversation data and convert the user input into audio format with Google TTS. As Google TTS has only a few voice types for each language, we also transcribe part of the data with <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-tts</span> to improve the diversity.</p>\n\n",
                "matched_terms": [
                    "only",
                    "convert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An overview of the datasets is provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S3.F4\" title=\"Figure 4 &#8227; 3 SeaBench-Audio &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a). For each language, we engage a professional native linguist to annotate 10 questions per task.\nOne exception is S2TT task, which requires the translation between two languages.\nFor id/th/vi, we construct two versions&#8212;native audio to English text and English audio to native text&#8212;each with 10 questions. For English, we omit this task to prevent redundancy. Consequently, there are 150 questions for each SEA language and 130 for English, yielding a total of 580 questions. For every question, a linguist supplies a reference to facilitate scoring. The benchmark underwent multiple rounds of careful review to ensure quality.</p>\n\n",
                "matched_terms": [
                    "two",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F5\" title=\"Figure 5 &#8227; 4.1 Main Results &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the human evaluations. Evaluators assessed both overall performance and language quality, the latter referring to the correctness of language usage in responses. Language quality was rated on a 1-5 scale, where 5 indicated entirely correct language devoid of code-switching. We can see that SeaLLMs-Audio achieves the best language quality for the three SEA languages.\nThe LLM-as-a-judge evaluation result is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F6\" title=\"Figure 6 &#8227; 4.1 Main Results &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nFrom these results, SeaLLMs-Audio attains the strongest performance on id/th/vi, irrespective of whether evaluation is conducted by human annotators or Gemini. Additional observations include: (1) MERaLiON-2 surpasses MERaLiON, which is expected given that MERaLiON-2 is the newer iteration; and (2) Qwen-Omni outperforms Qwen2-Audio across the three languages, consistent with its more recent release. This alignment with prior expectations supports the validity of the LLM-as-a-judge framework.</p>\n\n",
                "matched_terms": [
                    "human",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to assessing average performance across languages, we further examine model outcomes by task. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.T1\" title=\"Table 1 &#8227; How does SeaLLMs-Audio perform on each task? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents average scores for each model across all evaluated tasks. Since the English subset excludes the S2TT task, averages for that setting are computed only over id, th, and vi. MERaLiON-2 consistently achieves the strongest results in audio comprehension tasks&#8212;specifically ASR, S2TT, and SER&#8212;which we attribute to its substantially larger and more diverse training corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib8\" title=\"\">2025</a>)</cite>. Conversely, SeaLLMs-Audio attains state-of-the-art performance in selected categories, including <span class=\"ltx_text ltx_font_italic\">fact, life, MED</span>, and <span class=\"ltx_text ltx_font_italic\">math</span>. We ascribe SeaLLMs-Audio&#8217;s advantages to the extensive scope and heterogeneity of its training data, encompassing both varied task types and multimodal input formats.</p>\n\n",
                "matched_terms": [
                    "only",
                    "results",
                    "scores"
                ]
            }
        ]
    },
    "A1.T3": {
        "caption": "Table 3: Verbalizers for the evaluation datasets.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">Task</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\"><span class=\"ltx_text ltx_font_bold\">Task description and requirements</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">text?</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">ASR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Automatic Speech Recognition. For this task, each input consists of an audio file and a text instruction. Some clips feature regional accents, and every language in this category includes at least one example of informal speech. The set also contains multi-sentence audio clips. Each text instruction is uniquely phrased.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">Yes</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">SS</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Speech Summarization. Each unit includes a text instruction to convert an audio file into a shorter text. The instruction may limit output length by word count, number of sentences, or by specifying a short format (e.g., headline or title).</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">Yes</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">SER</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Speaker Emotion and Sentiment Recognition. This category includes 5 emotion detection units and 5 sentiment detection units. Each unit&#8217;s text instruction provides options from which the model must choose the correct answer. Every audio clip contains speech whose lexical content does not reflect the speaker&#8217;s emotion or sentiment. This setup compels the model to rely on paralinguistic features to determine the correct answer.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">Yes</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">S2TT_EX, S2TT_XE</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Speech to Text Translation. There are two speech translation categories in this task: English-to-local (EX) and local-to-English (XE). This category includes units from specialized domains (e.g., courtroom, military, medical, addressing royalty) across languages. Translation challenges&#8212;such as idioms, informal varieties, appropriate first- and second-person pronouns, and polysemy&#8212;are represented in these units.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">Yes</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">SQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Speech Question Answering. This task evaluates a model&#8217;s ability to extract or infer specific information from an audio file. Each unit&#8217;s text instruction is crafted to elicit a precise answer for reliable evaluation. The targeted information may extend beyond named entities present in the audio.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">Yes</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">AC</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Audio Captioning. Each audio unit includes non-speech sounds from both animate and inanimate sources, as well as natural or man-made scenes. The accompanying text prompts are crafted to elicit detailed descriptions of the entire audio content.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">Yes</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">AQA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Audio Question Answering. The audio input for AQA consists of non-speech snippets, including event sounds or sequences of related sounds from human activities or nature. The text instructions test a model&#8217;s abilities in object/event detection, sound tracking (e.g., duration), segmentation, and sequential analysis. Some prompts include multiple-choice options (e.g., &#8220;&#8230; takes place outdoors or indoors?&#8221;).</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">Yes</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">SKI</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Speaker Identifiers. This category focuses on non-linguistic aspects of audio (e.g., number of speakers, turn-taking, speaking rate) and speaker-related tasks such as gender, relative age, and accent or regional variety identification.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">Yes</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">Life</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Life. The audio files in this set come from text questions posted on popular forums and social media in the respective SEA-language countries. Questions are carefully chosen to cover diverse topics, suit model prompting, and remain manageable for evaluation. Each question is recorded with a unique voice. There are no separate text instructions, as the audio explicitly contains the questions.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">No</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">CS</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Customer Service. This set includes 10 customer care scenarios. Each audio clip features either a single customer or a dialogue between a customer and a CS officer. The scripts simulate real calls&#8212;for example, clarifying product or price information, checking delivery status, requesting refunds, or reporting faulty products. The instructions include 6 units with answer options and 4 open-ended units. These units are designed to assess fine-grained CS knowledge and were curated in consultation with a customer care professional.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">Yes</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">MED</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Medical Patient Question. Each unit contains an audio recording of a patient describing their condition and requesting medical advice. These recordings are human-voiced readings of questions sourced mainly from publicly available hospital websites, with some from open-source Q&amp;As. This category evaluates a model&#8217;s ability to respond within a specific medical domain. References are provided by doctors from the respective hospitals or are human-verified.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">No</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">Safety</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Safety. Containing an audio file and a text instruction, each unit in this category is designed to provoke an unsafe response from models. The set includes both country-specific safety violations and universally unsafe topics. Each unit is curated so that a safe, desired response requires analyzing the audio content&#8212;the text instruction alone does not reveal anything unsafe. This ensures that any model rejection stems from understanding the speech/audio elements.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">Yes</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">Math</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Math. There are no written instructions in this set; each audio clip contains a complete math question. The questions span various math topics for grades 7&#8211;12.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">No</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">Fact</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">Fact. Audio clips in this set pose explicit factual questions across diverse topics. No text instructions accompany the audio files. Subjects include history, economics, medicine, technology, and more.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:10.3pt;\">No</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "aspects",
            "models",
            "speakers",
            "history",
            "evaluation",
            "each",
            "must",
            "files",
            "math",
            "related",
            "response",
            "sequences",
            "informal",
            "entire",
            "provides",
            "hospitals",
            "ser",
            "clarifying",
            "evaluates",
            "duration",
            "natural",
            "extend",
            "simulate",
            "nature",
            "domains",
            "event",
            "not",
            "entities",
            "voice",
            "input",
            "descriptions",
            "patient",
            "text",
            "count",
            "provided",
            "units",
            "objectevent",
            "identifiers",
            "secondperson",
            "respond",
            "universally",
            "humanvoiced",
            "consists",
            "englishtolocal",
            "answering",
            "delivery",
            "understanding",
            "short",
            "economics",
            "animate",
            "reflect",
            "asr",
            "anything",
            "named",
            "challengessuch",
            "span",
            "diverse",
            "automatic",
            "clip",
            "identification",
            "price",
            "includes",
            "multisentence",
            "humanverified",
            "paralinguistic",
            "prompts",
            "their",
            "safe",
            "detection",
            "questions",
            "information",
            "also",
            "focuses",
            "abilities",
            "posted",
            "advice",
            "such",
            "from",
            "options",
            "appropriate",
            "elements",
            "hospital",
            "reporting",
            "safety",
            "localtoenglish",
            "recorded",
            "speakerrelated",
            "precise",
            "categories",
            "format",
            "example",
            "refunds",
            "speaking",
            "subjects",
            "aqa",
            "customer",
            "correct",
            "ensures",
            "across",
            "royalty",
            "condition",
            "medicine",
            "forums",
            "emotion",
            "separate",
            "there",
            "every",
            "choose",
            "word",
            "reliable",
            "inanimate",
            "requires",
            "remain",
            "tasks",
            "sentences",
            "sounds",
            "test",
            "pose",
            "analyzing",
            "units",
            "extract",
            "speech",
            "features",
            "task",
            "summarization",
            "model",
            "uniquely",
            "question",
            "answer",
            "countryspecific",
            "more",
            "opensource",
            "sourced",
            "convert",
            "indoors",
            "yes",
            "lexical",
            "712",
            "two",
            "professional",
            "category",
            "speechaudio",
            "explicitly",
            "sound",
            "doctors",
            "one",
            "specialized",
            "unit",
            "include",
            "officer",
            "social",
            "translation",
            "complete",
            "gender",
            "determine",
            "first",
            "file",
            "analysis",
            "products",
            "factual",
            "designed",
            "languages",
            "media",
            "assess",
            "segmentation",
            "domain",
            "respective",
            "come",
            "care",
            "compels",
            "desired",
            "rate",
            "least",
            "explicit",
            "rely",
            "idioms",
            "scripts",
            "topics",
            "grades",
            "manmade",
            "prompting",
            "specific",
            "human",
            "clips",
            "detailed",
            "life",
            "content",
            "output",
            "setup",
            "which",
            "nonspeech",
            "dialogue",
            "mainly",
            "multiplechoice",
            "military",
            "instructions",
            "whose",
            "product",
            "containing",
            "beyond",
            "between",
            "phrased",
            "recognition",
            "relative",
            "unique",
            "title",
            "contains",
            "websites",
            "tracking",
            "varieties",
            "feature",
            "sealanguage",
            "accents",
            "contentthe",
            "elicit",
            "accompanying",
            "age",
            "fact",
            "accompany",
            "activities",
            "place",
            "nonlinguistic",
            "faulty",
            "does",
            "shorter",
            "popular",
            "readings",
            "consultation",
            "sequential",
            "various",
            "polysemyare",
            "captioning",
            "crafted",
            "callsfor",
            "violations",
            "language",
            "references",
            "well",
            "both",
            "snippets",
            "available",
            "countries",
            "datasets",
            "carefully",
            "s2ttex",
            "sqa",
            "pronouns",
            "either",
            "stems",
            "infer",
            "sources",
            "requesting",
            "chosen",
            "audio",
            "recording",
            "speaker",
            "speakers",
            "real",
            "requirements",
            "openended",
            "provoke",
            "verbalizers",
            "variety",
            "suit",
            "some",
            "knowledge",
            "description",
            "turntaking",
            "models",
            "specifying",
            "addressing",
            "represented",
            "cover",
            "regional",
            "service",
            "instruction",
            "any",
            "headline",
            "finegrained",
            "accent",
            "qas",
            "manageable",
            "reveal",
            "courtroom",
            "into",
            "ability",
            "describing",
            "number",
            "set",
            "scenes",
            "within",
            "recordings",
            "including",
            "rejection",
            "publicly",
            "ski",
            "present",
            "status",
            "scenarios",
            "curated",
            "med",
            "length",
            "technology",
            "limit",
            "s2ttxe",
            "outdoors",
            "single",
            "unsafe",
            "written",
            "takes",
            "checking",
            "medical",
            "sentiment",
            "alone",
            "targeted"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Due to the absence of standard audio benchmarks for evaluating audio LLMs in SEA languages, we manually create a benchmark called SeaBench-Audio. It comprises 14 tasks: 1) Tasks with both audio and text inputs:\nAutomatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Speech Summarization (SS), Speech Question Answering (SQA), Customer Service (CS), Safety, Audio Cationing (AC), Audio Question Answering (AQA), Speaker Identifiers (SKI), and Speech Emotion Recognition (SER); 2) Tasks with only audio inputs: Life, Medical (MED), Math, and Fact.\nThe task descriptions and annotation criteria are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1.T3\" title=\"Table 3 &#8227; Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1\" title=\"Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">SeaLLMs-Audio</span>, the first large audio-language model (LALM) tailored for multiple Southeast Asian (SEA) languages&#8212;Indonesian (id), Thai (th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across diverse audio-centric tasks, spanning fine-grained audio understanding and voice-based interaction. Its key features include: 1) Multilingual: the model primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English, and Chinese; 2) Multimodal: the model accepts flexible input modalities, including audio only, text only, as well as audio with text; 3) Multi-task: the model supports a wide range of tasks, including audio analysis tasks such as Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation, Speech Emotion Recognition, Speech Question Answering, and Speech Summarization. It also enables voice-based dialogue, including answering factual, mathematical, and general knowledge queries. As a significant step towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to benefit both the regional research community and industry. To automate LALM evaluation for Southeast Asia, we introduce <span class=\"ltx_text ltx_font_bold\">SeaBench-Audio</span>, a benchmark spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves competitive performance compared with other LALMs on SEA languages.&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>SeaLLMs-Audio is publicly available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/DAMO-NLP-SG/SeaLLMs-Audio\" title=\"\">https://github.com/DAMO-NLP-SG/SeaLLMs-Audio</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "automatic",
                    "include",
                    "dialogue",
                    "answering",
                    "evaluation",
                    "regional",
                    "translation",
                    "also",
                    "captioning",
                    "audio",
                    "finegrained",
                    "understanding",
                    "such",
                    "tasks",
                    "input",
                    "text",
                    "first",
                    "across",
                    "analysis",
                    "well",
                    "factual",
                    "both",
                    "speech",
                    "available",
                    "emotion",
                    "languages",
                    "recognition",
                    "features",
                    "knowledge",
                    "summarization",
                    "model",
                    "including",
                    "question",
                    "publicly",
                    "diverse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large audio-language models (LALMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib4\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib3\" title=\"\">2024</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib9\" title=\"\">2024</a>; Pipatanakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib15\" title=\"\">2024</a>; Held et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib10\" title=\"\">2024</a>)</cite> have shown impressive capabilities in understanding the rich information contained in audio signals. However, most existing LALMs support only one or two languages, most typically English, leaving multilingual and low-resource regions under-represented.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages",
                    "information",
                    "audio",
                    "understanding",
                    "one",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Southeast Asia (SEA), significant progress has been made in developing multilingual large language models (LLMs), such as SeaLLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib14\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib22\" title=\"\">2025</a>; Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib23\" title=\"\">2025</a>)</cite>, Sailor <cite class=\"ltx_cite ltx_citemacro_citep\">(Dou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib6\" title=\"\">2024</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib7\" title=\"\">2025</a>)</cite>, and SEA-LION<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/aisingapore/collections\" title=\"\">https://huggingface.co/aisingapore/collections</a></span></span></span> series. Despite their multilingual reach, these models operate solely in the textual modality and lack the ability to process audio inputs&#8212;an essential component of natural human communication.</p>\n\n",
                "matched_terms": [
                    "models",
                    "natural",
                    "audio",
                    "their",
                    "language",
                    "ability",
                    "such",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the absence of LALMs tailored for Southeast Asian languages, progress is further constrained by the lack of comprehensive and rigorous evaluation frameworks. Existing benchmarks, such as SeaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib17\" title=\"\">2024</a>)</cite>, SeaExam, and SeaBench <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib13\" title=\"\">2025</a>)</cite>, focus primarily on textual evaluation within SEA contexts. Meanwhile, audio-related benchmarks remain limited to specific tasks like automatic speech recognition (ASR) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib18\" title=\"\">2025</a>)</cite>, without providing a holistic assessment of audio understanding and voice-based interaction. This lack of broad, multimodal benchmarks continue to impede the advancement of audio-language modeling in SEA languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "recognition",
                    "automatic",
                    "within",
                    "audio",
                    "understanding",
                    "evaluation",
                    "beyond",
                    "such",
                    "remain",
                    "tasks",
                    "asr",
                    "speech",
                    "specific"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge above gaps, we introduce SeaLLMs-Audio (Southeast Asian Large Language Models with audio capabilities), a Large Audio-Language Model designed specifically for Southeast Asia. SeaLLMs-Audio is trained using data from a comprehensive curation pipeline that aggregates, organizes, and synthesizes multimodal resources across SEA languages, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The curated dataset spans diverse tasks, including automatic speech recognition (ASR), audio captioning (AC), speech-to-text translation (S2TT), speech summarization (SS), audio question answering (AQA), and multimodal reasoning.</p>\n\n",
                "matched_terms": [
                    "models",
                    "automatic",
                    "curated",
                    "answering",
                    "aqa",
                    "translation",
                    "audio",
                    "captioning",
                    "from",
                    "tasks",
                    "across",
                    "language",
                    "designed",
                    "asr",
                    "speech",
                    "languages",
                    "recognition",
                    "summarization",
                    "model",
                    "including",
                    "question",
                    "diverse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, to facilitate standardized evaluation, we present SeaBench-Audio, a manually curated benchmark for assessing LALMs in Southeast Asian languages. SeaBench-Audio encompasses multiple open-ended task categories that reflect real-world, multimodal language understanding scenarios. To facilitate consistent and scalable evaluation, we adopt an LLM-as-a-judge framework with task-specific prompt templates, achieving high agreement with human annotations. Experimental results on SeaBench-Audio demonstrate that SeaLLMs-Audio delivers robust and competitive performance across a wide range of audio-language tasks.</p>\n\n",
                "matched_terms": [
                    "categories",
                    "languages",
                    "across",
                    "present",
                    "scenarios",
                    "curated",
                    "language",
                    "task",
                    "understanding",
                    "evaluation",
                    "tasks",
                    "reflect",
                    "human",
                    "openended"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present <span class=\"ltx_text ltx_font_bold\">SeaLLMs-Audio</span>, a large-scale audio&#8211;language model specifically designed for Southeast Asian contexts.</p>\n\n",
                "matched_terms": [
                    "model",
                    "present",
                    "designed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This training dataset for SeaLLMs-Audio contains 1.58M conversations for multiple tasks, including 7% multi-turn dialogues that better reflect real-world interactive scenarios. The tasks can be roughly classified as the following categories: automatic speech recognition (ASR), audio captioning (AC), speech-to-text translation (S2TT), question answering (QA), speech summarization (SS), audio question answering (AQA), chat, math, and factoid QA (fact) and other tasks (mixed).</p>\n\n",
                "matched_terms": [
                    "categories",
                    "automatic",
                    "scenarios",
                    "answering",
                    "aqa",
                    "translation",
                    "audio",
                    "captioning",
                    "fact",
                    "tasks",
                    "math",
                    "reflect",
                    "asr",
                    "speech",
                    "recognition",
                    "summarization",
                    "including",
                    "question",
                    "contains"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training dataset was curated from multiple data sources, including public datasets and private data. Public datasets include: GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib2\" title=\"\">2021</a>)</cite>, GigaSpeech2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib21\" title=\"\">2025</a>)</cite>, Common Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib1\" title=\"\">2020</a>)</cite>, AudioCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib11\" title=\"\">2019</a>)</cite>, VoiceAssistant-400 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib19\" title=\"\">2024</a>)</cite>, YODAS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib12\" title=\"\">2024</a>)</cite>, and Multitask National Speech Corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib9\" title=\"\">2024</a>)</cite>.\nAs these datasets span multiple sources with disparate formats (e.g., different audio encodings, annotation schemas, and text structures), they cannot be directly used for end-to-end training. We therefore perform comprehensive preprocessing to unify the data.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the overall data curation pipeline with some examples.\nThe following describes the construction process for each task.</p>\n\n",
                "matched_terms": [
                    "some",
                    "task",
                    "curated",
                    "audio",
                    "include",
                    "including",
                    "voice",
                    "span",
                    "each",
                    "from",
                    "speech",
                    "text",
                    "datasets",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR datasets such as GigaSpeech, we normalize transcripts to improve readability. For example, we transform <span class=\"ltx_text ltx_font_typewriter\">\"AND LOOK AT THE PERCENTAGE OF REPORTS &lt;PERIOD&gt;\"</span> into <span class=\"ltx_text ltx_font_typewriter\">\"And look at the percentage of reports.\"</span> For GigaSpeech2, which includes Thai, Indonesian, and Vietnamese while its text does not contain punctuations, we employ a selected LLM to restore punctuations and spacing, producing more reader-friendly text for each language. As LLMs may introduce errors, we discard samples whose outputs are inconsistent with the original transcripts.</p>\n\n",
                "matched_terms": [
                    "does",
                    "into",
                    "includes",
                    "language",
                    "whose",
                    "which",
                    "example",
                    "more",
                    "text",
                    "such",
                    "each",
                    "asr",
                    "not",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the absence of open-source S2TT datasets for SEA languages, we construct such data by leveraging ASR corpora in different languages.\nMore specifically, since each unit of the ASR data comprises the same-language speech audio plus its text transcription, we utilize this text to create translations into multiple targeted languages. This results in data pairs of speech audio in one language plus their translated text in another language.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "into",
                    "unit",
                    "audio",
                    "their",
                    "language",
                    "more",
                    "one",
                    "such",
                    "each",
                    "asr",
                    "speech",
                    "opensource",
                    "targeted",
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AudioCaps provides captions exclusively in English. To accommodate Southeast Asian languages, we translate these captions into the respective target languages.</p>\n\n",
                "matched_terms": [
                    "respective",
                    "languages",
                    "into",
                    "provides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set is curated to obtain audio questions with text answers. To do this, we make use of existing question-answer pairs in text format. The answers are kept unchanged in text form, while the text questions are converted into audio with text-to-speech (TTS) models. No translation is involved. After manually assessing samples of the quality of TTS outputs from several models, we finally select Google Text-to-Speech <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cloud.google.com/text-to-speech\" title=\"\">https://cloud.google.com/text-to-speech</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "models",
                    "format",
                    "translation",
                    "into",
                    "curated",
                    "audio",
                    "from",
                    "questions",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to curate SS dataset, we sample a piece of speech audio from YODAS2 dataset and ask Gemini-2.0-Flash to summarize it in a specified language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "audio",
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to create natural questions and audio about a piece of audio, we first sample a piece of audio from the YODAS2 dataset, which contains audio for YouTube videos. After that, we prompt Gemini-2.0-Flash to generate a question about the audio and provide the corresponding answer.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "audio",
                    "which",
                    "question",
                    "answer",
                    "from",
                    "questions",
                    "contains",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to create voice chat data, we make use of existing text conversation data and convert the user input into audio format with Google TTS. As Google TTS has only a few voice types for each language, we also transcribe part of the data with <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-tts</span> to improve the diversity.</p>\n\n",
                "matched_terms": [
                    "format",
                    "into",
                    "also",
                    "audio",
                    "language",
                    "convert",
                    "voice",
                    "each",
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For math and factoid QA instruction data, we also transcribe the prompts into speech with Google TTS.</p>\n\n",
                "matched_terms": [
                    "into",
                    "also",
                    "prompts",
                    "speech",
                    "math",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The distribution of the training data by language and by task type is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S2.F2\" title=\"Figure 2 &#8227; math and fact &#8227; 2.1 Comprehensive Data Curation Pipeline &#8227; 2 SeaLLMs-Audio &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeaLLMs-Audio builts upon Qwen2-Audio-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib4\" title=\"\">2023</a>)</cite> and Qwen2.5-7B-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib16\" title=\"\">2025</a>)</cite>. The architecture is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Model Architecture &#8227; 2 SeaLLMs-Audio &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. We replace the LLM module in Qwen2-Audio-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib3\" title=\"\">2024</a>)</cite> by Qwen2.5-7B-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib16\" title=\"\">2025</a>)</cite>. In this way, we harness the advantages of both models: Qwen2-Audio-7B audio encoder can encode the audio features for speech and non-speech audios effectively, and Qwen2.5-7B-Instruct has strong multilingual capabilities. Due to the hidden embedding mismatch, the audio adapter is newly initialized. After that, we do full-parameter fine-tuning on our newly curated large-scale audio dataset, which contains multiple tasks. Given paired data <math alttext=\"(a,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a,x)</annotation></semantics></math>, with <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> denoting the audio sequences and <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> denoting the optional corresponding text sequences, the training objective is to maximize the likelihood of the subsequent text token, formulated as</p>\n\n",
                "matched_terms": [
                    "models",
                    "features",
                    "contains",
                    "curated",
                    "audio",
                    "which",
                    "nonspeech",
                    "tasks",
                    "sequences",
                    "both",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">conditioned on the audio representations and the preceding text tokens <math alttext=\"x_{&lt;t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mrow><mi/><mo>&lt;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">x_{&lt;t}</annotation></semantics></math>, where <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> represents the trainable parameters of the LALM. We train the model on the dataset for 1 epoch, which took &#160;6 days to complete on 32 A800 GPUs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "complete",
                    "model",
                    "which",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An overview of the datasets is provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S3.F4\" title=\"Figure 4 &#8227; 3 SeaBench-Audio &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a). For each language, we engage a professional native linguist to annotate 10 questions per task.\nOne exception is S2TT task, which requires the translation between two languages.\nFor id/th/vi, we construct two versions&#8212;native audio to English text and English audio to native text&#8212;each with 10 questions. For English, we omit this task to prevent redundancy. Consequently, there are 150 questions for each SEA language and 130 for English, yielding a total of 580 questions. For every question, a linguist supplies a reference to facilitate scoring. The benchmark underwent multiple rounds of careful review to ensure quality.</p>\n\n",
                "matched_terms": [
                    "which",
                    "each",
                    "questions",
                    "two",
                    "translation",
                    "audio",
                    "professional",
                    "requires",
                    "between",
                    "text",
                    "language",
                    "provided",
                    "one",
                    "datasets",
                    "languages",
                    "task",
                    "question",
                    "there",
                    "every"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, qualified native speakers rated each response on a scale of 1 to 5, with 5 representing the highest quality. However, human evaluations are expensive and time-consuming. In order to facilitate automatic evaluation, we employ an LLM-as-a-judge framework. We choose Gemini-2.5-flash (Gemini) <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib5\" title=\"\">2025</a>)</cite>, due to its capabilities of audio understanding and good balance of cost and performance. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S3.F4\" title=\"Figure 4 &#8227; 3 SeaBench-Audio &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(b), the procedures to evaluate an LALM are: 1) Generate responses for each instance with the LALM; 2) Construct an evaluation prompt with the text instruction (optional), reference answer, response, rubrics, and the template; 3) Prompt Gemini with the audio and evaluation prompt; 4) Extract the score from the final response. The prompt template for LLM-as-a-judge is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1.F8\" title=\"Figure 8 &#8227; Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1\" title=\"Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. For each task, we additionally engage linguists to develop a task-specific evaluation rubric for the responses on a scale of 1 to 5.\nWe hypothesize that tasks exhibit distinct characteristics, and a dedicated rubric more accurately captures the nuances of each task.</p>\n\n",
                "matched_terms": [
                    "automatic",
                    "task",
                    "audio",
                    "response",
                    "extract",
                    "understanding",
                    "more",
                    "evaluation",
                    "text",
                    "each",
                    "speakers",
                    "from",
                    "tasks",
                    "answer",
                    "human",
                    "choose",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This is the latest version of Qwen-Audio series <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib3\" title=\"\">2024</a>)</cite>, which mainly focus on English and Chinese. It shares the same base audio encoder as SeaLLMs-Audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "which",
                    "mainly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This is a multimodal model that perceive diverse modalities including text, audio, image, and video <cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib20\" title=\"\">2025</a>)</cite>. It also adopted the audio encoder from Qwen2-Audio. In this work, we only compare its performance with audio input.</p>\n\n",
                "matched_terms": [
                    "diverse",
                    "also",
                    "audio",
                    "model",
                    "including",
                    "from",
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This model was trained to understand Singlish, which was trained on 62 million multimodal instruction samples <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib8\" title=\"\">2025</a>)</cite>. Since it was trained with both audio and text instructions, we add a text instruction <span class=\"ltx_text ltx_font_italic\">\"Please follow the instruction in the speech.\"</span> for tasks with no text input.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "which",
                    "text",
                    "speech",
                    "tasks",
                    "both",
                    "input",
                    "instructions",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This is a concurrent work with SeaLLMs-Audio. Compared with MERaLion, it supports more languages, including English, Chinese, Indonesian, Thai, and Vietnamese; thus, it has a similar motivation to SeaLLMs-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib8\" title=\"\">2025</a>)</cite>. Like MERaLiON, we add a text instruction <span class=\"ltx_text ltx_font_italic\">\"Please follow the instruction in the speech.\"</span> for tasks without text input.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "instruction",
                    "including",
                    "more",
                    "speech",
                    "tasks",
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All the LALMs can accept audio with text as input. In order to evaluate the quality of LLM-as-a-judge, we also conducted human evaluations. The judging criteria are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1\" title=\"Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. We engage the benchmark annotators to do the evaluations as they are familiar with the benchmarks.</p>\n\n",
                "matched_terms": [
                    "also",
                    "audio",
                    "human",
                    "input",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F5\" title=\"Figure 5 &#8227; 4.1 Main Results &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the human evaluations. Evaluators assessed both overall performance and language quality, the latter referring to the correctness of language usage in responses. Language quality was rated on a 1-5 scale, where 5 indicated entirely correct language devoid of code-switching. We can see that SeaLLMs-Audio achieves the best language quality for the three SEA languages.\nThe LLM-as-a-judge evaluation result is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F6\" title=\"Figure 6 &#8227; 4.1 Main Results &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nFrom these results, SeaLLMs-Audio attains the strongest performance on id/th/vi, irrespective of whether evaluation is conducted by human annotators or Gemini. Additional observations include: (1) MERaLiON-2 surpasses MERaLiON, which is expected given that MERaLiON-2 is the newer iteration; and (2) Qwen-Omni outperforms Qwen2-Audio across the three languages, consistent with its more recent release. This alignment with prior expectations supports the validity of the LLM-as-a-judge framework.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "correct",
                    "language",
                    "include",
                    "which",
                    "more",
                    "evaluation",
                    "from",
                    "both",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further understand the capabilities of SeaLLMs-Audio and SeaBench-Audio, we conduct more analysis on the results.</p>\n\n",
                "matched_terms": [
                    "more",
                    "analysis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to assessing average performance across languages, we further examine model outcomes by task. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.T1\" title=\"Table 1 &#8227; How does SeaLLMs-Audio perform on each task? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents average scores for each model across all evaluated tasks. Since the English subset excludes the S2TT task, averages for that setting are computed only over id, th, and vi. MERaLiON-2 consistently achieves the strongest results in audio comprehension tasks&#8212;specifically ASR, S2TT, and SER&#8212;which we attribute to its substantially larger and more diverse training corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib8\" title=\"\">2025</a>)</cite>. Conversely, SeaLLMs-Audio attains state-of-the-art performance in selected categories, including <span class=\"ltx_text ltx_font_italic\">fact, life, MED</span>, and <span class=\"ltx_text ltx_font_italic\">math</span>. We ascribe SeaLLMs-Audio&#8217;s advantages to the extensive scope and heterogeneity of its training data, encompassing both varied task types and multimodal input formats.</p>\n\n",
                "matched_terms": [
                    "categories",
                    "languages",
                    "across",
                    "life",
                    "task",
                    "audio",
                    "model",
                    "med",
                    "including",
                    "more",
                    "fact",
                    "each",
                    "tasks",
                    "both",
                    "asr",
                    "math",
                    "diverse",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that the scores by human judgments and LLM-as-a-judge evaluations are not perfectly aligned. To evaluate their correlation, we calculate their Pearson correlation coefficient. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F7\" title=\"Figure 7 &#8227; How is LLM-as-a-judge consistent with human judges? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, LLM-as-a-judge and human judges have an average correlation coefficient of 0.8, which shows high correlation between the scores by humans and by the LLM judge.\nWe also calculate the agreement between human judges and LLM judges when comparing the responses from two models. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.T2\" title=\"Table 2 &#8227; How is LLM-as-a-judge consistent with human judges? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, they have an average agreement of 69% with tie and 93% without tie, which is even higher than the result in MT-bench <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib24\" title=\"\">2023</a>)</cite>. Such high agreement between humans and the LLM judge shows the reliability of SeaBench-Audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "also",
                    "their",
                    "which",
                    "such",
                    "from",
                    "human",
                    "between",
                    "not",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduce a large audio-language model specifically designed for Southeast Asian languages, named SeaLLMs-Audio. Trained on an extensive multilingual audio corpus, SeaLLMs-Audio exhibits robust audio understanding and generation capabilities across Indonesian, Thai, and Vietnamese. To systematically assess LALMs within this region, we construct the SeaBench-Audio benchmark, encompassing multiple-choice and open-ended questions spanning 14 distinct tasks. Experimental outcomes highlight the strong performance of SeaLLMs-Audio on the proposed benchmark. We anticipate that SeaLLMs-Audio and SeaBench-Audio will promote further research on LALMs for Southeast Asia and stimulate broader efforts toward supporting low-resource languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "named",
                    "assess",
                    "audio",
                    "within",
                    "model",
                    "designed",
                    "understanding",
                    "tasks",
                    "multiplechoice",
                    "questions",
                    "openended"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limitations in manpower and computational resources, we confined SeaLLMs-Audio and SeaBench-Audio to three selected Southeast Asian languages. Nevertheless, the proposed methodology can be easily extended to a broader range of languages. Although SeaLLMs-Audio demonstrated strong performance, instances of language mixing still exist, a behavior commonly observed in other LALMs. We anticipate that this issue can be mitigated through reinforcement learning, which we identify as a promising direction for future work.</p>\n\n",
                "matched_terms": [
                    "language",
                    "languages",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We would like to express our special thanks to our professional, native linguists, Tantong Champaiboon, Nguyen Ngoc Yen Nhi and Tara Devina Putri, who helped build, evaluate, and fact-check our SeaBench-Audio dataset as well as evaluating our models across different aspects. We sincerely appreciate the valuable suggestions from Hao Zhang (DAMO Academy, Alibaba Group) on improving SeaLLMs-Audio.</p>\n\n",
                "matched_terms": [
                    "aspects",
                    "models",
                    "across",
                    "professional",
                    "well",
                    "from"
                ]
            }
        ]
    },
    "A1.T4": {
        "caption": "Table 4: General scoring criteria for assessing the overall quality of responses by human judges. For each score, we provide guidelines to promote consistency and reliability in human evaluations.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">Score</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\"><span class=\"ltx_text ltx_font_bold\">Criteria</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">The response is largely inaccurate, irrelevant, or incomplete, with poor language quality. It does not effectively address the question or may be incoherent.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">The response contains significant inaccuracies or is missing key details. It may be unclear or poorly structured, and the language quality could be improved.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">The response is generally accurate but may contain noticeable errors or omissions. It addresses the question with moderate clarity and completeness, but could be better structured or more detailed.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">The response is mostly accurate and relevant, with a few minor errors or omissions. It is clear and well-structured but could benefit from slight improvements in detail or language quality.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:24.2pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:258.8pt;\">The response is accurate, relevant, coherent, and complete, with excellent language quality. It answers the question thoroughly, clearly, and correctly, with no significant errors.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "poor",
            "does",
            "answers",
            "significant",
            "structured",
            "slight",
            "consistency",
            "key",
            "guidelines",
            "overall",
            "clearly",
            "each",
            "details",
            "detailed",
            "address",
            "addresses",
            "general",
            "not",
            "inaccuracies",
            "noticeable",
            "thoroughly",
            "reliability",
            "moderate",
            "completeness",
            "provide",
            "scoring",
            "evaluations",
            "complete",
            "promote",
            "criteria",
            "inaccurate",
            "benefit",
            "from",
            "contain",
            "detail",
            "omissions",
            "excellent",
            "coherent",
            "unclear",
            "clarity",
            "incoherent",
            "effectively",
            "responses",
            "could",
            "language",
            "largely",
            "mostly",
            "accurate",
            "response",
            "few",
            "clear",
            "missing",
            "irrelevant",
            "incomplete",
            "improved",
            "assessing",
            "minor",
            "errors",
            "score",
            "wellstructured",
            "poorly",
            "better",
            "correctly",
            "question",
            "more",
            "relevant",
            "generally",
            "human",
            "judges",
            "improvements",
            "contains",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">All the LALMs can accept audio with text as input. In order to evaluate the quality of LLM-as-a-judge, we also conducted human evaluations. The judging criteria are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1\" title=\"Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. We engage the benchmark annotators to do the evaluations as they are familiar with the benchmarks.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">SeaLLMs-Audio</span>, the first large audio-language model (LALM) tailored for multiple Southeast Asian (SEA) languages&#8212;Indonesian (id), Thai (th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across diverse audio-centric tasks, spanning fine-grained audio understanding and voice-based interaction. Its key features include: 1) Multilingual: the model primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English, and Chinese; 2) Multimodal: the model accepts flexible input modalities, including audio only, text only, as well as audio with text; 3) Multi-task: the model supports a wide range of tasks, including audio analysis tasks such as Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation, Speech Emotion Recognition, Speech Question Answering, and Speech Summarization. It also enables voice-based dialogue, including answering factual, mathematical, and general knowledge queries. As a significant step towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to benefit both the regional research community and industry. To automate LALM evaluation for Southeast Asia, we introduce <span class=\"ltx_text ltx_font_bold\">SeaBench-Audio</span>, a benchmark spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves competitive performance compared with other LALMs on SEA languages.&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>SeaLLMs-Audio is publicly available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/DAMO-NLP-SG/SeaLLMs-Audio\" title=\"\">https://github.com/DAMO-NLP-SG/SeaLLMs-Audio</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "significant",
                    "key",
                    "question",
                    "benefit",
                    "general"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Southeast Asia (SEA), significant progress has been made in developing multilingual large language models (LLMs), such as SeaLLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib14\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib22\" title=\"\">2025</a>; Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib23\" title=\"\">2025</a>)</cite>, Sailor <cite class=\"ltx_cite ltx_citemacro_citep\">(Dou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib6\" title=\"\">2024</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib7\" title=\"\">2025</a>)</cite>, and SEA-LION<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/aisingapore/collections\" title=\"\">https://huggingface.co/aisingapore/collections</a></span></span></span> series. Despite their multilingual reach, these models operate solely in the textual modality and lack the ability to process audio inputs&#8212;an essential component of natural human communication.</p>\n\n",
                "matched_terms": [
                    "language",
                    "human",
                    "significant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To bridge above gaps, we introduce SeaLLMs-Audio (Southeast Asian Large Language Models with audio capabilities), a Large Audio-Language Model designed specifically for Southeast Asia. SeaLLMs-Audio is trained using data from a comprehensive curation pipeline that aggregates, organizes, and synthesizes multimodal resources across SEA languages, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The curated dataset spans diverse tasks, including automatic speech recognition (ASR), audio captioning (AC), speech-to-text translation (S2TT), speech summarization (SS), audio question answering (AQA), and multimodal reasoning.</p>\n\n",
                "matched_terms": [
                    "question",
                    "language",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, to facilitate standardized evaluation, we present SeaBench-Audio, a manually curated benchmark for assessing LALMs in Southeast Asian languages. SeaBench-Audio encompasses multiple open-ended task categories that reflect real-world, multimodal language understanding scenarios. To facilitate consistent and scalable evaluation, we adopt an LLM-as-a-judge framework with task-specific prompt templates, achieving high agreement with human annotations. Experimental results on SeaBench-Audio demonstrate that SeaLLMs-Audio delivers robust and competitive performance across a wide range of audio-language tasks.</p>\n\n",
                "matched_terms": [
                    "language",
                    "assessing",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This training dataset for SeaLLMs-Audio contains 1.58M conversations for multiple tasks, including 7% multi-turn dialogues that better reflect real-world interactive scenarios. The tasks can be roughly classified as the following categories: automatic speech recognition (ASR), audio captioning (AC), speech-to-text translation (S2TT), question answering (QA), speech summarization (SS), audio question answering (AQA), chat, math, and factoid QA (fact) and other tasks (mixed).</p>\n\n",
                "matched_terms": [
                    "question",
                    "contains",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training dataset was curated from multiple data sources, including public datasets and private data. Public datasets include: GigaSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib2\" title=\"\">2021</a>)</cite>, GigaSpeech2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib21\" title=\"\">2025</a>)</cite>, Common Voice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib1\" title=\"\">2020</a>)</cite>, AudioCaps <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib11\" title=\"\">2019</a>)</cite>, VoiceAssistant-400 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie and Wu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib19\" title=\"\">2024</a>)</cite>, YODAS2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib12\" title=\"\">2024</a>)</cite>, and Multitask National Speech Corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib9\" title=\"\">2024</a>)</cite>.\nAs these datasets span multiple sources with disparate formats (e.g., different audio encodings, annotation schemas, and text structures), they cannot be directly used for end-to-end training. We therefore perform comprehensive preprocessing to unify the data.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the overall data curation pipeline with some examples.\nThe following describes the construction process for each task.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR datasets such as GigaSpeech, we normalize transcripts to improve readability. For example, we transform <span class=\"ltx_text ltx_font_typewriter\">\"AND LOOK AT THE PERCENTAGE OF REPORTS &lt;PERIOD&gt;\"</span> into <span class=\"ltx_text ltx_font_typewriter\">\"And look at the percentage of reports.\"</span> For GigaSpeech2, which includes Thai, Indonesian, and Vietnamese while its text does not contain punctuations, we employ a selected LLM to restore punctuations and spacing, producing more reader-friendly text for each language. As LLMs may introduce errors, we discard samples whose outputs are inconsistent with the original transcripts.</p>\n\n",
                "matched_terms": [
                    "does",
                    "language",
                    "more",
                    "each",
                    "contain",
                    "not",
                    "errors"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given the absence of open-source S2TT datasets for SEA languages, we construct such data by leveraging ASR corpora in different languages.\nMore specifically, since each unit of the ASR data comprises the same-language speech audio plus its text transcription, we utilize this text to create translations into multiple targeted languages. This results in data pairs of speech audio in one language plus their translated text in another language.</p>\n\n",
                "matched_terms": [
                    "more",
                    "language",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set is curated to obtain audio questions with text answers. To do this, we make use of existing question-answer pairs in text format. The answers are kept unchanged in text form, while the text questions are converted into audio with text-to-speech (TTS) models. No translation is involved. After manually assessing samples of the quality of TTS outputs from several models, we finally select Google Text-to-Speech <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cloud.google.com/text-to-speech\" title=\"\">https://cloud.google.com/text-to-speech</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "assessing",
                    "from",
                    "answers",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to curate SS dataset, we sample a piece of speech audio from YODAS2 dataset and ask Gemini-2.0-Flash to summarize it in a specified language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to create natural questions and audio about a piece of audio, we first sample a piece of audio from the YODAS2 dataset, which contains audio for YouTube videos. After that, we prompt Gemini-2.0-Flash to generate a question about the audio and provide the corresponding answer.</p>\n\n",
                "matched_terms": [
                    "question",
                    "from",
                    "contains",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to create voice chat data, we make use of existing text conversation data and convert the user input into audio format with Google TTS. As Google TTS has only a few voice types for each language, we also transcribe part of the data with <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-tts</span> to improve the diversity.</p>\n\n",
                "matched_terms": [
                    "language",
                    "few",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeaLLMs-Audio builts upon Qwen2-Audio-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib4\" title=\"\">2023</a>)</cite> and Qwen2.5-7B-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib16\" title=\"\">2025</a>)</cite>. The architecture is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S2.F3\" title=\"Figure 3 &#8227; 2.2 Model Architecture &#8227; 2 SeaLLMs-Audio &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. We replace the LLM module in Qwen2-Audio-7B <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib3\" title=\"\">2024</a>)</cite> by Qwen2.5-7B-Instruct <cite class=\"ltx_cite ltx_citemacro_citep\">(Qwen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib16\" title=\"\">2025</a>)</cite>. In this way, we harness the advantages of both models: Qwen2-Audio-7B audio encoder can encode the audio features for speech and non-speech audios effectively, and Qwen2.5-7B-Instruct has strong multilingual capabilities. Due to the hidden embedding mismatch, the audio adapter is newly initialized. After that, we do full-parameter fine-tuning on our newly curated large-scale audio dataset, which contains multiple tasks. Given paired data <math alttext=\"(a,x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a,x)</annotation></semantics></math>, with <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> denoting the audio sequences and <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> denoting the optional corresponding text sequences, the training objective is to maximize the likelihood of the subsequent text token, formulated as</p>\n\n",
                "matched_terms": [
                    "contains",
                    "effectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the absence of standard audio benchmarks for evaluating audio LLMs in SEA languages, we manually create a benchmark called SeaBench-Audio. It comprises 14 tasks: 1) Tasks with both audio and text inputs:\nAutomatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Speech Summarization (SS), Speech Question Answering (SQA), Customer Service (CS), Safety, Audio Cationing (AC), Audio Question Answering (AQA), Speaker Identifiers (SKI), and Speech Emotion Recognition (SER); 2) Tasks with only audio inputs: Life, Medical (MED), Math, and Fact.\nThe task descriptions and annotation criteria are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1.T3\" title=\"Table 3 &#8227; Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1\" title=\"Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "criteria",
                    "question"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An overview of the datasets is provided in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S3.F4\" title=\"Figure 4 &#8227; 3 SeaBench-Audio &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a). For each language, we engage a professional native linguist to annotate 10 questions per task.\nOne exception is S2TT task, which requires the translation between two languages.\nFor id/th/vi, we construct two versions&#8212;native audio to English text and English audio to native text&#8212;each with 10 questions. For English, we omit this task to prevent redundancy. Consequently, there are 150 questions for each SEA language and 130 for English, yielding a total of 580 questions. For every question, a linguist supplies a reference to facilitate scoring. The benchmark underwent multiple rounds of careful review to ensure quality.</p>\n\n",
                "matched_terms": [
                    "language",
                    "scoring",
                    "question",
                    "each",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, qualified native speakers rated each response on a scale of 1 to 5, with 5 representing the highest quality. However, human evaluations are expensive and time-consuming. In order to facilitate automatic evaluation, we employ an LLM-as-a-judge framework. We choose Gemini-2.5-flash (Gemini) <cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib5\" title=\"\">2025</a>)</cite>, due to its capabilities of audio understanding and good balance of cost and performance. As shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S3.F4\" title=\"Figure 4 &#8227; 3 SeaBench-Audio &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(b), the procedures to evaluate an LALM are: 1) Generate responses for each instance with the LALM; 2) Construct an evaluation prompt with the text instruction (optional), reference answer, response, rubrics, and the template; 3) Prompt Gemini with the audio and evaluation prompt; 4) Extract the score from the final response. The prompt template for LLM-as-a-judge is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1.F8\" title=\"Figure 8 &#8227; Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#A1\" title=\"Appendix A Appendix &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>. For each task, we additionally engage linguists to develop a task-specific evaluation rubric for the responses on a scale of 1 to 5.\nWe hypothesize that tasks exhibit distinct characteristics, and a dedicated rubric more accurately captures the nuances of each task.</p>\n\n",
                "matched_terms": [
                    "score",
                    "responses",
                    "evaluations",
                    "response",
                    "more",
                    "each",
                    "from",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F5\" title=\"Figure 5 &#8227; 4.1 Main Results &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows the human evaluations. Evaluators assessed both overall performance and language quality, the latter referring to the correctness of language usage in responses. Language quality was rated on a 1-5 scale, where 5 indicated entirely correct language devoid of code-switching. We can see that SeaLLMs-Audio achieves the best language quality for the three SEA languages.\nThe LLM-as-a-judge evaluation result is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F6\" title=\"Figure 6 &#8227; 4.1 Main Results &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nFrom these results, SeaLLMs-Audio attains the strongest performance on id/th/vi, irrespective of whether evaluation is conducted by human annotators or Gemini. Additional observations include: (1) MERaLiON-2 surpasses MERaLiON, which is expected given that MERaLiON-2 is the newer iteration; and (2) Qwen-Omni outperforms Qwen2-Audio across the three languages, consistent with its more recent release. This alignment with prior expectations supports the validity of the LLM-as-a-judge framework.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "evaluations",
                    "language",
                    "overall",
                    "more",
                    "from",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to assessing average performance across languages, we further examine model outcomes by task. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.T1\" title=\"Table 1 &#8227; How does SeaLLMs-Audio perform on each task? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents average scores for each model across all evaluated tasks. Since the English subset excludes the S2TT task, averages for that setting are computed only over id, th, and vi. MERaLiON-2 consistently achieves the strongest results in audio comprehension tasks&#8212;specifically ASR, S2TT, and SER&#8212;which we attribute to its substantially larger and more diverse training corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib8\" title=\"\">2025</a>)</cite>. Conversely, SeaLLMs-Audio attains state-of-the-art performance in selected categories, including <span class=\"ltx_text ltx_font_italic\">fact, life, MED</span>, and <span class=\"ltx_text ltx_font_italic\">math</span>. We ascribe SeaLLMs-Audio&#8217;s advantages to the extensive scope and heterogeneity of its training data, encompassing both varied task types and multimodal input formats.</p>\n\n",
                "matched_terms": [
                    "more",
                    "assessing",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observed that the scores by human judgments and LLM-as-a-judge evaluations are not perfectly aligned. To evaluate their correlation, we calculate their Pearson correlation coefficient. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.F7\" title=\"Figure 7 &#8227; How is LLM-as-a-judge consistent with human judges? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, LLM-as-a-judge and human judges have an average correlation coefficient of 0.8, which shows high correlation between the scores by humans and by the LLM judge.\nWe also calculate the agreement between human judges and LLM judges when comparing the responses from two models. As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#S4.T2\" title=\"Table 2 &#8227; How is LLM-as-a-judge consistent with human judges? &#8227; 4.2 Analysis &#8227; 4 Experiments &#8227; SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, they have an average agreement of 69% with tie and 93% without tie, which is even higher than the result in MT-bench <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01670v1#bib.bib24\" title=\"\">2023</a>)</cite>. Such high agreement between humans and the LLM judge shows the reliability of SeaBench-Audio.</p>\n\n",
                "matched_terms": [
                    "reliability",
                    "responses",
                    "evaluations",
                    "from",
                    "human",
                    "judges",
                    "not"
                ]
            }
        ]
    }
}