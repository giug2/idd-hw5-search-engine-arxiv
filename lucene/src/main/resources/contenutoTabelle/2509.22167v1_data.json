{
    "S3.T1": {
        "source_file": "Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis",
        "caption": "Table 1: Zero-shot TTS results on the LibriSpeech-PC test-clean. The best results in the low-resource setting are marked in bold. High-resource results are quoted from the F5-TTS for comparison.",
        "body": "Method\n\n# Param.\nData\nWER(%)(\\%)↓\\downarrow\nSIM↑\\uparrow\n\n\nGround Truth\n\n\n2.23\n0.69\n\n\nHigh-resource\n\n\n\nCosyVoice [37]\n\n300M\n170kh\n3.59\n0.66\n\n\n\nFireRedTTS [38]\n\n580M\n248kh\n2.69\n0.47\n\n\n\nE2 TTS [15]\n\n333M\n100kh\n2.95\n0.69\n\n\n\nF5-TTS [9]\n\n336M\n100kh\n2.42\n0.66\n\n\nLow-resource\n\n\n\nUSLM [25]\n\n361M\n0.6kh\n6.11\n0.43\n\n\n\nE2 TTS [15]\n\n157M\n0.6kh\n3.51\n0.61\n\n\n\n+ Semantic-VAE\n\n157M\n0.6kh\n2.41\n0.62\n\n\n\nF5-TTS [9]\n\n159M\n0.6kh\n2.23\n0.60\n\n\n\n+ Vanilla VAE\n\n159M\n0.6kh\n2.65\n0.60\n\n\n\n+ Semantic-VAE\n\n159M\n0.6kh\n2.10\n0.64",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\"># Param.</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Data</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER<math alttext=\"(\\%)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>%</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\%)</annotation></semantics></math><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ground Truth</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t\" style=\"padding:1.4pt 0.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t\" style=\"padding:1.4pt 0.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.23</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.69</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" colspan=\"5\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">High-resource</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib37\" title=\"\">37</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">300M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">170kh</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.59</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.66</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FireRedTTS&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib38\" title=\"\">38</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">580M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">248kh</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.69</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">E2 TTS&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">333M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">100kh</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.95</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.69</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">336M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">100kh</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.42</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.66</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" colspan=\"5\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Low-resource</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">USLM&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib25\" title=\"\">25</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">361M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.6kh</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.11</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.43</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">E2 TTS&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">157M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.6kh</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.51</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">+ </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Semantic-VAE</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">157M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.6kh</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.41</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">159M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.6kh</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.23</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">+ </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Vanilla VAE</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">159M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.6kh</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.65</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">+ </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Semantic-VAE</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">159M</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.6kh</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.10</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.64</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" style=\"padding:1.4pt 0.0pt;\"><span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span></td>\n<td class=\"ltx_td\" style=\"padding:1.4pt 0.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1.4pt 0.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1.4pt 0.0pt;\"/>\n<td class=\"ltx_td\" style=\"padding:1.4pt 0.0pt;\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "param",
            "semanticvae",
            "data",
            "580m",
            "cosyvoice",
            "best",
            "100kh",
            "f5tts",
            "results",
            "lowresource",
            "336m",
            "truth",
            "06kh",
            "marked",
            "bold",
            "wer↓downarrow",
            "248kh",
            "157m",
            "method",
            "fireredtts",
            "361m",
            "comparison",
            "vanilla",
            "librispeechpc",
            "vae",
            "170kh",
            "testclean",
            "from",
            "zeroshot",
            "sim↑uparrow",
            "159m",
            "quoted",
            "ground",
            "300m",
            "333m",
            "tts",
            "uslm",
            "highresource",
            "setting"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.4 Evaluation &#8227; 3 Experimental Setup &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports zero-shot TTS results on the LibriSpeech-PC test-clean dataset, with results presented separately for high-resource and low-resource settings.\nIn the high-resource scenario, NAR models (E2 TTS and F5-TTS) achieve performance comparable to AR models such as CosyVoice&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib37\" title=\"\">37</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and FireRedTTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib38\" title=\"\">38</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While mel-spectrograms have been widely utilized as intermediate representations in zero-shot text-to-speech (TTS), their inherent redundancy leads to inefficiency in learning text-speech alignment. Compact VAE-based latent representations have recently emerged as a stronger alternative, but they also face a fundamental optimization dilemma: higher-dimensional latent spaces improve reconstruction quality and speaker similarity, but degrade intelligibility, while lower-dimensional spaces improve intelligibility at the expense of reconstruction fidelity. To overcome this dilemma, we propose Semantic-VAE, a novel VAE framework that utilizes semantic alignment regularization in the latent space. This design alleviates the reconstruction-generation trade-off by capturing semantic structure in high-dimensional latent representations. Extensive experiments demonstrate that Semantic-VAE significantly improves synthesis quality and training efficiency.\nWhen integrated into F5-TTS, our method achieves 2.10% WER and 0.64 speaker similarity on LibriSpeech-PC, outperforming mel-based systems (2.23%, 0.60) and vanilla acoustic VAE baselines (2.65%, 0.59).\nWe also release the <a class=\"ltx_ref ltx_href\" href=\"https://github.com/ZhikangNiu/Semantic-VAE\" title=\"\">code</a>\nand <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/zkniu/Semantic-VAE\" title=\"\">models</a>\nto facilitate further research.</span>\n</p>\n\n",
                "matched_terms": [
                    "method",
                    "semanticvae",
                    "tts",
                    "f5tts",
                    "librispeechpc",
                    "vae",
                    "zeroshot",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Zero-shot Text-to-Speech (TTS) aims to synthesize natural human speech from text inputs, conditioned on a reference speech prompt to closely mimic the speaker&#8217;s timbre. In recent years, zero-shot TTS models have achieved remarkable progress by scaling up both data and model size. Existing methods for zero-shot TTS can be broadly categorized into two approaches: Autoregressive (AR) &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Non-Autoregressive (NAR)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib11\" title=\"\">11</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> models.</span>\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "zeroshot",
                    "tts",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1) Models conditioned on the mel-spectrogram</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which constitute the predominant paradigm in NAR-based TTS, utilize a pre-trained vocoder to reconstruct the waveform from the generated mel-spectrogram. Representative examples include VoiceBox </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, E2 TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and F5-TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Despite the strong performance of mel-based systems, they still suffer from several inherent limitations </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, including loss of phase information, high dimensionality with substantial redundancy, and the absence of fine-grained high-frequency details, which limit high-fidelity speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "tts",
                    "f5tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2) Models conditioned on latent representations</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> produced by a Variational Autoencoder (VAE) encoder and synthesize waveforms directly using the corresponding decoder. VAE-based systems mitigate the drawbacks mentioned above by learning compact latent representations that preserve essential acoustic information while eliminating redundancy. These compact representations not only accelerate training convergence but also yield improved synthesis quality&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For instance, </span>\n  <math alttext=\"\\text{SeedTTS}_{DiT}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SeedTTS</mtext>\n        <mrow>\n          <mi mathsize=\"0.900em\">D</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">T</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\text{SeedTTS}_{DiT}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a large-scale state-of-the-art industrial TTS model that leverages VAE latent representations, highlighting the effectiveness of compact representation for high-fidelity speech synthesis. Building on this idea, MegaTTS-3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further introduces a novel sparse alignment strategy to guide a latent diffusion transformer over the VAE latent space, achieving performance gains.</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Consistent with findings in the vision domain </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, latent diffusion models with vanilla acoustic VAE systems face an </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">optimization dilemma</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between reconstruction and generation performance. The results of preliminary experiments are shown in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Specifically, increasing the latent dimension enhances reconstruction quality and speaker similarity but impairs intelligibility in zero-shot TTS, whereas reducing the latent dimension improves intelligibility at the expense of reconstruction quality and speaker similarity. This trade-off exemplifies the information bottleneck: lower-dimensional representations capture core semantic content, whereas higher-dimensional representations preserve richer acoustic details but introduce redundancy and complicate semantic modeling.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "results",
                    "vae",
                    "zeroshot",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this challenge, we introduce </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Semantic-VAE</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a novel variational autoencoder with semantic alignment regularization in the latent space to mitigate the difficulty of semantic modeling in zero-shot TTS tasks with unconstrained high-dimensional latent representations. Extensive experiments demonstrate that Semantic-VAE mitigates the generation&#8211;reconstruction dilemma in high-dimensional latent spaces, accelerates training convergence, and achieves state-of-the-art performance in zero-shot TTS tasks. Specifically, the F5-TTS model with the proposed Semantic-VAE features achieves </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.10</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">% WER and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.64</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> speaker similarity on the LibriSpeech-PC test-clean dataset, outperforming the mel-based F5-TTS baseline (2.23%, 0.60) and the vanilla acoustic VAE baseline (2.65%, 0.59). Additional reconstruction experiments show that Semantic-VAE achieves reconstruction quality comparable to vanilla VAE and substantially better than the vocoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with mel-spectrograms. This demonstrates that our proposed semantic alignment regularization enables more effective generative modeling without compromising the representational capacity of the latent features. We release the full Semantic-VAE framework with code and checkpoints at </span>\n  <a class=\"ltx_ref ltx_href\" href=\"https://github.com/ZhikangNiu/Semantic-VAE\" style=\"font-size:90%;\" title=\"\">https://github.com/ZhikangNiu/Semantic-VAE</a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "semanticvae",
                    "tts",
                    "f5tts",
                    "librispeechpc",
                    "vae",
                    "testclean",
                    "zeroshot",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2 Method &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we propose an improved training paradigm for acoustic VAE systems that incorporates semantic regularization to mitigate the reconstruction and generation optimization dilemma.\nIn this section, we provide a detailed introduction of our proposed method, highlighting its key components and underlying motivations. Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S2.SS1\" style=\"font-size:90%;\" title=\"2.1 Variational Autoencoder &#8227; 2 Method &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduces our baseline VAE architecture and objective. Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 Semantic Representation Alignment &#8227; 2 Method &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents our strategy for semantic regularizing latent representations using pre-trained self-supervised learning (SSL) speech models.</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To obtain latent representations for speech generation, we employ a Variational Autoencoder (VAE) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that models speech waveforms.\nThe VAE consists of an encoder that maps the input speech </span>\n  <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119961;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{x}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into a latent representation </span>\n  <math alttext=\"\\bm{z}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119963;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{z}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and a decoder that reconstructs the signal </span>\n  <math alttext=\"\\hat{\\bm{x}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mo mathsize=\"0.900em\">^</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\hat{\\bm{x}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from </span>\n  <math alttext=\"\\bm{z}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119963;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{z}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The standard training objective of a VAE maximizes the Evidence Lower Bound (ELBO):</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, to the best of our knowledge, no prior work has explored applying semantic regularization to continuous latent spaces, nor examined its potential to improve downstream task performance. Motivated by these insights, we introduce a semantic regularization loss into the VAE training framework. This addition aims to investigate whether semantic-aligned VAEs can improve both the convergence speed and performance of NAR-based TTS models compared to vanilla acoustic VAEs. Specifically, we employ a pre-trained SSL speech model </span>\n  <math alttext=\"f(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to extract structured semantic representations from the same speech </span>\n  <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119961;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{x}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The hidden states </span>\n  <math alttext=\"\\bm{h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119945;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{h}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of the SSL model, aligned in both temporal and feature dimensions with the VAE latent representation, are obtained through an interpolation layer followed by a 1D convolution layer, formulated as</span>\n</p>\n\n",
                "matched_terms": [
                    "best",
                    "tts",
                    "vae",
                    "from",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">T</mi>\n      <annotation encoding=\"application/x-tex\">T</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the sequence length and </span>\n  <math alttext=\"\\text{cos}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">cos</mtext>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo mathsize=\"0.900em\" rspace=\"0em\">,</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{cos}(\\cdot,\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> computes the cosine similarity between the aligned VAE latent and the SSL feature at each time step </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This loss regularizes the VAE latent space, encouraging the VAE to learn a structured semantic representation from SSL models. To jointly optimize for high-fidelity speech reconstruction and semantically meaningful latent representations, we integrate the VAE training objective with the proposed semantic regularization loss. The overall training objective is formulated as</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use the F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Small model as the baseline and adopt its training setup. In our approach, the mel-spectrograms are replaced with VAE latent representations. The model is optimized using the AdamW optimizer with </span>\n  <math alttext=\"7.5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">7.5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">5</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">7.5\\times 10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, following a warm-up schedule for the first 20,000 updates and linear decay thereafter. For inference, we follow the same setup as F5-TTS, applying the sway sampling strategy and using the Euler ODE solver.</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "f5tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use two large-scale English corpora: Libriheavy&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and LibriTTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nLibriheavy contains around 50k hours of re-segmented audiobook speech at 16 kHz, and LibriTTS contains approximately 585 hours of high-quality multi-speaker speech segments at 24 kHz.\nIn our experiments, Semantic-VAE is trained on LibriTTS as well as the small and medium subsets of Libriheavy, over 6k hours in total, while the F5-TTS model is trained on LibriTTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "semanticvae",
                    "f5tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To comprehensively evaluate Semantic-VAE regarding the optimization dilemma between reconstruction and generation, we consider two tasks: 1) reconstruction and 2) downstream zero-shot TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "semanticvae",
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the reconstruction evaluation of Semantic-VAE, we utilize the LibriTTS test-other as our test set, and employ UTMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PESQ (Perceptual Evaluation of Speech Quality)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib35\" title=\"\">35</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and STOI (Short-time Objective Intelligibility) as the evaluation metrics to quantify perceptual quality and intelligibility. For the zero-shot TTS evaluation, we follow the setup in F5-TTS</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://github.com/SWivid/F5-TTS/tree/main/src/f5_tts/eval\" title=\"\">https://github.com/SWivid/F5-TTS/tree/main/src/f5_tts/eval</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and evaluate on the LibriSpeech-PC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib36\" title=\"\">36</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test-clean. We perform the cross-sentence generation and evaluate the performance using the Word Error Rate (WER), Speaker Similarity (SIM-o), and UTMOS, which respectively measure intelligibility, speaker consistency, and naturalness.</span>\n</p>\n\n",
                "matched_terms": [
                    "semanticvae",
                    "tts",
                    "librispeechpc",
                    "testclean",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For a fair comparison, we evaluated the models in the low-resource setting, which highlights the advantages of Semantic-VAE. When integrated into F5-TTS, Semantic-VAE achieves state-of-the-art results, reducing WER to 2.10% and increasing speaker similarity (SIM) to 0.64 under the same inference setting, outperforming both F5-TTS Small baseline and its vanilla VAE variant. A similar trend is observed when applying Semantic-VAE to E2 TTS, yielding consistent gains over the baseline. To further assess convergence, we compare the proposed Semantic-VAE against the vanilla VAE and the mel-based F5-TTS baseline across different training steps. As shown in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.1 Downstream Zero-shot TTS Results &#8227; 4 Results &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Semantic-VAE converges faster and achieves better performance than both the vanilla VAE and the mel-based baseline at the same step. These results demonstrate that Semantic-VAE not only improves the final performance, especially speaker similarity, but also accelerates convergence and reduces training costs compared with the baseline F5-TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "semanticvae",
                    "comparison",
                    "f5tts",
                    "tts",
                    "results",
                    "lowresource",
                    "vae",
                    "vanilla",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Reconstruction Results &#8227; 4 Results &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports reconstruction results on LibriTTS test-other. Despite its aggressively compact representation in both temporal and latent dimensions, vanilla VAE outperforms Vocos in reconstruction. It indicates that end-to-end VAE training encourages the latent to retain essential information while discarding redundancy. Moreover, Semantic-VAE maintains reconstruction performance comparable to vanilla VAE, demonstrating that semantic alignment simplified generative modeling without sacrificing reconstruction performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "vae",
                    "semanticvae",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we investigate the optimization dilemma in high-dimensional latent spaces and propose Semantic-VAE, a speech variational autoencoder with semantic alignment regularization. By learning structured and semantically aligned latent representations, Semantic-VAE alleviates the reconstruction&#8211;generation dilemma and improves downstream zero-shot TTS performance. Experiments demonstrate that our approach accelerates convergence and achieves state-of-the-art results on the zero-shot TTS task. In addition, comprehensive analyses and ablation studies confirm the effectiveness of each proposed component. We believe that these promising findings pave the way for future research on advanced modality alignment techniques and further optimization of latent diffusion-based text-to-speech systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "semanticvae",
                    "tts",
                    "zeroshot"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis",
        "caption": "Table 2: Reconstruction performance on LibriTTS test-other",
        "body": "Model\n\nframe/s\ndim\nPESQ↑\\uparrow\nSTOI↑\\uparrow\nUTMOS↑\\uparrow\n\n\nground truth\n-\n-\n-\n-\n3.48\n\n\nVocos\n93.75\n100\n3.57\n0.96\n3.24\n\n\nVanilla VAE\n40\n64\n3.75\n0.97\n3.57\n\n\nSemantic-VAE\n40\n64\n3.74\n0.96\n3.56",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">frame/s</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">dim</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PESQ<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">STOI<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UTMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ground truth</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.48</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Vocos</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">93.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Vanilla VAE</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.75</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.97</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.57</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Semantic-VAE</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.74</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.56</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span></td>\n<td class=\"ltx_td\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"/>\n<td class=\"ltx_td\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"/>\n<td class=\"ltx_td\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"/>\n<td class=\"ltx_td\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"/>\n<td class=\"ltx_td\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "libritts",
            "vocos",
            "pesq↑uparrow",
            "performance",
            "testother",
            "frames",
            "ground",
            "semanticvae",
            "utmos↑uparrow",
            "vae",
            "dim",
            "reconstruction",
            "truth",
            "model",
            "stoi↑uparrow",
            "vanilla"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Reconstruction Results &#8227; 4 Results &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports reconstruction results on LibriTTS test-other. Despite its aggressively compact representation in both temporal and latent dimensions, vanilla VAE outperforms Vocos in reconstruction. It indicates that end-to-end VAE training encourages the latent to retain essential information while discarding redundancy. Moreover, Semantic-VAE maintains reconstruction performance comparable to vanilla VAE, demonstrating that semantic alignment simplified generative modeling without sacrificing reconstruction performance.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While mel-spectrograms have been widely utilized as intermediate representations in zero-shot text-to-speech (TTS), their inherent redundancy leads to inefficiency in learning text-speech alignment. Compact VAE-based latent representations have recently emerged as a stronger alternative, but they also face a fundamental optimization dilemma: higher-dimensional latent spaces improve reconstruction quality and speaker similarity, but degrade intelligibility, while lower-dimensional spaces improve intelligibility at the expense of reconstruction fidelity. To overcome this dilemma, we propose Semantic-VAE, a novel VAE framework that utilizes semantic alignment regularization in the latent space. This design alleviates the reconstruction-generation trade-off by capturing semantic structure in high-dimensional latent representations. Extensive experiments demonstrate that Semantic-VAE significantly improves synthesis quality and training efficiency.\nWhen integrated into F5-TTS, our method achieves 2.10% WER and 0.64 speaker similarity on LibriSpeech-PC, outperforming mel-based systems (2.23%, 0.60) and vanilla acoustic VAE baselines (2.65%, 0.59).\nWe also release the <a class=\"ltx_ref ltx_href\" href=\"https://github.com/ZhikangNiu/Semantic-VAE\" title=\"\">code</a>\nand <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/zkniu/Semantic-VAE\" title=\"\">models</a>\nto facilitate further research.</span>\n</p>\n\n",
                "matched_terms": [
                    "vanilla",
                    "vae",
                    "semanticvae",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2) Models conditioned on latent representations</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> produced by a Variational Autoencoder (VAE) encoder and synthesize waveforms directly using the corresponding decoder. VAE-based systems mitigate the drawbacks mentioned above by learning compact latent representations that preserve essential acoustic information while eliminating redundancy. These compact representations not only accelerate training convergence but also yield improved synthesis quality&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For instance, </span>\n  <math alttext=\"\\text{SeedTTS}_{DiT}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SeedTTS</mtext>\n        <mrow>\n          <mi mathsize=\"0.900em\">D</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">T</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\text{SeedTTS}_{DiT}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a large-scale state-of-the-art industrial TTS model that leverages VAE latent representations, highlighting the effectiveness of compact representation for high-fidelity speech synthesis. Building on this idea, MegaTTS-3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further introduces a novel sparse alignment strategy to guide a latent diffusion transformer over the VAE latent space, achieving performance gains.</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Consistent with findings in the vision domain </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, latent diffusion models with vanilla acoustic VAE systems face an </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">optimization dilemma</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between reconstruction and generation performance. The results of preliminary experiments are shown in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Specifically, increasing the latent dimension enhances reconstruction quality and speaker similarity but impairs intelligibility in zero-shot TTS, whereas reducing the latent dimension improves intelligibility at the expense of reconstruction quality and speaker similarity. This trade-off exemplifies the information bottleneck: lower-dimensional representations capture core semantic content, whereas higher-dimensional representations preserve richer acoustic details but introduce redundancy and complicate semantic modeling.</span>\n</p>\n\n",
                "matched_terms": [
                    "vanilla",
                    "vae",
                    "reconstruction",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this challenge, we introduce </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Semantic-VAE</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a novel variational autoencoder with semantic alignment regularization in the latent space to mitigate the difficulty of semantic modeling in zero-shot TTS tasks with unconstrained high-dimensional latent representations. Extensive experiments demonstrate that Semantic-VAE mitigates the generation&#8211;reconstruction dilemma in high-dimensional latent spaces, accelerates training convergence, and achieves state-of-the-art performance in zero-shot TTS tasks. Specifically, the F5-TTS model with the proposed Semantic-VAE features achieves </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.10</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">% WER and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.64</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> speaker similarity on the LibriSpeech-PC test-clean dataset, outperforming the mel-based F5-TTS baseline (2.23%, 0.60) and the vanilla acoustic VAE baseline (2.65%, 0.59). Additional reconstruction experiments show that Semantic-VAE achieves reconstruction quality comparable to vanilla VAE and substantially better than the vocoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with mel-spectrograms. This demonstrates that our proposed semantic alignment regularization enables more effective generative modeling without compromising the representational capacity of the latent features. We release the full Semantic-VAE framework with code and checkpoints at </span>\n  <a class=\"ltx_ref ltx_href\" href=\"https://github.com/ZhikangNiu/Semantic-VAE\" style=\"font-size:90%;\" title=\"\">https://github.com/ZhikangNiu/Semantic-VAE</a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "semanticvae",
                    "vae",
                    "reconstruction",
                    "model",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2 Method &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we propose an improved training paradigm for acoustic VAE systems that incorporates semantic regularization to mitigate the reconstruction and generation optimization dilemma.\nIn this section, we provide a detailed introduction of our proposed method, highlighting its key components and underlying motivations. Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S2.SS1\" style=\"font-size:90%;\" title=\"2.1 Variational Autoencoder &#8227; 2 Method &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduces our baseline VAE architecture and objective. Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 Semantic Representation Alignment &#8227; 2 Method &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents our strategy for semantic regularizing latent representations using pre-trained self-supervised learning (SSL) speech models.</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Following the standard VAE objective, we express the generative loss as a weighted sum of the reconstruction loss </span>\n  <math alttext=\"\\mathcal{L}_{\\text{recon}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mtext mathsize=\"0.900em\">recon</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{recon}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the KL divergence loss </span>\n  <math alttext=\"\\mathcal{L}_{\\text{KL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mtext mathsize=\"0.900em\">KL</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{KL}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The reconstruction and generation dilemma is a widely discussed challenge in discrete audio codecs. Previous studies&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have demonstrated that aligning with pre-trained SSL speech models (e.g., HuBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, WavLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, w2v-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, etc.), which capture rich phonetic and semantic representations, can provide more informative supervision for audio codec training and improve downstream AR model performance and convergence. Similarly, incorporating SSL alignment into Diffusion Transformers (DiT) training accelerates convergence and yields better performance in speech synthesis&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and image generation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "reconstruction",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, to the best of our knowledge, no prior work has explored applying semantic regularization to continuous latent spaces, nor examined its potential to improve downstream task performance. Motivated by these insights, we introduce a semantic regularization loss into the VAE training framework. This addition aims to investigate whether semantic-aligned VAEs can improve both the convergence speed and performance of NAR-based TTS models compared to vanilla acoustic VAEs. Specifically, we employ a pre-trained SSL speech model </span>\n  <math alttext=\"f(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to extract structured semantic representations from the same speech </span>\n  <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119961;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{x}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The hidden states </span>\n  <math alttext=\"\\bm{h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119945;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{h}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of the SSL model, aligned in both temporal and feature dimensions with the VAE latent representation, are obtained through an interpolation layer followed by a 1D convolution layer, formulated as</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "model",
                    "vanilla",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">T</mi>\n      <annotation encoding=\"application/x-tex\">T</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the sequence length and </span>\n  <math alttext=\"\\text{cos}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">cos</mtext>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo mathsize=\"0.900em\" rspace=\"0em\">,</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{cos}(\\cdot,\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> computes the cosine similarity between the aligned VAE latent and the SSL feature at each time step </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This loss regularizes the VAE latent space, encouraging the VAE to learn a structured semantic representation from SSL models. To jointly optimize for high-fidelity speech reconstruction and semantically meaningful latent representations, we integrate the VAE training objective with the proposed semantic regularization loss. The overall training objective is formulated as</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Following DAC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_markedasmath\" style=\"font-size:90%;\">Semantic-VAE</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> adopts the same convolutional encoder, discriminator, and loss weights.\nTo improve reconstruction performance, we replace the original convolutional decoder with an AMP Block-based decoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The encoder downsamples the 16 kHz input </span>\n  <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119961;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{x}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with factors of [4, 4, 5, 5], producing a 64-dimensional latent representation </span>\n  <math alttext=\"\\bm{z}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119963;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{z}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> at a frame rate of 40 Hz, while the decoder upsamples </span>\n  <math alttext=\"\\bm{z}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119963;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{z}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with factors of [5, 5, 4, 4] to reconstruct the waveform.</span>\n</p>\n\n",
                "matched_terms": [
                    "semanticvae",
                    "reconstruction",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use the F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Small model as the baseline and adopt its training setup. In our approach, the mel-spectrograms are replaced with VAE latent representations. The model is optimized using the AdamW optimizer with </span>\n  <math alttext=\"7.5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">7.5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">5</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">7.5\\times 10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, following a warm-up schedule for the first 20,000 updates and linear decay thereafter. For inference, we follow the same setup as F5-TTS, applying the sway sampling strategy and using the Euler ODE solver.</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use two large-scale English corpora: Libriheavy&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and LibriTTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nLibriheavy contains around 50k hours of re-segmented audiobook speech at 16 kHz, and LibriTTS contains approximately 585 hours of high-quality multi-speaker speech segments at 24 kHz.\nIn our experiments, Semantic-VAE is trained on LibriTTS as well as the small and medium subsets of Libriheavy, over 6k hours in total, while the F5-TTS model is trained on LibriTTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "libritts",
                    "semanticvae",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To comprehensively evaluate Semantic-VAE regarding the optimization dilemma between reconstruction and generation, we consider two tasks: 1) reconstruction and 2) downstream zero-shot TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "semanticvae",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the reconstruction evaluation of Semantic-VAE, we utilize the LibriTTS test-other as our test set, and employ UTMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PESQ (Perceptual Evaluation of Speech Quality)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib35\" title=\"\">35</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and STOI (Short-time Objective Intelligibility) as the evaluation metrics to quantify perceptual quality and intelligibility. For the zero-shot TTS evaluation, we follow the setup in F5-TTS</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://github.com/SWivid/F5-TTS/tree/main/src/f5_tts/eval\" title=\"\">https://github.com/SWivid/F5-TTS/tree/main/src/f5_tts/eval</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and evaluate on the LibriSpeech-PC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib36\" title=\"\">36</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test-clean. We perform the cross-sentence generation and evaluate the performance using the Word Error Rate (WER), Speaker Similarity (SIM-o), and UTMOS, which respectively measure intelligibility, speaker consistency, and naturalness.</span>\n</p>\n\n",
                "matched_terms": [
                    "libritts",
                    "performance",
                    "testother",
                    "semanticvae",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For a fair comparison, we evaluated the models in the low-resource setting, which highlights the advantages of Semantic-VAE. When integrated into F5-TTS, Semantic-VAE achieves state-of-the-art results, reducing WER to 2.10% and increasing speaker similarity (SIM) to 0.64 under the same inference setting, outperforming both F5-TTS Small baseline and its vanilla VAE variant. A similar trend is observed when applying Semantic-VAE to E2 TTS, yielding consistent gains over the baseline. To further assess convergence, we compare the proposed Semantic-VAE against the vanilla VAE and the mel-based F5-TTS baseline across different training steps. As shown in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.1 Downstream Zero-shot TTS Results &#8227; 4 Results &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Semantic-VAE converges faster and achieves better performance than both the vanilla VAE and the mel-based baseline at the same step. These results demonstrate that Semantic-VAE not only improves the final performance, especially speaker similarity, but also accelerates convergence and reduces training costs compared with the baseline F5-TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "vae",
                    "semanticvae",
                    "vanilla",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we investigate the optimization dilemma in high-dimensional latent spaces and propose Semantic-VAE, a speech variational autoencoder with semantic alignment regularization. By learning structured and semantically aligned latent representations, Semantic-VAE alleviates the reconstruction&#8211;generation dilemma and improves downstream zero-shot TTS performance. Experiments demonstrate that our approach accelerates convergence and achieves state-of-the-art results on the zero-shot TTS task. In addition, comprehensive analyses and ablation studies confirm the effectiveness of each proposed component. We believe that these promising findings pave the way for future research on advanced modality alignment techniques and further optimization of latent diffusion-based text-to-speech systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "semanticvae",
                    "performance"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis",
        "caption": "Table 3: Effect of different SSL models, layer, and alignment methods on zero-shot TTS performance on LibriSpeech-PC test-clean. “Avg.” = average of all layers; “Last” = final layer output.",
        "body": "Align method\n\nLayer\nWER (%)↓\\downarrow\nSIM↑\\uparrow\nUTMOS↑\\uparrow\n\n\nBaseline\n-\n2.65\n0.59\n4.42\n\n\n|Φn−f​(x)||\\Phi_{n}-f(x)|\n\n\n\nWavLM\n\n23rd\n \n3.12\n0.47\n3.29\n\n\n‖Φn−f​(x)‖2\\|\\Phi_{n}-f(x)\\|_{2}\n4.37\n0.48\n4.16\n\n\n−cos⁡(Φn,f​(x))-\\cos(\\Phi_{n},f(x))\n2.10\n0.64\n4.39\n\n\nSSL Models\nLayer\nWER (%)↓\\downarrow\nSIM↑\\uparrow\nUTMOS↑\\uparrow\n\n\nHuBERT\n23\n2.28\n0.63\n4.38\n\n\nLast\n2.33\n0.50\n4.41\n\n\nAvg.\n2.29\n0.61\n4.39\n\n\nWavLM\n23\n2.10\n0.64\n4.39\n\n\nLast\n2.62\n0.58\n4.34\n\n\nAvg.\n2.31\n0.63\n4.42",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\">\n<span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Align method</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Layer</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER (%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UTMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Baseline</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.42</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><math alttext=\"|\\Phi_{n}-f(x)|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo><mrow><msub><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#934;</mi><mi mathsize=\"0.900em\">n</mi></msub><mo mathsize=\"0.900em\">&#8722;</mo><mrow><mi mathsize=\"0.900em\">f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">x</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo></mrow><annotation encoding=\"application/x-tex\">|\\Phi_{n}-f(x)|</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\">WavLM</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\">23rd</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.29</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><math alttext=\"\\|\\Phi_{n}-f(x)\\|_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><msub><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">&#8214;</mo><mrow><msub><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#934;</mi><mi mathsize=\"0.900em\">n</mi></msub><mo mathsize=\"0.900em\">&#8722;</mo><mrow><mi mathsize=\"0.900em\">f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">x</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">&#8214;</mo></mrow><mn mathsize=\"0.900em\">2</mn></msub><annotation encoding=\"application/x-tex\">\\|\\Phi_{n}-f(x)\\|_{2}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.37</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.48</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><math alttext=\"-\\cos(\\Phi_{n},f(x))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.900em\" rspace=\"0.167em\">&#8722;</mo><mrow><mi mathsize=\"0.900em\">cos</mi><mo>&#8289;</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><msub><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#934;</mi><mi mathsize=\"0.900em\">n</mi></msub><mo mathsize=\"0.900em\">,</mo><mrow><mi mathsize=\"0.900em\">f</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mi mathsize=\"0.900em\">x</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">-\\cos(\\Phi_{n},f(x))</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.10</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.64</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SSL Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Layer</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER (%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UTMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">HuBERT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.38</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Last</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.33</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.41</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.29</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WavLM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Last</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.34</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.31</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.42</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"><span class=\"ltx_rule\" style=\"width:100%;height:1.2pt;--ltx-bg-color:black;display:inline-block;\">&#160;</span></td>\n<td class=\"ltx_td\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"/>\n<td class=\"ltx_td\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"/>\n<td class=\"ltx_td\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"/>\n<td class=\"ltx_td\" style=\"padding-top:1.4pt;padding-bottom:1.4pt;\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "“avg”",
            "ssl",
            "“last”",
            "wer",
            "effect",
            "avg",
            "−cos⁡φnf​xcosphinfx",
            "methods",
            "different",
            "‖φn−f​x‖2phinfx2",
            "wavlm",
            "layer",
            "method",
            "performance",
            "↓downarrow",
            "librispeechpc",
            "average",
            "φn−f​xphinfx",
            "alignment",
            "testclean",
            "zeroshot",
            "sim↑uparrow",
            "align",
            "hubert",
            "all",
            "baseline",
            "models",
            "tts",
            "utmos↑uparrow",
            "23rd",
            "last",
            "final",
            "layers",
            "output"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.3 Ablation Study &#8227; 4 Results &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents a comprehensive ablation study on semantic alignment methods and semantic guidance selection. The results indicate that negative cosine similarity loss provides a more effective alignment mechanism than the L1 or MSE losses. This can be intuitively explained that L1 and MSE overly constrain absolute numerical differences between the latent space and the semantic guidance, whereas cosine loss focuses on directional alignment in the latent space, which better captures structured semantic representations.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While mel-spectrograms have been widely utilized as intermediate representations in zero-shot text-to-speech (TTS), their inherent redundancy leads to inefficiency in learning text-speech alignment. Compact VAE-based latent representations have recently emerged as a stronger alternative, but they also face a fundamental optimization dilemma: higher-dimensional latent spaces improve reconstruction quality and speaker similarity, but degrade intelligibility, while lower-dimensional spaces improve intelligibility at the expense of reconstruction fidelity. To overcome this dilemma, we propose Semantic-VAE, a novel VAE framework that utilizes semantic alignment regularization in the latent space. This design alleviates the reconstruction-generation trade-off by capturing semantic structure in high-dimensional latent representations. Extensive experiments demonstrate that Semantic-VAE significantly improves synthesis quality and training efficiency.\nWhen integrated into F5-TTS, our method achieves 2.10% WER and 0.64 speaker similarity on LibriSpeech-PC, outperforming mel-based systems (2.23%, 0.60) and vanilla acoustic VAE baselines (2.65%, 0.59).\nWe also release the <a class=\"ltx_ref ltx_href\" href=\"https://github.com/ZhikangNiu/Semantic-VAE\" title=\"\">code</a>\nand <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/zkniu/Semantic-VAE\" title=\"\">models</a>\nto facilitate further research.</span>\n</p>\n\n",
                "matched_terms": [
                    "method",
                    "wer",
                    "models",
                    "tts",
                    "librispeechpc",
                    "alignment",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Zero-shot Text-to-Speech (TTS) aims to synthesize natural human speech from text inputs, conditioned on a reference speech prompt to closely mimic the speaker&#8217;s timbre. In recent years, zero-shot TTS models have achieved remarkable progress by scaling up both data and model size. Existing methods for zero-shot TTS can be broadly categorized into two approaches: Autoregressive (AR) &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Non-Autoregressive (NAR)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib11\" title=\"\">11</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> models.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "methods",
                    "tts",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Most AR models directly quantize speech into discrete tokens and achieve exceptional performance in capturing and reproducing the acoustic characteristics of a reference speaker through in-context learning. However, they suffer from slow inference speed and error accumulation due to autoregressive generation of speech representation and information loss from quantization&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. NAR models, primarily built on diffusion and flow matching models and using continuous speech representations, address the inherent limitations of AR models and overcome the information loss introduced by quantization, thereby improving both inference speed and output quality. NAR-based models can be grouped into two main categories based on their intermediate representation:</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "output",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1) Models conditioned on the mel-spectrogram</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which constitute the predominant paradigm in NAR-based TTS, utilize a pre-trained vocoder to reconstruct the waveform from the generated mel-spectrogram. Representative examples include VoiceBox </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, E2 TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and F5-TTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Despite the strong performance of mel-based systems, they still suffer from several inherent limitations </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, including loss of phase information, high dimensionality with substantial redundancy, and the absence of fine-grained high-frequency details, which limit high-fidelity speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2) Models conditioned on latent representations</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> produced by a Variational Autoencoder (VAE) encoder and synthesize waveforms directly using the corresponding decoder. VAE-based systems mitigate the drawbacks mentioned above by learning compact latent representations that preserve essential acoustic information while eliminating redundancy. These compact representations not only accelerate training convergence but also yield improved synthesis quality&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For instance, </span>\n  <math alttext=\"\\text{SeedTTS}_{DiT}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SeedTTS</mtext>\n        <mrow>\n          <mi mathsize=\"0.900em\">D</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">T</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\text{SeedTTS}_{DiT}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a large-scale state-of-the-art industrial TTS model that leverages VAE latent representations, highlighting the effectiveness of compact representation for high-fidelity speech synthesis. Building on this idea, MegaTTS-3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further introduces a novel sparse alignment strategy to guide a latent diffusion transformer over the VAE latent space, achieving performance gains.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "performance",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Consistent with findings in the vision domain </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, latent diffusion models with vanilla acoustic VAE systems face an </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">optimization dilemma</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between reconstruction and generation performance. The results of preliminary experiments are shown in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Specifically, increasing the latent dimension enhances reconstruction quality and speaker similarity but impairs intelligibility in zero-shot TTS, whereas reducing the latent dimension improves intelligibility at the expense of reconstruction quality and speaker similarity. This trade-off exemplifies the information bottleneck: lower-dimensional representations capture core semantic content, whereas higher-dimensional representations preserve richer acoustic details but introduce redundancy and complicate semantic modeling.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "tts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this challenge, we introduce </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Semantic-VAE</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a novel variational autoencoder with semantic alignment regularization in the latent space to mitigate the difficulty of semantic modeling in zero-shot TTS tasks with unconstrained high-dimensional latent representations. Extensive experiments demonstrate that Semantic-VAE mitigates the generation&#8211;reconstruction dilemma in high-dimensional latent spaces, accelerates training convergence, and achieves state-of-the-art performance in zero-shot TTS tasks. Specifically, the F5-TTS model with the proposed Semantic-VAE features achieves </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.10</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">% WER and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.64</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> speaker similarity on the LibriSpeech-PC test-clean dataset, outperforming the mel-based F5-TTS baseline (2.23%, 0.60) and the vanilla acoustic VAE baseline (2.65%, 0.59). Additional reconstruction experiments show that Semantic-VAE achieves reconstruction quality comparable to vanilla VAE and substantially better than the vocoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with mel-spectrograms. This demonstrates that our proposed semantic alignment regularization enables more effective generative modeling without compromising the representational capacity of the latent features. We release the full Semantic-VAE framework with code and checkpoints at </span>\n  <a class=\"ltx_ref ltx_href\" href=\"https://github.com/ZhikangNiu/Semantic-VAE\" style=\"font-size:90%;\" title=\"\">https://github.com/ZhikangNiu/Semantic-VAE</a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "baseline",
                    "wer",
                    "tts",
                    "librispeechpc",
                    "alignment",
                    "testclean",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2 Method &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we propose an improved training paradigm for acoustic VAE systems that incorporates semantic regularization to mitigate the reconstruction and generation optimization dilemma.\nIn this section, we provide a detailed introduction of our proposed method, highlighting its key components and underlying motivations. Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S2.SS1\" style=\"font-size:90%;\" title=\"2.1 Variational Autoencoder &#8227; 2 Method &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduces our baseline VAE architecture and objective. Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 Semantic Representation Alignment &#8227; 2 Method &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents our strategy for semantic regularizing latent representations using pre-trained self-supervised learning (SSL) speech models.</span>\n</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "models",
                    "method",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The reconstruction and generation dilemma is a widely discussed challenge in discrete audio codecs. Previous studies&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have demonstrated that aligning with pre-trained SSL speech models (e.g., HuBERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, WavLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, w2v-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, etc.), which capture rich phonetic and semantic representations, can provide more informative supervision for audio codec training and improve downstream AR model performance and convergence. Similarly, incorporating SSL alignment into Diffusion Transformers (DiT) training accelerates convergence and yields better performance in speech synthesis&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and image generation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "hubert",
                    "performance",
                    "models",
                    "alignment",
                    "wavlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, to the best of our knowledge, no prior work has explored applying semantic regularization to continuous latent spaces, nor examined its potential to improve downstream task performance. Motivated by these insights, we introduce a semantic regularization loss into the VAE training framework. This addition aims to investigate whether semantic-aligned VAEs can improve both the convergence speed and performance of NAR-based TTS models compared to vanilla acoustic VAEs. Specifically, we employ a pre-trained SSL speech model </span>\n  <math alttext=\"f(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to extract structured semantic representations from the same speech </span>\n  <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119961;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{x}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The hidden states </span>\n  <math alttext=\"\\bm{h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119945;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{h}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of the SSL model, aligned in both temporal and feature dimensions with the VAE latent representation, are obtained through an interpolation layer followed by a 1D convolution layer, formulated as</span>\n</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "layer",
                    "performance",
                    "models",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">T</mi>\n      <annotation encoding=\"application/x-tex\">T</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the sequence length and </span>\n  <math alttext=\"\\text{cos}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext mathsize=\"0.900em\">cos</mtext>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo mathsize=\"0.900em\" rspace=\"0em\">,</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{cos}(\\cdot,\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> computes the cosine similarity between the aligned VAE latent and the SSL feature at each time step </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This loss regularizes the VAE latent space, encouraging the VAE to learn a structured semantic representation from SSL models. To jointly optimize for high-fidelity speech reconstruction and semantically meaningful latent representations, we integrate the VAE training objective with the proposed semantic regularization loss. The overall training objective is formulated as</span>\n</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To comprehensively evaluate Semantic-VAE regarding the optimization dilemma between reconstruction and generation, we consider two tasks: 1) reconstruction and 2) downstream zero-shot TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the reconstruction evaluation of Semantic-VAE, we utilize the LibriTTS test-other as our test set, and employ UTMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PESQ (Perceptual Evaluation of Speech Quality)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib35\" title=\"\">35</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and STOI (Short-time Objective Intelligibility) as the evaluation metrics to quantify perceptual quality and intelligibility. For the zero-shot TTS evaluation, we follow the setup in F5-TTS</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://github.com/SWivid/F5-TTS/tree/main/src/f5_tts/eval\" title=\"\">https://github.com/SWivid/F5-TTS/tree/main/src/f5_tts/eval</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and evaluate on the LibriSpeech-PC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib36\" title=\"\">36</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test-clean. We perform the cross-sentence generation and evaluate the performance using the Word Error Rate (WER), Speaker Similarity (SIM-o), and UTMOS, which respectively measure intelligibility, speaker consistency, and naturalness.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "wer",
                    "tts",
                    "librispeechpc",
                    "testclean",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.4 Evaluation &#8227; 3 Experimental Setup &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports zero-shot TTS results on the LibriSpeech-PC test-clean dataset, with results presented separately for high-resource and low-resource settings.\nIn the high-resource scenario, NAR models (E2 TTS and F5-TTS) achieve performance comparable to AR models such as CosyVoice&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib37\" title=\"\">37</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and FireRedTTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib38\" title=\"\">38</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "models",
                    "tts",
                    "librispeechpc",
                    "testclean",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For a fair comparison, we evaluated the models in the low-resource setting, which highlights the advantages of Semantic-VAE. When integrated into F5-TTS, Semantic-VAE achieves state-of-the-art results, reducing WER to 2.10% and increasing speaker similarity (SIM) to 0.64 under the same inference setting, outperforming both F5-TTS Small baseline and its vanilla VAE variant. A similar trend is observed when applying Semantic-VAE to E2 TTS, yielding consistent gains over the baseline. To further assess convergence, we compare the proposed Semantic-VAE against the vanilla VAE and the mel-based F5-TTS baseline across different training steps. As shown in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.1 Downstream Zero-shot TTS Results &#8227; 4 Results &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Semantic-VAE converges faster and achieves better performance than both the vanilla VAE and the mel-based baseline at the same step. These results demonstrate that Semantic-VAE not only improves the final performance, especially speaker similarity, but also accelerates convergence and reduces training costs compared with the baseline F5-TTS.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "baseline",
                    "wer",
                    "models",
                    "tts",
                    "different",
                    "final"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Reconstruction Results &#8227; 4 Results &#8227; Semantic-VAE: Semantic-Alignment Latent Representation for Better Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports reconstruction results on LibriTTS test-other. Despite its aggressively compact representation in both temporal and latent dimensions, vanilla VAE outperforms Vocos in reconstruction. It indicates that end-to-end VAE training encourages the latent to retain essential information while discarding redundancy. Moreover, Semantic-VAE maintains reconstruction performance comparable to vanilla VAE, demonstrating that semantic alignment simplified generative modeling without sacrificing reconstruction performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Moreover, we explore </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">which semantic guidance is most effective for semantic alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. It observes that WavLM</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> generally outperforms HuBERT</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on SIM and comparable performance on WER, which can be attributed to its speaker-aware training objectives. Moreover, we compare different SSL layers and find that while all SSL layers help reduce WER, their impact on SIM varies considerably. In particular, the final layer consistently leads to a substantial drop in SIM performance, likely due to distributional adjustments required for aligning with the SSL training target, thereby discarding speaker-specific cues. Averaging SSL features across all layers improves SIM compared to the last-layer feature, which suggests that averaging all layers introduces more information that can improve acoustic details. However, incorporating too much information may also introduce redundancy, which could degrade semantic modeling. To verify this hypothesis, we chose the 23rd layer of WavLM for supervision. In SUPERB</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22167v1#bib.bib39\" title=\"\">39</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, this layer has the highest weight for WER and provides richer semantic representations.\nExperimental results confirm that this layer offers an optimal trade-off between WER and SIM, providing practical guidance for selecting pre-trained SSL features for alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "layer",
                    "performance",
                    "all",
                    "wer",
                    "different",
                    "23rd",
                    "layers",
                    "final",
                    "alignment",
                    "wavlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we investigate the optimization dilemma in high-dimensional latent spaces and propose Semantic-VAE, a speech variational autoencoder with semantic alignment regularization. By learning structured and semantically aligned latent representations, Semantic-VAE alleviates the reconstruction&#8211;generation dilemma and improves downstream zero-shot TTS performance. Experiments demonstrate that our approach accelerates convergence and achieves state-of-the-art results on the zero-shot TTS task. In addition, comprehensive analyses and ablation studies confirm the effectiveness of each proposed component. We believe that these promising findings pave the way for future research on advanced modality alignment techniques and further optimization of latent diffusion-based text-to-speech systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "performance",
                    "alignment"
                ]
            }
        ]
    }
}