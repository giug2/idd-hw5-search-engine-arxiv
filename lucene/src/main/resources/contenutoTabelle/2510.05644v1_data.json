{
    "S1.T1": {
        "source_file": "The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP",
        "caption": "Table 1: Publication volume analysis comparing top 10 global languages versus top 10 African languages across major academic databases (2020-2024). The disparity reveals a 20-70× underrepresentation of African languages in computational linguistics research.",
        "body": "High-Resource Languages\nAfrican Languages\n\n\n\n\nLanguage\nGS\narXiv\nIEEE\nCORE\nLanguage\nGS\narXiv\nIEEE\nCORE\n\n\nEnglish\n14,700\n323\n256\n3,095\nSwahili\n617\n10\n3\n114\n\n\nChinese\n7,710\n60\n85\n1,694\nHausa\n261\n1\n0\n49\n\n\nHindi\n1,980\n20\n41\n336\nYoruba\n276\n1\n0\n59\n\n\nSpanish\n4,240\n29\n24\n908\nIgbo\n203\n0\n0\n38\n\n\nArabic\n3,150\n25\n24\n616\nAmharic\n338\n2\n2\n49\n\n\nFrench\n4,490\n38\n17\n1,037\nOromo\n104\n1\n1\n21\n\n\nBengali\n943\n9\n8\n183\nBerber\n55\n0\n0\n11\n\n\nPortuguese\n1,980\n13\n7\n400\nZulu\n175\n1\n1\n38\n\n\nRussian\n2,950\n19\n16\n611\nFula\n20\n0\n0\n7\n\n\nUrdu\n728\n3\n9\n131\nMalagasy\n72\n0\n0\n15",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">High-Resource Languages</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">African Languages</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Language</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">GS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">arXiv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">IEEE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CORE</span></td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Language</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">GS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">arXiv</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">IEEE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CORE</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><code class=\"ltx_verbatim ltx_font_typewriter\">English</code></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14,700</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">323</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">256</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">3,095</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><code class=\"ltx_verbatim ltx_font_typewriter\">Swahili</code></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">617</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">114</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Chinese</code></th>\n<td class=\"ltx_td ltx_align_center\">7,710</td>\n<td class=\"ltx_td ltx_align_center\">60</td>\n<td class=\"ltx_td ltx_align_center\">85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\">1,694</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Hausa</code></th>\n<td class=\"ltx_td ltx_align_center\">261</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center\">0</td>\n<td class=\"ltx_td ltx_align_center\">49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Hindi</code></th>\n<td class=\"ltx_td ltx_align_center\">1,980</td>\n<td class=\"ltx_td ltx_align_center\">20</td>\n<td class=\"ltx_td ltx_align_center\">41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\">336</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Yoruba</code></th>\n<td class=\"ltx_td ltx_align_center\">276</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center\">0</td>\n<td class=\"ltx_td ltx_align_center\">59</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Spanish</code></th>\n<td class=\"ltx_td ltx_align_center\">4,240</td>\n<td class=\"ltx_td ltx_align_center\">29</td>\n<td class=\"ltx_td ltx_align_center\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\">908</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Igbo</code></th>\n<td class=\"ltx_td ltx_align_center\">203</td>\n<td class=\"ltx_td ltx_align_center\">0</td>\n<td class=\"ltx_td ltx_align_center\">0</td>\n<td class=\"ltx_td ltx_align_center\">38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Arabic</code></th>\n<td class=\"ltx_td ltx_align_center\">3,150</td>\n<td class=\"ltx_td ltx_align_center\">25</td>\n<td class=\"ltx_td ltx_align_center\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\">616</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Amharic</code></th>\n<td class=\"ltx_td ltx_align_center\">338</td>\n<td class=\"ltx_td ltx_align_center\">2</td>\n<td class=\"ltx_td ltx_align_center\">2</td>\n<td class=\"ltx_td ltx_align_center\">49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">French</code></th>\n<td class=\"ltx_td ltx_align_center\">4,490</td>\n<td class=\"ltx_td ltx_align_center\">38</td>\n<td class=\"ltx_td ltx_align_center\">17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\">1,037</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Oromo</code></th>\n<td class=\"ltx_td ltx_align_center\">104</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center\">21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Bengali</code></th>\n<td class=\"ltx_td ltx_align_center\">943</td>\n<td class=\"ltx_td ltx_align_center\">9</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\">183</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Berber</code></th>\n<td class=\"ltx_td ltx_align_center\">55</td>\n<td class=\"ltx_td ltx_align_center\">0</td>\n<td class=\"ltx_td ltx_align_center\">0</td>\n<td class=\"ltx_td ltx_align_center\">11</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Portuguese</code></th>\n<td class=\"ltx_td ltx_align_center\">1,980</td>\n<td class=\"ltx_td ltx_align_center\">13</td>\n<td class=\"ltx_td ltx_align_center\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\">400</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Zulu</code></th>\n<td class=\"ltx_td ltx_align_center\">175</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center\">38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Russian</code></th>\n<td class=\"ltx_td ltx_align_center\">2,950</td>\n<td class=\"ltx_td ltx_align_center\">19</td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr\">611</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Fula</code></th>\n<td class=\"ltx_td ltx_align_center\">20</td>\n<td class=\"ltx_td ltx_align_center\">0</td>\n<td class=\"ltx_td ltx_align_center\">0</td>\n<td class=\"ltx_td ltx_align_center\">7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Urdu</code></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">728</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_rr\">131</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\"><code class=\"ltx_verbatim ltx_font_typewriter\">Malagasy</code></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">15</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "global",
            "underrepresentation",
            "urdu",
            "hindi",
            "french",
            "bengali",
            "volume",
            "publication",
            "linguistics",
            "igbo",
            "malagasy",
            "core",
            "computational",
            "zulu",
            "reveals",
            "ieee",
            "across",
            "databases",
            "swahili",
            "african",
            "top",
            "english",
            "russian",
            "analysis",
            "fula",
            "2070×",
            "highresource",
            "language",
            "chinese",
            "arabic",
            "portuguese",
            "arxiv",
            "disparity",
            "amharic",
            "comparing",
            "spanish",
            "yoruba",
            "versus",
            "academic",
            "berber",
            "research",
            "hausa",
            "languages",
            "major",
            "oromo"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">This problem is compounded by a severe underrepresentation in the global NLP research community. Our analysis of mentions of the top 10 global languages versus the top 10 African languages across major academic databases reveals a stark imbalance. On average, for every paper discussing African languages in multilingual LLM contexts, there are 20 papers on global languages in Google Scholar (GS), 23 in COnnecting REpositories(CORE), 34 in arXiv, and 70 in The Institute of Electrical and Electronics Engineers (IEEE) (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Aggregated Paper Count Approach &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> in the Appendix). This 20-70x representation gap reinforces a self-perpetuating cycle of marginalization where limited research attention leads to poor technological support, which in turn discourages further research investment.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite representing nearly one-third of the world&#8217;s languages, African languages remain critically underserved by modern NLP technologies, with 88% classified as severely underrepresented or completely ignored in computational linguistics. We present the African Languages Lab (All Lab), a comprehensive research initiative that addresses this technological gap through systematic data collection, model development, and capacity building. Our contributions include: (1) a quality-controlled data collection pipeline, yielding the largest validated African multi-modal speech and text dataset spanning 40 languages with 19 billion tokens of monolingual text and 12,628 hours of aligned speech data; (2) extensive experimental validation demonstrating that our dataset, combined with fine-tuning, achieves substantial improvements over baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages; and (3) a structured research program that has successfully mentored fifteen early-career researchers, establishing sustainable local capacity. Our comparative evaluation against Google Translate reveals competitive performance in several languages while identifying areas that require continued development.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>To promote accessibility, we provide translations of this abstract in 10 African languages in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#A2\" title=\"Appendix B Abstract Translations &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, generated using our fine-tuned models.</span></span></span></p>\n\n",
                "matched_terms": [
                    "across",
                    "african",
                    "linguistics",
                    "research",
                    "computational",
                    "languages",
                    "reveals"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP</span>\n</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The promise of artificial intelligence (AI) and natural language processing (NLP) to democratize information access remains unfulfilled for billions of speakers worldwide. Among the approximately 7,000 languages spoken globally, fewer than 20 receive substantial attention in NLP research <cite class=\"ltx_cite ltx_citemacro_citep\">(Magueresse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib27\" title=\"\">2020</a>)</cite>. This technological marginalization particularly affects low-resource languages (LRLs). Without a clearly established definition, LRLs are languages that exist at the periphery of the digital transformation, characterized by three critical deficits: (1) a scarcity of machine-readable corpora, (2) limited personalized computational technologies and trained language models, and (3) insufficient representation in global research communities <cite class=\"ltx_cite ltx_citemacro_citep\">(Nigatu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib32\" title=\"\">2024</a>; Issaka et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib23\" title=\"\">2024</a>; Magueresse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib27\" title=\"\">2020</a>)</cite>.\nWhile often serving substantial speaker populations, these languages face significant challenges in participating fully in the AI-driven information economy.</p>\n\n",
                "matched_terms": [
                    "global",
                    "language",
                    "research",
                    "computational",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Africa, the scale of this crisis is staggering: over 2,000 languages are spoken across Africa (nearly one-third of all languages worldwide). Yet, a stunning 88% of African languages are \"severely underrepresented\" or \"completely ignored\" in computational linguistics <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib24\" title=\"\">2020</a>)</cite>. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S2.F1\" title=\"Figure 1 &#8227; 2.1 Community-Driven Research Initiatives &#8227; 2 Related Work &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, approximately 814 African languages are in danger of extinction. Countries like Nigeria, Cameroon, and the Ivory Coast have 171, 75, and 65 languages facing the most severe threats, respectively <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.ethnologue.com/\" title=\"\">https://www.ethnologue.com/</a></span></span></span>. This exclusion has far-reaching consequences, from poor educational and healthcare outcomes to preventing full participation in the digital economy <cite class=\"ltx_cite ltx_citemacro_citep\">(Laitin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib25\" title=\"\">2019</a>; Gessler and von&#160;der Wense, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "african",
                    "linguistics",
                    "computational",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Contributing to broader efforts to bridge this systemic technological gap, we present the African Languages Lab (All Lab), an initiative to democratize NLP technology for African languages. Founded in 2020, the All Lab operates through a coordinated team of dedicated researchers who combine three innovative elements:</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Systematic data infrastructure:</span> We developed a systematic, quality-controlled data collection framework powered by our \"All Voices\" platform. All Voices is a mobile-first platform specifically designed for community-driven multilingual data collection in low-resource contexts, enabling direct translation between African languages without English intermediation</p>\n\n",
                "matched_terms": [
                    "african",
                    "english",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comprehensive dataset development:</span> Through coordinated collection and validation efforts, we assembled the largest multi-modal dataset for African LRLs, encompassing 19 billion tokens across 40 languages with 12,628 hours of aligned speech data.</p>\n\n",
                "matched_terms": [
                    "african",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The landscape of African NLP research has evolved through three interconnected streams: community-driven initiatives, advances in multilingual modeling, and the development of evaluation frameworks. We examine how these efforts have shaped current capabilities and identify gaps our work addresses.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of African NLP has been shaped by complementary community and institutional efforts. Masakhane, comprising over 3,000 Slack members, exemplifies successful community-driven research, demonstrating that participatory approaches can produce high-quality datasets and models <cite class=\"ltx_cite ltx_citemacro_citep\">(Orife et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib35\" title=\"\">2020</a>)</cite>. Complementing this work, the \"Breaking the Unwritten Language Barrier\" project addresses challenges specific to unwritten and under-documented languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adda et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib1\" title=\"\">2016</a>)</cite>. Their work on languages like Basaa, Myene, and Embosi has established methodological approaches for speech recognition in LRLs.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These community efforts have been supported by institutional initiatives providing essential infrastructure. The Lacuna Fund has enabled dataset development <cite class=\"ltx_cite ltx_citemacro_citep\">(Rathi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib37\" title=\"\">2023</a>)</cite>, while Meta&#8217;s No Language Left Behind project has contributed architectural innovations for massively multilingual models <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib47\" title=\"\">2022</a>)</cite>. Additional infrastructure support has come from Mozilla&#8217;s Common Voice project <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib8\" title=\"\">2020</a>)</cite> for speech resources and the AI4D African Language Program <cite class=\"ltx_cite ltx_citemacro_citep\">(Siminyu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib42\" title=\"\">2021</a>)</cite> for benchmark development. The Deep Learning Indaba<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://deeplearningindaba.com/\" title=\"\">https://deeplearningindaba.com/</a></span></span></span> has contributed to research capacity building through its convenings, while platforms like Lanfrica have improved resource discoverability and research sharing across the continent <cite class=\"ltx_cite ltx_citemacro_citep\">(Emezue and Dossou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib17\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "language",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evolution of multilingual LLMs has shown steady progress in language coverage and capabilities. Early approaches like mBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Muller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib30\" title=\"\">2021</a>)</cite> and XLM-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib12\" title=\"\">2020</a>)</cite> established initial benchmarks, supporting approximately 100 languages each. Subsequent developments included more focused models like mBART <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib26\" title=\"\">2020</a>)</cite>, mT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xue et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib57\" title=\"\">2021</a>)</cite>, and XGLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Ersoy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib19\" title=\"\">2023</a>)</cite>, which traded broader language coverage for improved performance on specific language sets. The advent of massive LLMs further expanded these capabilities, with models like GPT-3, mGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Shliazhko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib41\" title=\"\">2024</a>)</cite>, and BLOOM <cite class=\"ltx_cite ltx_citemacro_citep\">(Workshop et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib56\" title=\"\">2023</a>)</cite> supporting varying numbers of African languages. Also, Glot500-m <cite class=\"ltx_cite ltx_citemacro_citep\">(Imani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib22\" title=\"\">2023</a>)</cite> extends support to 511 languages and the SERENGETI and Cheetah models supports about 517 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adebara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib4\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib3\" title=\"\">2024</a>)</cite>. Additional progress has come from the Aya model, which demonstrates instruction-following capabilities across 101 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(&#220;st&#252;n et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib59\" title=\"\">2024</a>)</cite>, and specialized models like AfroLM, which focuses on 23 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Dossou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib14\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While not specifically trained in African languages, English-centric LLMs such as GPT-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib34\" title=\"\">2024</a>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib46\" title=\"\">2025</a>)</cite>, and Llama <cite class=\"ltx_cite ltx_citemacro_citep\">(Wendler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib53\" title=\"\">2024</a>)</cite> have shown capability in handling some African languages, <cite class=\"ltx_cite ltx_citemacro_citep\">(Robinson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib40\" title=\"\">2023</a>; Ojo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib33\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib58\" title=\"\">2024</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib13\" title=\"\">2024</a>)</cite>, though their performance generally does not match that of specialized models; underscoring the need for dedicated resources and architectures.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of evaluation frameworks has enabled systematic progress measurement in African NLP across diverse task domains. MasakhaNER provides NER datasets for 10 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib5\" title=\"\">2021</a>)</cite>, AfriSenti offers sentiment analysis benchmarks in 14 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib28\" title=\"\">2023</a>)</cite>, and AFROMT establishes standardized translation benchmarks for 8 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Reid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib39\" title=\"\">2021</a>)</cite>. IrokoBench unifies evaluation across natural language inference, mathematical reasoning, and multiple-choice QA in 17 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib6\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "african",
                    "analysis",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these developments and advances, significant challenges remain in African NLP research <cite class=\"ltx_cite ltx_citemacro_citep\">(Adebara and Abdul-Mageed, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib2\" title=\"\">2022</a>; Issaka et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib23\" title=\"\">2024</a>)</cite>. Our work builds upon these foundations while addressing several key limitations in existing approaches, such as robust team coordination, cross-initiative knowledge transfer, deduplication of efforts, and intentional skill set development.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">All Voices Platform.</span> To address the fundamental challenge of data scarcity in African languages, we developed All Voices, a mobile-first platform that stands as the only solution specifically designed for data collection in any LRL. The platform&#8217;s innovative approach enables direct translation between LRLs without requiring English as an intermediary, addressing a critical gap in the existing data collection infrastructure. In addition, All Voices distinguishes itself through its multimodal capabilities, which support the collection and validation of text and audio data. The platform features an intuitive, user-friendly interface that encourages broad participation, complemented by gamification elements, including a global leaderboard system that promotes user engagement. Importantly, All Voices is open and free to everyone, aligning with our mission to democratize language technology development. All Voices contributors provide informed consent for their contributions to be used for research purposes, including dataset creation, model training, and open-source distribution, with full transparency regarding data usage and the right to withdraw consent at any time.</p>\n\n",
                "matched_terms": [
                    "global",
                    "language",
                    "african",
                    "english",
                    "research",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data processing implements a robust two-tier approach combining general normalization with language-specific processing. The general normalization phase addresses universal text artifacts through Unicode normalization, character encoding standardization, and structural cleaning, including HTML removal and symbol standardization. The language-specific processing phase implements specialized handling for African language features, including morphological analysis, script variant normalization, and tone mark standardization, language identification, with custom rule sets developed for specific language families.</p>\n\n",
                "matched_terms": [
                    "african",
                    "analysis",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, our translation validation methodology implements a robust statistical framework for assessing translation quality through quantitative analysis of character-level distributions. The validation metric employs character ratio analysis between source and target texts, computed as the ratio of target text length to source text length. We analyze these ratios using z-score normalization within language-specific distribution, enabling the detection of statistical outliers while accounting for natural variations in text length across different language pairs. This approach is augmented with character overlap detection to identify potential artifacts or inappropriate text preservation, particularly crucial for languages sharing similar orthographic features.</p>\n\n",
                "matched_terms": [
                    "language",
                    "analysis",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Also, the threshold determination process implements an adaptive sampling methodology. For each language pair, we establish baseline distributions through initial sampling of 10,000 translation pairs, employing Kernel Density Estimation for robust distribution modeling. This approach effectively captures the non-Gaussian characteristics frequently observed in cross-lingual character distributions. Thresholds are dynamically computed using a modified Tukey method with an adaptive multiplier. This adaptive threshold mechanism automatically calibrates to language-specific characteristics, implementing more stringent filtering for language pairs that exhibit consistent ratios while allowing appropriate flexibility for pairs with inherently higher variability. The resulting validation framework effectively identifies and filters anomalous translations while maintaining sensitivity to legitimate linguistic variations across diverse African language families. The processed data sets are structured according to HuggingFace&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/\" title=\"\">https://huggingface.co/</a></span></span></span> Dataset specifications, enabling seamless API integration.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the utility of our dataset and establish baselines for our languages, we experimented with Llama-3.2-1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib21\" title=\"\">2024</a>)</cite>. We chose this model as our base because of its demonstrated multilingual capabilities and efficient parameter scaling, making it suitable for LRLs. Critically, while capable of tokenizing, this model has not been explicitly trained on any African language in our dataset. Thus, providing a cleaner baseline for measuring the utility of our dataset during fine-tuning without risk of data contamination.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed full fine-tuning rather than parameter-efficient methods, as preliminary experiments with Quantization-aware Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_citep\">(&#220;st&#252;n et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib59\" title=\"\">2024</a>)</cite> yielded insufficient performance gains for our target languages. Our training pipeline uses supervised learning with a standardized instruction template: \"Translate the following English text to X:\", where X denotes the target African language.</p>\n\n",
                "matched_terms": [
                    "african",
                    "english",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model performance was evaluated using a complementary set of metrics: BiLingual Evaluation Understudy (BLEU) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wieting et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib55\" title=\"\">2019</a>)</cite>, which measures n-gram precision; METEOR <cite class=\"ltx_cite ltx_citemacro_citep\">(Banerjee and Lavie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib9\" title=\"\">2005</a>)</cite>, which accounts for word stems and synonyms; COMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib38\" title=\"\">2020</a>)</cite>, which leverages multilingual embeddings to assess semantic similarity; and ChrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib51\" title=\"\">2025</a>)</cite>, which operates on character-level n-grams to better capture morphological variations common in African languages. Additionally, we employed Translation Edit Rate (TER) <cite class=\"ltx_cite ltx_citemacro_citep\">(Snover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib45\" title=\"\">2006</a>)</cite>, which quantifies the minimum number of edits required to transform the hypothesis into the reference translation (where lower scores indicate better quality), and AfriCOMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib50\" title=\"\">2024</a>)</cite>, a neural metric specifically trained on African language pairs to better capture language-specific quality nuances.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these metrics comprehensively assess translation quality across different linguistic aspects, from surface-level n-gram matching to semantic preservation and post-editing effort. We utilized the FLORES-200 dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib47\" title=\"\">2022</a>)</cite> as our standardized test set, ensuring consistency across all languages and enabling direct comparison with other multilingual systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our comprehensive data collection yielded 19,012,019,696 tokens of monolingual text and 12,628 hours of aligned audio across 40 African languages (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Our analysis reveals distinct stratification patterns highlighting the digital divide within African languages themselves.</p>\n\n",
                "matched_terms": [
                    "across",
                    "african",
                    "analysis",
                    "languages",
                    "reveals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Primary resource languages (&gt;2B tokens):</span> This tier includes Amharic (2.94B), Arabic (2.40B), Yoruba (2.36B), and Afrikaans (2.30B), reflecting sustained digitization efforts and strong institutional support.</p>\n\n",
                "matched_terms": [
                    "arabic",
                    "amharic",
                    "languages",
                    "yoruba"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Established digital languages (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2B tokens):</span> Languages such as Hausa (1.54B) and Tigrinya (0.92B) demonstrate robust digital presence, likely owing to consistent documentation and preservation initiatives.</p>\n\n",
                "matched_terms": [
                    "hausa",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emerging digital languages (250M&#8211;1B tokens):</span> A substantial group including Malagasy (839.12M), Somali (751.13M), Swahili (700.39M), and Xhosa (563.07M), show growing digital footprints but still lagging behind the top tiers.</p>\n\n",
                "matched_terms": [
                    "malagasy",
                    "top",
                    "swahili",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">High-resource audio languages (&gt;1,000 hours):</span> Kinyarwanda (3,839.00h), Luganda (1,727.80h), Swahili (1,115.00h), and Arabic (2,721.52h) dominate in audio availability, often due to large-scale speech corpora or broadcast archives.</p>\n\n",
                "matched_terms": [
                    "highresource",
                    "languages",
                    "swahili",
                    "arabic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Established audio languages (500&#8211;1,000 hours):</span> This tier is notably sparse, underscoring the scarcity of mid-scale speech datasets in African languages.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Moderate audio languages (100&#8211;500 hours):</span> Includes Malagasy (325.14h), Twi (227.03h), Bemba (230.30h), and Ewe (147.00h), representing a mix of widely spoken languages and those with targeted speech collection efforts.</p>\n\n",
                "matched_terms": [
                    "malagasy",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, our analysis underscores two parallel digital divides: a textual divide, where a small set of languages capture the majority of tokens, and an audio divide, where a different but equally narrow set of languages dominate. Notably, the top three languages by text volume account for a disproportionate share of tokens, while the top three by audio hours similarly capture the bulk of recorded speech. This imbalance highlights the urgent need for targeted development of both textual and audio resources, particularly for languages with substantial speaker populations but limited digital presence.</p>\n\n",
                "matched_terms": [
                    "top",
                    "volume",
                    "analysis",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation across 31 African languages reveals substantial and systematic improvements through fine-tuning, with distinct patterns emerging across language families and resource levels (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S4.T3\" title=\"Table 3 &#8227; 4 Results &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Nine languages were excluded from evaluation due to insufficient training data or absence from the FLORES-200 benchmark.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "african",
                    "languages",
                    "reveals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance.</span> The base Llama-3.2-1B model demonstrates limited but non-trivial capability for African languages, revealing interesting patterns of cross-lingual transfer. ChrF++ scores range from 2.00 (Wolof) to 44.76 (Afrikaans), with a mean of 8.10, indicating minimal character-level understanding for most languages. COMET scores cluster between 0.16-0.68 (mean: 0.32), suggesting some semantic comprehension despite poor surface realization. Notably, Afrikaans shows exceptional baseline performance (ChrF++ 44.76, BLEU 32.98), leveraging its Germanic roots and Latin script. The extremely low baseline BLEU scores (mean: 2.27) across most languages confirm the model&#8217;s inability to produce accurate n-gram sequences without language-specific training.</p>\n\n",
                "matched_terms": [
                    "african",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-tuning Impact.</span> Our dataset enables noticeable performance improvements, with average gains of +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points. The magnitude of improvement correlates inversely with baseline performance, suggesting effective transfer learning rather than simple memorization. Swahili exhibits the largest absolute ChrF++ improvement (+63.27 points), achieving near-parity with Google Translate (72.27 vs 75.81). Sesotho shows remarkable gains across all metrics (+61.79 ChrF++, +51.16 BLEU), while maintaining competitive performance against Google Translate. Languages with minimal baselines, including Fula, Wolof, and Kikongo, demonstrate that even severely under-resourced languages benefit substantially from targeted fine-tuning, achieving functional translation capability where none existed before.</p>\n\n",
                "matched_terms": [
                    "swahili",
                    "fula",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparison with Google Translate.</span> Where available (22 languages), comparison with Google Translate reveals three distinct performance categories. First, languages where our models achieve competitive or superior performance: Yoruba (30.88 vs 21.05 ChrF++), Arabic (31.52 vs 28.46), and notably Twi, where we substantially outperform Google Translate (46.80 vs 31.48). Second, languages where Google maintains clear advantages, particularly in high-resource cases like Kinyarwanda (24.65 vs 70.27) and Swahili (72.27 vs 75.81), reflecting their extensive training data. Third, the languages where Google Translate offers no support, highlighting our contribution to genuinely LRL coverage.</p>\n\n",
                "matched_terms": [
                    "highresource",
                    "yoruba",
                    "swahili",
                    "arabic",
                    "languages",
                    "reveals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language-Specific Patterns.</span> Three response profiles emerge from our analysis. High-responder languages (Swahili, Sesotho, Hausa) show dramatic improvements exceeding 40 ChrF++ points, suggesting optimal alignment between our dataset characteristics and model architecture. Steady improvers (Igbo, Shona, Somali) demonstrate consistent gains of 25-30 points across metrics, indicating robust but not exceptional adaptation. Challenging cases (Fon, Wolof, Bambara) show limited improvements despite fine-tuning, likely requiring specialized tokenization or architectural modifications to address their unique linguistic features.</p>\n\n",
                "matched_terms": [
                    "swahili",
                    "across",
                    "igbo",
                    "analysis",
                    "hausa",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation Edit Rate Analysis.</span> The dramatic TER reductions, averaging 580.13 points lower after fine-tuning, provide crucial practical insights. Languages like Swahili achieve TER scores comparable to Google Translate (23.77), indicating production-ready quality requiring minimal post-editing. Even languages with modest BLEU improvements show substantial TER reductions, suggesting improved fluency and coherence that traditional metrics may not fully capture. This pattern holds particular significance for scenarios where post-editing cost determines practical viability.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "swahili",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Correlations.</span> While surface metrics (BLEU, ChrF++) show high correlation, the divergence between these and neural metrics (COMET, AfriCOMET) reveals important quality dimensions. Languages like Ewe show minimal COMET improvement (0.04) despite substantial ChrF++ gains (30.43 points), suggesting character-level improvements without the corresponding semantic enhancement. In contrast, Arabic shows strong COMET gains (0.33) with modest improvement in ChrF++, indicating semantic preservation despite surface-level challenges. These patterns underscore the importance of multi-metric evaluation for morphologically diverse African languages.</p>\n\n",
                "matched_terms": [
                    "arabic",
                    "african",
                    "reveals",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We have presented the African Languages Lab, a research initiative addressing the critical underrepresentation of African languages in NLP through systematic data collection, model development, and capacity building. Our contributions include a validated dataset of 19 billion tokens and 12,628 hours of aligned speech across 40 languages, substantial performance improvements averaging +23.69 ChrF++ over baseline models, the All Voices platform, and a structured mentorship program developing fifteen early-career researchers. Thus, we demonstrate that the technological marginalization of African languages, while severe, is not intractable.</p>\n\n",
                "matched_terms": [
                    "underrepresentation",
                    "across",
                    "african",
                    "research",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As language technologies increasingly mediate access to information, education, and economic opportunities, ensuring equitable coverage becomes not merely a technical challenge but a moral imperative. The African Languages Lab demonstrates that this imperative can be met through coordinated research, community engagement, and sustained investment in both technical infrastructure and human capacity, establishing a sustainable path forward for the world&#8217;s underserved linguistic communities.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments utilize Llama-3.2-1B as the sole base model, which, while demonstrating the utility of our dataset, may underestimate potential gains achievable with larger-scale architectures. The performance variance across language families, from 63.27 ChrF++ improvement for Swahili to minimal gains for Fon (-1.10), suggests that optimal model selection likely varies by linguistic typology. Additionally, our evaluation of 31 of 40 collected languages reflects FLORES-200 coverage limitations, potentially obscuring insights from the most critically under-resourced languages in our dataset.</p>\n\n",
                "matched_terms": [
                    "swahili",
                    "language",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite assembling 19 billion tokens, our dataset exhibits a 147,000&#215; disparity between the highest-resourced (Amharic: 2,944.95M tokens) and lowest-resourced (Fang: 0.02M tokens) languages. This imbalance directly correlates with performance outcomes: languages with &gt;1B tokens achieve average ChrF++ scores of 45.66, while those with &lt;100M tokens average 24.31. Furthermore, 13 languages lack audio data entirely, limiting multimodal model development. Our validation pipeline, while statistically grounded, operates without native speaker verification for 73% of languages, potentially missing dialectal variations that affect 28% of evaluated translations showing COMET-ChrF++ divergence exceeding 0.3.</p>\n\n",
                "matched_terms": [
                    "disparity",
                    "amharic",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these constraints delineate several clear pathways for advancement. One direction is to explore architecture-specific optimizations for morphologically complex languages. Another is to implement active learning strategies that help address data imbalances. It is also important to develop evaluation metrics that are more sensitive to African language typologies. These limitations inform our ongoing work and highlight key areas for future research in African NLP.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">They also underscore the need for continued investment in computational resources, human expertise, and infrastructure development to support comprehensive technology development for African languages.</p>\n\n",
                "matched_terms": [
                    "african",
                    "computational",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our structured research development program has mentored fifteen early-career researchers across four institutions through one-on-one mentorship, project development support, and transitions into extended research roles. This investment in local research leadership establishes sustainable capacity for African NLP development, ensuring that technical advancement aligns with cultural and linguistic expertise. By prioritizing skill development alongside technical innovation, we contribute to a sustainable talent pipeline that positions African researchers to lead future developments in their languages.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work directly advances United Nations Sustainable Development Goals in education and inequality reduction through increased digital representation of marginalized languages. The platform enables community-led content creation and facilitates open knowledge transfer, democratizing access to digital tools while preserving linguistic and cultural heritage. With 88% of African languages severely underrepresented or completely ignored in computational linguistics, and 814 languages facing extinction risk, our framework provides critical infrastructure for language preservation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "african",
                    "linguistics",
                    "computational",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Guided by Ubuntu philosophy&#8212;emphasizing inclusivity, interdependence, and openness&#8212;we establish a framework for equitable NLP development. Our roadmap encompasses expanding language coverage, optimizing model architectures for low-resource languages, and deepening research collaborations. We acknowledge persistent challenges including limited commercial viability for some LRL technologies and infrastructural constraints, yet our results demonstrate that systematic community engagement can effectively address technological marginalization.\nThrough this comprehensive approach integrating technical innovation, cultural preservation, educational empowerment, and economic inclusion, we provide replicable models for equitable language technology development that can benefit millions of African language speakers while contributing to global linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "global",
                    "language",
                    "african",
                    "research",
                    "languages"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP",
        "caption": "Table 2: Dataset composition across our 40 African languages sorted by token count, showing the distribution of tokens (in millions) and hours of audio data. Dashes (-) indicate no audio data available.",
        "body": "Language\nTokens\nHours\nLanguage\nTokens\nHours\nLanguage\nTokens\nHours\n\n\n\n\nAmharic\n2,944.95\n238.00\nSesotho\n274.61\n114.70\nTshiluba\n54.93\n-\n\n\nArabic\n2,400.00\n2,721.52\nOromo\n252.82\n145.00\nMossi\n50.59\n-\n\n\nYoruba\n2,362.70\n128.30\nChewa\n230.63\n35.00\nKikongo\n46.59\n-\n\n\nAfrikaans\n2,295.09\n138.00\nRundi\n172.61\n-\nEwe\n31.74\n147.00\n\n\nHausa\n1,538.84\n239.00\nLuganda\n121.17\n1,727.80\nBerber\n28.86\n19.33\n\n\nTigrinya\n916.42\n1.00\nTswana\n118.84\n111.70\nKrio\n22.76\n80.00\n\n\nMalagasy\n839.12\n325.14\nBambara\n109.49\n30.60\nBemba\n8.60\n230.30\n\n\nSomali\n751.13\n115.40\nLingala\n102.19\n194.30\nKanuri\n6.18\n-\n\n\nSwahili\n700.39\n1,115.00\nTwi\n86.49\n227.03\nUmbundu\n5.10\n-\n\n\nXhosa\n563.07\n123.70\nFon\n77.27\n18.50\nKiluba\n2.02\n-\n\n\nZulu\n553.67\n83.20\nFula\n72.40\n124.00\nNgambay\n1.03\n-\n\n\nIgbo\n433.28\n25.00\nKikuyu\n66.34\n44.00\nMandinka\n0.41\n-\n\n\nShona\n428.25\n103.00\nWolof\n57.46\n183.20\nFang\n0.02\n-\n\n\nKinyarwanda\n283.40\n3,839.00",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Language</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Tokens</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Hours</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Language</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Tokens</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Hours</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Language</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Tokens</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Hours</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Amharic</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2,944.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">238.00</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Sesotho</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">274.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">114.70</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Tshiluba</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Arabic</th>\n<td class=\"ltx_td ltx_align_center\">2,400.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2,721.52</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Oromo</th>\n<td class=\"ltx_td ltx_align_center\">252.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">145.00</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mossi</th>\n<td class=\"ltx_td ltx_align_center\">50.59</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Yoruba</th>\n<td class=\"ltx_td ltx_align_center\">2,362.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">128.30</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Chewa</th>\n<td class=\"ltx_td ltx_align_center\">230.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">35.00</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Kikongo</th>\n<td class=\"ltx_td ltx_align_center\">46.59</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Afrikaans</th>\n<td class=\"ltx_td ltx_align_center\">2,295.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">138.00</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Rundi</th>\n<td class=\"ltx_td ltx_align_center\">172.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Ewe</th>\n<td class=\"ltx_td ltx_align_center\">31.74</td>\n<td class=\"ltx_td ltx_align_center\">147.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Hausa</th>\n<td class=\"ltx_td ltx_align_center\">1,538.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">239.00</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Luganda</th>\n<td class=\"ltx_td ltx_align_center\">121.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1,727.80</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Berber</th>\n<td class=\"ltx_td ltx_align_center\">28.86</td>\n<td class=\"ltx_td ltx_align_center\">19.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Tigrinya</th>\n<td class=\"ltx_td ltx_align_center\">916.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.00</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Tswana</th>\n<td class=\"ltx_td ltx_align_center\">118.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">111.70</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Krio</th>\n<td class=\"ltx_td ltx_align_center\">22.76</td>\n<td class=\"ltx_td ltx_align_center\">80.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Malagasy</th>\n<td class=\"ltx_td ltx_align_center\">839.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">325.14</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Bambara</th>\n<td class=\"ltx_td ltx_align_center\">109.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">30.60</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Bemba</th>\n<td class=\"ltx_td ltx_align_center\">8.60</td>\n<td class=\"ltx_td ltx_align_center\">230.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Somali</th>\n<td class=\"ltx_td ltx_align_center\">751.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">115.40</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Lingala</th>\n<td class=\"ltx_td ltx_align_center\">102.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">194.30</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Kanuri</th>\n<td class=\"ltx_td ltx_align_center\">6.18</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Swahili</th>\n<td class=\"ltx_td ltx_align_center\">700.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1,115.00</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Twi</th>\n<td class=\"ltx_td ltx_align_center\">86.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">227.03</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Umbundu</th>\n<td class=\"ltx_td ltx_align_center\">5.10</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Xhosa</th>\n<td class=\"ltx_td ltx_align_center\">563.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">123.70</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Fon</th>\n<td class=\"ltx_td ltx_align_center\">77.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">18.50</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Kiluba</th>\n<td class=\"ltx_td ltx_align_center\">2.02</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Zulu</th>\n<td class=\"ltx_td ltx_align_center\">553.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">83.20</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Fula</th>\n<td class=\"ltx_td ltx_align_center\">72.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">124.00</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Ngambay</th>\n<td class=\"ltx_td ltx_align_center\">1.03</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Igbo</th>\n<td class=\"ltx_td ltx_align_center\">433.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">25.00</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Kikuyu</th>\n<td class=\"ltx_td ltx_align_center\">66.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">44.00</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mandinka</th>\n<td class=\"ltx_td ltx_align_center\">0.41</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Shona</th>\n<td class=\"ltx_td ltx_align_center\">428.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">103.00</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Wolof</th>\n<td class=\"ltx_td ltx_align_center\">57.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">183.20</td>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Fang</th>\n<td class=\"ltx_td ltx_align_center\">0.02</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">Kinyarwanda</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">283.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">3,839.00</td>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_b\"/>\n<td class=\"ltx_td ltx_border_b\"/>\n<td class=\"ltx_td ltx_border_b ltx_border_r\"/>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_b\"/>\n<td class=\"ltx_td ltx_border_b\"/>\n<td class=\"ltx_td ltx_border_b\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "umbundu",
            "ewe",
            "luganda",
            "malagasy",
            "xhosa",
            "available",
            "fon",
            "tshiluba",
            "igbo",
            "tswana",
            "kikuyu",
            "hours",
            "zulu",
            "our",
            "across",
            "swahili",
            "african",
            "sorted",
            "shona",
            "indicate",
            "kiluba",
            "ngambay",
            "fula",
            "kikongo",
            "afrikaans",
            "lingala",
            "language",
            "sesotho",
            "mandinka",
            "distribution",
            "kanuri",
            "dataset",
            "dashes",
            "arabic",
            "wolof",
            "token",
            "millions",
            "mossi",
            "count",
            "somali",
            "amharic",
            "chewa",
            "showing",
            "oromo",
            "fang",
            "yoruba",
            "composition",
            "twi",
            "berber",
            "bemba",
            "tigrinya",
            "krio",
            "kinyarwanda",
            "bambara",
            "hausa",
            "languages",
            "data",
            "tokens",
            "audio",
            "rundi"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our comprehensive data collection yielded 19,012,019,696 tokens of monolingual text and 12,628 hours of aligned audio across 40 African languages (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Our analysis reveals distinct stratification patterns highlighting the digital divide within African languages themselves.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite representing nearly one-third of the world&#8217;s languages, African languages remain critically underserved by modern NLP technologies, with 88% classified as severely underrepresented or completely ignored in computational linguistics. We present the African Languages Lab (All Lab), a comprehensive research initiative that addresses this technological gap through systematic data collection, model development, and capacity building. Our contributions include: (1) a quality-controlled data collection pipeline, yielding the largest validated African multi-modal speech and text dataset spanning 40 languages with 19 billion tokens of monolingual text and 12,628 hours of aligned speech data; (2) extensive experimental validation demonstrating that our dataset, combined with fine-tuning, achieves substantial improvements over baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages; and (3) a structured research program that has successfully mentored fifteen early-career researchers, establishing sustainable local capacity. Our comparative evaluation against Google Translate reveals competitive performance in several languages while identifying areas that require continued development.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>To promote accessibility, we provide translations of this abstract in 10 African languages in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#A2\" title=\"Appendix B Abstract Translations &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, generated using our fine-tuned models.</span></span></span></p>\n\n",
                "matched_terms": [
                    "across",
                    "african",
                    "dataset",
                    "hours",
                    "languages",
                    "data",
                    "tokens",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP</span>\n</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The promise of artificial intelligence (AI) and natural language processing (NLP) to democratize information access remains unfulfilled for billions of speakers worldwide. Among the approximately 7,000 languages spoken globally, fewer than 20 receive substantial attention in NLP research <cite class=\"ltx_cite ltx_citemacro_citep\">(Magueresse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib27\" title=\"\">2020</a>)</cite>. This technological marginalization particularly affects low-resource languages (LRLs). Without a clearly established definition, LRLs are languages that exist at the periphery of the digital transformation, characterized by three critical deficits: (1) a scarcity of machine-readable corpora, (2) limited personalized computational technologies and trained language models, and (3) insufficient representation in global research communities <cite class=\"ltx_cite ltx_citemacro_citep\">(Nigatu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib32\" title=\"\">2024</a>; Issaka et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib23\" title=\"\">2024</a>; Magueresse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib27\" title=\"\">2020</a>)</cite>.\nWhile often serving substantial speaker populations, these languages face significant challenges in participating fully in the AI-driven information economy.</p>\n\n",
                "matched_terms": [
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Africa, the scale of this crisis is staggering: over 2,000 languages are spoken across Africa (nearly one-third of all languages worldwide). Yet, a stunning 88% of African languages are \"severely underrepresented\" or \"completely ignored\" in computational linguistics <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib24\" title=\"\">2020</a>)</cite>. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S2.F1\" title=\"Figure 1 &#8227; 2.1 Community-Driven Research Initiatives &#8227; 2 Related Work &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, approximately 814 African languages are in danger of extinction. Countries like Nigeria, Cameroon, and the Ivory Coast have 171, 75, and 65 languages facing the most severe threats, respectively <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.ethnologue.com/\" title=\"\">https://www.ethnologue.com/</a></span></span></span>. This exclusion has far-reaching consequences, from poor educational and healthcare outcomes to preventing full participation in the digital economy <cite class=\"ltx_cite ltx_citemacro_citep\">(Laitin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib25\" title=\"\">2019</a>; Gessler and von&#160;der Wense, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "african",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This problem is compounded by a severe underrepresentation in the global NLP research community. Our analysis of mentions of the top 10 global languages versus the top 10 African languages across major academic databases reveals a stark imbalance. On average, for every paper discussing African languages in multilingual LLM contexts, there are 20 papers on global languages in Google Scholar (GS), 23 in COnnecting REpositories(CORE), 34 in arXiv, and 70 in The Institute of Electrical and Electronics Engineers (IEEE) (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Aggregated Paper Count Approach &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> in the Appendix). This 20-70x representation gap reinforces a self-perpetuating cycle of marginalization where limited research attention leads to poor technological support, which in turn discourages further research investment.</p>\n\n",
                "matched_terms": [
                    "african",
                    "across",
                    "our",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Contributing to broader efforts to bridge this systemic technological gap, we present the African Languages Lab (All Lab), an initiative to democratize NLP technology for African languages. Founded in 2020, the All Lab operates through a coordinated team of dedicated researchers who combine three innovative elements:</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Systematic data infrastructure:</span> We developed a systematic, quality-controlled data collection framework powered by our \"All Voices\" platform. All Voices is a mobile-first platform specifically designed for community-driven multilingual data collection in low-resource contexts, enabling direct translation between African languages without English intermediation</p>\n\n",
                "matched_terms": [
                    "african",
                    "data",
                    "our",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comprehensive dataset development:</span> Through coordinated collection and validation efforts, we assembled the largest multi-modal dataset for African LRLs, encompassing 19 billion tokens across 40 languages with 12,628 hours of aligned speech data.</p>\n\n",
                "matched_terms": [
                    "across",
                    "african",
                    "dataset",
                    "hours",
                    "languages",
                    "data",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Empirical validation and capacity building:</span> We demonstrate the effectiveness of our approach through extensive experiments showing average improvements of +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points, while simultaneously developing local research capacity through structured mentorship programs.</p>\n\n",
                "matched_terms": [
                    "showing",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The landscape of African NLP research has evolved through three interconnected streams: community-driven initiatives, advances in multilingual modeling, and the development of evaluation frameworks. We examine how these efforts have shaped current capabilities and identify gaps our work addresses.</p>\n\n",
                "matched_terms": [
                    "african",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of African NLP has been shaped by complementary community and institutional efforts. Masakhane, comprising over 3,000 Slack members, exemplifies successful community-driven research, demonstrating that participatory approaches can produce high-quality datasets and models <cite class=\"ltx_cite ltx_citemacro_citep\">(Orife et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib35\" title=\"\">2020</a>)</cite>. Complementing this work, the \"Breaking the Unwritten Language Barrier\" project addresses challenges specific to unwritten and under-documented languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adda et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib1\" title=\"\">2016</a>)</cite>. Their work on languages like Basaa, Myene, and Embosi has established methodological approaches for speech recognition in LRLs.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These community efforts have been supported by institutional initiatives providing essential infrastructure. The Lacuna Fund has enabled dataset development <cite class=\"ltx_cite ltx_citemacro_citep\">(Rathi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib37\" title=\"\">2023</a>)</cite>, while Meta&#8217;s No Language Left Behind project has contributed architectural innovations for massively multilingual models <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib47\" title=\"\">2022</a>)</cite>. Additional infrastructure support has come from Mozilla&#8217;s Common Voice project <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib8\" title=\"\">2020</a>)</cite> for speech resources and the AI4D African Language Program <cite class=\"ltx_cite ltx_citemacro_citep\">(Siminyu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib42\" title=\"\">2021</a>)</cite> for benchmark development. The Deep Learning Indaba<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://deeplearningindaba.com/\" title=\"\">https://deeplearningindaba.com/</a></span></span></span> has contributed to research capacity building through its convenings, while platforms like Lanfrica have improved resource discoverability and research sharing across the continent <cite class=\"ltx_cite ltx_citemacro_citep\">(Emezue and Dossou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib17\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "dataset",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evolution of multilingual LLMs has shown steady progress in language coverage and capabilities. Early approaches like mBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Muller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib30\" title=\"\">2021</a>)</cite> and XLM-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib12\" title=\"\">2020</a>)</cite> established initial benchmarks, supporting approximately 100 languages each. Subsequent developments included more focused models like mBART <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib26\" title=\"\">2020</a>)</cite>, mT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xue et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib57\" title=\"\">2021</a>)</cite>, and XGLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Ersoy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib19\" title=\"\">2023</a>)</cite>, which traded broader language coverage for improved performance on specific language sets. The advent of massive LLMs further expanded these capabilities, with models like GPT-3, mGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Shliazhko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib41\" title=\"\">2024</a>)</cite>, and BLOOM <cite class=\"ltx_cite ltx_citemacro_citep\">(Workshop et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib56\" title=\"\">2023</a>)</cite> supporting varying numbers of African languages. Also, Glot500-m <cite class=\"ltx_cite ltx_citemacro_citep\">(Imani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib22\" title=\"\">2023</a>)</cite> extends support to 511 languages and the SERENGETI and Cheetah models supports about 517 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adebara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib4\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib3\" title=\"\">2024</a>)</cite>. Additional progress has come from the Aya model, which demonstrates instruction-following capabilities across 101 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(&#220;st&#252;n et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib59\" title=\"\">2024</a>)</cite>, and specialized models like AfroLM, which focuses on 23 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Dossou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib14\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While not specifically trained in African languages, English-centric LLMs such as GPT-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib34\" title=\"\">2024</a>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib46\" title=\"\">2025</a>)</cite>, and Llama <cite class=\"ltx_cite ltx_citemacro_citep\">(Wendler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib53\" title=\"\">2024</a>)</cite> have shown capability in handling some African languages, <cite class=\"ltx_cite ltx_citemacro_citep\">(Robinson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib40\" title=\"\">2023</a>; Ojo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib33\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib58\" title=\"\">2024</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib13\" title=\"\">2024</a>)</cite>, though their performance generally does not match that of specialized models; underscoring the need for dedicated resources and architectures.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of evaluation frameworks has enabled systematic progress measurement in African NLP across diverse task domains. MasakhaNER provides NER datasets for 10 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib5\" title=\"\">2021</a>)</cite>, AfriSenti offers sentiment analysis benchmarks in 14 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib28\" title=\"\">2023</a>)</cite>, and AFROMT establishes standardized translation benchmarks for 8 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Reid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib39\" title=\"\">2021</a>)</cite>. IrokoBench unifies evaluation across natural language inference, mathematical reasoning, and multiple-choice QA in 17 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib6\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "across",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More targeted evaluation resources include NaijaSenti for Nigerian languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib29\" title=\"\">2022</a>)</cite> and Kencorpus for Kenyan languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Wanjawa et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib52\" title=\"\">2023</a>)</cite>. These Africa-focused frameworks complement broader initiatives like FLORES200 <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib47\" title=\"\">2022</a>)</cite>, the Aya Dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib44\" title=\"\">2024b</a>)</cite>, and Global-MMLU <cite class=\"ltx_cite ltx_citemacro_citep\">(Singh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib43\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these developments and advances, significant challenges remain in African NLP research <cite class=\"ltx_cite ltx_citemacro_citep\">(Adebara and Abdul-Mageed, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib2\" title=\"\">2022</a>; Issaka et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib23\" title=\"\">2024</a>)</cite>. Our work builds upon these foundations while addressing several key limitations in existing approaches, such as robust team coordination, cross-initiative knowledge transfer, deduplication of efforts, and intentional skill set development.</p>\n\n",
                "matched_terms": [
                    "african",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">All Voices Platform.</span> To address the fundamental challenge of data scarcity in African languages, we developed All Voices, a mobile-first platform that stands as the only solution specifically designed for data collection in any LRL. The platform&#8217;s innovative approach enables direct translation between LRLs without requiring English as an intermediary, addressing a critical gap in the existing data collection infrastructure. In addition, All Voices distinguishes itself through its multimodal capabilities, which support the collection and validation of text and audio data. The platform features an intuitive, user-friendly interface that encourages broad participation, complemented by gamification elements, including a global leaderboard system that promotes user engagement. Importantly, All Voices is open and free to everyone, aligning with our mission to democratize language technology development. All Voices contributors provide informed consent for their contributions to be used for research purposes, including dataset creation, model training, and open-source distribution, with full transparency regarding data usage and the right to withdraw consent at any time.</p>\n\n",
                "matched_terms": [
                    "language",
                    "african",
                    "distribution",
                    "dataset",
                    "languages",
                    "data",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The platform&#8217;s architecture, built using ReactNative&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://reactnative.dev/\" title=\"\">https://reactnative.dev/</a></span></span></span> and Firebase&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://firebase.google.com/\" title=\"\">https://firebase.google.com/</a></span></span></span>, integrates user authentication and analytics, translation corpus management, and quality control components. Our authentication system provides comprehensive user profiling, tracking contributor demographics and expertise through quantifiable metrics, including successful translations and community validation scores. This system implements OAuth 2.0 authentication and role-based access control to ensure data integrity and user privacy. The translation corpus management system centrally stores both text and audio translations along with their metadata, and protects all data using AES-256 encryption at rest and TLS 1.3 during transmission. Translations undergo peer review requiring both a minimum threshold of positive validation (&gt;5 upvotes) and an acceptable error margin (&lt;3 downvotes) to achieve verified status. A key innovation is our recursive translation pipeline: verified translations become eligible source material for subsequent translations, creating a multiplicative effect in data collection.</p>\n\n",
                "matched_terms": [
                    "data",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Collection and Processing.</span> Our dataset development methodology combines crowd-sourced translations through All Voices (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 Datasets &#8227; 3 Methodology &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) with carefully curated open-source corpora. We integrate validated translations from our platform with established datasets, including NLLB <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib47\" title=\"\">2022</a>)</cite>, CCMatrix <cite class=\"ltx_cite ltx_citemacro_citep\">(Wenzek et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib54\" title=\"\">2019</a>)</cite>, OpenSubtitles <cite class=\"ltx_cite ltx_citemacro_citep\">(Tiedemann, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib48\" title=\"\">2016</a>)</cite>, MultiCCAligned <cite class=\"ltx_cite ltx_citemacro_citep\">(El-Kishky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib15\" title=\"\">2020</a>)</cite>, ParaCrawl <cite class=\"ltx_cite ltx_citemacro_citep\">(Ba&#241;&#243;n et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib10\" title=\"\">2020</a>)</cite>, XLEnt <cite class=\"ltx_cite ltx_citemacro_citep\">(El-Kishky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib16\" title=\"\">2021</a>)</cite>, MultiParaCrawl<cite class=\"ltx_cite ltx_citemacro_citep\">(Ba&#241;&#243;n et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib10\" title=\"\">2020</a>)</cite>, LinguaTools-WikiTitles <cite class=\"ltx_cite ltx_citemacro_citep\">(Tiedemann, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib49\" title=\"\">2012</a>)</cite>, and CCAligned <cite class=\"ltx_cite ltx_citemacro_citep\">(El-Kishky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib15\" title=\"\">2020</a>)</cite>. Additionally, we collect new datasets through our community partners.</p>\n\n",
                "matched_terms": [
                    "data",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data processing implements a robust two-tier approach combining general normalization with language-specific processing. The general normalization phase addresses universal text artifacts through Unicode normalization, character encoding standardization, and structural cleaning, including HTML removal and symbol standardization. The language-specific processing phase implements specialized handling for African language features, including morphological analysis, script variant normalization, and tone mark standardization, language identification, with custom rule sets developed for specific language families.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, our translation validation methodology implements a robust statistical framework for assessing translation quality through quantitative analysis of character-level distributions. The validation metric employs character ratio analysis between source and target texts, computed as the ratio of target text length to source text length. We analyze these ratios using z-score normalization within language-specific distribution, enabling the detection of statistical outliers while accounting for natural variations in text length across different language pairs. This approach is augmented with character overlap detection to identify potential artifacts or inappropriate text preservation, particularly crucial for languages sharing similar orthographic features.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "distribution",
                    "languages",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Also, the threshold determination process implements an adaptive sampling methodology. For each language pair, we establish baseline distributions through initial sampling of 10,000 translation pairs, employing Kernel Density Estimation for robust distribution modeling. This approach effectively captures the non-Gaussian characteristics frequently observed in cross-lingual character distributions. Thresholds are dynamically computed using a modified Tukey method with an adaptive multiplier. This adaptive threshold mechanism automatically calibrates to language-specific characteristics, implementing more stringent filtering for language pairs that exhibit consistent ratios while allowing appropriate flexibility for pairs with inherently higher variability. The resulting validation framework effectively identifies and filters anomalous translations while maintaining sensitivity to legitimate linguistic variations across diverse African language families. The processed data sets are structured according to HuggingFace&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/\" title=\"\">https://huggingface.co/</a></span></span></span> Dataset specifications, enabling seamless API integration.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "african",
                    "distribution",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the utility of our dataset and establish baselines for our languages, we experimented with Llama-3.2-1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib21\" title=\"\">2024</a>)</cite>. We chose this model as our base because of its demonstrated multilingual capabilities and efficient parameter scaling, making it suitable for LRLs. Critically, while capable of tokenizing, this model has not been explicitly trained on any African language in our dataset. Thus, providing a cleaner baseline for measuring the utility of our dataset during fine-tuning without risk of data contamination.</p>\n\n",
                "matched_terms": [
                    "language",
                    "african",
                    "dataset",
                    "languages",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed full fine-tuning rather than parameter-efficient methods, as preliminary experiments with Quantization-aware Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_citep\">(&#220;st&#252;n et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib59\" title=\"\">2024</a>)</cite> yielded insufficient performance gains for our target languages. Our training pipeline uses supervised learning with a standardized instruction template: \"Translate the following English text to X:\", where X denotes the target African language.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "our",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training leveraged NVIDIA H100 GPUs with the following parameters: batch size of 64 with 4-step gradient accumulation, maximum sequence length of 1024 tokens for both input and output, single epoch training using all available parallel data per language, learning rate of <math alttext=\"5.0\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mn>5.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5.0\\times 10^{-5}</annotation></semantics></math> with cosine scheduling and 0.15 warm-up ratio, and BF16 mixed precision for memory efficiency. During inference, we maintained consistency with batch size 64 while using temperature 0.1, top-p 0.95, top-k 50, and maximum output length of 1024 tokens to balance diversity with quality and reproducibility.</p>\n\n",
                "matched_terms": [
                    "language",
                    "tokens",
                    "available",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model performance was evaluated using a complementary set of metrics: BiLingual Evaluation Understudy (BLEU) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wieting et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib55\" title=\"\">2019</a>)</cite>, which measures n-gram precision; METEOR <cite class=\"ltx_cite ltx_citemacro_citep\">(Banerjee and Lavie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib9\" title=\"\">2005</a>)</cite>, which accounts for word stems and synonyms; COMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib38\" title=\"\">2020</a>)</cite>, which leverages multilingual embeddings to assess semantic similarity; and ChrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib51\" title=\"\">2025</a>)</cite>, which operates on character-level n-grams to better capture morphological variations common in African languages. Additionally, we employed Translation Edit Rate (TER) <cite class=\"ltx_cite ltx_citemacro_citep\">(Snover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib45\" title=\"\">2006</a>)</cite>, which quantifies the minimum number of edits required to transform the hypothesis into the reference translation (where lower scores indicate better quality), and AfriCOMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib50\" title=\"\">2024</a>)</cite>, a neural metric specifically trained on African language pairs to better capture language-specific quality nuances.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages",
                    "language",
                    "indicate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these metrics comprehensively assess translation quality across different linguistic aspects, from surface-level n-gram matching to semantic preservation and post-editing effort. We utilized the FLORES-200 dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib47\" title=\"\">2022</a>)</cite> as our standardized test set, ensuring consistency across all languages and enabling direct comparison with other multilingual systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "dataset",
                    "our",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From a text perspective (measured in millions of tokens), we observe four distinct tiers:</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "millions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Primary resource languages (&gt;2B tokens):</span> This tier includes Amharic (2.94B), Arabic (2.40B), Yoruba (2.36B), and Afrikaans (2.30B), reflecting sustained digitization efforts and strong institutional support.</p>\n\n",
                "matched_terms": [
                    "yoruba",
                    "tokens",
                    "arabic",
                    "languages",
                    "amharic",
                    "afrikaans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Established digital languages (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2B tokens):</span> Languages such as Hausa (1.54B) and Tigrinya (0.92B) demonstrate robust digital presence, likely owing to consistent documentation and preservation initiatives.</p>\n\n",
                "matched_terms": [
                    "hausa",
                    "tokens",
                    "tigrinya",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emerging digital languages (250M&#8211;1B tokens):</span> A substantial group including Malagasy (839.12M), Somali (751.13M), Swahili (700.39M), and Xhosa (563.07M), show growing digital footprints but still lagging behind the top tiers.</p>\n\n",
                "matched_terms": [
                    "swahili",
                    "malagasy",
                    "xhosa",
                    "languages",
                    "somali",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Resource-constrained languages (&lt;250M tokens):</span> The majority of languages in our dataset fall into this category, including widely spoken languages such as Bambara (109.49M) and Luganda (121.17M). This tier reflects substantial gaps in textual data availability.</p>\n\n",
                "matched_terms": [
                    "luganda",
                    "dataset",
                    "bambara",
                    "languages",
                    "data",
                    "tokens",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For audio resources (measured in hours), the stratification follows a different pattern, highlighting a distinct set of leading languages:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "hours",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">High-resource audio languages (&gt;1,000 hours):</span> Kinyarwanda (3,839.00h), Luganda (1,727.80h), Swahili (1,115.00h), and Arabic (2,721.52h) dominate in audio availability, often due to large-scale speech corpora or broadcast archives.</p>\n\n",
                "matched_terms": [
                    "swahili",
                    "luganda",
                    "arabic",
                    "hours",
                    "languages",
                    "audio",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Established audio languages (500&#8211;1,000 hours):</span> This tier is notably sparse, underscoring the scarcity of mid-scale speech datasets in African languages.</p>\n\n",
                "matched_terms": [
                    "african",
                    "audio",
                    "hours",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Moderate audio languages (100&#8211;500 hours):</span> Includes Malagasy (325.14h), Twi (227.03h), Bemba (230.30h), and Ewe (147.00h), representing a mix of widely spoken languages and those with targeted speech collection efforts.</p>\n\n",
                "matched_terms": [
                    "twi",
                    "ewe",
                    "bemba",
                    "malagasy",
                    "hours",
                    "languages",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-resource audio languages (&lt;100 hours):</span> Many languages, including Kikongo, Rundi, Kanuri, Umbundu, and Fang, have either minimal or no audio data.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "umbundu",
                    "kanuri",
                    "kikongo",
                    "hours",
                    "languages",
                    "data",
                    "fang",
                    "rundi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, our analysis underscores two parallel digital divides: a textual divide, where a small set of languages capture the majority of tokens, and an audio divide, where a different but equally narrow set of languages dominate. Notably, the top three languages by text volume account for a disproportionate share of tokens, while the top three by audio hours similarly capture the bulk of recorded speech. This imbalance highlights the urgent need for targeted development of both textual and audio resources, particularly for languages with substantial speaker populations but limited digital presence.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "languages",
                    "tokens",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation across 31 African languages reveals substantial and systematic improvements through fine-tuning, with distinct patterns emerging across language families and resource levels (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S4.T3\" title=\"Table 3 &#8227; 4 Results &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Nine languages were excluded from evaluation due to insufficient training data or absence from the FLORES-200 benchmark.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "african",
                    "languages",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance.</span> The base Llama-3.2-1B model demonstrates limited but non-trivial capability for African languages, revealing interesting patterns of cross-lingual transfer. ChrF++ scores range from 2.00 (Wolof) to 44.76 (Afrikaans), with a mean of 8.10, indicating minimal character-level understanding for most languages. COMET scores cluster between 0.16-0.68 (mean: 0.32), suggesting some semantic comprehension despite poor surface realization. Notably, Afrikaans shows exceptional baseline performance (ChrF++ 44.76, BLEU 32.98), leveraging its Germanic roots and Latin script. The extremely low baseline BLEU scores (mean: 2.27) across most languages confirm the model&#8217;s inability to produce accurate n-gram sequences without language-specific training.</p>\n\n",
                "matched_terms": [
                    "across",
                    "african",
                    "wolof",
                    "languages",
                    "afrikaans"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-tuning Impact.</span> Our dataset enables noticeable performance improvements, with average gains of +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points. The magnitude of improvement correlates inversely with baseline performance, suggesting effective transfer learning rather than simple memorization. Swahili exhibits the largest absolute ChrF++ improvement (+63.27 points), achieving near-parity with Google Translate (72.27 vs 75.81). Sesotho shows remarkable gains across all metrics (+61.79 ChrF++, +51.16 BLEU), while maintaining competitive performance against Google Translate. Languages with minimal baselines, including Fula, Wolof, and Kikongo, demonstrate that even severely under-resourced languages benefit substantially from targeted fine-tuning, achieving functional translation capability where none existed before.</p>\n\n",
                "matched_terms": [
                    "sesotho",
                    "swahili",
                    "across",
                    "dataset",
                    "wolof",
                    "languages",
                    "fula",
                    "kikongo",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparison with Google Translate.</span> Where available (22 languages), comparison with Google Translate reveals three distinct performance categories. First, languages where our models achieve competitive or superior performance: Yoruba (30.88 vs 21.05 ChrF++), Arabic (31.52 vs 28.46), and notably Twi, where we substantially outperform Google Translate (46.80 vs 31.48). Second, languages where Google maintains clear advantages, particularly in high-resource cases like Kinyarwanda (24.65 vs 70.27) and Swahili (72.27 vs 75.81), reflecting their extensive training data. Third, the languages where Google Translate offers no support, highlighting our contribution to genuinely LRL coverage.</p>\n\n",
                "matched_terms": [
                    "yoruba",
                    "twi",
                    "swahili",
                    "available",
                    "arabic",
                    "languages",
                    "data",
                    "our",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language-Specific Patterns.</span> Three response profiles emerge from our analysis. High-responder languages (Swahili, Sesotho, Hausa) show dramatic improvements exceeding 40 ChrF++ points, suggesting optimal alignment between our dataset characteristics and model architecture. Steady improvers (Igbo, Shona, Somali) demonstrate consistent gains of 25-30 points across metrics, indicating robust but not exceptional adaptation. Challenging cases (Fon, Wolof, Bambara) show limited improvements despite fine-tuning, likely requiring specialized tokenization or architectural modifications to address their unique linguistic features.</p>\n\n",
                "matched_terms": [
                    "sesotho",
                    "across",
                    "swahili",
                    "fon",
                    "igbo",
                    "dataset",
                    "wolof",
                    "shona",
                    "bambara",
                    "hausa",
                    "languages",
                    "somali",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation Edit Rate Analysis.</span> The dramatic TER reductions, averaging 580.13 points lower after fine-tuning, provide crucial practical insights. Languages like Swahili achieve TER scores comparable to Google Translate (23.77), indicating production-ready quality requiring minimal post-editing. Even languages with modest BLEU improvements show substantial TER reductions, suggesting improved fluency and coherence that traditional metrics may not fully capture. This pattern holds particular significance for scenarios where post-editing cost determines practical viability.</p>\n\n",
                "matched_terms": [
                    "swahili",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Correlations.</span> While surface metrics (BLEU, ChrF++) show high correlation, the divergence between these and neural metrics (COMET, AfriCOMET) reveals important quality dimensions. Languages like Ewe show minimal COMET improvement (0.04) despite substantial ChrF++ gains (30.43 points), suggesting character-level improvements without the corresponding semantic enhancement. In contrast, Arabic shows strong COMET gains (0.33) with modest improvement in ChrF++, indicating semantic preservation despite surface-level challenges. These patterns underscore the importance of multi-metric evaluation for morphologically diverse African languages.</p>\n\n",
                "matched_terms": [
                    "ewe",
                    "african",
                    "languages",
                    "arabic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We have presented the African Languages Lab, a research initiative addressing the critical underrepresentation of African languages in NLP through systematic data collection, model development, and capacity building. Our contributions include a validated dataset of 19 billion tokens and 12,628 hours of aligned speech across 40 languages, substantial performance improvements averaging +23.69 ChrF++ over baseline models, the All Voices platform, and a structured mentorship program developing fifteen early-career researchers. Thus, we demonstrate that the technological marginalization of African languages, while severe, is not intractable.</p>\n\n",
                "matched_terms": [
                    "across",
                    "african",
                    "dataset",
                    "hours",
                    "languages",
                    "data",
                    "tokens",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As language technologies increasingly mediate access to information, education, and economic opportunities, ensuring equitable coverage becomes not merely a technical challenge but a moral imperative. The African Languages Lab demonstrates that this imperative can be met through coordinated research, community engagement, and sustained investment in both technical infrastructure and human capacity, establishing a sustainable path forward for the world&#8217;s underserved linguistic communities.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments utilize Llama-3.2-1B as the sole base model, which, while demonstrating the utility of our dataset, may underestimate potential gains achievable with larger-scale architectures. The performance variance across language families, from 63.27 ChrF++ improvement for Swahili to minimal gains for Fon (-1.10), suggests that optimal model selection likely varies by linguistic typology. Additionally, our evaluation of 31 of 40 collected languages reflects FLORES-200 coverage limitations, potentially obscuring insights from the most critically under-resourced languages in our dataset.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "swahili",
                    "fon",
                    "dataset",
                    "languages",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite assembling 19 billion tokens, our dataset exhibits a 147,000&#215; disparity between the highest-resourced (Amharic: 2,944.95M tokens) and lowest-resourced (Fang: 0.02M tokens) languages. This imbalance directly correlates with performance outcomes: languages with &gt;1B tokens achieve average ChrF++ scores of 45.66, while those with &lt;100M tokens average 24.31. Furthermore, 13 languages lack audio data entirely, limiting multimodal model development. Our validation pipeline, while statistically grounded, operates without native speaker verification for 73% of languages, potentially missing dialectal variations that affect 28% of evaluated translations showing COMET-ChrF++ divergence exceeding 0.3.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "tokens",
                    "showing",
                    "dataset",
                    "languages",
                    "data",
                    "amharic",
                    "fang",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these constraints delineate several clear pathways for advancement. One direction is to explore architecture-specific optimizations for morphologically complex languages. Another is to implement active learning strategies that help address data imbalances. It is also important to develop evaluation metrics that are more sensitive to African language typologies. These limitations inform our ongoing work and highlight key areas for future research in African NLP.</p>\n\n",
                "matched_terms": [
                    "language",
                    "african",
                    "languages",
                    "data",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">They also underscore the need for continued investment in computational resources, human expertise, and infrastructure development to support comprehensive technology development for African languages.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data collection through the All Voices platform operates on principles of voluntary, consent-based participation with full revocability rights. Contributors are informed of their rights, with explicit consent obtained for research purposes including dataset creation, model training, and open-source distribution. The platform implements embedded reporting mechanisms for flagging offensive or culturally inappropriate content, with trained moderators reviewing submissions to maintain quality and cultural sensitivity. We acknowledge that automated validation procedures may miss dialectal nuances or culturally specific meanings, necessitating our ongoing collaboration with native speakers and community experts to expand linguistic coverage and cultural sensitivity.</p>\n\n",
                "matched_terms": [
                    "data",
                    "distribution",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset management follows principles of responsible data stewardship. While we aim for maximum openness, certain data components are subject to agreements with contributing communities that restrict fully public release. We maintain a managed access framework that provides dataset access to qualified researchers while respecting community rights and contributor agreements. Access requests are evaluated based on research purpose, institutional affiliation, and commitment to ethical use.</p>\n\n",
                "matched_terms": [
                    "data",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our structured research development program has mentored fifteen early-career researchers across four institutions through one-on-one mentorship, project development support, and transitions into extended research roles. This investment in local research leadership establishes sustainable capacity for African NLP development, ensuring that technical advancement aligns with cultural and linguistic expertise. By prioritizing skill development alongside technical innovation, we contribute to a sustainable talent pipeline that positions African researchers to lead future developments in their languages.</p>\n\n",
                "matched_terms": [
                    "african",
                    "across",
                    "our",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work directly advances United Nations Sustainable Development Goals in education and inequality reduction through increased digital representation of marginalized languages. The platform enables community-led content creation and facilitates open knowledge transfer, democratizing access to digital tools while preserving linguistic and cultural heritage. With 88% of African languages severely underrepresented or completely ignored in computational linguistics, and 814 languages facing extinction risk, our framework provides critical infrastructure for language preservation.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "our",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Guided by Ubuntu philosophy&#8212;emphasizing inclusivity, interdependence, and openness&#8212;we establish a framework for equitable NLP development. Our roadmap encompasses expanding language coverage, optimizing model architectures for low-resource languages, and deepening research collaborations. We acknowledge persistent challenges including limited commercial viability for some LRL technologies and infrastructural constraints, yet our results demonstrate that systematic community engagement can effectively address technological marginalization.\nThrough this comprehensive approach integrating technical innovation, cultural preservation, educational empowerment, and economic inclusion, we provide replicable models for equitable language technology development that can benefit millions of African language speakers while contributing to global linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "language",
                    "african",
                    "millions",
                    "languages",
                    "our"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP",
        "caption": "Table 3: Performance comparison between base Llama-3.2-1B (Llama1B), our Finetuned Llama-3.2-1B models (Ours), and Google Translate (GT) across different metrics (ChrF++, COMET, BLEU, Africomet, METEOR, and TER). Higher scores indicate better performance (except for TER, where lower scores are better).",
        "body": "Language\n\nChrF++ (↑\\uparrow)\n\nCOMET (↑\\uparrow)\n\nBLEU (↑\\uparrow)\n\nAfricomet (↑\\uparrow)\n\nMETEOR (↑\\uparrow)\n\nTER (↓\\downarrow)\n\n\nLlama1B\nOurs\nGT\nLlama1B\nOurs\nGT\nLlama1B\nOurs\nGT\nLlama1B\nOurs\nGT\nLlama1B\nOurs\nGT\nLlama1B\nOurs\nGT\n\n\n\n\nAmharic\n3.26\n28.69\n30.24\n0.38\n0.82\n0.88\n3.83\n12.16\n16.11\n0.14\n0.56\n0.72\n0.01\n0.30\n0.40\n58.40\n64.24\n58.40\n\n\nFula\n2.97\n18.73\n-\n0.32\n0.55\n-\n0.14\n5.73\n-\n0.03\n0.15\n-\n0.08\n0.06\n-\n1151.08\n72.22\n-\n\n\nYoruba\n3.77\n30.88\n21.05\n0.23\n0.67\n0.56\n0.41\n32.33\n11.96\n0.01\n0.60\n0.55\n0.05\n0.27\n0.16\n525.95\n84.31\n76.28\n\n\nIgbo\n5.30\n33.42\n45.92\n0.27\n0.71\n0.72\n0.65\n14.10\n36.09\n-0.70\n0.52\n0.57\n0.06\n0.41\n0.43\n523.93\n73.11\n52.80\n\n\nOromo\n11.30\n27.74\n54.93\n0.31\n0.70\n0.80\n5.54\n5.72\n33.57\n0.13\n0.29\n0.66\n0.06\n0.12\n0.29\n77.62\n119.01\n56.92\n\n\nSwahili\n9.0\n72.27\n75.81\n0.40\n0.78\n0.85\n0.54\n56.23\n54.81\n-0.20\n0.62\n0.72\n0.07\n0.57\n0.65\n1021.93\n23.77\n23.77\n\n\nHausa\n7.75\n51.77\n54.60\n0.39\n0.70\n0.80\n0.57\n22.38\n45.49\n0.01\n0.47\n0.64\n0.07\n0.41\n0.53\n993.66\n52.3\n64.37\n\n\nTwi(Asante)\n4.0\n46.80\n31.48\n0.26\n0.71\n0.71\n0.29\n27.36\n17.81\n-0.06\n0.30\n0.36\n0.07\n0.29\n0.33\n656.31\n49.12\n64.24\n\n\nShona\n7.52\n36.55\n51.93\n0.27\n0.60\n0.63\n0.25\n12.06\n18.05\n0.01\n0.47\n0.59\n0.07\n0.28\n0.34\n1318.15\n99.15\n64.16\n\n\nKinyarwanda\n5.62\n24.65\n70.27\n0.28\n0.56\n0.67\n0.37\n17.25\n49.60\n-0.18\n0.40\n0.66\n0.04\n0.23\n0.48\n428.45\n65.53\n30.24\n\n\nEwe\n3.05\n33.48\n47.86\n0.22\n0.26\n0.37\n0.27\n31.55\n25.79\n0.06\n0.24\n0.33\n0.07\n0.26\n0.37\n689.73\n93.32\n48.69\n\n\nBambara\n4.06\n15.60\n27.02\n0.25\n0.65\n0.72\n0.18\n22.50\n13.60\n0.08\n0.22\n0.39\n0.08\n0.20\n0.29\n766.87\n49.33\n112.12\n\n\nWolof\n2.00\n12.01\n-\n0.30\n0.61\n-\n0.16\n2.68\n-\n0.07\n0.25\n-\n0.07\n0.28\n-\n1030.14\n84.83\n-\n\n\nLuganda\n7.08\n24.91\n23.55\n0.28\n0.64\n0.63\n0.41\n3.84\n3.96\n-0.05\n0.55\n0.57\n0.07\n0.31\n0.33\n980.96\n74.57\n63.10\n\n\nArabic\n8.67\n31.52\n28.46\n0.52\n0.85\n0.89\n0.30\n23.36\n21.41\n0.21\n0.70\n0.77\n0.08\n0.52\n0.55\n1164.70\n67.00\n67.00\n\n\nSomali\n7.11\n35.64\n47.32\n0.37\n0.76\n0.80\n0.41\n8.99\n12.90\n0.03\n0.51\n0.62\n0.07\n0.32\n0.37\n442.60\n65.09\n60.75\n\n\nAfrkaans\n44.76\n48.07\n52.10\n0.68\n0.86\n0.85\n32.98\n18.00\n27.25\n0.37\n0.74\n0.73\n0.35\n0.66\n0.65\n52.61\n48.23\n39.46\n\n\nTigrinya\n5.43\n24.12\n23.75\n0.27\n0.76\n0.83\n2.77\n13.65\n13.00\n0.04\n0.43\n0.70\n0.01\n0.16\n0.23\n78.33\n36.55\n78.33\n\n\nMalagasy\n9.39\n26.29\n43.06\n0.42\n0.80\n0.76\n0.32\n22.49\n9.10\n0.13\n0.61\n0.45\n0.06\n0.39\n0.36\n856.18\n57.08\n57.08\n\n\nXhosa\n6.58\n46.17\n47.88\n0.35\n0.75\n0.70\n0.63\n12.87\n14.95\n0.21\n0.55\n0.41\n0.09\n0.33\n0.31\n561.279\n71.79\n58.74\n\n\nZulu\n7.58\n38.06\n64.45\n0.32\n0.76\n0.73\n0.37\n5.90\n34.57\n0.09\n0.62\n0.54\n0.07\n0.41\n0.35\n728.98\n89.52\n51.16\n\n\nSesotho\n7.09\n61.79\n57.01\n0.28\n0.61\n0.66\n0.14\n51.16\n33.25\n0.14\n0.59\n0.65\n0.08\n0.31\n0.44\n1475.22\n63.22\n45.66\n\n\nChewa\n4.76\n28.57\n34.29\n0.28\n0.62\n0.61\n0.0\n13.00\n12.67\n-0.02\n0.56\n0.44\n0.07\n0.35\n0.31\n876.28\n72.16\n67.01\n\n\nLingala\n5.91\n33.32\n38.14\n0.30\n0.58\n0.65\n0.23\n18.21\n16.82\n0.00\n0.12\n0.40\n0.08\n0.23\n0.45\n955.39\n80.32\n54.96\n\n\nTswana\n8.59\n26.37\n-\n0.28\n0.63\n-\n0.69\n16.98\n-\n0.11\n0.38\n-\n0.07\n0.27\n-\n413.87\n65.69\n-\n\n\nFon\n10.57\n9.47\n-\n0.16\n0.60\n-\n5.14\n6.50\n-\n0.14\n0.04\n-\n0.06\n0.06\n-\n56.15\n29.73\n-\n\n\nKikuyu\n9.77\n25.80\n-\n0.27\n0.55\n-\n0.28\n17.92\n-\n0.03\n0.03\n-\n0.07\n0.09\n-\n943.70\n59.38\n-\n\n\nTshiluba\n15.68\n22.10\n-\n0.31\n0.55\n-\n5.856\n7.44\n-\n0.20\n0.12\n-\n0.11\n0.11\n-\n61.49\n52.71\n-\n\n\nMossi\n12.82\n23.99\n-\n0.28\n0.51\n-\n5.86\n9.62\n-\n0.18\n-0.03\n-\n0.08\n0.09\n-\n57.02\n81.46\n-\n\n\nKikongo\n6.18\n20.84\n-\n0.31\n0.55\n-\n0.38\n6.15\n-\n-0.05\n0.07\n-\n0.05\n0.16\n-\n318.97\n44.51\n-\n\n\nBemba\n3.59\n25.79\n-\n0.31\n0.46\n-\n0.25\n27.69\n-\n0.09\n0.07\n-\n0.10\n0.10\n-\n755.92\n48.63\n-\n\n\nAverage\n8.10\n31.79\n44.14\n0.32\n0.65\n0.72\n2.27\n17.61\n23.76\n0.04\n0.38\n0.57\n0.08\n0.28\n0.39\n645.87\n65.74\n58.87",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Language</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\">\n<span class=\"ltx_text ltx_font_bold\">ChrF++</span>&#160;(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\">\n<span class=\"ltx_text ltx_font_bold\">COMET</span>&#160;(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\">\n<span class=\"ltx_text ltx_font_bold\">BLEU</span>&#160;(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\">\n<span class=\"ltx_text ltx_font_bold\">Africomet</span>&#160;(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\">\n<span class=\"ltx_text ltx_font_bold\">METEOR</span>&#160;(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\">\n<span class=\"ltx_text ltx_font_bold\">TER</span>&#160;(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Llama1B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">GT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Llama1B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">GT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Llama1B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">GT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Llama1B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">GT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Llama1B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">GT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Llama1B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">GT</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Amharic</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">30.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Fula</span></th>\n<td class=\"ltx_td ltx_align_center\">2.97</td>\n<td class=\"ltx_td ltx_align_center\">18.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.32</td>\n<td class=\"ltx_td ltx_align_center\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.14</td>\n<td class=\"ltx_td ltx_align_center\">5.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.03</td>\n<td class=\"ltx_td ltx_align_center\">0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center\">0.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">1151.08</td>\n<td class=\"ltx_td ltx_align_center\">72.22</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Yoruba</span></th>\n<td class=\"ltx_td ltx_align_center\">3.77</td>\n<td class=\"ltx_td ltx_align_center\">30.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">21.05</td>\n<td class=\"ltx_td ltx_align_center\">0.23</td>\n<td class=\"ltx_td ltx_align_center\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.56</td>\n<td class=\"ltx_td ltx_align_center\">0.41</td>\n<td class=\"ltx_td ltx_align_center\">32.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">11.96</td>\n<td class=\"ltx_td ltx_align_center\">0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.55</td>\n<td class=\"ltx_td ltx_align_center\">0.05</td>\n<td class=\"ltx_td ltx_align_center\">0.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.16</td>\n<td class=\"ltx_td ltx_align_center\">525.95</td>\n<td class=\"ltx_td ltx_align_center\">84.31</td>\n<td class=\"ltx_td ltx_align_center\">76.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Igbo</span></th>\n<td class=\"ltx_td ltx_align_center\">5.30</td>\n<td class=\"ltx_td ltx_align_center\">33.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">45.92</td>\n<td class=\"ltx_td ltx_align_center\">0.27</td>\n<td class=\"ltx_td ltx_align_center\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.72</td>\n<td class=\"ltx_td ltx_align_center\">0.65</td>\n<td class=\"ltx_td ltx_align_center\">14.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">36.09</td>\n<td class=\"ltx_td ltx_align_center\">-0.70</td>\n<td class=\"ltx_td ltx_align_center\">0.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.57</td>\n<td class=\"ltx_td ltx_align_center\">0.06</td>\n<td class=\"ltx_td ltx_align_center\">0.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.43</td>\n<td class=\"ltx_td ltx_align_center\">523.93</td>\n<td class=\"ltx_td ltx_align_center\">73.11</td>\n<td class=\"ltx_td ltx_align_center\">52.80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Oromo</span></th>\n<td class=\"ltx_td ltx_align_center\">11.30</td>\n<td class=\"ltx_td ltx_align_center\">27.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">54.93</td>\n<td class=\"ltx_td ltx_align_center\">0.31</td>\n<td class=\"ltx_td ltx_align_center\">0.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.80</td>\n<td class=\"ltx_td ltx_align_center\">5.54</td>\n<td class=\"ltx_td ltx_align_center\">5.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">33.57</td>\n<td class=\"ltx_td ltx_align_center\">0.13</td>\n<td class=\"ltx_td ltx_align_center\">0.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.66</td>\n<td class=\"ltx_td ltx_align_center\">0.06</td>\n<td class=\"ltx_td ltx_align_center\">0.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.29</td>\n<td class=\"ltx_td ltx_align_center\">77.62</td>\n<td class=\"ltx_td ltx_align_center\">119.01</td>\n<td class=\"ltx_td ltx_align_center\">56.92</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Swahili</span></th>\n<td class=\"ltx_td ltx_align_center\">9.0</td>\n<td class=\"ltx_td ltx_align_center\">72.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">75.81</td>\n<td class=\"ltx_td ltx_align_center\">0.40</td>\n<td class=\"ltx_td ltx_align_center\">0.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.85</td>\n<td class=\"ltx_td ltx_align_center\">0.54</td>\n<td class=\"ltx_td ltx_align_center\">56.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">54.81</td>\n<td class=\"ltx_td ltx_align_center\">-0.20</td>\n<td class=\"ltx_td ltx_align_center\">0.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.72</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.65</td>\n<td class=\"ltx_td ltx_align_center\">1021.93</td>\n<td class=\"ltx_td ltx_align_center\">23.77</td>\n<td class=\"ltx_td ltx_align_center\">23.77</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Hausa</span></th>\n<td class=\"ltx_td ltx_align_center\">7.75</td>\n<td class=\"ltx_td ltx_align_center\">51.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">54.60</td>\n<td class=\"ltx_td ltx_align_center\">0.39</td>\n<td class=\"ltx_td ltx_align_center\">0.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.80</td>\n<td class=\"ltx_td ltx_align_center\">0.57</td>\n<td class=\"ltx_td ltx_align_center\">22.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">45.49</td>\n<td class=\"ltx_td ltx_align_center\">0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.64</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.53</td>\n<td class=\"ltx_td ltx_align_center\">993.66</td>\n<td class=\"ltx_td ltx_align_center\">52.3</td>\n<td class=\"ltx_td ltx_align_center\">64.37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Twi(Asante)</span></th>\n<td class=\"ltx_td ltx_align_center\">4.0</td>\n<td class=\"ltx_td ltx_align_center\">46.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">31.48</td>\n<td class=\"ltx_td ltx_align_center\">0.26</td>\n<td class=\"ltx_td ltx_align_center\">0.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.71</td>\n<td class=\"ltx_td ltx_align_center\">0.29</td>\n<td class=\"ltx_td ltx_align_center\">27.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">17.81</td>\n<td class=\"ltx_td ltx_align_center\">-0.06</td>\n<td class=\"ltx_td ltx_align_center\">0.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.36</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.33</td>\n<td class=\"ltx_td ltx_align_center\">656.31</td>\n<td class=\"ltx_td ltx_align_center\">49.12</td>\n<td class=\"ltx_td ltx_align_center\">64.24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Shona</span></th>\n<td class=\"ltx_td ltx_align_center\">7.52</td>\n<td class=\"ltx_td ltx_align_center\">36.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">51.93</td>\n<td class=\"ltx_td ltx_align_center\">0.27</td>\n<td class=\"ltx_td ltx_align_center\">0.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.63</td>\n<td class=\"ltx_td ltx_align_center\">0.25</td>\n<td class=\"ltx_td ltx_align_center\">12.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">18.05</td>\n<td class=\"ltx_td ltx_align_center\">0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.59</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.34</td>\n<td class=\"ltx_td ltx_align_center\">1318.15</td>\n<td class=\"ltx_td ltx_align_center\">99.15</td>\n<td class=\"ltx_td ltx_align_center\">64.16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Kinyarwanda</span></th>\n<td class=\"ltx_td ltx_align_center\">5.62</td>\n<td class=\"ltx_td ltx_align_center\">24.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">70.27</td>\n<td class=\"ltx_td ltx_align_center\">0.28</td>\n<td class=\"ltx_td ltx_align_center\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.67</td>\n<td class=\"ltx_td ltx_align_center\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">17.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">49.60</td>\n<td class=\"ltx_td ltx_align_center\">-0.18</td>\n<td class=\"ltx_td ltx_align_center\">0.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.66</td>\n<td class=\"ltx_td ltx_align_center\">0.04</td>\n<td class=\"ltx_td ltx_align_center\">0.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.48</td>\n<td class=\"ltx_td ltx_align_center\">428.45</td>\n<td class=\"ltx_td ltx_align_center\">65.53</td>\n<td class=\"ltx_td ltx_align_center\">30.24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Ewe</span></th>\n<td class=\"ltx_td ltx_align_center\">3.05</td>\n<td class=\"ltx_td ltx_align_center\">33.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">47.86</td>\n<td class=\"ltx_td ltx_align_center\">0.22</td>\n<td class=\"ltx_td ltx_align_center\">0.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">0.27</td>\n<td class=\"ltx_td ltx_align_center\">31.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">25.79</td>\n<td class=\"ltx_td ltx_align_center\">0.06</td>\n<td class=\"ltx_td ltx_align_center\">0.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.33</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">689.73</td>\n<td class=\"ltx_td ltx_align_center\">93.32</td>\n<td class=\"ltx_td ltx_align_center\">48.69</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Bambara</span></th>\n<td class=\"ltx_td ltx_align_center\">4.06</td>\n<td class=\"ltx_td ltx_align_center\">15.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">27.02</td>\n<td class=\"ltx_td ltx_align_center\">0.25</td>\n<td class=\"ltx_td ltx_align_center\">0.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.72</td>\n<td class=\"ltx_td ltx_align_center\">0.18</td>\n<td class=\"ltx_td ltx_align_center\">22.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">13.60</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center\">0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.39</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center\">0.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.29</td>\n<td class=\"ltx_td ltx_align_center\">766.87</td>\n<td class=\"ltx_td ltx_align_center\">49.33</td>\n<td class=\"ltx_td ltx_align_center\">112.12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Wolof</span></th>\n<td class=\"ltx_td ltx_align_center\">2.00</td>\n<td class=\"ltx_td ltx_align_center\">12.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.30</td>\n<td class=\"ltx_td ltx_align_center\">0.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.16</td>\n<td class=\"ltx_td ltx_align_center\">2.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">1030.14</td>\n<td class=\"ltx_td ltx_align_center\">84.83</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Luganda</span></th>\n<td class=\"ltx_td ltx_align_center\">7.08</td>\n<td class=\"ltx_td ltx_align_center\">24.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">23.55</td>\n<td class=\"ltx_td ltx_align_center\">0.28</td>\n<td class=\"ltx_td ltx_align_center\">0.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.63</td>\n<td class=\"ltx_td ltx_align_center\">0.41</td>\n<td class=\"ltx_td ltx_align_center\">3.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.96</td>\n<td class=\"ltx_td ltx_align_center\">-0.05</td>\n<td class=\"ltx_td ltx_align_center\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.57</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.33</td>\n<td class=\"ltx_td ltx_align_center\">980.96</td>\n<td class=\"ltx_td ltx_align_center\">74.57</td>\n<td class=\"ltx_td ltx_align_center\">63.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Arabic</span></th>\n<td class=\"ltx_td ltx_align_center\">8.67</td>\n<td class=\"ltx_td ltx_align_center\">31.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">28.46</td>\n<td class=\"ltx_td ltx_align_center\">0.52</td>\n<td class=\"ltx_td ltx_align_center\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.89</td>\n<td class=\"ltx_td ltx_align_center\">0.30</td>\n<td class=\"ltx_td ltx_align_center\">23.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">21.41</td>\n<td class=\"ltx_td ltx_align_center\">0.21</td>\n<td class=\"ltx_td ltx_align_center\">0.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.77</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center\">0.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.55</td>\n<td class=\"ltx_td ltx_align_center\">1164.70</td>\n<td class=\"ltx_td ltx_align_center\">67.00</td>\n<td class=\"ltx_td ltx_align_center\">67.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Somali</span></th>\n<td class=\"ltx_td ltx_align_center\">7.11</td>\n<td class=\"ltx_td ltx_align_center\">35.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">47.32</td>\n<td class=\"ltx_td ltx_align_center\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">0.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.80</td>\n<td class=\"ltx_td ltx_align_center\">0.41</td>\n<td class=\"ltx_td ltx_align_center\">8.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">12.90</td>\n<td class=\"ltx_td ltx_align_center\">0.03</td>\n<td class=\"ltx_td ltx_align_center\">0.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.62</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">442.60</td>\n<td class=\"ltx_td ltx_align_center\">65.09</td>\n<td class=\"ltx_td ltx_align_center\">60.75</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Afrkaans</span></th>\n<td class=\"ltx_td ltx_align_center\">44.76</td>\n<td class=\"ltx_td ltx_align_center\">48.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">52.10</td>\n<td class=\"ltx_td ltx_align_center\">0.68</td>\n<td class=\"ltx_td ltx_align_center\">0.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.85</td>\n<td class=\"ltx_td ltx_align_center\">32.98</td>\n<td class=\"ltx_td ltx_align_center\">18.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">27.25</td>\n<td class=\"ltx_td ltx_align_center\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">0.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.73</td>\n<td class=\"ltx_td ltx_align_center\">0.35</td>\n<td class=\"ltx_td ltx_align_center\">0.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.65</td>\n<td class=\"ltx_td ltx_align_center\">52.61</td>\n<td class=\"ltx_td ltx_align_center\">48.23</td>\n<td class=\"ltx_td ltx_align_center\">39.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Tigrinya</span></th>\n<td class=\"ltx_td ltx_align_center\">5.43</td>\n<td class=\"ltx_td ltx_align_center\">24.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">23.75</td>\n<td class=\"ltx_td ltx_align_center\">0.27</td>\n<td class=\"ltx_td ltx_align_center\">0.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.83</td>\n<td class=\"ltx_td ltx_align_center\">2.77</td>\n<td class=\"ltx_td ltx_align_center\">13.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">13.00</td>\n<td class=\"ltx_td ltx_align_center\">0.04</td>\n<td class=\"ltx_td ltx_align_center\">0.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.70</td>\n<td class=\"ltx_td ltx_align_center\">0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.23</td>\n<td class=\"ltx_td ltx_align_center\">78.33</td>\n<td class=\"ltx_td ltx_align_center\">36.55</td>\n<td class=\"ltx_td ltx_align_center\">78.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Malagasy</span></th>\n<td class=\"ltx_td ltx_align_center\">9.39</td>\n<td class=\"ltx_td ltx_align_center\">26.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">43.06</td>\n<td class=\"ltx_td ltx_align_center\">0.42</td>\n<td class=\"ltx_td ltx_align_center\">0.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.76</td>\n<td class=\"ltx_td ltx_align_center\">0.32</td>\n<td class=\"ltx_td ltx_align_center\">22.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">9.10</td>\n<td class=\"ltx_td ltx_align_center\">0.13</td>\n<td class=\"ltx_td ltx_align_center\">0.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.45</td>\n<td class=\"ltx_td ltx_align_center\">0.06</td>\n<td class=\"ltx_td ltx_align_center\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.36</td>\n<td class=\"ltx_td ltx_align_center\">856.18</td>\n<td class=\"ltx_td ltx_align_center\">57.08</td>\n<td class=\"ltx_td ltx_align_center\">57.08</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Xhosa</span></th>\n<td class=\"ltx_td ltx_align_center\">6.58</td>\n<td class=\"ltx_td ltx_align_center\">46.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">47.88</td>\n<td class=\"ltx_td ltx_align_center\">0.35</td>\n<td class=\"ltx_td ltx_align_center\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.70</td>\n<td class=\"ltx_td ltx_align_center\">0.63</td>\n<td class=\"ltx_td ltx_align_center\">12.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">14.95</td>\n<td class=\"ltx_td ltx_align_center\">0.21</td>\n<td class=\"ltx_td ltx_align_center\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.41</td>\n<td class=\"ltx_td ltx_align_center\">0.09</td>\n<td class=\"ltx_td ltx_align_center\">0.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.31</td>\n<td class=\"ltx_td ltx_align_center\">561.279</td>\n<td class=\"ltx_td ltx_align_center\">71.79</td>\n<td class=\"ltx_td ltx_align_center\">58.74</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Zulu</span></th>\n<td class=\"ltx_td ltx_align_center\">7.58</td>\n<td class=\"ltx_td ltx_align_center\">38.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">64.45</td>\n<td class=\"ltx_td ltx_align_center\">0.32</td>\n<td class=\"ltx_td ltx_align_center\">0.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.73</td>\n<td class=\"ltx_td ltx_align_center\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">5.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">34.57</td>\n<td class=\"ltx_td ltx_align_center\">0.09</td>\n<td class=\"ltx_td ltx_align_center\">0.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.54</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.35</td>\n<td class=\"ltx_td ltx_align_center\">728.98</td>\n<td class=\"ltx_td ltx_align_center\">89.52</td>\n<td class=\"ltx_td ltx_align_center\">51.16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Sesotho</span></th>\n<td class=\"ltx_td ltx_align_center\">7.09</td>\n<td class=\"ltx_td ltx_align_center\">61.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">57.01</td>\n<td class=\"ltx_td ltx_align_center\">0.28</td>\n<td class=\"ltx_td ltx_align_center\">0.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.66</td>\n<td class=\"ltx_td ltx_align_center\">0.14</td>\n<td class=\"ltx_td ltx_align_center\">51.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">33.25</td>\n<td class=\"ltx_td ltx_align_center\">0.14</td>\n<td class=\"ltx_td ltx_align_center\">0.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.65</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center\">0.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.44</td>\n<td class=\"ltx_td ltx_align_center\">1475.22</td>\n<td class=\"ltx_td ltx_align_center\">63.22</td>\n<td class=\"ltx_td ltx_align_center\">45.66</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Chewa</span></th>\n<td class=\"ltx_td ltx_align_center\">4.76</td>\n<td class=\"ltx_td ltx_align_center\">28.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">34.29</td>\n<td class=\"ltx_td ltx_align_center\">0.28</td>\n<td class=\"ltx_td ltx_align_center\">0.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.61</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">13.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">12.67</td>\n<td class=\"ltx_td ltx_align_center\">-0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.44</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.31</td>\n<td class=\"ltx_td ltx_align_center\">876.28</td>\n<td class=\"ltx_td ltx_align_center\">72.16</td>\n<td class=\"ltx_td ltx_align_center\">67.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Lingala</span></th>\n<td class=\"ltx_td ltx_align_center\">5.91</td>\n<td class=\"ltx_td ltx_align_center\">33.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">38.14</td>\n<td class=\"ltx_td ltx_align_center\">0.30</td>\n<td class=\"ltx_td ltx_align_center\">0.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.65</td>\n<td class=\"ltx_td ltx_align_center\">0.23</td>\n<td class=\"ltx_td ltx_align_center\">18.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">16.82</td>\n<td class=\"ltx_td ltx_align_center\">0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.40</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center\">0.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.45</td>\n<td class=\"ltx_td ltx_align_center\">955.39</td>\n<td class=\"ltx_td ltx_align_center\">80.32</td>\n<td class=\"ltx_td ltx_align_center\">54.96</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Tswana</span></th>\n<td class=\"ltx_td ltx_align_center\">8.59</td>\n<td class=\"ltx_td ltx_align_center\">26.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.28</td>\n<td class=\"ltx_td ltx_align_center\">0.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.69</td>\n<td class=\"ltx_td ltx_align_center\">16.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.11</td>\n<td class=\"ltx_td ltx_align_center\">0.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">413.87</td>\n<td class=\"ltx_td ltx_align_center\">65.69</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Fon</span></th>\n<td class=\"ltx_td ltx_align_center\">10.57</td>\n<td class=\"ltx_td ltx_align_center\">9.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.16</td>\n<td class=\"ltx_td ltx_align_center\">0.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">5.14</td>\n<td class=\"ltx_td ltx_align_center\">6.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.14</td>\n<td class=\"ltx_td ltx_align_center\">0.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.06</td>\n<td class=\"ltx_td ltx_align_center\">0.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">56.15</td>\n<td class=\"ltx_td ltx_align_center\">29.73</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Kikuyu</span></th>\n<td class=\"ltx_td ltx_align_center\">9.77</td>\n<td class=\"ltx_td ltx_align_center\">25.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.27</td>\n<td class=\"ltx_td ltx_align_center\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.28</td>\n<td class=\"ltx_td ltx_align_center\">17.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.03</td>\n<td class=\"ltx_td ltx_align_center\">0.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center\">0.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">943.70</td>\n<td class=\"ltx_td ltx_align_center\">59.38</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Tshiluba</span></th>\n<td class=\"ltx_td ltx_align_center\">15.68</td>\n<td class=\"ltx_td ltx_align_center\">22.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.31</td>\n<td class=\"ltx_td ltx_align_center\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">5.856</td>\n<td class=\"ltx_td ltx_align_center\">7.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.20</td>\n<td class=\"ltx_td ltx_align_center\">0.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.11</td>\n<td class=\"ltx_td ltx_align_center\">0.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">61.49</td>\n<td class=\"ltx_td ltx_align_center\">52.71</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Mossi</span></th>\n<td class=\"ltx_td ltx_align_center\">12.82</td>\n<td class=\"ltx_td ltx_align_center\">23.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.28</td>\n<td class=\"ltx_td ltx_align_center\">0.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">5.86</td>\n<td class=\"ltx_td ltx_align_center\">9.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.18</td>\n<td class=\"ltx_td ltx_align_center\">-0.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center\">0.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">57.02</td>\n<td class=\"ltx_td ltx_align_center\">81.46</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Kikongo</span></th>\n<td class=\"ltx_td ltx_align_center\">6.18</td>\n<td class=\"ltx_td ltx_align_center\">20.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.31</td>\n<td class=\"ltx_td ltx_align_center\">0.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.38</td>\n<td class=\"ltx_td ltx_align_center\">6.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">-0.05</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.05</td>\n<td class=\"ltx_td ltx_align_center\">0.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">318.97</td>\n<td class=\"ltx_td ltx_align_center\">44.51</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Bemba</span></th>\n<td class=\"ltx_td ltx_align_center\">3.59</td>\n<td class=\"ltx_td ltx_align_center\">25.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.31</td>\n<td class=\"ltx_td ltx_align_center\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.25</td>\n<td class=\"ltx_td ltx_align_center\">27.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.09</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.10</td>\n<td class=\"ltx_td ltx_align_center\">0.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">755.92</td>\n<td class=\"ltx_td ltx_align_center\">48.63</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Average</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">8.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">31.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">44.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">2.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">17.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">23.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">645.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">65.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">58.87</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ewe",
            "luganda",
            "malagasy",
            "xhosa",
            "fon",
            "↓downarrow",
            "ours",
            "igbo",
            "tswana",
            "kikuyu",
            "tshiluba",
            "lower",
            "zulu",
            "comet",
            "better",
            "ter",
            "our",
            "twiasante",
            "across",
            "swahili",
            "metrics",
            "where",
            "base",
            "shona",
            "between",
            "llama1b",
            "indicate",
            "fula",
            "africomet",
            "kikongo",
            "lingala",
            "language",
            "translate",
            "sesotho",
            "google",
            "wolof",
            "arabic",
            "mossi",
            "except",
            "somali",
            "average",
            "performance",
            "↑uparrow",
            "higher",
            "amharic",
            "chewa",
            "yoruba",
            "meteor",
            "finetuned",
            "bleu",
            "afrkaans",
            "models",
            "bemba",
            "different",
            "scores",
            "tigrinya",
            "chrf",
            "llama321b",
            "bambara",
            "hausa",
            "comparison",
            "oromo",
            "kinyarwanda"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our experimental evaluation across 31 African languages reveals substantial and systematic improvements through fine-tuning, with distinct patterns emerging across language families and resource levels (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S4.T3\" title=\"Table 3 &#8227; 4 Results &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Nine languages were excluded from evaluation due to insufficient training data or absence from the FLORES-200 benchmark.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite representing nearly one-third of the world&#8217;s languages, African languages remain critically underserved by modern NLP technologies, with 88% classified as severely underrepresented or completely ignored in computational linguistics. We present the African Languages Lab (All Lab), a comprehensive research initiative that addresses this technological gap through systematic data collection, model development, and capacity building. Our contributions include: (1) a quality-controlled data collection pipeline, yielding the largest validated African multi-modal speech and text dataset spanning 40 languages with 19 billion tokens of monolingual text and 12,628 hours of aligned speech data; (2) extensive experimental validation demonstrating that our dataset, combined with fine-tuning, achieves substantial improvements over baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages; and (3) a structured research program that has successfully mentored fifteen early-career researchers, establishing sustainable local capacity. Our comparative evaluation against Google Translate reveals competitive performance in several languages while identifying areas that require continued development.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>To promote accessibility, we provide translations of this abstract in 10 African languages in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#A2\" title=\"Appendix B Abstract Translations &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, generated using our fine-tuned models.</span></span></span></p>\n\n",
                "matched_terms": [
                    "translate",
                    "finetuned",
                    "across",
                    "bleu",
                    "models",
                    "google",
                    "chrf",
                    "comet",
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The promise of artificial intelligence (AI) and natural language processing (NLP) to democratize information access remains unfulfilled for billions of speakers worldwide. Among the approximately 7,000 languages spoken globally, fewer than 20 receive substantial attention in NLP research <cite class=\"ltx_cite ltx_citemacro_citep\">(Magueresse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib27\" title=\"\">2020</a>)</cite>. This technological marginalization particularly affects low-resource languages (LRLs). Without a clearly established definition, LRLs are languages that exist at the periphery of the digital transformation, characterized by three critical deficits: (1) a scarcity of machine-readable corpora, (2) limited personalized computational technologies and trained language models, and (3) insufficient representation in global research communities <cite class=\"ltx_cite ltx_citemacro_citep\">(Nigatu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib32\" title=\"\">2024</a>; Issaka et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib23\" title=\"\">2024</a>; Magueresse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib27\" title=\"\">2020</a>)</cite>.\nWhile often serving substantial speaker populations, these languages face significant challenges in participating fully in the AI-driven information economy.</p>\n\n",
                "matched_terms": [
                    "models",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This problem is compounded by a severe underrepresentation in the global NLP research community. Our analysis of mentions of the top 10 global languages versus the top 10 African languages across major academic databases reveals a stark imbalance. On average, for every paper discussing African languages in multilingual LLM contexts, there are 20 papers on global languages in Google Scholar (GS), 23 in COnnecting REpositories(CORE), 34 in arXiv, and 70 in The Institute of Electrical and Electronics Engineers (IEEE) (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Aggregated Paper Count Approach &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> in the Appendix). This 20-70x representation gap reinforces a self-perpetuating cycle of marginalization where limited research attention leads to poor technological support, which in turn discourages further research investment.</p>\n\n",
                "matched_terms": [
                    "across",
                    "google",
                    "where",
                    "average",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Systematic data infrastructure:</span> We developed a systematic, quality-controlled data collection framework powered by our \"All Voices\" platform. All Voices is a mobile-first platform specifically designed for community-driven multilingual data collection in low-resource contexts, enabling direct translation between African languages without English intermediation</p>\n\n",
                "matched_terms": [
                    "between",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Empirical validation and capacity building:</span> We demonstrate the effectiveness of our approach through extensive experiments showing average improvements of +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points, while simultaneously developing local research capacity through structured mentorship programs.</p>\n\n",
                "matched_terms": [
                    "bleu",
                    "average",
                    "chrf",
                    "comet",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of African NLP has been shaped by complementary community and institutional efforts. Masakhane, comprising over 3,000 Slack members, exemplifies successful community-driven research, demonstrating that participatory approaches can produce high-quality datasets and models <cite class=\"ltx_cite ltx_citemacro_citep\">(Orife et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib35\" title=\"\">2020</a>)</cite>. Complementing this work, the \"Breaking the Unwritten Language Barrier\" project addresses challenges specific to unwritten and under-documented languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adda et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib1\" title=\"\">2016</a>)</cite>. Their work on languages like Basaa, Myene, and Embosi has established methodological approaches for speech recognition in LRLs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These community efforts have been supported by institutional initiatives providing essential infrastructure. The Lacuna Fund has enabled dataset development <cite class=\"ltx_cite ltx_citemacro_citep\">(Rathi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib37\" title=\"\">2023</a>)</cite>, while Meta&#8217;s No Language Left Behind project has contributed architectural innovations for massively multilingual models <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib47\" title=\"\">2022</a>)</cite>. Additional infrastructure support has come from Mozilla&#8217;s Common Voice project <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib8\" title=\"\">2020</a>)</cite> for speech resources and the AI4D African Language Program <cite class=\"ltx_cite ltx_citemacro_citep\">(Siminyu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib42\" title=\"\">2021</a>)</cite> for benchmark development. The Deep Learning Indaba<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://deeplearningindaba.com/\" title=\"\">https://deeplearningindaba.com/</a></span></span></span> has contributed to research capacity building through its convenings, while platforms like Lanfrica have improved resource discoverability and research sharing across the continent <cite class=\"ltx_cite ltx_citemacro_citep\">(Emezue and Dossou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib17\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "language",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evolution of multilingual LLMs has shown steady progress in language coverage and capabilities. Early approaches like mBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Muller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib30\" title=\"\">2021</a>)</cite> and XLM-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib12\" title=\"\">2020</a>)</cite> established initial benchmarks, supporting approximately 100 languages each. Subsequent developments included more focused models like mBART <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib26\" title=\"\">2020</a>)</cite>, mT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xue et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib57\" title=\"\">2021</a>)</cite>, and XGLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Ersoy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib19\" title=\"\">2023</a>)</cite>, which traded broader language coverage for improved performance on specific language sets. The advent of massive LLMs further expanded these capabilities, with models like GPT-3, mGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Shliazhko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib41\" title=\"\">2024</a>)</cite>, and BLOOM <cite class=\"ltx_cite ltx_citemacro_citep\">(Workshop et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib56\" title=\"\">2023</a>)</cite> supporting varying numbers of African languages. Also, Glot500-m <cite class=\"ltx_cite ltx_citemacro_citep\">(Imani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib22\" title=\"\">2023</a>)</cite> extends support to 511 languages and the SERENGETI and Cheetah models supports about 517 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adebara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib4\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib3\" title=\"\">2024</a>)</cite>. Additional progress has come from the Aya model, which demonstrates instruction-following capabilities across 101 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(&#220;st&#252;n et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib59\" title=\"\">2024</a>)</cite>, and specialized models like AfroLM, which focuses on 23 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Dossou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib14\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "language",
                    "performance",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While not specifically trained in African languages, English-centric LLMs such as GPT-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib34\" title=\"\">2024</a>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib46\" title=\"\">2025</a>)</cite>, and Llama <cite class=\"ltx_cite ltx_citemacro_citep\">(Wendler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib53\" title=\"\">2024</a>)</cite> have shown capability in handling some African languages, <cite class=\"ltx_cite ltx_citemacro_citep\">(Robinson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib40\" title=\"\">2023</a>; Ojo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib33\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib58\" title=\"\">2024</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib13\" title=\"\">2024</a>)</cite>, though their performance generally does not match that of specialized models; underscoring the need for dedicated resources and architectures.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of evaluation frameworks has enabled systematic progress measurement in African NLP across diverse task domains. MasakhaNER provides NER datasets for 10 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib5\" title=\"\">2021</a>)</cite>, AfriSenti offers sentiment analysis benchmarks in 14 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib28\" title=\"\">2023</a>)</cite>, and AFROMT establishes standardized translation benchmarks for 8 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Reid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib39\" title=\"\">2021</a>)</cite>. IrokoBench unifies evaluation across natural language inference, mathematical reasoning, and multiple-choice QA in 17 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib6\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">All Voices Platform.</span> To address the fundamental challenge of data scarcity in African languages, we developed All Voices, a mobile-first platform that stands as the only solution specifically designed for data collection in any LRL. The platform&#8217;s innovative approach enables direct translation between LRLs without requiring English as an intermediary, addressing a critical gap in the existing data collection infrastructure. In addition, All Voices distinguishes itself through its multimodal capabilities, which support the collection and validation of text and audio data. The platform features an intuitive, user-friendly interface that encourages broad participation, complemented by gamification elements, including a global leaderboard system that promotes user engagement. Importantly, All Voices is open and free to everyone, aligning with our mission to democratize language technology development. All Voices contributors provide informed consent for their contributions to be used for research purposes, including dataset creation, model training, and open-source distribution, with full transparency regarding data usage and the right to withdraw consent at any time.</p>\n\n",
                "matched_terms": [
                    "language",
                    "between",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The platform&#8217;s architecture, built using ReactNative&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://reactnative.dev/\" title=\"\">https://reactnative.dev/</a></span></span></span> and Firebase&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://firebase.google.com/\" title=\"\">https://firebase.google.com/</a></span></span></span>, integrates user authentication and analytics, translation corpus management, and quality control components. Our authentication system provides comprehensive user profiling, tracking contributor demographics and expertise through quantifiable metrics, including successful translations and community validation scores. This system implements OAuth 2.0 authentication and role-based access control to ensure data integrity and user privacy. The translation corpus management system centrally stores both text and audio translations along with their metadata, and protects all data using AES-256 encryption at rest and TLS 1.3 during transmission. Translations undergo peer review requiring both a minimum threshold of positive validation (&gt;5 upvotes) and an acceptable error margin (&lt;3 downvotes) to achieve verified status. A key innovation is our recursive translation pipeline: verified translations become eligible source material for subsequent translations, creating a multiplicative effect in data collection.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "scores",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data processing implements a robust two-tier approach combining general normalization with language-specific processing. The general normalization phase addresses universal text artifacts through Unicode normalization, character encoding standardization, and structural cleaning, including HTML removal and symbol standardization. The language-specific processing phase implements specialized handling for African language features, including morphological analysis, script variant normalization, and tone mark standardization, language identification, with custom rule sets developed for specific language families.</p>\n\n",
                "matched_terms": [
                    "language",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, our translation validation methodology implements a robust statistical framework for assessing translation quality through quantitative analysis of character-level distributions. The validation metric employs character ratio analysis between source and target texts, computed as the ratio of target text length to source text length. We analyze these ratios using z-score normalization within language-specific distribution, enabling the detection of statistical outliers while accounting for natural variations in text length across different language pairs. This approach is augmented with character overlap detection to identify potential artifacts or inappropriate text preservation, particularly crucial for languages sharing similar orthographic features.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "different",
                    "between",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Also, the threshold determination process implements an adaptive sampling methodology. For each language pair, we establish baseline distributions through initial sampling of 10,000 translation pairs, employing Kernel Density Estimation for robust distribution modeling. This approach effectively captures the non-Gaussian characteristics frequently observed in cross-lingual character distributions. Thresholds are dynamically computed using a modified Tukey method with an adaptive multiplier. This adaptive threshold mechanism automatically calibrates to language-specific characteristics, implementing more stringent filtering for language pairs that exhibit consistent ratios while allowing appropriate flexibility for pairs with inherently higher variability. The resulting validation framework effectively identifies and filters anomalous translations while maintaining sensitivity to legitimate linguistic variations across diverse African language families. The processed data sets are structured according to HuggingFace&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/\" title=\"\">https://huggingface.co/</a></span></span></span> Dataset specifications, enabling seamless API integration.</p>\n\n",
                "matched_terms": [
                    "language",
                    "higher",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the utility of our dataset and establish baselines for our languages, we experimented with Llama-3.2-1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib21\" title=\"\">2024</a>)</cite>. We chose this model as our base because of its demonstrated multilingual capabilities and efficient parameter scaling, making it suitable for LRLs. Critically, while capable of tokenizing, this model has not been explicitly trained on any African language in our dataset. Thus, providing a cleaner baseline for measuring the utility of our dataset during fine-tuning without risk of data contamination.</p>\n\n",
                "matched_terms": [
                    "base",
                    "llama321b",
                    "language",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed full fine-tuning rather than parameter-efficient methods, as preliminary experiments with Quantization-aware Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_citep\">(&#220;st&#252;n et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib59\" title=\"\">2024</a>)</cite> yielded insufficient performance gains for our target languages. Our training pipeline uses supervised learning with a standardized instruction template: \"Translate the following English text to X:\", where X denotes the target African language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "translate",
                    "where",
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model performance was evaluated using a complementary set of metrics: BiLingual Evaluation Understudy (BLEU) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wieting et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib55\" title=\"\">2019</a>)</cite>, which measures n-gram precision; METEOR <cite class=\"ltx_cite ltx_citemacro_citep\">(Banerjee and Lavie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib9\" title=\"\">2005</a>)</cite>, which accounts for word stems and synonyms; COMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib38\" title=\"\">2020</a>)</cite>, which leverages multilingual embeddings to assess semantic similarity; and ChrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib51\" title=\"\">2025</a>)</cite>, which operates on character-level n-grams to better capture morphological variations common in African languages. Additionally, we employed Translation Edit Rate (TER) <cite class=\"ltx_cite ltx_citemacro_citep\">(Snover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib45\" title=\"\">2006</a>)</cite>, which quantifies the minimum number of edits required to transform the hypothesis into the reference translation (where lower scores indicate better quality), and AfriCOMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib50\" title=\"\">2024</a>)</cite>, a neural metric specifically trained on African language pairs to better capture language-specific quality nuances.</p>\n\n",
                "matched_terms": [
                    "language",
                    "meteor",
                    "bleu",
                    "metrics",
                    "scores",
                    "africomet",
                    "where",
                    "lower",
                    "chrf",
                    "indicate",
                    "comet",
                    "better",
                    "performance",
                    "ter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these metrics comprehensively assess translation quality across different linguistic aspects, from surface-level n-gram matching to semantic preservation and post-editing effort. We utilized the FLORES-200 dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib47\" title=\"\">2022</a>)</cite> as our standardized test set, ensuring consistency across all languages and enabling direct comparison with other multilingual systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "metrics",
                    "different",
                    "comparison",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our comprehensive data collection yielded 19,012,019,696 tokens of monolingual text and 12,628 hours of aligned audio across 40 African languages (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Our analysis reveals distinct stratification patterns highlighting the digital divide within African languages themselves.</p>\n\n",
                "matched_terms": [
                    "across",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Primary resource languages (&gt;2B tokens):</span> This tier includes Amharic (2.94B), Arabic (2.40B), Yoruba (2.36B), and Afrikaans (2.30B), reflecting sustained digitization efforts and strong institutional support.</p>\n\n",
                "matched_terms": [
                    "arabic",
                    "amharic",
                    "yoruba"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Established digital languages (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1&#8211;2B tokens):</span> Languages such as Hausa (1.54B) and Tigrinya (0.92B) demonstrate robust digital presence, likely owing to consistent documentation and preservation initiatives.</p>\n\n",
                "matched_terms": [
                    "hausa",
                    "tigrinya"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emerging digital languages (250M&#8211;1B tokens):</span> A substantial group including Malagasy (839.12M), Somali (751.13M), Swahili (700.39M), and Xhosa (563.07M), show growing digital footprints but still lagging behind the top tiers.</p>\n\n",
                "matched_terms": [
                    "somali",
                    "malagasy",
                    "xhosa",
                    "swahili"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Resource-constrained languages (&lt;250M tokens):</span> The majority of languages in our dataset fall into this category, including widely spoken languages such as Bambara (109.49M) and Luganda (121.17M). This tier reflects substantial gaps in textual data availability.</p>\n\n",
                "matched_terms": [
                    "luganda",
                    "bambara",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">High-resource audio languages (&gt;1,000 hours):</span> Kinyarwanda (3,839.00h), Luganda (1,727.80h), Swahili (1,115.00h), and Arabic (2,721.52h) dominate in audio availability, often due to large-scale speech corpora or broadcast archives.</p>\n\n",
                "matched_terms": [
                    "luganda",
                    "kinyarwanda",
                    "swahili",
                    "arabic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Moderate audio languages (100&#8211;500 hours):</span> Includes Malagasy (325.14h), Twi (227.03h), Bemba (230.30h), and Ewe (147.00h), representing a mix of widely spoken languages and those with targeted speech collection efforts.</p>\n\n",
                "matched_terms": [
                    "ewe",
                    "bemba",
                    "malagasy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, our analysis underscores two parallel digital divides: a textual divide, where a small set of languages capture the majority of tokens, and an audio divide, where a different but equally narrow set of languages dominate. Notably, the top three languages by text volume account for a disproportionate share of tokens, while the top three by audio hours similarly capture the bulk of recorded speech. This imbalance highlights the urgent need for targeted development of both textual and audio resources, particularly for languages with substantial speaker populations but limited digital presence.</p>\n\n",
                "matched_terms": [
                    "different",
                    "our",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance.</span> The base Llama-3.2-1B model demonstrates limited but non-trivial capability for African languages, revealing interesting patterns of cross-lingual transfer. ChrF++ scores range from 2.00 (Wolof) to 44.76 (Afrikaans), with a mean of 8.10, indicating minimal character-level understanding for most languages. COMET scores cluster between 0.16-0.68 (mean: 0.32), suggesting some semantic comprehension despite poor surface realization. Notably, Afrikaans shows exceptional baseline performance (ChrF++ 44.76, BLEU 32.98), leveraging its Germanic roots and Latin script. The extremely low baseline BLEU scores (mean: 2.27) across most languages confirm the model&#8217;s inability to produce accurate n-gram sequences without language-specific training.</p>\n\n",
                "matched_terms": [
                    "across",
                    "bleu",
                    "scores",
                    "wolof",
                    "base",
                    "between",
                    "chrf",
                    "llama321b",
                    "comet",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-tuning Impact.</span> Our dataset enables noticeable performance improvements, with average gains of +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points. The magnitude of improvement correlates inversely with baseline performance, suggesting effective transfer learning rather than simple memorization. Swahili exhibits the largest absolute ChrF++ improvement (+63.27 points), achieving near-parity with Google Translate (72.27 vs 75.81). Sesotho shows remarkable gains across all metrics (+61.79 ChrF++, +51.16 BLEU), while maintaining competitive performance against Google Translate. Languages with minimal baselines, including Fula, Wolof, and Kikongo, demonstrate that even severely under-resourced languages benefit substantially from targeted fine-tuning, achieving functional translation capability where none existed before.</p>\n\n",
                "matched_terms": [
                    "translate",
                    "sesotho",
                    "across",
                    "swahili",
                    "average",
                    "performance",
                    "bleu",
                    "metrics",
                    "google",
                    "where",
                    "wolof",
                    "chrf",
                    "comet",
                    "fula",
                    "kikongo",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparison with Google Translate.</span> Where available (22 languages), comparison with Google Translate reveals three distinct performance categories. First, languages where our models achieve competitive or superior performance: Yoruba (30.88 vs 21.05 ChrF++), Arabic (31.52 vs 28.46), and notably Twi, where we substantially outperform Google Translate (46.80 vs 31.48). Second, languages where Google maintains clear advantages, particularly in high-resource cases like Kinyarwanda (24.65 vs 70.27) and Swahili (72.27 vs 75.81), reflecting their extensive training data. Third, the languages where Google Translate offers no support, highlighting our contribution to genuinely LRL coverage.</p>\n\n",
                "matched_terms": [
                    "yoruba",
                    "translate",
                    "swahili",
                    "models",
                    "google",
                    "where",
                    "arabic",
                    "chrf",
                    "performance",
                    "comparison",
                    "our",
                    "kinyarwanda"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language-Specific Patterns.</span> Three response profiles emerge from our analysis. High-responder languages (Swahili, Sesotho, Hausa) show dramatic improvements exceeding 40 ChrF++ points, suggesting optimal alignment between our dataset characteristics and model architecture. Steady improvers (Igbo, Shona, Somali) demonstrate consistent gains of 25-30 points across metrics, indicating robust but not exceptional adaptation. Challenging cases (Fon, Wolof, Bambara) show limited improvements despite fine-tuning, likely requiring specialized tokenization or architectural modifications to address their unique linguistic features.</p>\n\n",
                "matched_terms": [
                    "sesotho",
                    "swahili",
                    "across",
                    "fon",
                    "metrics",
                    "igbo",
                    "wolof",
                    "shona",
                    "between",
                    "chrf",
                    "bambara",
                    "hausa",
                    "somali",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation Edit Rate Analysis.</span> The dramatic TER reductions, averaging 580.13 points lower after fine-tuning, provide crucial practical insights. Languages like Swahili achieve TER scores comparable to Google Translate (23.77), indicating production-ready quality requiring minimal post-editing. Even languages with modest BLEU improvements show substantial TER reductions, suggesting improved fluency and coherence that traditional metrics may not fully capture. This pattern holds particular significance for scenarios where post-editing cost determines practical viability.</p>\n\n",
                "matched_terms": [
                    "translate",
                    "swahili",
                    "bleu",
                    "metrics",
                    "google",
                    "scores",
                    "where",
                    "lower",
                    "ter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Correlations.</span> While surface metrics (BLEU, ChrF++) show high correlation, the divergence between these and neural metrics (COMET, AfriCOMET) reveals important quality dimensions. Languages like Ewe show minimal COMET improvement (0.04) despite substantial ChrF++ gains (30.43 points), suggesting character-level improvements without the corresponding semantic enhancement. In contrast, Arabic shows strong COMET gains (0.33) with modest improvement in ChrF++, indicating semantic preservation despite surface-level challenges. These patterns underscore the importance of multi-metric evaluation for morphologically diverse African languages.</p>\n\n",
                "matched_terms": [
                    "bleu",
                    "ewe",
                    "metrics",
                    "arabic",
                    "between",
                    "chrf",
                    "comet",
                    "africomet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We have presented the African Languages Lab, a research initiative addressing the critical underrepresentation of African languages in NLP through systematic data collection, model development, and capacity building. Our contributions include a validated dataset of 19 billion tokens and 12,628 hours of aligned speech across 40 languages, substantial performance improvements averaging +23.69 ChrF++ over baseline models, the All Voices platform, and a structured mentorship program developing fifteen early-career researchers. Thus, we demonstrate that the technological marginalization of African languages, while severe, is not intractable.</p>\n\n",
                "matched_terms": [
                    "across",
                    "models",
                    "chrf",
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments utilize Llama-3.2-1B as the sole base model, which, while demonstrating the utility of our dataset, may underestimate potential gains achievable with larger-scale architectures. The performance variance across language families, from 63.27 ChrF++ improvement for Swahili to minimal gains for Fon (-1.10), suggests that optimal model selection likely varies by linguistic typology. Additionally, our evaluation of 31 of 40 collected languages reflects FLORES-200 coverage limitations, potentially obscuring insights from the most critically under-resourced languages in our dataset.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "swahili",
                    "fon",
                    "base",
                    "chrf",
                    "llama321b",
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite assembling 19 billion tokens, our dataset exhibits a 147,000&#215; disparity between the highest-resourced (Amharic: 2,944.95M tokens) and lowest-resourced (Fang: 0.02M tokens) languages. This imbalance directly correlates with performance outcomes: languages with &gt;1B tokens achieve average ChrF++ scores of 45.66, while those with &lt;100M tokens average 24.31. Furthermore, 13 languages lack audio data entirely, limiting multimodal model development. Our validation pipeline, while statistically grounded, operates without native speaker verification for 73% of languages, potentially missing dialectal variations that affect 28% of evaluated translations showing COMET-ChrF++ divergence exceeding 0.3.</p>\n\n",
                "matched_terms": [
                    "amharic",
                    "scores",
                    "between",
                    "chrf",
                    "average",
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these constraints delineate several clear pathways for advancement. One direction is to explore architecture-specific optimizations for morphologically complex languages. Another is to implement active learning strategies that help address data imbalances. It is also important to develop evaluation metrics that are more sensitive to African language typologies. These limitations inform our ongoing work and highlight key areas for future research in African NLP.</p>\n\n",
                "matched_terms": [
                    "language",
                    "metrics",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our structured research development program has mentored fifteen early-career researchers across four institutions through one-on-one mentorship, project development support, and transitions into extended research roles. This investment in local research leadership establishes sustainable capacity for African NLP development, ensuring that technical advancement aligns with cultural and linguistic expertise. By prioritizing skill development alongside technical innovation, we contribute to a sustainable talent pipeline that positions African researchers to lead future developments in their languages.</p>\n\n",
                "matched_terms": [
                    "across",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work directly advances United Nations Sustainable Development Goals in education and inequality reduction through increased digital representation of marginalized languages. The platform enables community-led content creation and facilitates open knowledge transfer, democratizing access to digital tools while preserving linguistic and cultural heritage. With 88% of African languages severely underrepresented or completely ignored in computational linguistics, and 814 languages facing extinction risk, our framework provides critical infrastructure for language preservation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Guided by Ubuntu philosophy&#8212;emphasizing inclusivity, interdependence, and openness&#8212;we establish a framework for equitable NLP development. Our roadmap encompasses expanding language coverage, optimizing model architectures for low-resource languages, and deepening research collaborations. We acknowledge persistent challenges including limited commercial viability for some LRL technologies and infrastructural constraints, yet our results demonstrate that systematic community engagement can effectively address technological marginalization.\nThrough this comprehensive approach integrating technical innovation, cultural preservation, educational empowerment, and economic inclusion, we provide replicable models for equitable language technology development that can benefit millions of African language speakers while contributing to global linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "models",
                    "language",
                    "our"
                ]
            }
        ]
    },
    "A1.T4": {
        "source_file": "The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP",
        "caption": "Table 4: Aggregate paper counts and ratios between high-resource and African languages (2020-2024). The ratio shows the disparity in research visibility, with higher numbers indicating greater inequality in representation. Search term: “multilingual” “X” “large language models”",
        "body": "Source\nHigh-Resource\nAfrican\nRatio\n\n\nGS\n(Link)\n\n42,871\n2,121\n20.2\n\n\n\n\narXiv(Link)\n\n539\n16\n33.7\n\n\nIEEE (Link)\n\n487\n7\n69.6\n\n\nCORE(Link)\n\n9,011\n401\n22.5",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Source</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">High-Resource</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">African</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Ratio</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">GS\n<a class=\"ltx_ref ltx_href\" href=\"https://scholar.google.ca/scholar?as_q=%E2%80%9Cmultilingual%E2%80%9D+%E2%80%9CSwahili%E2%80%9D+%E2%80%9Clarge+language+models%E2%80%9D&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=&amp;as_publication=&amp;as_ylo=2020&amp;as_yhi=2024&amp;hl=en&amp;as_sdt=0%2C5\" title=\"\">(Link)</a>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t\">42,871</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t\">2,121</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\">20.2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">arXiv<a class=\"ltx_ref ltx_href\" href=\"https://arxiv.org/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=%22multilingual%22+%22Swahili%22+%22large+language+models%22&amp;terms-0-field=all&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-year=&amp;date-filter_by=date_range&amp;date-from_date=2020&amp;date-to_date=2024&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first\" title=\"\">(Link)</a>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">539</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">16</td>\n<td class=\"ltx_td ltx_align_right\">33.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">IEEE <a class=\"ltx_ref ltx_href\" href=\"https://ieeexplore.ieee.org/search/searchresult.jsp?queryText=%E2%80%9Cmultilingual%E2%80%9D%20%E2%80%9CSwahili%E2%80%9D%20%E2%80%9Clarge%20language%20models%E2%80%9D&amp;highlight=true&amp;returnFacets=ALL&amp;returnType=SEARCH&amp;matchPubs=true&amp;ranges=2020_2024_Year\" title=\"\">(Link)</a>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">487</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">7</td>\n<td class=\"ltx_td ltx_align_right\">69.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\">CORE<a class=\"ltx_ref ltx_href\" href=\"https://core.ac.uk/search/?q=%22multilingual%22+%22Swahili%22+%22large+language+models%22+AND+%28yearPublished%3E%3D2020+AND+yearPublished%3C%3D2024%29&amp;page=1\" title=\"\">(Link)</a>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">9,011</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">401</td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\">22.5</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "representation",
            "arxivlink",
            "aggregate",
            "source",
            "counts",
            "indicating",
            "paper",
            "term",
            "shows",
            "numbers",
            "ieee",
            "african",
            "between",
            "highresource",
            "language",
            "visibility",
            "ratios",
            "ratio",
            "inequality",
            "corelink",
            "disparity",
            "link",
            "models”",
            "greater",
            "“large",
            "research",
            "“multilingual”",
            "languages",
            "search",
            "“x”",
            "higher"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">This problem is compounded by a severe underrepresentation in the global NLP research community. Our analysis of mentions of the top 10 global languages versus the top 10 African languages across major academic databases reveals a stark imbalance. On average, for every paper discussing African languages in multilingual LLM contexts, there are 20 papers on global languages in Google Scholar (GS), 23 in COnnecting REpositories(CORE), 34 in arXiv, and 70 in The Institute of Electrical and Electronics Engineers (IEEE) (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Aggregated Paper Count Approach &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> in the Appendix). This 20-70x representation gap reinforces a self-perpetuating cycle of marginalization where limited research attention leads to poor technological support, which in turn discourages further research investment.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Despite representing nearly one-third of the world&#8217;s languages, African languages remain critically underserved by modern NLP technologies, with 88% classified as severely underrepresented or completely ignored in computational linguistics. We present the African Languages Lab (All Lab), a comprehensive research initiative that addresses this technological gap through systematic data collection, model development, and capacity building. Our contributions include: (1) a quality-controlled data collection pipeline, yielding the largest validated African multi-modal speech and text dataset spanning 40 languages with 19 billion tokens of monolingual text and 12,628 hours of aligned speech data; (2) extensive experimental validation demonstrating that our dataset, combined with fine-tuning, achieves substantial improvements over baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages; and (3) a structured research program that has successfully mentored fifteen early-career researchers, establishing sustainable local capacity. Our comparative evaluation against Google Translate reveals competitive performance in several languages while identifying areas that require continued development.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>To promote accessibility, we provide translations of this abstract in 10 African languages in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#A2\" title=\"Appendix B Abstract Translations &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, generated using our fine-tuned models.</span></span></span></p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP</span>\n</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The promise of artificial intelligence (AI) and natural language processing (NLP) to democratize information access remains unfulfilled for billions of speakers worldwide. Among the approximately 7,000 languages spoken globally, fewer than 20 receive substantial attention in NLP research <cite class=\"ltx_cite ltx_citemacro_citep\">(Magueresse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib27\" title=\"\">2020</a>)</cite>. This technological marginalization particularly affects low-resource languages (LRLs). Without a clearly established definition, LRLs are languages that exist at the periphery of the digital transformation, characterized by three critical deficits: (1) a scarcity of machine-readable corpora, (2) limited personalized computational technologies and trained language models, and (3) insufficient representation in global research communities <cite class=\"ltx_cite ltx_citemacro_citep\">(Nigatu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib32\" title=\"\">2024</a>; Issaka et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib23\" title=\"\">2024</a>; Magueresse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib27\" title=\"\">2020</a>)</cite>.\nWhile often serving substantial speaker populations, these languages face significant challenges in participating fully in the AI-driven information economy.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "language",
                    "research",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Africa, the scale of this crisis is staggering: over 2,000 languages are spoken across Africa (nearly one-third of all languages worldwide). Yet, a stunning 88% of African languages are \"severely underrepresented\" or \"completely ignored\" in computational linguistics <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib24\" title=\"\">2020</a>)</cite>. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S2.F1\" title=\"Figure 1 &#8227; 2.1 Community-Driven Research Initiatives &#8227; 2 Related Work &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, approximately 814 African languages are in danger of extinction. Countries like Nigeria, Cameroon, and the Ivory Coast have 171, 75, and 65 languages facing the most severe threats, respectively <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.ethnologue.com/\" title=\"\">https://www.ethnologue.com/</a></span></span></span>. This exclusion has far-reaching consequences, from poor educational and healthcare outcomes to preventing full participation in the digital economy <cite class=\"ltx_cite ltx_citemacro_citep\">(Laitin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib25\" title=\"\">2019</a>; Gessler and von&#160;der Wense, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib20\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Contributing to broader efforts to bridge this systemic technological gap, we present the African Languages Lab (All Lab), an initiative to democratize NLP technology for African languages. Founded in 2020, the All Lab operates through a coordinated team of dedicated researchers who combine three innovative elements:</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Systematic data infrastructure:</span> We developed a systematic, quality-controlled data collection framework powered by our \"All Voices\" platform. All Voices is a mobile-first platform specifically designed for community-driven multilingual data collection in low-resource contexts, enabling direct translation between African languages without English intermediation</p>\n\n",
                "matched_terms": [
                    "african",
                    "between",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comprehensive dataset development:</span> Through coordinated collection and validation efforts, we assembled the largest multi-modal dataset for African LRLs, encompassing 19 billion tokens across 40 languages with 12,628 hours of aligned speech data.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The landscape of African NLP research has evolved through three interconnected streams: community-driven initiatives, advances in multilingual modeling, and the development of evaluation frameworks. We examine how these efforts have shaped current capabilities and identify gaps our work addresses.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of African NLP has been shaped by complementary community and institutional efforts. Masakhane, comprising over 3,000 Slack members, exemplifies successful community-driven research, demonstrating that participatory approaches can produce high-quality datasets and models <cite class=\"ltx_cite ltx_citemacro_citep\">(Orife et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib35\" title=\"\">2020</a>)</cite>. Complementing this work, the \"Breaking the Unwritten Language Barrier\" project addresses challenges specific to unwritten and under-documented languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adda et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib1\" title=\"\">2016</a>)</cite>. Their work on languages like Basaa, Myene, and Embosi has established methodological approaches for speech recognition in LRLs.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These community efforts have been supported by institutional initiatives providing essential infrastructure. The Lacuna Fund has enabled dataset development <cite class=\"ltx_cite ltx_citemacro_citep\">(Rathi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib37\" title=\"\">2023</a>)</cite>, while Meta&#8217;s No Language Left Behind project has contributed architectural innovations for massively multilingual models <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib47\" title=\"\">2022</a>)</cite>. Additional infrastructure support has come from Mozilla&#8217;s Common Voice project <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib8\" title=\"\">2020</a>)</cite> for speech resources and the AI4D African Language Program <cite class=\"ltx_cite ltx_citemacro_citep\">(Siminyu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib42\" title=\"\">2021</a>)</cite> for benchmark development. The Deep Learning Indaba<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://deeplearningindaba.com/\" title=\"\">https://deeplearningindaba.com/</a></span></span></span> has contributed to research capacity building through its convenings, while platforms like Lanfrica have improved resource discoverability and research sharing across the continent <cite class=\"ltx_cite ltx_citemacro_citep\">(Emezue and Dossou, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib17\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evolution of multilingual LLMs has shown steady progress in language coverage and capabilities. Early approaches like mBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Muller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib30\" title=\"\">2021</a>)</cite> and XLM-R <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib12\" title=\"\">2020</a>)</cite> established initial benchmarks, supporting approximately 100 languages each. Subsequent developments included more focused models like mBART <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib26\" title=\"\">2020</a>)</cite>, mT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Xue et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib57\" title=\"\">2021</a>)</cite>, and XGLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Ersoy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib19\" title=\"\">2023</a>)</cite>, which traded broader language coverage for improved performance on specific language sets. The advent of massive LLMs further expanded these capabilities, with models like GPT-3, mGPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Shliazhko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib41\" title=\"\">2024</a>)</cite>, and BLOOM <cite class=\"ltx_cite ltx_citemacro_citep\">(Workshop et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib56\" title=\"\">2023</a>)</cite> supporting varying numbers of African languages. Also, Glot500-m <cite class=\"ltx_cite ltx_citemacro_citep\">(Imani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib22\" title=\"\">2023</a>)</cite> extends support to 511 languages and the SERENGETI and Cheetah models supports about 517 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adebara et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib4\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib3\" title=\"\">2024</a>)</cite>. Additional progress has come from the Aya model, which demonstrates instruction-following capabilities across 101 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(&#220;st&#252;n et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib59\" title=\"\">2024</a>)</cite>, and specialized models like AfroLM, which focuses on 23 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Dossou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib14\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages",
                    "language",
                    "numbers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While not specifically trained in African languages, English-centric LLMs such as GPT-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib34\" title=\"\">2024</a>)</cite>, Gemini <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib46\" title=\"\">2025</a>)</cite>, and Llama <cite class=\"ltx_cite ltx_citemacro_citep\">(Wendler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib53\" title=\"\">2024</a>)</cite> have shown capability in handling some African languages, <cite class=\"ltx_cite ltx_citemacro_citep\">(Robinson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib40\" title=\"\">2023</a>; Ojo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib33\" title=\"\">2024</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib58\" title=\"\">2024</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib13\" title=\"\">2024</a>)</cite>, though their performance generally does not match that of specialized models; underscoring the need for dedicated resources and architectures.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of evaluation frameworks has enabled systematic progress measurement in African NLP across diverse task domains. MasakhaNER provides NER datasets for 10 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib5\" title=\"\">2021</a>)</cite>, AfriSenti offers sentiment analysis benchmarks in 14 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Muhammad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib28\" title=\"\">2023</a>)</cite>, and AFROMT establishes standardized translation benchmarks for 8 languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Reid et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib39\" title=\"\">2021</a>)</cite>. IrokoBench unifies evaluation across natural language inference, mathematical reasoning, and multiple-choice QA in 17 African languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Adelani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib6\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these developments and advances, significant challenges remain in African NLP research <cite class=\"ltx_cite ltx_citemacro_citep\">(Adebara and Abdul-Mageed, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib2\" title=\"\">2022</a>; Issaka et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib23\" title=\"\">2024</a>)</cite>. Our work builds upon these foundations while addressing several key limitations in existing approaches, such as robust team coordination, cross-initiative knowledge transfer, deduplication of efforts, and intentional skill set development.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">All Voices Platform.</span> To address the fundamental challenge of data scarcity in African languages, we developed All Voices, a mobile-first platform that stands as the only solution specifically designed for data collection in any LRL. The platform&#8217;s innovative approach enables direct translation between LRLs without requiring English as an intermediary, addressing a critical gap in the existing data collection infrastructure. In addition, All Voices distinguishes itself through its multimodal capabilities, which support the collection and validation of text and audio data. The platform features an intuitive, user-friendly interface that encourages broad participation, complemented by gamification elements, including a global leaderboard system that promotes user engagement. Importantly, All Voices is open and free to everyone, aligning with our mission to democratize language technology development. All Voices contributors provide informed consent for their contributions to be used for research purposes, including dataset creation, model training, and open-source distribution, with full transparency regarding data usage and the right to withdraw consent at any time.</p>\n\n",
                "matched_terms": [
                    "language",
                    "african",
                    "research",
                    "between",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our data processing implements a robust two-tier approach combining general normalization with language-specific processing. The general normalization phase addresses universal text artifacts through Unicode normalization, character encoding standardization, and structural cleaning, including HTML removal and symbol standardization. The language-specific processing phase implements specialized handling for African language features, including morphological analysis, script variant normalization, and tone mark standardization, language identification, with custom rule sets developed for specific language families.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, our translation validation methodology implements a robust statistical framework for assessing translation quality through quantitative analysis of character-level distributions. The validation metric employs character ratio analysis between source and target texts, computed as the ratio of target text length to source text length. We analyze these ratios using z-score normalization within language-specific distribution, enabling the detection of statistical outliers while accounting for natural variations in text length across different language pairs. This approach is augmented with character overlap detection to identify potential artifacts or inappropriate text preservation, particularly crucial for languages sharing similar orthographic features.</p>\n\n",
                "matched_terms": [
                    "language",
                    "source",
                    "ratios",
                    "ratio",
                    "between",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Also, the threshold determination process implements an adaptive sampling methodology. For each language pair, we establish baseline distributions through initial sampling of 10,000 translation pairs, employing Kernel Density Estimation for robust distribution modeling. This approach effectively captures the non-Gaussian characteristics frequently observed in cross-lingual character distributions. Thresholds are dynamically computed using a modified Tukey method with an adaptive multiplier. This adaptive threshold mechanism automatically calibrates to language-specific characteristics, implementing more stringent filtering for language pairs that exhibit consistent ratios while allowing appropriate flexibility for pairs with inherently higher variability. The resulting validation framework effectively identifies and filters anomalous translations while maintaining sensitivity to legitimate linguistic variations across diverse African language families. The processed data sets are structured according to HuggingFace&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/\" title=\"\">https://huggingface.co/</a></span></span></span> Dataset specifications, enabling seamless API integration.</p>\n\n",
                "matched_terms": [
                    "higher",
                    "african",
                    "language",
                    "ratios"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the utility of our dataset and establish baselines for our languages, we experimented with Llama-3.2-1B <cite class=\"ltx_cite ltx_citemacro_citep\">(Grattafiori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib21\" title=\"\">2024</a>)</cite>. We chose this model as our base because of its demonstrated multilingual capabilities and efficient parameter scaling, making it suitable for LRLs. Critically, while capable of tokenizing, this model has not been explicitly trained on any African language in our dataset. Thus, providing a cleaner baseline for measuring the utility of our dataset during fine-tuning without risk of data contamination.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed full fine-tuning rather than parameter-efficient methods, as preliminary experiments with Quantization-aware Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_citep\">(&#220;st&#252;n et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib59\" title=\"\">2024</a>)</cite> yielded insufficient performance gains for our target languages. Our training pipeline uses supervised learning with a standardized instruction template: \"Translate the following English text to X:\", where X denotes the target African language.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training leveraged NVIDIA H100 GPUs with the following parameters: batch size of 64 with 4-step gradient accumulation, maximum sequence length of 1024 tokens for both input and output, single epoch training using all available parallel data per language, learning rate of <math alttext=\"5.0\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mn>5.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5.0\\times 10^{-5}</annotation></semantics></math> with cosine scheduling and 0.15 warm-up ratio, and BF16 mixed precision for memory efficiency. During inference, we maintained consistency with batch size 64 while using temperature 0.1, top-p 0.95, top-k 50, and maximum output length of 1024 tokens to balance diversity with quality and reproducibility.</p>\n\n",
                "matched_terms": [
                    "language",
                    "ratio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model performance was evaluated using a complementary set of metrics: BiLingual Evaluation Understudy (BLEU) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wieting et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib55\" title=\"\">2019</a>)</cite>, which measures n-gram precision; METEOR <cite class=\"ltx_cite ltx_citemacro_citep\">(Banerjee and Lavie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib9\" title=\"\">2005</a>)</cite>, which accounts for word stems and synonyms; COMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib38\" title=\"\">2020</a>)</cite>, which leverages multilingual embeddings to assess semantic similarity; and ChrF++ <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib51\" title=\"\">2025</a>)</cite>, which operates on character-level n-grams to better capture morphological variations common in African languages. Additionally, we employed Translation Edit Rate (TER) <cite class=\"ltx_cite ltx_citemacro_citep\">(Snover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib45\" title=\"\">2006</a>)</cite>, which quantifies the minimum number of edits required to transform the hypothesis into the reference translation (where lower scores indicate better quality), and AfriCOMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#bib.bib50\" title=\"\">2024</a>)</cite>, a neural metric specifically trained on African language pairs to better capture language-specific quality nuances.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our comprehensive data collection yielded 19,012,019,696 tokens of monolingual text and 12,628 hours of aligned audio across 40 African languages (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S4.T2\" title=\"Table 2 &#8227; 4 Results &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). Our analysis reveals distinct stratification patterns highlighting the digital divide within African languages themselves.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">High-resource audio languages (&gt;1,000 hours):</span> Kinyarwanda (3,839.00h), Luganda (1,727.80h), Swahili (1,115.00h), and Arabic (2,721.52h) dominate in audio availability, often due to large-scale speech corpora or broadcast archives.</p>\n\n",
                "matched_terms": [
                    "highresource",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Established audio languages (500&#8211;1,000 hours):</span> This tier is notably sparse, underscoring the scarcity of mid-scale speech datasets in African languages.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experimental evaluation across 31 African languages reveals substantial and systematic improvements through fine-tuning, with distinct patterns emerging across language families and resource levels (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.05644v1#S4.T3\" title=\"Table 3 &#8227; 4 Results &#8227; The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Nine languages were excluded from evaluation due to insufficient training data or absence from the FLORES-200 benchmark.</p>\n\n",
                "matched_terms": [
                    "african",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline Performance.</span> The base Llama-3.2-1B model demonstrates limited but non-trivial capability for African languages, revealing interesting patterns of cross-lingual transfer. ChrF++ scores range from 2.00 (Wolof) to 44.76 (Afrikaans), with a mean of 8.10, indicating minimal character-level understanding for most languages. COMET scores cluster between 0.16-0.68 (mean: 0.32), suggesting some semantic comprehension despite poor surface realization. Notably, Afrikaans shows exceptional baseline performance (ChrF++ 44.76, BLEU 32.98), leveraging its Germanic roots and Latin script. The extremely low baseline BLEU scores (mean: 2.27) across most languages confirm the model&#8217;s inability to produce accurate n-gram sequences without language-specific training.</p>\n\n",
                "matched_terms": [
                    "african",
                    "between",
                    "indicating",
                    "languages",
                    "shows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-tuning Impact.</span> Our dataset enables noticeable performance improvements, with average gains of +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points. The magnitude of improvement correlates inversely with baseline performance, suggesting effective transfer learning rather than simple memorization. Swahili exhibits the largest absolute ChrF++ improvement (+63.27 points), achieving near-parity with Google Translate (72.27 vs 75.81). Sesotho shows remarkable gains across all metrics (+61.79 ChrF++, +51.16 BLEU), while maintaining competitive performance against Google Translate. Languages with minimal baselines, including Fula, Wolof, and Kikongo, demonstrate that even severely under-resourced languages benefit substantially from targeted fine-tuning, achieving functional translation capability where none existed before.</p>\n\n",
                "matched_terms": [
                    "shows",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparison with Google Translate.</span> Where available (22 languages), comparison with Google Translate reveals three distinct performance categories. First, languages where our models achieve competitive or superior performance: Yoruba (30.88 vs 21.05 ChrF++), Arabic (31.52 vs 28.46), and notably Twi, where we substantially outperform Google Translate (46.80 vs 31.48). Second, languages where Google maintains clear advantages, particularly in high-resource cases like Kinyarwanda (24.65 vs 70.27) and Swahili (72.27 vs 75.81), reflecting their extensive training data. Third, the languages where Google Translate offers no support, highlighting our contribution to genuinely LRL coverage.</p>\n\n",
                "matched_terms": [
                    "highresource",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language-Specific Patterns.</span> Three response profiles emerge from our analysis. High-responder languages (Swahili, Sesotho, Hausa) show dramatic improvements exceeding 40 ChrF++ points, suggesting optimal alignment between our dataset characteristics and model architecture. Steady improvers (Igbo, Shona, Somali) demonstrate consistent gains of 25-30 points across metrics, indicating robust but not exceptional adaptation. Challenging cases (Fon, Wolof, Bambara) show limited improvements despite fine-tuning, likely requiring specialized tokenization or architectural modifications to address their unique linguistic features.</p>\n\n",
                "matched_terms": [
                    "between",
                    "indicating",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation Edit Rate Analysis.</span> The dramatic TER reductions, averaging 580.13 points lower after fine-tuning, provide crucial practical insights. Languages like Swahili achieve TER scores comparable to Google Translate (23.77), indicating production-ready quality requiring minimal post-editing. Even languages with modest BLEU improvements show substantial TER reductions, suggesting improved fluency and coherence that traditional metrics may not fully capture. This pattern holds particular significance for scenarios where post-editing cost determines practical viability.</p>\n\n",
                "matched_terms": [
                    "indicating",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Metric Correlations.</span> While surface metrics (BLEU, ChrF++) show high correlation, the divergence between these and neural metrics (COMET, AfriCOMET) reveals important quality dimensions. Languages like Ewe show minimal COMET improvement (0.04) despite substantial ChrF++ gains (30.43 points), suggesting character-level improvements without the corresponding semantic enhancement. In contrast, Arabic shows strong COMET gains (0.33) with modest improvement in ChrF++, indicating semantic preservation despite surface-level challenges. These patterns underscore the importance of multi-metric evaluation for morphologically diverse African languages.</p>\n\n",
                "matched_terms": [
                    "african",
                    "between",
                    "indicating",
                    "languages",
                    "shows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We have presented the African Languages Lab, a research initiative addressing the critical underrepresentation of African languages in NLP through systematic data collection, model development, and capacity building. Our contributions include a validated dataset of 19 billion tokens and 12,628 hours of aligned speech across 40 languages, substantial performance improvements averaging +23.69 ChrF++ over baseline models, the All Voices platform, and a structured mentorship program developing fifteen early-career researchers. Thus, we demonstrate that the technological marginalization of African languages, while severe, is not intractable.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As language technologies increasingly mediate access to information, education, and economic opportunities, ensuring equitable coverage becomes not merely a technical challenge but a moral imperative. The African Languages Lab demonstrates that this imperative can be met through coordinated research, community engagement, and sustained investment in both technical infrastructure and human capacity, establishing a sustainable path forward for the world&#8217;s underserved linguistic communities.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments utilize Llama-3.2-1B as the sole base model, which, while demonstrating the utility of our dataset, may underestimate potential gains achievable with larger-scale architectures. The performance variance across language families, from 63.27 ChrF++ improvement for Swahili to minimal gains for Fon (-1.10), suggests that optimal model selection likely varies by linguistic typology. Additionally, our evaluation of 31 of 40 collected languages reflects FLORES-200 coverage limitations, potentially obscuring insights from the most critically under-resourced languages in our dataset.</p>\n\n",
                "matched_terms": [
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite assembling 19 billion tokens, our dataset exhibits a 147,000&#215; disparity between the highest-resourced (Amharic: 2,944.95M tokens) and lowest-resourced (Fang: 0.02M tokens) languages. This imbalance directly correlates with performance outcomes: languages with &gt;1B tokens achieve average ChrF++ scores of 45.66, while those with &lt;100M tokens average 24.31. Furthermore, 13 languages lack audio data entirely, limiting multimodal model development. Our validation pipeline, while statistically grounded, operates without native speaker verification for 73% of languages, potentially missing dialectal variations that affect 28% of evaluated translations showing COMET-ChrF++ divergence exceeding 0.3.</p>\n\n",
                "matched_terms": [
                    "disparity",
                    "between",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, these constraints delineate several clear pathways for advancement. One direction is to explore architecture-specific optimizations for morphologically complex languages. Another is to implement active learning strategies that help address data imbalances. It is also important to develop evaluation metrics that are more sensitive to African language typologies. These limitations inform our ongoing work and highlight key areas for future research in African NLP.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "language",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">They also underscore the need for continued investment in computational resources, human expertise, and infrastructure development to support comprehensive technology development for African languages.</p>\n\n",
                "matched_terms": [
                    "african",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our structured research development program has mentored fifteen early-career researchers across four institutions through one-on-one mentorship, project development support, and transitions into extended research roles. This investment in local research leadership establishes sustainable capacity for African NLP development, ensuring that technical advancement aligns with cultural and linguistic expertise. By prioritizing skill development alongside technical innovation, we contribute to a sustainable talent pipeline that positions African researchers to lead future developments in their languages.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work directly advances United Nations Sustainable Development Goals in education and inequality reduction through increased digital representation of marginalized languages. The platform enables community-led content creation and facilitates open knowledge transfer, democratizing access to digital tools while preserving linguistic and cultural heritage. With 88% of African languages severely underrepresented or completely ignored in computational linguistics, and 814 languages facing extinction risk, our framework provides critical infrastructure for language preservation.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "language",
                    "african",
                    "inequality",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Guided by Ubuntu philosophy&#8212;emphasizing inclusivity, interdependence, and openness&#8212;we establish a framework for equitable NLP development. Our roadmap encompasses expanding language coverage, optimizing model architectures for low-resource languages, and deepening research collaborations. We acknowledge persistent challenges including limited commercial viability for some LRL technologies and infrastructural constraints, yet our results demonstrate that systematic community engagement can effectively address technological marginalization.\nThrough this comprehensive approach integrating technical innovation, cultural preservation, educational empowerment, and economic inclusion, we provide replicable models for equitable language technology development that can benefit millions of African language speakers while contributing to global linguistic diversity.</p>\n\n",
                "matched_terms": [
                    "african",
                    "research",
                    "language",
                    "languages"
                ]
            }
        ]
    }
}