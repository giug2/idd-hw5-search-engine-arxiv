{
    "S1.T1": {
        "source_file": "ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark",
        "caption": "Table 1: The comparative analysis of our dataset against previous speech style datasets. ✓ indicates that only part of the speeches meet the condition and * denotes style version of the dataset.",
        "body": "Dataset\nPaired speech\n# Styles\nDescription type\nInstruction\nDuration\nOpen-source\n\n\n\n\n\nPromptSpeech [6]\n\n✗\n-\nNatural language\nNone\nUnavailable\n\n✓\n\n\n\n\nCapSpeech* [11]\n\n✗\n28\nNatural language\nNone\n90h\n✓\n\n\n\nEars [12]\n\n✗\n22\nLabel\nNone\n100h\n✓\n\n\n\nExpresso [13]\n\n\n✓\n\n9\nLabel\nNone\n42h total / 11h paired\n✓\n\n\n\nESD [14]\n\n✓\n5\nLabel\nNone\n29h\n✓\n\n\n\nInstructSpeech [2]\n\n✓\n22\nNatural language\nCoarse-grained\nUnavailable\n✗\n\n\nISSE (Ours)\n✓\n28\nNatural language\nFine-grained\n382h\n✓",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Paired speech</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\"># Styles</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Description type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Instruction</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Duration</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Open-source</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">PromptSpeech </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#B80000;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Natural language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Unavailable</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"/><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"10.29\" id=\"S1.T1.pic3\" overflow=\"visible\" style=\"vertical-align:-2.47px\" version=\"1.1\" viewbox=\"0 0 12.33 10.29\" width=\"12.33\"><g transform=\"translate(0,10.29) matrix(1 0 0 -1 0 0) translate(6.16,0) translate(0,5.15)\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -5.33 -4.32)\"><foreignobject height=\"8.63\" overflow=\"visible\" style=\"--fo_width :0.83em;--fo_height:0.67em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.63)\" width=\"10.67\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#00E000;\">&#10003;</span></span></span></foreignobject></g><g color=\"#B80000\" fill=\"#B80000\" stroke=\"#B80000\" stroke-width=\"0.8pt\" style=\"--ltx-stroke-color:#B80000;--ltx-fill-color:#B80000;--ltx-fg-color:#B80000;\"><path d=\"M -5.61 4.59 L 5.61 -4.59\" style=\"fill:none\"/></g></g></svg></span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CapSpeech* </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib11\" title=\"\">11</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#B80000;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Natural language</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">90h</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#00E000;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Ears </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#B80000;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Label</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">100h</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#00E000;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Expresso </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\"/><span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"10.29\" id=\"S1.T1.pic4\" overflow=\"visible\" style=\"vertical-align:-2.47px\" version=\"1.1\" viewbox=\"0 0 12.33 10.29\" width=\"12.33\"><g transform=\"translate(0,10.29) matrix(1 0 0 -1 0 0) translate(6.16,0) translate(0,5.15)\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"matrix(1.0 0.0 0.0 1.0 -5.33 -4.32)\"><foreignobject height=\"8.63\" overflow=\"visible\" style=\"--fo_width :0.83em;--fo_height:0.67em;--fo_depth :0em;\" transform=\"matrix(1 0 0 -1 0 8.63)\" width=\"10.67\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#00E000;\">&#10003;</span></span></span></foreignobject></g><g color=\"#B80000\" fill=\"#B80000\" stroke=\"#B80000\" stroke-width=\"0.8pt\" style=\"--ltx-stroke-color:#B80000;--ltx-fill-color:#B80000;--ltx-fg-color:#B80000;\"><path d=\"M -5.61 4.59 L 5.61 -4.59\" style=\"fill:none\"/></g></g></svg></span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Label</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">42h total / 11h paired</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#00E000;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ESD </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#00E000;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Label</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29h</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#00E000;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">InstructSpeech </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#00E000;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Natural language</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Coarse-grained</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Unavailable</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#B80000;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ISSE (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#00E000;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Natural language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Fine-grained</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">382h</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#00E000;\">&#10003;</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "previous",
            "type",
            "esd",
            "ours",
            "datasets",
            "coarsegrained",
            "against",
            "42h",
            "our",
            "speech",
            "ears",
            "label",
            "finegrained",
            "11h",
            "promptspeech",
            "analysis",
            "instruction",
            "29h",
            "none",
            "styles",
            "language",
            "duration",
            "meet",
            "description",
            "dataset",
            "comparative",
            "opensource",
            "denotes",
            "version",
            "capspeech",
            "382h",
            "instructspeech",
            "only",
            "part",
            "indicates",
            "expresso",
            "90h",
            "100h",
            "paired",
            "total",
            "isse",
            "natural",
            "speeches",
            "style",
            "condition",
            "unavailable"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A key prerequisite for training text-guided speech generation models is the availability of paired &#8220;speech-text description&#8221; datasets.\nIn the context of text-to-speech (TTS), several studies have annotated speech with textual style descriptions to produce more expressive outputs </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as shown in&#160;Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (a).\nHowever, these datasets are constructed for stylized speech generation, which cannot describe the change between source speech and target speech when applying it to the style editing task.\nThis makes the editing objective ill-posed and hindering the disentanglement of style from content and timbre&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo enable attribute-specific editing, Huang et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> constructed a dataset of source-target speech pairs with corresponding editing instructions, as illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(b).\nDespite its contribution, these datasets exhibit two notable limitations.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">First, existing datasets suffer from limitations in the design of instructions.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nThe provided instructions are often overly simplistic and lack linguistic diversity, typically restricted to templated commands such as &#8220;Change the emotion of speech&#8221; or &#8220;Change the speaking speed of speech.&#8221;\nMoreover, they are usually coarse-grained, focusing on only a single attribute to modify.\nSuch constraints hinder the ability to represent more realistic and expressive editing scenarios.\nIn contrast, fine-grained instructions (e.g., &#8220;Convert the source speech to a medium-pitched, raspy monotone that conveys awe&#8221;) help enable flexible, multi-attribute speech style editing, as shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(c).\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Secondly, most existing open-source paired speech style datasets remain limited in scale and diversity. </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAs shown in&#160;Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ESD&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> contains only 29 hours of recordings, while other datasets such as Ears&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Expresso&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are similarly constrained.\nThese datasets not only suffer from relatively small amounts of data, but also lack sufficient variety in speaking styles, prosodic patterns, and content coverage.\nSuch limitations in both quantity and diversity pose significant challenges and have become a major bottleneck for advancing research in speech style editing.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we present ISSE, a novel </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">I</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">nstruction-guided </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">peech </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">tyle </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">E</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">diting dataset.\nISSE contains nearly 400 hours of speech and over 100,000 source-target pairs, and is designed with fine-grained editing instructions, substantially exceeding the simplicity and limited coverage of prior resources (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nAs the performance of style editing models hinges on the consistency between fine-grained instructions and the paired speech content, we design an effective data construction pipeline.\nSpecifically, we integrate expressive TTS and voice conversion (VC) to generate reliable source-target pairs that preserve content and timbre while varying style attributes.\nWe further conduct quality evaluation of content preservation, speaker identity consistency, and style similarity to filter low-quality data.\nFinally, a large language model (LLM) is employed to describe the transformation between paired speech samples and generate the fine-grained instruction.\nTo validate the effectiveness of the proposed ISSE, we establish a benchmark using a baseline instruction-guided autoregressive speech model, LlasaEdit.\nWe evaluate LlasaEdit trained on ISSE against the same model trained on ESD under both in-domain and cross-domain settings.\nThe experimental results consistently confirm the effectiveness of ISSE for instruction-guided speech style editing.\n</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech style editing refers to modifying the stylistic properties of speech while preserving its linguistic content and speaker identity.\nHowever, most existing approaches depend on explicit labels or reference audio, which limits both flexibility and scalability.\nMore recent attempts to use natural language descriptions remain constrained by oversimplified instructions and coarse style control.\nTo address these limitations, we introduce an <span class=\"ltx_text ltx_font_bold\">I</span>nstruction-guided <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">S</span>tyle <span class=\"ltx_text ltx_font_bold\">E</span>diting Dataset (ISSE).\nThe dataset comprises nearly 400 hours of speech and over 100,000 source-target pairs, each aligned with diverse and detailed textual editing instructions.\nWe also build a systematic instructed speech data generation pipeline leveraging large language model, expressive text-to-speech and voice conversion technologies to construct high-quality paired samples.\nFurthermore, we train an instruction-guided autoregressive speech model on ISSE and evaluate it in terms of instruction adherence, timbre preservation, and content consistency.\nExperimental results demonstrate that ISSE enables accurate, controllable, and generalizable speech style editing compared to other datasets.\nThe project page of ISSE is available at <a class=\"ltx_ref ltx_href\" href=\"https://ychenn1.github.io/ISSE/\" title=\"\">https://ychenn1.github.io/ISSE/</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "dataset",
                    "paired",
                    "isse",
                    "natural",
                    "datasets",
                    "style",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nISSE dataset, Speech style editing, Instruction-guided editing, Natural language instructions</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "dataset",
                    "isse",
                    "natural",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech style editing refers to the task of modifying the stylistic properties of speech, such as emotional tone, speaking rate, and prosodic delivery, while preserving its linguistic content and speaker identity &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nExisting approaches in this domain largely rely on explicit labels&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or reference audio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to provide editing signals, which constrains their flexibility and scalability&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWith the rapid advances in natural language modeling </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a new paradigm has emerged: guiding speech generation through natural language descriptions </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "natural",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We release the ISSE dataset, which contains approximately 400 hours of speech and more than 100,000 source-target pairs annotated with fine-grained editing instructions.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "isse",
                    "finegrained",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose an instructed speech data pipeline that integrates existing speech datasets with TTS, VC, quality assessment, and LLM tools to generate high-quality speech pairs with fine-grained editing instructions.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "finegrained",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We establish a benchmark with an instruction-guided autoregressive model trained on ISSE. Experiments demonstrate the effectiveness of both the dataset and the proposed model in editing style while preserving transcript and identity.</span>\n</p>\n\n",
                "matched_terms": [
                    "isse",
                    "dataset",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we propose an instructed speech data pipeline to generate source and target speech pairs with corresponding style instructions.\nAs shown in&#160;Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the pipeline consists of three stages of data processing:\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Stage 1: Paired Speech Generation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Generates speech samples in diverse styles and then converts them to share the same speaker identity while maintaining the style differences;\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Stage 2: Quality Evaluation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Validates the generated speech quality and consistency based on predefined criteria;\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Stage 3: Instruction Design</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Formulates natural language instructions describing the style transformation between the paired speech samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "styles",
                    "paired",
                    "natural",
                    "style",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We adopt two existing speech datasets, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Ears</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Expresso</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as the source data.\nBoth datasets contain speeches, transcriptions, and the corresponding style descriptions annotated with the tool used in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo minimize the influence of speaker identity during data construction, non-stylistic words (e.g., he, she) are removed from the style descriptions.\nFrom the processed datasets, we first sample an anchor speech, with its transcription and speaker identity serving as fixed attributes for the subsequent synthesis process.\nWe then randomly select </span>\n  <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">n</mi>\n      <annotation encoding=\"application/x-tex\">n</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> additional samples with distinct speaking styles, referred to as style reference speeches.\nTheir style descriptions are used to specify the target styles for synthesis.\nTo produce expressive speech variations, we employ EmoCapTTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor each case, EmoCapTTS takes as input the transcription of the anchor speech and the style description of a style reference speech, and generates a stylized target that preserves the linguistic content while adopting the style of the style reference speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ears",
                    "styles",
                    "expresso",
                    "description",
                    "datasets",
                    "speeches",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The EmoCapTTS model lacks explicit control over speaker identity, so the generated speech differs from the anchor speech in timbre.\nThis variability makes it difficult to design editing instructions that isolate style changes without being confounded by speaker changes.\nTo address this issue, we employ Chatterbox </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a voice conversion model, to adjust the timbre of each generated sample so that it matches the anchor speech while retaining the intended style.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To ensure the quality and controllability of the generated speech, we propose a quality assessment process that evaluates generated samples along three aspects: content preservation, style similarity, and speaker identity consistency.\nFor content intelligibility and accuracy, we employ a widely-used speech recognition model, Whisper-large-v3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to transcribe the generated speech into text.\nThis transcription is then compared against the source text transcript to calculate word error rate (WER) for content accuracy and intelligibility verification.\nFor style alignment, we utilize a Speech Emotion Recognition (SER) model, emotion2vec </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to assess style similarity </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{sty}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">sty</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{sty}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between the generated speech and its style reference speeches.\nFor speaker consistency, a speaker verification (SV) model ECAPA-TDNN </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used to extract speaker embeddings from both the generated and anchor speech samples.\nThe cosine similarity </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{spk}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">spk</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{spk}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between these embeddings is computed to quantitatively assess whether the speaker&#8217;s identity is accurately preserved.\nGenerated samples are retained only when they satisfy all these criteria: WER </span>\n  <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&lt;</mo>\n      <annotation encoding=\"application/x-tex\">&lt;</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 10, </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{sty}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">sty</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{sty}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&gt;</mo>\n      <annotation encoding=\"application/x-tex\">&gt;</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.5 , and </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{spk}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">spk</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{spk}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&gt;</mo>\n      <annotation encoding=\"application/x-tex\">&gt;</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.5.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speeches",
                    "style",
                    "only",
                    "against"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Through Stages 1 and 2, we obtain high-quality source-target speech pairs in which both source and target speech samples share the same linguistic content and speaker identity but differ in style descriptions.\nThe goal of Stage 3 is to generate natural language editing instructions that accurately describe the transformation from the source style to the target style.\nLeveraging the language understanding capabilities of Qwen3-8B </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we transform the style description into high-quality editing instructions for each source-target speech pair.\nSpecifically, we first prompt Qwen3-8B to compare the style descriptions of the source and target speech samples and explicitly identify the differences between them.\nBased on this comparison, we instruct the LLM to formulate a natural language editing instruction using the template: &#8220;Convert the source speech to </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">{target style}</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.&#8221;\nTo enhance instruction diversity and avoid repetitive phrasing, we further prompt the model to generate multiple alternative formulations for each transformation, such as &#8220;Convert the source speech to a whispering style with a high-pitched voice.&#8221; or &#8220;Convert the source speech into a high-pitched whisper.&#8221;</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "description",
                    "natural",
                    "style",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Overview of the ISSE.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nUsing the proposed pipeline, we construct the Instructed Speech Style Editing Dataset (ISSE).\nIt is an open-source paired speech style dataset comprising approximately 382 hours of speech.\nWithin it, the &#8220;Real&#8221; portion (recorded human speech) amounts to around 90 hours, and the generated speech occupies about 292 hours.\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates the data distribution in ISSE.\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(a) shows the distribution of audio length, where most speech samples fall into short-to-medium ranges, providing balanced coverage across durations.\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(b) illustrates the style distribution, demonstrating that ISSE encompasses a broad variety of speaking styles with sufficient samples per category, in contrast to prior datasets that are limited to a few emotional labels </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "styles",
                    "dataset",
                    "opensource",
                    "paired",
                    "isse",
                    "datasets",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Quality of the ISSE.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (c)-(e) present the quality evaluation of the ISSE dataset. Specifically, Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(c) reports the quality of intelligibility, measured using ASR-based word error rate to ensure that the generated speeches remain linguistically accurate. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(d) evaluates the quality of identity preservation through speaker similarity, confirming that speaker characteristics are maintained across paired samples. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(e) assesses the quality of style preservation, where style similarity against reference speeches demonstrates that stylistic variations are effectively retained. Together, these results highlight that ISSE achieves a balance between intelligibility, identity consistency, and style fidelity.</span>\n</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "isse",
                    "paired",
                    "speeches",
                    "style",
                    "against"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we establish a benchmark for the instruction-guided speech style editing task.\nFirst, we introduce a baseline model, LlasaEdit, built upon </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe then train LlasaEdit separately on ISSE and ESD to demonstrate the effectiveness of the model and the advantages of our proposed dataset in editing style while preserving transcript and speaker identity.\nFinally, we validate the generalization of ISSE by evaluating through TTS and expressive speech synthesis (ESS) conditioned on style descriptions.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "esd",
                    "dataset",
                    "isse",
                    "style",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.1 LlasaEdit &#8227; 4 Benchmark &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we propose LlasaEdit, which consists of the XCodec2 audio codec and a large transformer-based speech generation model initialized from Llasa&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nEach training instance is represented as a triplet </span>\n  <math alttext=\"(x_{i},y_{i},c_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">x</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">y</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(x_{i},y_{i},c_{i})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the source speech, </span>\n  <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the target speech (same speaker and transcript but different style), and </span>\n  <math alttext=\"c_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">c_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the corresponding style instruction.\nWe first encode </span>\n  <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into discrete token sequences using XCodec2, while </span>\n  <math alttext=\"c_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">c_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is tokenized into text tokens.\nThe generation model is then trained to model the conditional distribution of target tokens given the source tokens and the instruction, predicting the target sequence token by token under teacher forcing.\nThe training objective is to minimize the negative log-likelihood of the target tokens.\nIn our experiments, we fine-tune the Llasa model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with Low-Rank Adaptation (LoRA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using rank </span>\n  <math alttext=\"r=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">64</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r=64</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\alpha=128\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">128</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha=128</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applied to both the attention and MLP layers.\nWe train with AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (learning rate </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) for 5 epochs with a batch size of 2, employing learning-rate decay and periodic checkpointing, on 8&#215; NVIDIA RTX A100 GPUs.\nAt inference, the model takes the source tokens and style instruction, autoregressively generates the edited target tokens, and reconstructs the waveform using the XCodec2 decoder.\nFor evaluation, we adopt the same metrics as in Quality Evaluation (Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 Quality Evaluation &#8227; 2 Dataset Construction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), along with UTMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for naturalness assessment.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "denotes",
                    "style",
                    "instruction",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of ISSE, we train the LlasaEdit model on the ISSE dataset and compare its performance with the same model trained on ESD, the closest open-source paired speech dataset. Following </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we convert the categorical labels in ESD into natural language instructions and adopt the same training parameters as used for ISSE. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Speech Style Editing &#8227; 4 Benchmark &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, in the in-domain setting, the model trained and tested on ISSE consistently outperforms its ESD counterpart across all metrics, achieving lower WER (8.06 vs. 10.07), higher </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{sty}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">sty</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{sty}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (0.68 vs. 0.64), higher </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{spk}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">spk</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{spk}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (0.58 vs. 0.49), and higher UTMOS (4.29 vs. 4.01). These results highlight the benefits of larger and more diverse data for enhancing generative capabilities. Furthermore, under the cross-domain setting, the model trained on ISSE even surpasses the in-domain performance of the ESD-trained model, with only a marginal shortfall in style similarity. In contrast, the model trained on ESD suffers from severe degradation when evaluated on the ISSE test set, yielding a catastrophic WER of 68.17.\nThose results demonstrate the comprehensive advantages of ISSE over ESD and confirm the effectiveness of its scale, data diversity, and fine-grained style instructions.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "esd",
                    "finegrained",
                    "dataset",
                    "opensource",
                    "paired",
                    "isse",
                    "natural",
                    "style",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The proposed ISSE can also be extended to TTS tasks by leveraging speech samples paired with their style transcriptions.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Speech Style Editing &#8227; 4 Benchmark &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, fine-tuning Llasa on the TTS task reduces WER from 5.48 to 3.74 and yields a slight improvement in naturalness.\nMoreover, the ESS model conditioned on style descriptions further enhances style controllability, achieving a higher style similarity score (0.60) and gender accuracy (0.96), while maintaining comparable naturalness.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "isse",
                    "paired",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we introduced ISSE, a dataset for instruction-guided speech style editing, constructed through an instructed speech data pipeline with LLM, TTS, and VC technologies.\nBenchmarking with our LlasaEdit model demonstrates that ISSE enables accurate and controllable style editing, providing a valuable resource for the speech community.\nNevertheless, the dataset is limited to English, which may constrain its applicability in multilingual scenarios.\nFuture work could extend ISSE to broader languages and richer style dimensions, and explore integration with larger generative models to further advance natural language-guided speech editing.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "isse",
                    "natural",
                    "style",
                    "our"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark",
        "caption": "Table 2: Results of LlasaEdit trained on different datasets. LlasaEdit-ESD and LlasaEdit-ISSE denote models trained on ESD and ISSE, respectively.",
        "body": "Model\nEvalset\nWER\nSIMsty\\textrm{SIM}_{\\textrm{sty}}\nSIMspk\\textrm{SIM}_{\\textrm{spk}}\nUTMOS\n\n\n\n\nLlasaEdit-ESD\nESD\n10.07\n0.64\n0.49\n4.01\n\n\nISSE\n68.17\n0.54\n0.49\n4.12\n\n\nLlasaEdit-ISSE\nESD\n7.69\n0.59\n0.51\n4.31\n\n\nISSE\n8.06\n0.68\n0.58\n4.29",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Evalset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math alttext=\"\\textrm{SIM}_{\\textrm{sty}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><msub><mtext mathsize=\"0.900em\">SIM</mtext><mtext mathsize=\"0.900em\">sty</mtext></msub><annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{sty}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math alttext=\"\\textrm{SIM}_{\\textrm{spk}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><msub><mtext mathsize=\"0.900em\">SIM</mtext><mtext mathsize=\"0.900em\">spk</mtext></msub><annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{spk}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">UTMOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LlasaEdit-ESD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ESD</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ISSE</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.17</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.54</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.12</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LlasaEdit-ISSE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ESD</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.31</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ISSE</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.29</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "evalset",
            "model",
            "simspktextrmsimtextrmspk",
            "llasaeditisse",
            "models",
            "esd",
            "different",
            "wer",
            "isse",
            "utmos",
            "llasaeditesd",
            "llasaedit",
            "datasets",
            "denote",
            "trained",
            "respectively",
            "simstytextrmsimtextrmsty",
            "results"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of ISSE, we train the LlasaEdit model on the ISSE dataset and compare its performance with the same model trained on ESD, the closest open-source paired speech dataset. Following </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we convert the categorical labels in ESD into natural language instructions and adopt the same training parameters as used for ISSE. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Speech Style Editing &#8227; 4 Benchmark &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, in the in-domain setting, the model trained and tested on ISSE consistently outperforms its ESD counterpart across all metrics, achieving lower WER (8.06 vs. 10.07), higher </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{sty}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">sty</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{sty}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (0.68 vs. 0.64), higher </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{spk}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">spk</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{spk}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (0.58 vs. 0.49), and higher UTMOS (4.29 vs. 4.01). These results highlight the benefits of larger and more diverse data for enhancing generative capabilities. Furthermore, under the cross-domain setting, the model trained on ISSE even surpasses the in-domain performance of the ESD-trained model, with only a marginal shortfall in style similarity. In contrast, the model trained on ESD suffers from severe degradation when evaluated on the ISSE test set, yielding a catastrophic WER of 68.17.\nThose results demonstrate the comprehensive advantages of ISSE over ESD and confirm the effectiveness of its scale, data diversity, and fine-grained style instructions.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech style editing refers to modifying the stylistic properties of speech while preserving its linguistic content and speaker identity.\nHowever, most existing approaches depend on explicit labels or reference audio, which limits both flexibility and scalability.\nMore recent attempts to use natural language descriptions remain constrained by oversimplified instructions and coarse style control.\nTo address these limitations, we introduce an <span class=\"ltx_text ltx_font_bold\">I</span>nstruction-guided <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">S</span>tyle <span class=\"ltx_text ltx_font_bold\">E</span>diting Dataset (ISSE).\nThe dataset comprises nearly 400 hours of speech and over 100,000 source-target pairs, each aligned with diverse and detailed textual editing instructions.\nWe also build a systematic instructed speech data generation pipeline leveraging large language model, expressive text-to-speech and voice conversion technologies to construct high-quality paired samples.\nFurthermore, we train an instruction-guided autoregressive speech model on ISSE and evaluate it in terms of instruction adherence, timbre preservation, and content consistency.\nExperimental results demonstrate that ISSE enables accurate, controllable, and generalizable speech style editing compared to other datasets.\nThe project page of ISSE is available at <a class=\"ltx_ref ltx_href\" href=\"https://ychenn1.github.io/ISSE/\" title=\"\">https://ychenn1.github.io/ISSE/</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "isse",
                    "model",
                    "datasets",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A key prerequisite for training text-guided speech generation models is the availability of paired &#8220;speech-text description&#8221; datasets.\nIn the context of text-to-speech (TTS), several studies have annotated speech with textual style descriptions to produce more expressive outputs </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as shown in&#160;Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (a).\nHowever, these datasets are constructed for stylized speech generation, which cannot describe the change between source speech and target speech when applying it to the style editing task.\nThis makes the editing objective ill-posed and hindering the disentanglement of style from content and timbre&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo enable attribute-specific editing, Huang et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> constructed a dataset of source-target speech pairs with corresponding editing instructions, as illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(b).\nDespite its contribution, these datasets exhibit two notable limitations.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">First, existing datasets suffer from limitations in the design of instructions.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nThe provided instructions are often overly simplistic and lack linguistic diversity, typically restricted to templated commands such as &#8220;Change the emotion of speech&#8221; or &#8220;Change the speaking speed of speech.&#8221;\nMoreover, they are usually coarse-grained, focusing on only a single attribute to modify.\nSuch constraints hinder the ability to represent more realistic and expressive editing scenarios.\nIn contrast, fine-grained instructions (e.g., &#8220;Convert the source speech to a medium-pitched, raspy monotone that conveys awe&#8221;) help enable flexible, multi-attribute speech style editing, as shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(c).\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Secondly, most existing open-source paired speech style datasets remain limited in scale and diversity. </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAs shown in&#160;Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ESD&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> contains only 29 hours of recordings, while other datasets such as Ears&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Expresso&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are similarly constrained.\nThese datasets not only suffer from relatively small amounts of data, but also lack sufficient variety in speaking styles, prosodic patterns, and content coverage.\nSuch limitations in both quantity and diversity pose significant challenges and have become a major bottleneck for advancing research in speech style editing.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "esd",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we present ISSE, a novel </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">I</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">nstruction-guided </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">peech </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">tyle </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">E</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">diting dataset.\nISSE contains nearly 400 hours of speech and over 100,000 source-target pairs, and is designed with fine-grained editing instructions, substantially exceeding the simplicity and limited coverage of prior resources (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nAs the performance of style editing models hinges on the consistency between fine-grained instructions and the paired speech content, we design an effective data construction pipeline.\nSpecifically, we integrate expressive TTS and voice conversion (VC) to generate reliable source-target pairs that preserve content and timbre while varying style attributes.\nWe further conduct quality evaluation of content preservation, speaker identity consistency, and style similarity to filter low-quality data.\nFinally, a large language model (LLM) is employed to describe the transformation between paired speech samples and generate the fine-grained instruction.\nTo validate the effectiveness of the proposed ISSE, we establish a benchmark using a baseline instruction-guided autoregressive speech model, LlasaEdit.\nWe evaluate LlasaEdit trained on ISSE against the same model trained on ESD under both in-domain and cross-domain settings.\nThe experimental results consistently confirm the effectiveness of ISSE for instruction-guided speech style editing.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "esd",
                    "isse",
                    "llasaedit",
                    "trained",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We establish a benchmark with an instruction-guided autoregressive model trained on ISSE. Experiments demonstrate the effectiveness of both the dataset and the proposed model in editing style while preserving transcript and identity.</span>\n</p>\n\n",
                "matched_terms": [
                    "isse",
                    "model",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To ensure the quality and controllability of the generated speech, we propose a quality assessment process that evaluates generated samples along three aspects: content preservation, style similarity, and speaker identity consistency.\nFor content intelligibility and accuracy, we employ a widely-used speech recognition model, Whisper-large-v3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to transcribe the generated speech into text.\nThis transcription is then compared against the source text transcript to calculate word error rate (WER) for content accuracy and intelligibility verification.\nFor style alignment, we utilize a Speech Emotion Recognition (SER) model, emotion2vec </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to assess style similarity </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{sty}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">sty</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{sty}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between the generated speech and its style reference speeches.\nFor speaker consistency, a speaker verification (SV) model ECAPA-TDNN </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used to extract speaker embeddings from both the generated and anchor speech samples.\nThe cosine similarity </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{spk}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">spk</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{spk}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between these embeddings is computed to quantitatively assess whether the speaker&#8217;s identity is accurately preserved.\nGenerated samples are retained only when they satisfy all these criteria: WER </span>\n  <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&lt;</mo>\n      <annotation encoding=\"application/x-tex\">&lt;</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 10, </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{sty}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">sty</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{sty}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&gt;</mo>\n      <annotation encoding=\"application/x-tex\">&gt;</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.5 , and </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{spk}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">spk</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{spk}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&gt;</mo>\n      <annotation encoding=\"application/x-tex\">&gt;</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.5.</span>\n</p>\n\n",
                "matched_terms": [
                    "simstytextrmsimtextrmsty",
                    "model",
                    "simspktextrmsimtextrmspk",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Overview of the ISSE.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nUsing the proposed pipeline, we construct the Instructed Speech Style Editing Dataset (ISSE).\nIt is an open-source paired speech style dataset comprising approximately 382 hours of speech.\nWithin it, the &#8220;Real&#8221; portion (recorded human speech) amounts to around 90 hours, and the generated speech occupies about 292 hours.\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates the data distribution in ISSE.\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(a) shows the distribution of audio length, where most speech samples fall into short-to-medium ranges, providing balanced coverage across durations.\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(b) illustrates the style distribution, demonstrating that ISSE encompasses a broad variety of speaking styles with sufficient samples per category, in contrast to prior datasets that are limited to a few emotional labels </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "isse",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Quality of the ISSE.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (c)-(e) present the quality evaluation of the ISSE dataset. Specifically, Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(c) reports the quality of intelligibility, measured using ASR-based word error rate to ensure that the generated speeches remain linguistically accurate. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(d) evaluates the quality of identity preservation through speaker similarity, confirming that speaker characteristics are maintained across paired samples. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(e) assesses the quality of style preservation, where style similarity against reference speeches demonstrates that stylistic variations are effectively retained. Together, these results highlight that ISSE achieves a balance between intelligibility, identity consistency, and style fidelity.</span>\n</p>\n\n",
                "matched_terms": [
                    "isse",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we establish a benchmark for the instruction-guided speech style editing task.\nFirst, we introduce a baseline model, LlasaEdit, built upon </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe then train LlasaEdit separately on ISSE and ESD to demonstrate the effectiveness of the model and the advantages of our proposed dataset in editing style while preserving transcript and speaker identity.\nFinally, we validate the generalization of ISSE by evaluating through TTS and expressive speech synthesis (ESS) conditioned on style descriptions.</span>\n</p>\n\n",
                "matched_terms": [
                    "isse",
                    "esd",
                    "model",
                    "llasaedit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.1 LlasaEdit &#8227; 4 Benchmark &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we propose LlasaEdit, which consists of the XCodec2 audio codec and a large transformer-based speech generation model initialized from Llasa&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nEach training instance is represented as a triplet </span>\n  <math alttext=\"(x_{i},y_{i},c_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">x</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">y</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(x_{i},y_{i},c_{i})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the source speech, </span>\n  <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the target speech (same speaker and transcript but different style), and </span>\n  <math alttext=\"c_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">c_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the corresponding style instruction.\nWe first encode </span>\n  <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into discrete token sequences using XCodec2, while </span>\n  <math alttext=\"c_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">c_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is tokenized into text tokens.\nThe generation model is then trained to model the conditional distribution of target tokens given the source tokens and the instruction, predicting the target sequence token by token under teacher forcing.\nThe training objective is to minimize the negative log-likelihood of the target tokens.\nIn our experiments, we fine-tune the Llasa model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with Low-Rank Adaptation (LoRA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using rank </span>\n  <math alttext=\"r=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">64</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r=64</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\alpha=128\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">128</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha=128</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applied to both the attention and MLP layers.\nWe train with AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (learning rate </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) for 5 epochs with a batch size of 2, employing learning-rate decay and periodic checkpointing, on 8&#215; NVIDIA RTX A100 GPUs.\nAt inference, the model takes the source tokens and style instruction, autoregressively generates the edited target tokens, and reconstructs the waveform using the XCodec2 decoder.\nFor evaluation, we adopt the same metrics as in Quality Evaluation (Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 Quality Evaluation &#8227; 2 Dataset Construction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), along with UTMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for naturalness assessment.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "different",
                    "utmos",
                    "llasaedit",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The proposed ISSE can also be extended to TTS tasks by leveraging speech samples paired with their style transcriptions.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Speech Style Editing &#8227; 4 Benchmark &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, fine-tuning Llasa on the TTS task reduces WER from 5.48 to 3.74 and yields a slight improvement in naturalness.\nMoreover, the ESS model conditioned on style descriptions further enhances style controllability, achieving a higher style similarity score (0.60) and gender accuracy (0.96), while maintaining comparable naturalness.</span>\n</p>\n\n",
                "matched_terms": [
                    "isse",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we introduced ISSE, a dataset for instruction-guided speech style editing, constructed through an instructed speech data pipeline with LLM, TTS, and VC technologies.\nBenchmarking with our LlasaEdit model demonstrates that ISSE enables accurate and controllable style editing, providing a valuable resource for the speech community.\nNevertheless, the dataset is limited to English, which may constrain its applicability in multilingual scenarios.\nFuture work could extend ISSE to broader languages and richer style dimensions, and explore integration with larger generative models to further advance natural language-guided speech editing.</span>\n</p>\n\n",
                "matched_terms": [
                    "isse",
                    "models",
                    "model",
                    "llasaedit"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark",
        "caption": "Table 3: Scalability of ISSE on TTS tasks. “Codec Rec” indicates speech reconstruction with an audio codec, while “Finetuning” denotes fine-tuning the Llasa-1B model on ISSE.",
        "body": "Task\nFinetuning\nWER\nSIMsty\\textrm{SIM}_{\\textrm{sty}}\nGender\nUTMOS\n\n\nCodec Rec (GT)\n✗\n5.22\n-\n0.97\n3.83\n\n\n\n\nTTS\n✗\n5.48\n0.48\n0.54\n3.93\n\n\n✓\n3.74\n0.49\n0.54\n4.03\n\n\nESS\n✓\n5.25\n0.60\n0.96\n3.94",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Task</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Finetuning</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><math alttext=\"\\textrm{SIM}_{\\textrm{sty}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><msub><mtext mathsize=\"0.900em\">SIM</mtext><mtext mathsize=\"0.900em\">sty</mtext></msub><annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{sty}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gender</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">UTMOS</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Codec Rec (GT)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.22</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.97</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.83</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.93</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.74</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.54</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.03</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ESS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.94</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "task",
            "tasks",
            "utmos",
            "finetuning",
            "ess",
            "speech",
            "tts",
            "wer",
            "“finetuning”",
            "gender",
            "rec",
            "“codec",
            "model",
            "llasa1b",
            "denotes",
            "codec",
            "indicates",
            "reconstruction",
            "rec”",
            "isse",
            "while",
            "simstytextrmsimtextrmsty",
            "audio",
            "scalability"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The proposed ISSE can also be extended to TTS tasks by leveraging speech samples paired with their style transcriptions.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.2 Speech Style Editing &#8227; 4 Benchmark &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, fine-tuning Llasa on the TTS task reduces WER from 5.48 to 3.74 and yields a slight improvement in naturalness.\nMoreover, the ESS model conditioned on style descriptions further enhances style controllability, achieving a higher style similarity score (0.60) and gender accuracy (0.96), while maintaining comparable naturalness.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech style editing refers to modifying the stylistic properties of speech while preserving its linguistic content and speaker identity.\nHowever, most existing approaches depend on explicit labels or reference audio, which limits both flexibility and scalability.\nMore recent attempts to use natural language descriptions remain constrained by oversimplified instructions and coarse style control.\nTo address these limitations, we introduce an <span class=\"ltx_text ltx_font_bold\">I</span>nstruction-guided <span class=\"ltx_text ltx_font_bold\">S</span>peech <span class=\"ltx_text ltx_font_bold\">S</span>tyle <span class=\"ltx_text ltx_font_bold\">E</span>diting Dataset (ISSE).\nThe dataset comprises nearly 400 hours of speech and over 100,000 source-target pairs, each aligned with diverse and detailed textual editing instructions.\nWe also build a systematic instructed speech data generation pipeline leveraging large language model, expressive text-to-speech and voice conversion technologies to construct high-quality paired samples.\nFurthermore, we train an instruction-guided autoregressive speech model on ISSE and evaluate it in terms of instruction adherence, timbre preservation, and content consistency.\nExperimental results demonstrate that ISSE enables accurate, controllable, and generalizable speech style editing compared to other datasets.\nThe project page of ISSE is available at <a class=\"ltx_ref ltx_href\" href=\"https://ychenn1.github.io/ISSE/\" title=\"\">https://ychenn1.github.io/ISSE/</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "isse",
                    "while",
                    "audio",
                    "scalability"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nISSE dataset, Speech style editing, Instruction-guided editing, Natural language instructions</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "isse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech style editing refers to the task of modifying the stylistic properties of speech, such as emotional tone, speaking rate, and prosodic delivery, while preserving its linguistic content and speaker identity &#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nExisting approaches in this domain largely rely on explicit labels&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or reference audio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to provide editing signals, which constrains their flexibility and scalability&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWith the rapid advances in natural language modeling </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a new paradigm has emerged: guiding speech generation through natural language descriptions </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "task",
                    "while",
                    "audio",
                    "scalability"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A key prerequisite for training text-guided speech generation models is the availability of paired &#8220;speech-text description&#8221; datasets.\nIn the context of text-to-speech (TTS), several studies have annotated speech with textual style descriptions to produce more expressive outputs </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as shown in&#160;Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (a).\nHowever, these datasets are constructed for stylized speech generation, which cannot describe the change between source speech and target speech when applying it to the style editing task.\nThis makes the editing objective ill-posed and hindering the disentanglement of style from content and timbre&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo enable attribute-specific editing, Huang et al. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> constructed a dataset of source-target speech pairs with corresponding editing instructions, as illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(b).\nDespite its contribution, these datasets exhibit two notable limitations.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">First, existing datasets suffer from limitations in the design of instructions.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nThe provided instructions are often overly simplistic and lack linguistic diversity, typically restricted to templated commands such as &#8220;Change the emotion of speech&#8221; or &#8220;Change the speaking speed of speech.&#8221;\nMoreover, they are usually coarse-grained, focusing on only a single attribute to modify.\nSuch constraints hinder the ability to represent more realistic and expressive editing scenarios.\nIn contrast, fine-grained instructions (e.g., &#8220;Convert the source speech to a medium-pitched, raspy monotone that conveys awe&#8221;) help enable flexible, multi-attribute speech style editing, as shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(c).\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Secondly, most existing open-source paired speech style datasets remain limited in scale and diversity. </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAs shown in&#160;Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ESD&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> contains only 29 hours of recordings, while other datasets such as Ears&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Expresso&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are similarly constrained.\nThese datasets not only suffer from relatively small amounts of data, but also lack sufficient variety in speaking styles, prosodic patterns, and content coverage.\nSuch limitations in both quantity and diversity pose significant challenges and have become a major bottleneck for advancing research in speech style editing.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "while",
                    "tts",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we present ISSE, a novel </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">I</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">nstruction-guided </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">peech </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">tyle </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">E</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">diting dataset.\nISSE contains nearly 400 hours of speech and over 100,000 source-target pairs, and is designed with fine-grained editing instructions, substantially exceeding the simplicity and limited coverage of prior resources (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nAs the performance of style editing models hinges on the consistency between fine-grained instructions and the paired speech content, we design an effective data construction pipeline.\nSpecifically, we integrate expressive TTS and voice conversion (VC) to generate reliable source-target pairs that preserve content and timbre while varying style attributes.\nWe further conduct quality evaluation of content preservation, speaker identity consistency, and style similarity to filter low-quality data.\nFinally, a large language model (LLM) is employed to describe the transformation between paired speech samples and generate the fine-grained instruction.\nTo validate the effectiveness of the proposed ISSE, we establish a benchmark using a baseline instruction-guided autoregressive speech model, LlasaEdit.\nWe evaluate LlasaEdit trained on ISSE against the same model trained on ESD under both in-domain and cross-domain settings.\nThe experimental results consistently confirm the effectiveness of ISSE for instruction-guided speech style editing.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts",
                    "isse",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We release the ISSE dataset, which contains approximately 400 hours of speech and more than 100,000 source-target pairs annotated with fine-grained editing instructions.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "isse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose an instructed speech data pipeline that integrates existing speech datasets with TTS, VC, quality assessment, and LLM tools to generate high-quality speech pairs with fine-grained editing instructions.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We establish a benchmark with an instruction-guided autoregressive model trained on ISSE. Experiments demonstrate the effectiveness of both the dataset and the proposed model in editing style while preserving transcript and identity.</span>\n</p>\n\n",
                "matched_terms": [
                    "isse",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we propose an instructed speech data pipeline to generate source and target speech pairs with corresponding style instructions.\nAs shown in&#160;Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the pipeline consists of three stages of data processing:\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Stage 1: Paired Speech Generation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Generates speech samples in diverse styles and then converts them to share the same speaker identity while maintaining the style differences;\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Stage 2: Quality Evaluation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Validates the generated speech quality and consistency based on predefined criteria;\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Stage 3: Instruction Design</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Formulates natural language instructions describing the style transformation between the paired speech samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We adopt two existing speech datasets, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Ears</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Expresso</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as the source data.\nBoth datasets contain speeches, transcriptions, and the corresponding style descriptions annotated with the tool used in </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo minimize the influence of speaker identity during data construction, non-stylistic words (e.g., he, she) are removed from the style descriptions.\nFrom the processed datasets, we first sample an anchor speech, with its transcription and speaker identity serving as fixed attributes for the subsequent synthesis process.\nWe then randomly select </span>\n  <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">n</mi>\n      <annotation encoding=\"application/x-tex\">n</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> additional samples with distinct speaking styles, referred to as style reference speeches.\nTheir style descriptions are used to specify the target styles for synthesis.\nTo produce expressive speech variations, we employ EmoCapTTS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor each case, EmoCapTTS takes as input the transcription of the anchor speech and the style description of a style reference speech, and generates a stylized target that preserves the linguistic content while adopting the style of the style reference speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The EmoCapTTS model lacks explicit control over speaker identity, so the generated speech differs from the anchor speech in timbre.\nThis variability makes it difficult to design editing instructions that isolate style changes without being confounded by speaker changes.\nTo address this issue, we employ Chatterbox </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a voice conversion model, to adjust the timbre of each generated sample so that it matches the anchor speech while retaining the intended style.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To ensure the quality and controllability of the generated speech, we propose a quality assessment process that evaluates generated samples along three aspects: content preservation, style similarity, and speaker identity consistency.\nFor content intelligibility and accuracy, we employ a widely-used speech recognition model, Whisper-large-v3 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to transcribe the generated speech into text.\nThis transcription is then compared against the source text transcript to calculate word error rate (WER) for content accuracy and intelligibility verification.\nFor style alignment, we utilize a Speech Emotion Recognition (SER) model, emotion2vec </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to assess style similarity </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{sty}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">sty</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{sty}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between the generated speech and its style reference speeches.\nFor speaker consistency, a speaker verification (SV) model ECAPA-TDNN </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used to extract speaker embeddings from both the generated and anchor speech samples.\nThe cosine similarity </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{spk}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">spk</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{spk}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between these embeddings is computed to quantitatively assess whether the speaker&#8217;s identity is accurately preserved.\nGenerated samples are retained only when they satisfy all these criteria: WER </span>\n  <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&lt;</mo>\n      <annotation encoding=\"application/x-tex\">&lt;</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 10, </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{sty}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">sty</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{sty}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&gt;</mo>\n      <annotation encoding=\"application/x-tex\">&gt;</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.5 , and </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{spk}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">spk</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{spk}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&gt;</mo>\n      <annotation encoding=\"application/x-tex\">&gt;</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> 0.5.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "simstytextrmsimtextrmsty",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Through Stages 1 and 2, we obtain high-quality source-target speech pairs in which both source and target speech samples share the same linguistic content and speaker identity but differ in style descriptions.\nThe goal of Stage 3 is to generate natural language editing instructions that accurately describe the transformation from the source style to the target style.\nLeveraging the language understanding capabilities of Qwen3-8B </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we transform the style description into high-quality editing instructions for each source-target speech pair.\nSpecifically, we first prompt Qwen3-8B to compare the style descriptions of the source and target speech samples and explicitly identify the differences between them.\nBased on this comparison, we instruct the LLM to formulate a natural language editing instruction using the template: &#8220;Convert the source speech to </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">{target style}</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.&#8221;\nTo enhance instruction diversity and avoid repetitive phrasing, we further prompt the model to generate multiple alternative formulations for each transformation, such as &#8220;Convert the source speech to a whispering style with a high-pitched voice.&#8221; or &#8220;Convert the source speech into a high-pitched whisper.&#8221;</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Overview of the ISSE.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nUsing the proposed pipeline, we construct the Instructed Speech Style Editing Dataset (ISSE).\nIt is an open-source paired speech style dataset comprising approximately 382 hours of speech.\nWithin it, the &#8220;Real&#8221; portion (recorded human speech) amounts to around 90 hours, and the generated speech occupies about 292 hours.\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates the data distribution in ISSE.\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(a) shows the distribution of audio length, where most speech samples fall into short-to-medium ranges, providing balanced coverage across durations.\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;(b) illustrates the style distribution, demonstrating that ISSE encompasses a broad variety of speaking styles with sufficient samples per category, in contrast to prior datasets that are limited to a few emotional labels </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "isse",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we establish a benchmark for the instruction-guided speech style editing task.\nFirst, we introduce a baseline model, LlasaEdit, built upon </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe then train LlasaEdit separately on ISSE and ESD to demonstrate the effectiveness of the model and the advantages of our proposed dataset in editing style while preserving transcript and speaker identity.\nFinally, we validate the generalization of ISSE by evaluating through TTS and expressive speech synthesis (ESS) conditioned on style descriptions.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "task",
                    "tts",
                    "isse",
                    "while",
                    "ess"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.1 LlasaEdit &#8227; 4 Benchmark &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we propose LlasaEdit, which consists of the XCodec2 audio codec and a large transformer-based speech generation model initialized from Llasa&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nEach training instance is represented as a triplet </span>\n  <math alttext=\"(x_{i},y_{i},c_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">x</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">y</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(x_{i},y_{i},c_{i})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the source speech, </span>\n  <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the target speech (same speaker and transcript but different style), and </span>\n  <math alttext=\"c_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">c_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the corresponding style instruction.\nWe first encode </span>\n  <math alttext=\"x_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into discrete token sequences using XCodec2, while </span>\n  <math alttext=\"c_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">c</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">c_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is tokenized into text tokens.\nThe generation model is then trained to model the conditional distribution of target tokens given the source tokens and the instruction, predicting the target sequence token by token under teacher forcing.\nThe training objective is to minimize the negative log-likelihood of the target tokens.\nIn our experiments, we fine-tune the Llasa model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with Low-Rank Adaptation (LoRA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, using rank </span>\n  <math alttext=\"r=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">64</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r=64</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\alpha=128\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">128</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha=128</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, applied to both the attention and MLP layers.\nWe train with AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (learning rate </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) for 5 epochs with a batch size of 2, employing learning-rate decay and periodic checkpointing, on 8&#215; NVIDIA RTX A100 GPUs.\nAt inference, the model takes the source tokens and style instruction, autoregressively generates the edited target tokens, and reconstructs the waveform using the XCodec2 decoder.\nFor evaluation, we adopt the same metrics as in Quality Evaluation (Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S2.SS2\" style=\"font-size:90%;\" title=\"2.2 Quality Evaluation &#8227; 2 Dataset Construction &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), along with UTMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for naturalness assessment.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "denotes",
                    "utmos",
                    "while",
                    "codec",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of ISSE, we train the LlasaEdit model on the ISSE dataset and compare its performance with the same model trained on ESD, the closest open-source paired speech dataset. Following </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we convert the categorical labels in ESD into natural language instructions and adopt the same training parameters as used for ISSE. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.24570v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Speech Style Editing &#8227; 4 Benchmark &#8227; ISSE: An Instruction-Guided Speech Style Editing Dataset and Benchmark\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, in the in-domain setting, the model trained and tested on ISSE consistently outperforms its ESD counterpart across all metrics, achieving lower WER (8.06 vs. 10.07), higher </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{sty}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">sty</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{sty}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (0.68 vs. 0.64), higher </span>\n  <math alttext=\"\\textrm{SIM}_{\\textrm{spk}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mtext mathsize=\"0.900em\">SIM</mtext>\n        <mtext mathsize=\"0.900em\">spk</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\textrm{SIM}_{\\textrm{spk}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (0.58 vs. 0.49), and higher UTMOS (4.29 vs. 4.01). These results highlight the benefits of larger and more diverse data for enhancing generative capabilities. Furthermore, under the cross-domain setting, the model trained on ISSE even surpasses the in-domain performance of the ESD-trained model, with only a marginal shortfall in style similarity. In contrast, the model trained on ESD suffers from severe degradation when evaluated on the ISSE test set, yielding a catastrophic WER of 68.17.\nThose results demonstrate the comprehensive advantages of ISSE over ESD and confirm the effectiveness of its scale, data diversity, and fine-grained style instructions.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "isse",
                    "utmos",
                    "wer",
                    "simstytextrmsimtextrmsty"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we introduced ISSE, a dataset for instruction-guided speech style editing, constructed through an instructed speech data pipeline with LLM, TTS, and VC technologies.\nBenchmarking with our LlasaEdit model demonstrates that ISSE enables accurate and controllable style editing, providing a valuable resource for the speech community.\nNevertheless, the dataset is limited to English, which may constrain its applicability in multilingual scenarios.\nFuture work could extend ISSE to broader languages and richer style dimensions, and explore integration with larger generative models to further advance natural language-guided speech editing.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "isse",
                    "tts",
                    "model"
                ]
            }
        ]
    }
}