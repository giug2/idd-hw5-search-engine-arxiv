{
    "S2.T1": {
        "source_file": "Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation",
        "caption": "Table 1: Performance comparison of different emotional TTS models on ESD and FEDD datasets.",
        "body": "Model\nEmotion\nDataset\nEmo SIM(%)\nDTW\nWER(%)\nEMOS\nNMOS\n\n\n\n\nEmoSpeech\nLabel\nESD\n98.25\n47.34\n7.92\n4.09\n3.93\n\n\nGenerSpeech\nAudio\nESD\n97.84\n42.68\n12.35\n3.72\n3.81\n\n\nCosyVoice2\nPrompt\nESD\n98.73\n27.48\n6.21\n4.07\n4.19\n\n\nEmo-FiLM\nGlobal Label\nESD\n98.78\n23.98\n3.12\n4.13\n4.23\n\n\nEmoSpeech\nLabel\nFEDD\n98.33\n59.89\n8.04\n3.99\n3.96\n\n\nGenerSpeech\nAudio\nFEDD\n98.17\n65.63\n9.58\n3.62\n3.82\n\n\nCosyVoice2\nPrompt\nFEDD\n99.13\n54.57\n9.93\n3.84\n4.17\n\n\nEmo-FiLM\nFine-grained Label\nFEDD\n99.32\n49.62\n7.32\n4.19\n4.23",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Emotion</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Emo SIM(%)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DTW</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER(%)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EMOS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">NMOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">EmoSpeech</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Label</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ESD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">47.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.93</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">GenerSpeech</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Audio</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">ESD</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.68</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.35</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.81</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prompt</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">ESD</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.73</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.48</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.07</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.19</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Emo-FiLM</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Global Label</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">ESD</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.98</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.13</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">EmoSpeech</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Label</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FEDD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.96</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">GenerSpeech</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Audio</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">FEDD</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.63</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.58</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.62</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.82</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prompt</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">FEDD</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">99.13</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Emo-FiLM</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Fine-grained Label</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">FEDD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">99.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">49.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.23</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "global",
            "emotional",
            "esd",
            "sim",
            "dtw",
            "datasets",
            "nmos",
            "emo",
            "label",
            "tts",
            "finegrained",
            "wer",
            "fedd",
            "model",
            "emospeech",
            "dataset",
            "generspeech",
            "performance",
            "cosyvoice2",
            "emofilm",
            "prompt",
            "emos",
            "models",
            "emotion",
            "different",
            "comparison",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The main experiment aims to evaluate Emo-FiLM against representative emotional TTS baselines under both global and word-level control tasks. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.1 Fine-grained Emotion Annotation &#8227; 2 Methodology &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, results on the ESD dataset show that Emo-FiLM achieves the best or competitive scores across all metrics, including a 12.7% relative improvement in DTW over CosyVoice2 and a low WER of 3.12%, indicating strong emotional expressiveness while maintaining intelligibility. On the more challenging FEDD dataset, Emo-FiLM further surpasses baselines with a 9.1% DTW gain and higher subjective ratings in both emotion similarity and naturalness. These findings confirm that the proposed feature-wise modulation approach effectively captures dynamic emotions, and demonstrate that Emo-FiLM delivers robust performance in both global and fine-grained emotional speech synthesis.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Emotional text-to-speech (E-TTS) is central to creating natural and trustworthy human&#8211;computer interaction. Existing systems typically rely on sentence-level control through predefined labels, reference audio, or natural language prompts. While effective for global emotion expression, these approaches fail to capture dynamic shifts within a sentence. To address this limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to words to obtain word-level emotion annotations, and maps them through a Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion control by directly modulating text embeddings. To support evaluation, we construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed annotations of emotional transitions. Experiments show that Emo-FiLM outperforms existing approaches on both global and fine-grained tasks, demonstrating its effectiveness and generality for expressive speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "audio",
                    "emotional",
                    "fedd",
                    "finegrained",
                    "tts",
                    "emotion",
                    "dataset",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nEmotional text-to-speech, Fine-grained emotion modeling, Large Language Models</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "finegrained",
                    "emotional",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Most existing E-TTS methods control emotion at the sentence level. Current methods can be grouped into three main categories. The first is predefined emotion labels&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where models are trained with discrete labels such as happy, sad, or angry. These labels are used to supervise the learning of emotional embeddings or classifiers. The second is reference audio-based control&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which extracts style or emotion embeddings from reference audio for zero-shot emotion transfer. This allows cross-speaker and cross-domain transfer but depends on whole-sentence reference audio. The third is natural language prompts with Large Language Models TTS (LLM-TTS)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where natural language descriptions are prepended to the text along with special tokens (e.g., [laughter], [breath]) to specify emotion, speed, role, or accent. While these methods offer flexibility, they rely on global control signals and cannot capture changes in emotion over time. For example, a sentence may begin with a surprised tone and shift to joy, but such dynamic variations are hard to express using global controls.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "emotional",
                    "models",
                    "emotion",
                    "tts",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this limitation, we propose Emo-FiLM, a fine-grained emotion modeling method based on Feature-wise Linear Modulation (FiLM). Our approach first uses an emotion2vec model to extract frame-level emotion features and aligns them to generate word-level dynamic emotion annotations. We then introduce an emotion modulation layer (E-FiLM) into an LLM-TTS framework. This layer transforms word-level emotion signals into dimension-wise scale and shift parameters that modulate the text embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "finegrained",
                    "model",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, unlike existing approaches that rely on global emotion control, our method achieves fine-grained word-level modulation, enabling the modeling of dynamic emotional transitions that global methods cannot capture. To evaluate the model&#8217;s ability in fine-grained emotion control, we construct a new dataset named Fine-grained Emotion Dynamics Dataset (FEDD), which fills the gap in existing benchmarks lacking detailed emotion annotations.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "emotional",
                    "fedd",
                    "finegrained",
                    "emotion",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Compared to global emotion control methods, our proposed approach achieves better results on both the ESD global emotion synthesis task and the FEDD dynamic emotion task. On the global task, Emo-FiLM achieves higher emotion similarity (Emo SIM). On the dynamic task, it significantly outperforms baselines in emotion dynamic matching (DTW) and subjective similarity (EMOS). Further analysis shows that combining frame-level emotion features with FiLM-based modulation improves the model&#8217;s ability to model emotion shifts within a sentence. Overall, our results demonstrate the effectiveness and generality of the proposed method for fine-grained controllable emotional TTS. To the best of our knowledge, this is the first work to leverage LLM-TTS for dynamic control of fine-grained emotions in speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "emos",
                    "emotional",
                    "fedd",
                    "model",
                    "emo",
                    "esd",
                    "tts",
                    "finegrained",
                    "emotion",
                    "dtw",
                    "sim",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose Emo-FiLM for word-level controllable emotional speech synthesis. As shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, it consists of two modules: (1) Fine-grained Emotion Annotation, where emotion2vec extracts frame-level features and aligns them with words to form word-level trajectories; (2) Emotion-modulated Generation, where an E-FiLM layer maps these trajectories into scale and shift parameters to modulate text states, enabling dynamic prosody and emotion control.</span>\n</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "finegrained",
                    "emotional",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Most existing emotional speech datasets are annotated at the sentence level, which limits their ability to support fine-grained modeling of dynamic emotions within a sentence. To obtain more fine-grained text&#8211;speech emotion alignment, we extract frame-level emotion features using emotion2vec and further infer word-level emotion representations. emotion2vec is a self-supervised speech emotion recognition model capable of capturing high-dimensional emotion-related features from speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "model",
                    "emotion",
                    "finegrained",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (a),\nwe train a lightweight Transformer model with multi-head self-attention layers, feed-forward networks, and residual connections to model contextual dependencies of frame sequences and generate enhanced representations. Specifically, we first use the </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Montreal Forced Aligner (MFA)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to align speech with transcribed text, obtaining frame-level emotion feature sequences corresponding to each word. Then, masked average pooling is applied to aggregate each sequence into a fixed-dimensional vector, which serves as the word-level emotion representation. This representation is passed through two parallel output heads: a classification head for predicting discrete emotion categories, and a regression head for estimating continuous emotion intensity (normalized to the range </span>\n  <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). For a frame-level emotion feature sequence </span>\n  <math alttext=\"\\mathbf{x}_{1:T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mi mathsize=\"0.900em\">T</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}_{1:T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> corresponding to a word, the prediction targets include the discrete category </span>\n  <math alttext=\"\\hat{y}^{\\text{class}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">y</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mtext mathsize=\"0.900em\">class</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{y}^{\\text{class}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the continuous intensity </span>\n  <math alttext=\"\\hat{y}^{\\text{dim}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">y</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mtext mathsize=\"0.900em\">dim</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{y}^{\\text{dim}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mathbf{x}_{1:T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mi mathsize=\"0.900em\">T</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}_{1:T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the frame-level emotion feature sequence aligned with a word; </span>\n  <math alttext=\"\\hat{y}^{\\text{class}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">y</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mtext mathsize=\"0.900em\">class</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{y}^{\\text{class}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the predicted emotion category; and </span>\n  <math alttext=\"\\hat{y}^{\\text{dim}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">y</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mtext mathsize=\"0.900em\">dim</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{y}^{\\text{dim}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the predicted emotion intensity. During training, the model jointly optimizes classification and regression tasks for fine-grained emotion modeling. The loss function is defined as:</span>\n</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "finegrained",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mathcal{L}_{\\text{CE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mtext mathsize=\"0.900em\">CE</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CE}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathcal{L}_{\\text{MSE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mtext mathsize=\"0.900em\">MSE</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{MSE}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the classification and regression losses weighted by </span>\n  <math alttext=\"\\lambda_{\\text{cls}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#955;</mi>\n        <mtext mathsize=\"0.900em\">cls</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\lambda_{\\text{cls}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\lambda_{\\text{reg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#955;</mi>\n        <mtext mathsize=\"0.900em\">reg</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\lambda_{\\text{reg}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the model infers word-level dynamic annotations from frame-level features to provide fine-grained control for speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "finegrained",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To achieve fine-grained emotional control, we introduce the </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Emotion Feature-wise Linear Modulation (E-FiLM)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> module into the pretrained LLM-TTS framework. As shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (b), this module consists of three components: an </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Emotion Encoder</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">FiLM Layer</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and an </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Emotion Supervision Mechanism</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The emotion encoder maps discrete emotion categories and continuous intensity labels into dense vector representations, which are fused with text embeddings to obtain unified emotional feature sequences. The FiLM layer then projects these emotion features into dimension-wise scaling factors </span>\n  <math alttext=\"\\gamma\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#947;</mi>\n      <annotation encoding=\"application/x-tex\">\\gamma</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and shifting factors </span>\n  <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#946;</mi>\n      <annotation encoding=\"application/x-tex\">\\beta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and applies affine modulation to the text hidden representations </span>\n  <math alttext=\"\\mathbf{h}_{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mtext mathsize=\"0.900em\">text</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}_{\\text{text}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "finegrained",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">During training, the model adopts a multi-task learning strategy to jointly optimize speech generation and emotion classification objectives. The speech generation task employs a label-smoothed cross-entropy loss to predict the next speech token, preserving the synthesis ability of the pretrained model:</span>\n</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the model&#8217;s performance on both global and fine-grained emotional synthesis, we conduct experiments on two test sets.\nFor the global task, we build a test set from the English portion of ESD&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, covering 10 speakers and 5 emotions (Angry, Happy, Sad, Surprise, Neutral), with 30 utterances per category, totaling 1,500 samples.\nFor the fine-grained task, we construct the FEDD dataset, which contains 1,000 utterances with emotional shifts across 5 speakers and 5 emotions. Among them, 500 are mild transitions generated by natural language instructions, while 500 are strong transitions created by concatenating segments with different emotions from the same speaker. Global emotion labels are automatically assigned using emotion2vec-plus-large. This dataset is designed to assess the model&#8217;s ability to capture and generate emotional transitions.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "emotional",
                    "fedd",
                    "esd",
                    "finegrained",
                    "emotion",
                    "different",
                    "dataset",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use the global emotion labels from IEMOCAP</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ESD to pseudo-label words and train a word-level emotion prediction model, which maps frame-level features to word-level representations for generating fine-grained speech&#8211;text pairs. Based on this, Emo-FiLM is built on the frozen pre-trained CosyVoice2 framework, where only the E-FiLM module and the decoding head are optimized, and integrates Flow Matching with HiFi-GAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to achieve word-level emotion-controllable speech synthesis. Training is performed with the Adam optimizer, a batch size of 4, and 5 epochs.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "model",
                    "esd",
                    "finegrained",
                    "emotion",
                    "cosyvoice2",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To comprehensively evaluate the advantages of Emo-FiLM in emotion control and speech generation, we compare it with three representative baselines: the label-based fastspeech2-EmoSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the reference-audio-based GenerSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the natural-language-instruction-based CosyVoice2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For EmoSpeech and CosyVoice2, we use test data with global emotion labels, while for GenerSpeech, reference audio with the same emotion and speaker as the ground truth is provided.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "audio",
                    "emospeech",
                    "emotion",
                    "generspeech",
                    "cosyvoice2",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate Emo-FiLM and baseline models using both objective and subjective metrics. Objective metrics include Emo SIM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to measure emotion similarity between synthesized and real speech, &#8212;note that Emo SIM averages frame-level emotion vectors, which may partially obscure dynamic emotional information. To mitigate this limitation, we further introduce Dynamic Time Warping (DTW) to provide an auxiliary assessment of emotion similarity. WER is computed with Whisper-Large-v3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to evaluate intelligibility. Subjective metrics include Mean Emotion Similarity Opinion Score (EMOS) and the Mean Opinion Score of Speech Naturalness (NMOS)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. EMOS measures the perceived similarity of listeners between the emotion conveyed by the synthesized speech and the target emotion, while NMOS quantifies the perceived naturalness of the speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "emos",
                    "emotional",
                    "nmos",
                    "emo",
                    "models",
                    "emotion",
                    "dtw",
                    "sim",
                    "wer",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The goal of this experiment is to assess the effectiveness of different components in Emo-FiLM. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Ablation Study &#8227; 4 Results and Discussion &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, training with only global-level labels significantly increases DTW on FEDD from 49.6 to 52.7, while removing word-level data causes the most severe degradation, with DTW rising to 134.0, clearly showing the necessity of fine-grained supervision. Removing the auxiliary emotion loss also leads to large drops (DTW 73.9 vs. 49.6), further confirming the benefit of explicit emotion classification. Finally, replacing the FiLM Layer with simple addition severely hurts performance (DTW 70.5 vs. 49.6), highlighting the importance of nonlinear affine modulation in capturing nuanced emotional variations. Overall, all ablations consistently degrade performance across both ESD and FEDD, thereby validating that each design choice is indeed critical for the overall effectiveness of Emo-FiLM.</span>\n</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "fedd",
                    "esd",
                    "finegrained",
                    "emotion",
                    "different",
                    "dtw",
                    "performance",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The purpose of this experiment is to further analyze the emotion expressiveness of each model by comparing their classification accuracy across emotion categories on the ESD dataset. As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.3 Per-Emotion Classification Accuracy Analysis &#8227; 4 Results and Discussion &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Emo-FiLM achieves the highest accuracy in Happy (65.6%), Surprise (71.2%), Angry (70.4%), and Neutral (78.5%), consistently outperforming the baselines. For example, on the Neutral class, Emo-FiLM improves over CosyVoice2 (69.8%) and EmoSpeech (26.7%) by a large margin, while in Happy it surpasses GenerSpeech (10.3%) and EmoSpeech (18.3%) by over 47% absolute gain. These results demonstrate that Emo-FiLM delivers more precise and robust emotion rendering across multiple key categories, highlighting its superior discriminative power and expressiveness in emotional speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "model",
                    "esd",
                    "emospeech",
                    "emotion",
                    "dataset",
                    "generspeech",
                    "cosyvoice2",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To intuitively evaluate the fine-grained emotion control of Emo-FiLM, we visualize the mel-spectrograms and pitch (F0) contours of an utterance with emotion transitions. As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.4 Case Study: Visualization of Emotional Dynamics &#8227; 4 Results and Discussion &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice2 and EmoSpeech produce nearly flat F0 trajectories, failing to capture local prosodic variations such as pitch rises or falls around emotion boundaries. In contrast, Emo-FiLM generates F0 contours that closely follow the ground truth, reproducing both the global trend and local fluctuations of emotional dynamics. This indicates that our model can not only preserve the overall prosody but also capture subtle shifts in intonation, stress, and rhythm that correspond to fine-grained emotional changes. Such improvements highlight Emo-FiLM&#8217;s capability to deliver speech with richer expressiveness and more natural emotional transitions, providing direct acoustic evidence of its effectiveness in fine-grained emotion modeling.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "emotional",
                    "model",
                    "finegrained",
                    "emospeech",
                    "emotion",
                    "cosyvoice2",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we proposed Emo-FiLM, a word-level controllable framework for fine-grained emotional speech synthesis. By aligning frame-level features with words to obtain word-level emotion annotations and applying FiLM-based modulation, our method enables dynamic emotion control beyond global signals. Experiments on both global and fine-grained tasks show consistent improvements in emotion similarity, dynamic alignment, and classification accuracy over strong baselines. These results confirm the effectiveness and generality of Emo-FiLM, providing a promising direction for expressive and trustworthy human&#8211;computer interaction.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "emotional",
                    "finegrained",
                    "emotion",
                    "emofilm"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation",
        "caption": "Table 2: Ablation study results on ESD and FEDD datasets.",
        "body": "Model\nESD\nFEDD\n\n\n\nEmo SIM\nDTW\nEmo SIM\nDTW\n\n\n\n\nEmo-FiLM\n98.78\n23.98\n99.32\n49.62\n\n\n    - Global Level Data Tuning\n98.45\n30.08\n99.20\n52.72\n\n\n    - Word Level Data Tuning\n98.45\n34.00\n95.28\n133.97\n\n\n    - Word Level Data Tuning\n98.58\n25.96\n98.99\n55.91\n\n\n    - Emo Loss\n98.26\n34.36\n98.83\n73.96",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ESD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FEDD</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Emo SIM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">DTW</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Emo SIM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">DTW</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Emo-FiLM</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">99.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">49.62</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8202;  - Global Level Data Tuning</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.45</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">99.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">52.72</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8202;  - Word Level Data Tuning</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.45</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">95.28</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">133.97</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8202;  - Word Level Data Tuning</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.58</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.91</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8202;  - Emo Loss</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.96</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "global",
            "fedd",
            "model",
            "study",
            "ablation",
            "emo",
            "esd",
            "sim",
            "dtw",
            "word",
            "datasets",
            "loss",
            "tuning",
            "data",
            "results",
            "emofilm",
            "level"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The goal of this experiment is to assess the effectiveness of different components in Emo-FiLM. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.2 Ablation Study &#8227; 4 Results and Discussion &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, training with only global-level labels significantly increases DTW on FEDD from 49.6 to 52.7, while removing word-level data causes the most severe degradation, with DTW rising to 134.0, clearly showing the necessity of fine-grained supervision. Removing the auxiliary emotion loss also leads to large drops (DTW 73.9 vs. 49.6), further confirming the benefit of explicit emotion classification. Finally, replacing the FiLM Layer with simple addition severely hurts performance (DTW 70.5 vs. 49.6), highlighting the importance of nonlinear affine modulation in capturing nuanced emotional variations. Overall, all ablations consistently degrade performance across both ESD and FEDD, thereby validating that each design choice is indeed critical for the overall effectiveness of Emo-FiLM.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Emotional text-to-speech (E-TTS) is central to creating natural and trustworthy human&#8211;computer interaction. Existing systems typically rely on sentence-level control through predefined labels, reference audio, or natural language prompts. While effective for global emotion expression, these approaches fail to capture dynamic shifts within a sentence. To address this limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to words to obtain word-level emotion annotations, and maps them through a Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion control by directly modulating text embeddings. To support evaluation, we construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed annotations of emotional transitions. Experiments show that Emo-FiLM outperforms existing approaches on both global and fine-grained tasks, demonstrating its effectiveness and generality for expressive speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "fedd",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Most existing E-TTS methods control emotion at the sentence level. Current methods can be grouped into three main categories. The first is predefined emotion labels&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where models are trained with discrete labels such as happy, sad, or angry. These labels are used to supervise the learning of emotional embeddings or classifiers. The second is reference audio-based control&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which extracts style or emotion embeddings from reference audio for zero-shot emotion transfer. This allows cross-speaker and cross-domain transfer but depends on whole-sentence reference audio. The third is natural language prompts with Large Language Models TTS (LLM-TTS)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where natural language descriptions are prepended to the text along with special tokens (e.g., [laughter], [breath]) to specify emotion, speed, role, or accent. While these methods offer flexibility, they rely on global control signals and cannot capture changes in emotion over time. For example, a sentence may begin with a surprised tone and shift to joy, but such dynamic variations are hard to express using global controls.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "level"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this limitation, we propose Emo-FiLM, a fine-grained emotion modeling method based on Feature-wise Linear Modulation (FiLM). Our approach first uses an emotion2vec model to extract frame-level emotion features and aligns them to generate word-level dynamic emotion annotations. We then introduce an emotion modulation layer (E-FiLM) into an LLM-TTS framework. This layer transforms word-level emotion signals into dimension-wise scale and shift parameters that modulate the text embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, unlike existing approaches that rely on global emotion control, our method achieves fine-grained word-level modulation, enabling the modeling of dynamic emotional transitions that global methods cannot capture. To evaluate the model&#8217;s ability in fine-grained emotion control, we construct a new dataset named Fine-grained Emotion Dynamics Dataset (FEDD), which fills the gap in existing benchmarks lacking detailed emotion annotations.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "fedd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Compared to global emotion control methods, our proposed approach achieves better results on both the ESD global emotion synthesis task and the FEDD dynamic emotion task. On the global task, Emo-FiLM achieves higher emotion similarity (Emo SIM). On the dynamic task, it significantly outperforms baselines in emotion dynamic matching (DTW) and subjective similarity (EMOS). Further analysis shows that combining frame-level emotion features with FiLM-based modulation improves the model&#8217;s ability to model emotion shifts within a sentence. Overall, our results demonstrate the effectiveness and generality of the proposed method for fine-grained controllable emotional TTS. To the best of our knowledge, this is the first work to leverage LLM-TTS for dynamic control of fine-grained emotions in speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "fedd",
                    "model",
                    "emo",
                    "esd",
                    "sim",
                    "dtw",
                    "results",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Most existing emotional speech datasets are annotated at the sentence level, which limits their ability to support fine-grained modeling of dynamic emotions within a sentence. To obtain more fine-grained text&#8211;speech emotion alignment, we extract frame-level emotion features using emotion2vec and further infer word-level emotion representations. emotion2vec is a self-supervised speech emotion recognition model capable of capturing high-dimensional emotion-related features from speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "datasets",
                    "level"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (a),\nwe train a lightweight Transformer model with multi-head self-attention layers, feed-forward networks, and residual connections to model contextual dependencies of frame sequences and generate enhanced representations. Specifically, we first use the </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Montreal Forced Aligner (MFA)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to align speech with transcribed text, obtaining frame-level emotion feature sequences corresponding to each word. Then, masked average pooling is applied to aggregate each sequence into a fixed-dimensional vector, which serves as the word-level emotion representation. This representation is passed through two parallel output heads: a classification head for predicting discrete emotion categories, and a regression head for estimating continuous emotion intensity (normalized to the range </span>\n  <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). For a frame-level emotion feature sequence </span>\n  <math alttext=\"\\mathbf{x}_{1:T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mi mathsize=\"0.900em\">T</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}_{1:T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> corresponding to a word, the prediction targets include the discrete category </span>\n  <math alttext=\"\\hat{y}^{\\text{class}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">y</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mtext mathsize=\"0.900em\">class</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{y}^{\\text{class}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the continuous intensity </span>\n  <math alttext=\"\\hat{y}^{\\text{dim}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">y</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mtext mathsize=\"0.900em\">dim</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{y}^{\\text{dim}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "word",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mathbf{x}_{1:T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mi mathsize=\"0.900em\">T</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}_{1:T}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the frame-level emotion feature sequence aligned with a word; </span>\n  <math alttext=\"\\hat{y}^{\\text{class}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">y</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mtext mathsize=\"0.900em\">class</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{y}^{\\text{class}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the predicted emotion category; and </span>\n  <math alttext=\"\\hat{y}^{\\text{dim}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">y</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mtext mathsize=\"0.900em\">dim</mtext>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{y}^{\\text{dim}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the predicted emotion intensity. During training, the model jointly optimizes classification and regression tasks for fine-grained emotion modeling. The loss function is defined as:</span>\n</p>\n\n",
                "matched_terms": [
                    "word",
                    "model",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">During training, the model adopts a multi-task learning strategy to jointly optimize speech generation and emotion classification objectives. The speech generation task employs a label-smoothed cross-entropy loss to predict the next speech token, preserving the synthesis ability of the pretrained model:</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the model&#8217;s performance on both global and fine-grained emotional synthesis, we conduct experiments on two test sets.\nFor the global task, we build a test set from the English portion of ESD&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, covering 10 speakers and 5 emotions (Angry, Happy, Sad, Surprise, Neutral), with 30 utterances per category, totaling 1,500 samples.\nFor the fine-grained task, we construct the FEDD dataset, which contains 1,000 utterances with emotional shifts across 5 speakers and 5 emotions. Among them, 500 are mild transitions generated by natural language instructions, while 500 are strong transitions created by concatenating segments with different emotions from the same speaker. Global emotion labels are automatically assigned using emotion2vec-plus-large. This dataset is designed to assess the model&#8217;s ability to capture and generate emotional transitions.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "esd",
                    "fedd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We use the global emotion labels from IEMOCAP</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ESD to pseudo-label words and train a word-level emotion prediction model, which maps frame-level features to word-level representations for generating fine-grained speech&#8211;text pairs. Based on this, Emo-FiLM is built on the frozen pre-trained CosyVoice2 framework, where only the E-FiLM module and the decoding head are optimized, and integrates Flow Matching with HiFi-GAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to achieve word-level emotion-controllable speech synthesis. Training is performed with the Adam optimizer, a batch size of 4, and 5 epochs.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "esd",
                    "model",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To comprehensively evaluate the advantages of Emo-FiLM in emotion control and speech generation, we compare it with three representative baselines: the label-based fastspeech2-EmoSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the reference-audio-based GenerSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the natural-language-instruction-based CosyVoice2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For EmoSpeech and CosyVoice2, we use test data with global emotion labels, while for GenerSpeech, reference audio with the same emotion and speaker as the ground truth is provided.</span>\n</p>\n\n",
                "matched_terms": [
                    "emofilm",
                    "global",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate Emo-FiLM and baseline models using both objective and subjective metrics. Objective metrics include Emo SIM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to measure emotion similarity between synthesized and real speech, &#8212;note that Emo SIM averages frame-level emotion vectors, which may partially obscure dynamic emotional information. To mitigate this limitation, we further introduce Dynamic Time Warping (DTW) to provide an auxiliary assessment of emotion similarity. WER is computed with Whisper-Large-v3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to evaluate intelligibility. Subjective metrics include Mean Emotion Similarity Opinion Score (EMOS) and the Mean Opinion Score of Speech Naturalness (NMOS)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. EMOS measures the perceived similarity of listeners between the emotion conveyed by the synthesized speech and the target emotion, while NMOS quantifies the perceived naturalness of the speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "sim",
                    "emofilm",
                    "dtw",
                    "emo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The main experiment aims to evaluate Emo-FiLM against representative emotional TTS baselines under both global and word-level control tasks. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.1 Fine-grained Emotion Annotation &#8227; 2 Methodology &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, results on the ESD dataset show that Emo-FiLM achieves the best or competitive scores across all metrics, including a 12.7% relative improvement in DTW over CosyVoice2 and a low WER of 3.12%, indicating strong emotional expressiveness while maintaining intelligibility. On the more challenging FEDD dataset, Emo-FiLM further surpasses baselines with a 9.1% DTW gain and higher subjective ratings in both emotion similarity and naturalness. These findings confirm that the proposed feature-wise modulation approach effectively captures dynamic emotions, and demonstrate that Emo-FiLM delivers robust performance in both global and fine-grained emotional speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "fedd",
                    "esd",
                    "dtw",
                    "results",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The purpose of this experiment is to further analyze the emotion expressiveness of each model by comparing their classification accuracy across emotion categories on the ESD dataset. As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.3 Per-Emotion Classification Accuracy Analysis &#8227; 4 Results and Discussion &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Emo-FiLM achieves the highest accuracy in Happy (65.6%), Surprise (71.2%), Angry (70.4%), and Neutral (78.5%), consistently outperforming the baselines. For example, on the Neutral class, Emo-FiLM improves over CosyVoice2 (69.8%) and EmoSpeech (26.7%) by a large margin, while in Happy it surpasses GenerSpeech (10.3%) and EmoSpeech (18.3%) by over 47% absolute gain. These results demonstrate that Emo-FiLM delivers more precise and robust emotion rendering across multiple key categories, highlighting its superior discriminative power and expressiveness in emotional speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "esd",
                    "model",
                    "emofilm",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To intuitively evaluate the fine-grained emotion control of Emo-FiLM, we visualize the mel-spectrograms and pitch (F0) contours of an utterance with emotion transitions. As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20378v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.4 Case Study: Visualization of Emotional Dynamics &#8227; 4 Results and Discussion &#8227; Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CosyVoice2 and EmoSpeech produce nearly flat F0 trajectories, failing to capture local prosodic variations such as pitch rises or falls around emotion boundaries. In contrast, Emo-FiLM generates F0 contours that closely follow the ground truth, reproducing both the global trend and local fluctuations of emotional dynamics. This indicates that our model can not only preserve the overall prosody but also capture subtle shifts in intonation, stress, and rhythm that correspond to fine-grained emotional changes. Such improvements highlight Emo-FiLM&#8217;s capability to deliver speech with richer expressiveness and more natural emotional transitions, providing direct acoustic evidence of its effectiveness in fine-grained emotion modeling.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "model",
                    "emofilm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we proposed Emo-FiLM, a word-level controllable framework for fine-grained emotional speech synthesis. By aligning frame-level features with words to obtain word-level emotion annotations and applying FiLM-based modulation, our method enables dynamic emotion control beyond global signals. Experiments on both global and fine-grained tasks show consistent improvements in emotion similarity, dynamic alignment, and classification accuracy over strong baselines. These results confirm the effectiveness and generality of Emo-FiLM, providing a promising direction for expressive and trustworthy human&#8211;computer interaction.</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "results",
                    "emofilm"
                ]
            }
        ]
    }
}