{
    "Sx4.T1": {
        "source_file": "StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak",
        "caption": "Table 1: Experimental results of baselines before and after applying StyleBreak with three query iterations. Values in parentheses denote improvements over each corresponding baseline.",
        "body": "Models\nMetric\nVanilla\nVanilla+Ours\nGCG∗\n\nGCG∗+Ours\nAutoDAN∗\n\nAutoDAN∗+Ours\nSSJ\nSSJ+Ours\n\n\nQwen2-Audio\nARR(%)\n58.0\n98.0 (40.0↑\\uparrow)\n47.4\n100.0  (52.6↑\\uparrow)\n\n98.0\n100.0 (2.0↑\\uparrow)\n\n24.0\n93.8 (69.8↑\\uparrow)\n\n\n\nASR(%)\n10.0\n30.5 (20.5↑\\uparrow)\n\n6.9\n33.3 (26.4↑\\uparrow)\n\n11.8\n16.7 (4.9↑\\uparrow)\n\n8.0\n41.7 (33.7↑\\uparrow)\n\n\n\nPV(%)\n10.0\n20.2 (10.2↑\\uparrow)\n\n17.1\n20.8 (3.7↑\\uparrow)\n\n20.3\n16.7 (3.6↓\\downarrow)\n\n10.0\n33.3 (23.3↑\\uparrow)\n\n\n\nTS(%)\n24.0\n47.0 (23.0↑\\uparrow)\n\n23.2\n52.1 (28.9↑\\uparrow)\n\n82.4\n78.0 (4.4↓\\downarrow)\n\n18.0\n64.6 (46.4↑\\uparrow)\n\n\n\nQwen-Omni\nARR(%)\n16.0\n86.8 (70.8↑\\uparrow)\n\n24.0\n93.7 (69.7↑\\uparrow)\n\n3.9\n66.7 (62.8↑\\uparrow)\n\n62.0\n62.5 (0.5↑\\uparrow)\n\n\n\nASR(%)\n0.0\n22.2 (22.2↑\\uparrow)\n\n2.0\n18.8 (16.8↑\\uparrow)\n\n0.0\n16.7 (16.7↑\\uparrow)\n\n2.0\n8.3 (6.3↑\\uparrow)\n\n\n\nPV(%)\n0.0\n7.6 (7.6↑\\uparrow)\n\n2.0\n6.3 (4.3↑\\uparrow)\n\n0.0\n0.3 (0.3↑\\uparrow)\n\n2.0\n4.2 (2.2↑\\uparrow)\n\n\n\nTS(%)\n0.0\n20.9 (20.9↑\\uparrow)\n\n2.0\n16.7 (14.7↑\\uparrow)\n\n0.0\n6.3 (6.3↑\\uparrow)\n\n18.0\n18.8 (0.8↑\\uparrow)\n\n\n\nMERaLiON\nARR(%)\n34.0\n97.7 (63.7↑\\uparrow)\n\n48.0\n97.9 (49.9↑\\uparrow)\n\n90.2\n100.0 (9.8↑\\uparrow)\n\n100.0\n100.0 (0.0)\n\n\n\nASR(%)\n4.0\n37.8 (33.8↑\\uparrow)\n\n11.0\n39.6 (28.6↑\\uparrow)\n\n52.8\n47.9 (4.9↓\\downarrow)\n\n8.0\n47.9 (39.9↑\\uparrow)\n\n\n\nPV(%)\n2.0\n28.2 (26.2↑\\uparrow)\n\n20.0\n25.0 (5.0↑\\uparrow)\n\n32.9\n29.2 (3.7↓\\downarrow)\n\n22.0\n22.9 (0.9↑\\uparrow)\n\n\n\nTS(%)\n8.0\n51.3 (43.3↑\\uparrow)\n\n22.0\n52.1 (30.1↑\\uparrow)\n\n66.5\n62.5 (4↓\\downarrow)\n\n66.0\n66.7 (0.7↑\\uparrow)\n\n\n\nUltravox\nARR(%)\n96.0\n100.0 (4.0↑\\uparrow)\n\n100.0\n100.0 (0.0)\n\n100.0\n100.0 (0.0)\n\n44.0\n85.4 (41.4↑\\uparrow)\n\n\n\nASR(%)\n4.0\n16.9 (12.9↑\\uparrow)\n\n4.0\n16.7 (12.7↑\\uparrow)\n\n2.0\n14.6 (12.6↑\\uparrow)\n\n0.0\n4.1 (4.1↑\\uparrow)\n\n\n\nPV(%)\n6.0\n10.3 (4.3↑\\uparrow)\n\n4.0\n20.8 (16.8↑\\uparrow)\n\n0.0\n10.4 (10.4↑\\uparrow)\n\n10.0\n14.6 (4.6↑\\uparrow)\n\n\n\nTS(%)\n0.0\n20.9 (20.9↑\\uparrow)\n\n12.0\n27.1 (15.1↑\\uparrow)\n\n0.8\n12.5 (11.7↑\\uparrow)\n\n10.0\n25.0 (15.0↑\\uparrow)",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Models</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Metric</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Vanilla+Ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">GCG<sup class=\"ltx_sup\">&#8727;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">GCG<sup class=\"ltx_sup\">&#8727;</sup>+Ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">AutoDAN<sup class=\"ltx_sup\">&#8727;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">AutoDAN<sup class=\"ltx_sup\">&#8727;</sup>+Ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">SSJ</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">SSJ+Ours</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\">Qwen2-Audio</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">ARR(%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">98.0 (40.0<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m5\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">47.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">100.0 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\"> (52.6<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m6\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">98.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">100.0 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(2.0<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m7\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">24.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">93.8 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(69.8<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m8\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">ASR(%)</td>\n<td class=\"ltx_td ltx_align_center\">10.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">30.5 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(20.5<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m9\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">6.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">33.3 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(26.4<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m10\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">11.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">16.7 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(4.9<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m11\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">8.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">41.7 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(33.7<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m12\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">PV(%)</td>\n<td class=\"ltx_td ltx_align_center\">10.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">20.2 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(10.2<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m13\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">17.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">20.8 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(3.7<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m14\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">20.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F9F9F9;\">16.7 <span class=\"ltx_text\" style=\"--ltx-bg-color:#F9F9F9;\">(3.6<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m15\" intent=\":literal\"><semantics><mo mathbackground=\"#F9F9F9\" stretchy=\"false\" style=\"--ltx-bg-color:#F9F9F9;\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">10.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">33.3 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(23.3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m16\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">TS(%)</td>\n<td class=\"ltx_td ltx_align_center\">24.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">47.0 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(23.0<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m17\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">23.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">52.1 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(28.9<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m18\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">82.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F9F9F9;\">78.0 <span class=\"ltx_text\" style=\"--ltx-bg-color:#F9F9F9;\">(4.4<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m19\" intent=\":literal\"><semantics><mo mathbackground=\"#F9F9F9\" stretchy=\"false\" style=\"--ltx-bg-color:#F9F9F9;\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">18.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">64.6 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(46.4<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m20\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\">Qwen-Omni</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">ARR(%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">86.8 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(70.8<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m21\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">24.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">93.7 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(69.7<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m22\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">66.7 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(62.8<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m23\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">62.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">62.5 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(0.5<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m24\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">ASR(%)</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">22.2 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(22.2<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m25\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">18.8 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(16.8<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m26\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">16.7 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(16.7<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m27\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">8.3 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(6.3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m28\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">PV(%)</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">7.6 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(7.6<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m29\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">6.3 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(4.3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m30\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">0.3 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(0.3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m31\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">4.2 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(2.2<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m32\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">TS(%)</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">20.9 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(20.9<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m33\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">16.7 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(14.7<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m34\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">6.3 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(6.3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m35\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">18.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">18.8 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(0.8<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m36\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\">MERaLiON</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">ARR(%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">97.7 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(63.7<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m37\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">48.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">97.9 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(49.9<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m38\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">90.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">100.0 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(9.8<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m39\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">100.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F2F2F2;\">100.0 <span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">(0.0)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">ASR(%)</td>\n<td class=\"ltx_td ltx_align_center\">4.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">37.8 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(33.8<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m40\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">11.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">39.6 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(28.6<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m41\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">52.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F9F9F9;\">47.9 <span class=\"ltx_text\" style=\"--ltx-bg-color:#F9F9F9;\">(4.9<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m42\" intent=\":literal\"><semantics><mo mathbackground=\"#F9F9F9\" stretchy=\"false\" style=\"--ltx-bg-color:#F9F9F9;\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">8.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">47.9 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(39.9<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m43\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">PV(%)</td>\n<td class=\"ltx_td ltx_align_center\">2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">28.2 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(26.2<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m44\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">20.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">25.0 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(5.0<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m45\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">32.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F9F9F9;\">29.2 <span class=\"ltx_text\" style=\"--ltx-bg-color:#F9F9F9;\">(3.7<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m46\" intent=\":literal\"><semantics><mo mathbackground=\"#F9F9F9\" stretchy=\"false\" style=\"--ltx-bg-color:#F9F9F9;\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">22.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">22.9 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(0.9<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m47\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">TS(%)</td>\n<td class=\"ltx_td ltx_align_center\">8.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">51.3 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(43.3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m48\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">22.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">52.1 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(30.1<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m49\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">66.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F9F9F9;\">62.5 <span class=\"ltx_text\" style=\"--ltx-bg-color:#F9F9F9;\">(4<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m50\" intent=\":literal\"><semantics><mo mathbackground=\"#F9F9F9\" stretchy=\"false\" style=\"--ltx-bg-color:#F9F9F9;\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">66.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">66.7 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(0.7<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m51\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\">Ultravox</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">ARR(%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">96.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">100.0 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(4.0<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m52\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">100.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F2F2F2;\">100.0 <span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">(0.0)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">100.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F2F2F2;\">100.0 <span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">(0.0)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\">85.4 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(41.4<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m53\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">ASR(%)</td>\n<td class=\"ltx_td ltx_align_center\">4.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">16.9 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(12.9<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m54\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">16.7 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(12.7<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m55\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">14.6 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(12.6<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m56\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">4.1 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(4.1<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m57\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">PV(%)</td>\n<td class=\"ltx_td ltx_align_center\">6.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">10.3 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(4.3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m58\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">20.8 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(16.8<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m59\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">10.4 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(10.4<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m60\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">10.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\">14.6 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(4.6<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m61\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">TS(%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#E6E6E6;\">20.9 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(20.9<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m62\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">12.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#E6E6E6;\">27.1 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(15.1<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m63\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#E6E6E6;\">12.5 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(11.7<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m64\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">10.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#E6E6E6;\">25.0 <span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">(15.0<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T1.m65\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" stretchy=\"false\" style=\"--ltx-bg-color:#E6E6E6;\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "baselines",
            "126↑uparrow",
            "337↑uparrow",
            "parentheses",
            "698↑uparrow",
            "222↑uparrow",
            "37↑uparrow",
            "over",
            "44↓downarrow",
            "499↑uparrow",
            "gcg∗",
            "49↑uparrow",
            "09↑uparrow",
            "526↑uparrow",
            "464↑uparrow",
            "autodan∗",
            "708↑uparrow",
            "264↑uparrow",
            "289↑uparrow",
            "76↑uparrow",
            "37↓downarrow",
            "233↑uparrow",
            "286↑uparrow",
            "147↑uparrow",
            "baseline",
            "models",
            "before",
            "205↑uparrow",
            "arr",
            "03↑uparrow",
            "05↑uparrow",
            "168↑uparrow",
            "63↑uparrow",
            "230↑uparrow",
            "102↑uparrow",
            "262↑uparrow",
            "129↑uparrow",
            "each",
            "414↑uparrow",
            "applying",
            "22↑uparrow",
            "20↑uparrow",
            "117↑uparrow",
            "4↓downarrow",
            "49↓downarrow",
            "three",
            "07↑uparrow",
            "400↑uparrow",
            "corresponding",
            "ssjours",
            "improvements",
            "151↑uparrow",
            "ssj",
            "40↑uparrow",
            "338↑uparrow",
            "qwen2audio",
            "metric",
            "41↑uparrow",
            "results",
            "98↑uparrow",
            "167↑uparrow",
            "meralion",
            "gcg∗ours",
            "autodan∗ours",
            "399↑uparrow",
            "experimental",
            "50↑uparrow",
            "46↑uparrow",
            "vanillaours",
            "150↑uparrow",
            "values",
            "qwenomni",
            "vanilla",
            "127↑uparrow",
            "628↑uparrow",
            "ultravox",
            "104↑uparrow",
            "209↑uparrow",
            "asr",
            "697↑uparrow",
            "43↑uparrow",
            "433↑uparrow",
            "query",
            "iterations",
            "637↑uparrow",
            "36↓downarrow",
            "08↑uparrow",
            "stylebreak",
            "denote",
            "after",
            "301↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attack Effectiveness.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.T1\" title=\"Table 1 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that StyleBreak consistently boosts the attack performance across all baselines, with ASR gains ranging from 7.1% to 22.3%, demonstrating strong effectiveness and broad applicability.\nDespite the overall improvements, attack effectiveness varied across methods and models.\nFor signal-level attack, SSJ suffers from low ASR (avg. 4.5%) but high ARR (avg. 57.5%), as LAMs tend to repeat spelled-out prompts rather than provide direct answers.\nHowever, applying StyleBreak on SSJ effectively mitigates this behavior, boosting ASR by 4.7&#215;.\nFor text semantic-level attacks GCG<sup class=\"ltx_sup\">&#8727;</sup> and AutoDAN<sup class=\"ltx_sup\">&#8727;</sup>, although the attack performance is significantly improved after combining with StyleBreak, both the original and StyleBreak-enhanced versions exhibit comparable performance to those of Vanilla except on MERaLiON. We attribute this to limited model capacity to process long audio or semantic loss during text transformation.\nMoreover, models exhibit distinct behaviors. For Ultravox, StyleBreak tends to trigger affirmative replies (e.g., &#8220;Yes, I can help you to&#8230;&#8221;) rather than explicit harmful content, resulting in a notable ARR increase but only modestly affecting other metrics.\nInterestingly, under multi-attribute composite attacks, MERaLiON demonstrates the highest vulnerability, contrary to its robustness under single-attribute perturbations shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. This may stem from MERaLiON&#8217;s stronger generalization in multicultural contexts, which makes it more sensitive to complex style-aware audio prompts.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Audio-language Models (LAMs) have recently enabled powerful speech-based interactions by coupling audio encoders with Large Language Models (LLMs).\nHowever, the security of LAMs under adversarial attacks remains underexplored, especially through audio jailbreaks that craft malicious audio prompts to bypass alignment. Existing efforts primarily rely on converting text-based attacks into speech or applying shallow signal-level perturbations, overlooking the impact of human speech&#8217;s expressive variations on LAM alignment robustness. To address this gap, we propose StyleBreak, a novel style-aware audio jailbreak framework that systematically investigates how diverse human speech attributes affect LAM alignment robustness. Specifically, StyleBreak employs a two-stage style-aware transformation pipeline that perturbs both textual content and audio to control linguistic, paralinguistic, and extralinguistic attributes. Furthermore, we develop a query-adaptive policy network that automatically searches for adversarial styles to enhance the efficiency of LAM jailbreak exploration. Extensive evaluations demonstrate that LAMs exhibit critical vulnerabilities when exposed to diverse human speech attributes. Moreover, StyleBreak achieves substantial improvements in attack effectiveness and efficiency across multiple attack paradigms, highlighting the urgent need for more robust alignment in LAMs.</p>\n\n",
                "matched_terms": [
                    "stylebreak",
                    "models",
                    "improvements",
                    "applying"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Typically, human speech conveys three types of information: linguistic, paralinguistic, and extralinguistic, corresponding to spoken semantic content, emotion, and speaker-specific traits, respectively&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">11-lu2023speechtriplenet</span>)</cite>.\nWhile the expressive richness of human speech substantially enlarges the input space, its impact on amplifying LAM vulnerabilities under audio jailbreak remains unexplored.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this critical gap, we introduce StyleBreak, a novel style-aware audio jailbreak framework that systematically investigates how the attributes of human speech inputs affect LAM alignment robustness. Specifically, we construct a two-stage style-aware transformation pipeline, which perturbs both the textual content and audio to enable fine-grained control over speech attributes. For text prompt transformation, prompts are rewritten with emotional semantics to simulate linguistic variations. For audio generation, speech is synthesized using a controllable text-to-speech (TTS) system that incorporates fine-grained paralinguistic traits such as emotion, as well as extralinguistic traits including age and gender.\nTo further enhance attack effectiveness, we design a query-adaptive policy network that automatically searches for adversarial style configurations per query, enabling efficient and targeted jailbreak exploration.\nExtensive experiments show that StyleBreak reveals critical LAM vulnerabilities, exposing their lack of robustness to perturbations across three key human speech attributes.\nBy adaptively targeting these weaknesses, StyleBreak achieves strong attack effectiveness and efficiency under various attack paradigms, with attack success rate improvements ranging from 7.1% to 22.3% within only three query iterations.\nGenerally, the contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "three",
                    "query",
                    "over",
                    "iterations",
                    "stylebreak",
                    "improvements"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Alignment</span> is a nascent research field that aims to align models&#8217; behaviors with the expected intentions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">26-shen2023large</span>)</cite>.\nTo prevent responding to malicious instructions, LLMs are trained with safety-enhancing techniques such as RLHF&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">23-ouyang2022training</span>)</cite> and DPO&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">24-rafailov2023direct</span>)</cite>, which have led to significant progress in safety alignment.\nDespite these practical advancements, the alignment robustness of LAMs that extend LLMs with audio modalities remain under-explored in jailbreak-related contexts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">21-peri2024speechguard</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">22-wang2024audiobench</span>)</cite>, especially when compared to the growing literature on LLM security&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">25-li2025privacy</span>)</cite>.\nIn this work, we systematically investigate vulnerabilities in LAMs by exploring a previously overlooked surface, namely the expressive attributes of human speech. Our findings reveal critical shortcomings in LAM alignment robustness, highlighting the urgent need for improved safety alignment in these models before widespread deployment.</p>\n\n",
                "matched_terms": [
                    "models",
                    "before"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Threat Model &amp; Objective.</span> The goal of the adversary is to bypass the safety alignment of a target LAM by crafting harmful queries in diverse human speech attributes, inducing malicious responses rather than refusals.\nFormally, we assume black-box access to a target LAM represented as a function <math alttext=\"M:\\mathcal{A}\\times\\mathcal{T}\\rightarrow\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi></mrow><mo stretchy=\"false\">&#8594;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">M:\\mathcal{A}\\times\\mathcal{T}\\rightarrow\\mathcal{Y}</annotation></semantics></math>, where <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math>, and <math alttext=\"\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Y}</annotation></semantics></math> denote the audio input space, textual instruction space, and textual response space, respectively. The adversary can query <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m5\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> using audio and/or textual inputs without access to model parameters.\nGiven a set of harmful textual queries <math alttext=\"\\mathcal{Q}=\\{q\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m6\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>q</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{Q}=\\{q\\}</annotation></semantics></math>, StyleBreak aims to generate adversarial audio prompts <math alttext=\"a_{p}=C(q,x_{ins})\\in\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>p</mi></msub><mo>=</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi></mrow><annotation encoding=\"application/x-tex\">a_{p}=C(q,x_{ins})\\in\\mathcal{A}</annotation></semantics></math>, where <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m8\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> is a controllable TTS system and <math alttext=\"x_{ins}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m9\" intent=\":literal\"><semantics><msub><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">x_{ins}</annotation></semantics></math> describes the characteristics of voice. When paired with a fixed textual prompt <math alttext=\"t_{i}\\in\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m10\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi></mrow><annotation encoding=\"application/x-tex\">t_{i}\\in\\mathcal{T}</annotation></semantics></math> (e.g., &#8220;Answer the question in the audio&#8221;), the goal is to induce an affirmative response <math alttext=\"y=M(a_{p},t_{i})\\in\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m11\" intent=\":literal\"><semantics><mrow><mi>y</mi><mo>=</mo><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>p</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi></mrow><annotation encoding=\"application/x-tex\">y=M(a_{p},t_{i})\\in\\mathcal{Y}</annotation></semantics></math> that aligns with the adversarial intent.</p>\n\n",
                "matched_terms": [
                    "denote",
                    "query",
                    "stylebreak"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to prior work that primarily focuses on text semantic-level prompts or signal-level perturbations, we introduce StyleBreak, a novel style-aware audio jailbreak framework designed to explore LAM alignment robustness under diverse human speech attributes.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx3.F1\" title=\"Figure 1 &#8227; Overview &#8227; Methodology &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, StyleBreak begins with a two-stage style-aware transformation pipeline, which includes emotion-driven prompt transformation and style-controlled audio attack generation to craft adversarial audio <math alttext=\"a_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">a_{p}</annotation></semantics></math> from origin query <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m2\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> by varying speech styles.\nAs not all style combinations are equally effective at inducing jailbreaks, a query-adaptive policy strategy <math alttext=\"\\pi_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta}</annotation></semantics></math> is introduced to automatically identify effective style configurations for each input query, enabling scalable and efficient jailbreak.\nFinally, the generated stylized adversarial audio is submitted to the target LAM to obtain responses and assess jailbreak performance.</p>\n\n",
                "matched_terms": [
                    "each",
                    "stylebreak",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Collection.</span> Most existing studies rely on AdvBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>, which contains 520 harmful textual queries. Following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2-ying2024unveiling</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">8-shen2024voice</span>)</cite>, we select 200 representative queries from this benchmark as our origin harmful query set to balance coverage and practicality.\nTo guide the generation of audio with diverse human speech attributes, we define a discrete style configuration space <math alttext=\"\\mathcal{S}=\\mathcal{E}\\times\\mathcal{G}\\times\\mathcal{A}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>g</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\mathcal{E}\\times\\mathcal{G}\\times\\mathcal{A}_{g}</annotation></semantics></math>, where <math alttext=\"e\\in\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>e</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi></mrow><annotation encoding=\"application/x-tex\">e\\in\\mathcal{E}</annotation></semantics></math>, <math alttext=\"g\\in\\mathcal{G}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi></mrow><annotation encoding=\"application/x-tex\">g\\in\\mathcal{G}</annotation></semantics></math>, and <math alttext=\"a_{g}\\in\\mathcal{A}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>g</mi></msub><mo>&#8712;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>g</mi></msub></mrow><annotation encoding=\"application/x-tex\">a_{g}\\in\\mathcal{A}_{g}</annotation></semantics></math> denote emotion, gender, and age group, respectively.\nBased on this, we construct a style reference set <math alttext=\"\\mathcal{X}=\\{x_{ins}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{X}=\\{x_{ins}\\}</annotation></semantics></math> from the GigaSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">33-GigaSpeech2021</span>)</cite>, which provides labeled speech samples annotated with the required attributes (as summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx3.F1\" title=\"Figure 1 &#8227; Overview &#8227; Methodology &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Each style instance <math alttext=\"x_{ins}=(t_{ins},a_{ref})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo>,</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{ins}=(t_{ins},a_{ref})</annotation></semantics></math> consists of a natural language description <math alttext=\"t_{ins}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m7\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">t_{ins}</annotation></semantics></math> (e.g., &#8220;A young male speaker expressing anger&#8221;) and a corresponding reference audio clip <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m8\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> exemplifying the specified style configuration <math alttext=\"(e,g,a_{g})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo>,</mo><mi>g</mi><mo>,</mo><msub><mi>a</mi><mi>g</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(e,g,a_{g})</annotation></semantics></math>.\nFor each unique configuration in <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, we randomly sample 5 diverse reference instances to ensure sufficient coverage and variation during audio generation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "denote",
                    "query",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Observation.</span> The generated adversarial audio samples <math alttext=\"a_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">a_{p}</annotation></semantics></math>, enriched with diverse speech styles, enable systematic investigation into how various human speech attributes influence the LAM alignment robustness. However, the combinatorial space of style configurations&#8212;spanning emotions <math alttext=\"|\\mathcal{E}|=7\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mn>7</mn></mrow><annotation encoding=\"application/x-tex\">|\\mathcal{E}|=7</annotation></semantics></math>, age groups <math alttext=\"|\\mathcal{A}_{g}|=5\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>g</mi></msub><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">|\\mathcal{A}_{g}|=5</annotation></semantics></math>, and genders <math alttext=\"|\\mathcal{G}|=2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">|\\mathcal{G}|=2</annotation></semantics></math> &#8212;yields distinct variants <math alttext=\"|\\mathcal{S}|=70\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mn>70</mn></mrow><annotation encoding=\"application/x-tex\">|\\mathcal{S}|=70</annotation></semantics></math>. Exhaustively pairing each query with all possible configurations is computationally expensive and constrained by practical limitations such as API rate limits. These challenges hinder scalability to newly emerging LAMs and reduce adaptability to evolving jailbreak queries. Therefore, we wonder whether an effective configuration can be identified to improve evaluation efficiency.</p>\n\n",
                "matched_terms": [
                    "each",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by prior work showing that different transformations exhibit varying effectiveness across inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">36-ji2024defending</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">37-DBLP:conf/icml/YangDHSR020</span>)</cite>, we conduct a preliminary study on how style configurations influence jailbreak success. Specifically, we apply diverse style configurations to each query <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p2.m1\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> and measure the resulting attack success rates. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx3.F2\" title=\"Figure 2 &#8227; Style-aware Transformations Pipelines &#8227; Methodology &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, each curve represents the success trend under varying styles for a specific query. The variation in peak positions reveals that jailbreak effectiveness is highly query-specific rather than uniform across queries.</p>\n\n",
                "matched_terms": [
                    "each",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Query-adaptive Policy Strategy.</span>\nBuilding on our observation, we learn a policy network that adaptively chooses style configurations based on the input query, avoiding exhaustive search while preserving effectiveness. Specifically, we introduce a multi-head policy network <math alttext=\"\\pi_{\\theta}:\\mathcal{Q}\\rightarrow\\Delta(\\mathcal{S})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#960;</mi><mi>&#952;</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><mo stretchy=\"false\">&#8594;</mo><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\theta}:\\mathcal{Q}\\rightarrow\\Delta(\\mathcal{S})</annotation></semantics></math> that maps a harmful query <math alttext=\"q\\in\\mathcal{Q}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p3.m2\" intent=\":literal\"><semantics><mrow><mi>q</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi></mrow><annotation encoding=\"application/x-tex\">q\\in\\mathcal{Q}</annotation></semantics></math> to a categorical distribution over the style configuration space <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p3.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, where each head independently predicts the distribution over a specific attribute dimension, and <math alttext=\"\\Delta(\\mathcal{S})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p3.m4\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta(\\mathcal{S})</annotation></semantics></math> denotes the probability simplex over possible configurations.\nThe distribution parameterized by learnable weights <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p3.m5\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> adaptively selects effective style configurations by maximizing the following reward:</p>\n\n",
                "matched_terms": [
                    "each",
                    "query",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides comprehensive results to understand both LAM robustness and StyleBreak. We begin by analyzing the impact of human speech attributes, followed by evaluating StyleBreak performance across diverse attack paradigms, and conclude with further exploration of StyleBreak capabilities.</p>\n\n",
                "matched_terms": [
                    "results",
                    "stylebreak"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span>\nTo comprehensively evaluate the model alignment robustness under attacks, we adopt four main metrics: Attack Response Rate (ARR), Attack Success Rate (ASR), Toxicity Score (TS), and Policy Violation (PV).\nARR measures the proportion of prompts that receive non-refusal responses, using predefined refusal patterns (e.g., &#8221;I&#8217;m sorry&#8221;)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>.\nASR employs a RoBERTa-based binary classifier&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">39-DBLP:journals/corr/abs-2402-13457</span>)</cite> to assess whether the model directly answers harmful inputs.\nLower ARR and ASR indicate stronger alignment robustness against jailbreak.\nTo further assess the quality of response, we employ a LLaMA3-Guard-based evaluation framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">40-dubey2024llama3herdmodels</span>)</cite>.\nBased on this framework, each response is assigned a vigilance score ranging from 0 to 9, which is subsequently binarized into a TS indicator, where scores above 4 are labeled as high-risk (TS = 1).\nSimilarly, PV denotes whether the response violates predefined safety policies, with PV = 1 indicating a violation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "asr",
                    "arr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Models.</span>\nWe consider four open-source LAMs with general capabilities for our major evaluation: Qwen2-Audio-7B-Instruct (Qwen2-Audio)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">14-chu2024qwen2</span>)</cite>, MERaLiON-AudioLLM-Whisper-SEA-LION (MERaLiON)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">16-he2024meralion</span>)</cite>, Ultravox-v0.4.1-Llama-3.1-8B (Ultravox)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">19-ultravox2025</span>)</cite>, and Qwen2.5-Omni-7B (Qwen-Omni)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">18-xu2025qwen2</span>)</cite>. The first three models are selected based on their relatively low ARR reported in VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">41-chen2024voicebench</span>)</cite>, indicating stronger resistance to adversarial prompts. Qwen2.5-Omni serves as a representative state-of-the-art multimodal model with strong general performance. All tested models are safety-aligned to reject harmful instructions and evaluated locally on 2 &#215; A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "three",
                    "models",
                    "qwenomni",
                    "arr",
                    "qwen2audio",
                    "meralion",
                    "ultravox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span>\nTo assess StyleBreak under diverse attack paradigms, we evaluate it with four representative audio jailbreak methods: Vanilla&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2-ying2024unveiling</span>)</cite>, AutoDAN<sup class=\"ltx_sup\">&#8727;</sup>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>, GCG<sup class=\"ltx_sup\">&#8727;</sup>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">30-liu2024autodan</span>)</cite>, and SSJ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">1-yang2024audio</span>)</cite>. Vanilla directly converts the original text queries into speech, while AutoDAN<sup class=\"ltx_sup\">&#8727;</sup> and GCG<sup class=\"ltx_sup\">&#8727;</sup> are text semantic-level attacks that manipulate the textual prompts before audio synthesis. In contrast, SSJ introduces perturbations at the audio level.\nTo ensure fairness and effectiveness under the black-box setting, adversarial examples are first optimized using AutoDAN and GCG on LLaMA2, a well-aligned LLM, and then transferred to the target LAMs.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "gcg∗",
                    "ssj",
                    "before",
                    "stylebreak",
                    "autodan∗",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets &amp; Settings.</span>\nA 200-query subset of AdvBench, as mentioned in the methodology section, is used to evaluate the impact of speech attributes and to train our adaptive policy, which is further assessed on StyleBreak and other baselines using 50 additional, non-overlapping queries.\nAll evaluations are conducted with default settings and no modifications.\nTo ensure consistency, we employ CosyVoice2-0.5B as the unified TTS model, and each test is repeated five times to mitigate randomness.\nFurther implementation details for policy and evaluation are in Appendix B.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "stylebreak",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Attributes.</span>\nWe explore how emotion control in linguistic attributes affects LAMs by altering the textual semantic content of adversarial audio prompts.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) reveals that emotional variations in linguistic attributes lead to significant increases across all jailbreak metrics for all target LAMs. Even the most robust model, Qwen-Omni, shows an average ASR increase from 0% to 9.1%.\nMoreover, specific emotional styles can strongly impact certain models. For instance, on MERaLiON, the surprised variant yields an ASR 8.57% higher than the second-highest, highlighting the nuanced influence of different emotional semantics on LAM alignment robustness.</p>\n\n",
                "matched_terms": [
                    "meralion",
                    "asr",
                    "models",
                    "qwenomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Attributes.</span>\nWe investigate LAM vulnerability to emotional manipulation in paralinguistic attributes by modulating acoustic emotional features in audio prompts with original textual semantic content.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b), emotional variations in paralinguistic attributes significantly increase jailbreak performance across models compared to the Vanilla setting.\nNotably, Ultravox is particularly sensitive to paralinguistic variations, with ASR increasing by 4.6-6.8&#215; over the original input and averaging 21.6% higher than its linguistic counterpart&#8212;likely due to its enhanced performance on emotion-related tasks.\nAlthough less effective than linguistic emotional rewriting which yields 3.9&#215; higher ARR and better conceals intent, paralinguistic emotional control still induces notable jailbreaks, with ASR rising by 9.1% on average. This underscores that even subtle acoustic features can compromise LAM safety alignment.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "vanilla",
                    "arr",
                    "over",
                    "ultravox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extralinguistic Attributes.</span>\nTo analyze the impact of extralinguistic attributes, we fix the original textual semantic content and generate adversarial audio prompts by individually varying the age and gender in the style configuration when querying the target LAMs.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c)(d), both age and gender variations show internally consistent trends across all four models and evaluation metrics, respectively.\nFor age, LAMs are most robust to child voices, showing the lowest ASR, while elderly voices yield the highest jailbreak success, with ASR averaging 13.3% higher than that of child voices.\nFor gender, male voices consistently result in higher ASR than female voices, with an average increase of 8.3% across the target LAMs.\nThese findings suggest that LAMs are generally more robust to higher-pitched voices such as those of children and females, but show increased vulnerability to lower-pitched voices such as those of males and the elderly.\nConsistently, among the target LAMs, Qwen2-Audio demonstrates the strongest alignment robustness to extralinguistic variations, while Ultravox remains the most susceptible.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr",
                    "ultravox",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Efficiency.</span>\nIn Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F4\" title=\"Figure 4 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we illustrate the ASR of all baselines with and without StyleBreak in different query iterations to investigate its effects on LAM alignment robustness.\nThe results reflect that integrating StyleBreak rapidly enhances attack success with minimal additional queries, confirming its effectiveness.\nNotably, Vanilla and SSJ initially fail to improve ASR through repeated queries alone but achieve 30.5% and 40.5% gains respectively after applying StyleBreak within just 10 iterations. In addition, an appropriate number of queries can achieve satisfactory attack coverage at an acceptable cost. Detailed results on additional metrics are available in Appendix C.1.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "ssj",
                    "asr",
                    "query",
                    "vanilla",
                    "iterations",
                    "results",
                    "stylebreak",
                    "after",
                    "applying"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Studies.</span>\nWe evaluate the impact of three key modules in StyleBreak: emotion-driven prompt transformation (EPT), style-controlled audio attack generation (EAG), and the query-adaptive policy (QP). To this end, we design the following variants: <math alttext=\"+EPT\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx3.p3.m1\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EPT</annotation></semantics></math> applies emotional rewriting to alter the semantic content of original queries; <math alttext=\"+EAG\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx3.p3.m2\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>G</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EAG</annotation></semantics></math> introduces paralinguistic and extralinguistic perturbations to synthesize adversarial audio; <math alttext=\"+QP\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx3.p3.m3\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+QP</annotation></semantics></math> replaces random style selection with a learned policy for adaptive configuration.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.T2\" title=\"Table 2 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the full StyleBreak consistently outperforms all variants, confirming the complementary benefits of each module. Moreover, audio-based attacks achieve markedly higher ASR than their text-only counterparts, underscoring the heightened vulnerability of LAMs to audio modality.\nAdditional analyses on policy selection distributions are presented in Appendices C.2 and C.3.</p>\n\n",
                "matched_terms": [
                    "each",
                    "stylebreak",
                    "asr",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representations of Attacks.</span>\nAs Qwen-Omni does not provide embedding representations, we visualize the internal representations on the other three LAMs to further explore LAMs&#8217; robustness.\nTo analyze how these models encode different types of inputs, we use the final layer&#8217;s last hidden state to represent each input query, capturing the model&#8217;s latent response&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">42-gong2025figstep</span>)</cite>. Then, t-SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">43-JMLR:v9:vandermaaten08a</span>)</cite> is applied to reduce these high-dimensional embeddings to two dimensions for visualization.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F5\" title=\"Figure 5 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the representation visualization of benign and harmful queries across text and audio modalities, along with StyleBreak based on Vanilla.\nQuery transformation for benign queries is conducted following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">7-peng2025jalmbench</span>)</cite> with details found in Appendix D.\nThe results indicate that the representations of the same content across text and audio modalities show large discrepancies, with Qwen2-Audio exhibiting the smallest cross-modal representation gap, demonstrating better multimodal alignment.\nIn terms of modality,\nwhile LAMs exhibit some ability to distinguish benign from harmful inputs in the text modality, this capability is significantly weaker in the audio modality, where the two types of queries often overlap.\nMoreover, StyleBreak effectively triggers model biases, inducing substantial semantic perturbations relative to the other two audio query types. This finding highlights human speech attributes as a potent factor for revealing LAM vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "three",
                    "models",
                    "query",
                    "qwenomni",
                    "results",
                    "stylebreak",
                    "qwen2audio",
                    "each",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Experiments on Advanced Models.</span>\nWe conduct experiments on two advanced commercial LAMs, GPT-4o and Gemini-2.5-flash. The style configurations for our method are directly transferred from the policy trained on Qwen2-Audio, without any fine-tuning on the target models. As shown in Figure 6, StyleBreak consistently improves attack performance across all evaluated baselines. Notably, even on the most robust GPT-4o, ASR increases by 2.1%<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx4.p2.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>9.5% after applying StyleBreak, demonstrating the generalization ability and effectiveness of the learned policy. Furthermore, we observe a significant increase in TS after applying StyleBreak by 4.7% and 9.7% on average across the two models, further highlighting that human speech attributes perturbations can substantially compromise LAM safety alignment.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "models",
                    "asr",
                    "stylebreak",
                    "after",
                    "qwen2audio",
                    "applying"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we expose a critical and previously underestimated threat: LAMs are inherently more vulnerable to jailbreak when exposed to audio prompts with perturbed human speech attributes.\nTo investigate this threat, we propose StyleBreak, a novel style-aware audio jailbreak framework that integrates a two-stage transformation pipeline and a query-adaptive policy to generate adversarial audio with controllable linguistic, paralinguistic, and extralinguistic attributes.\nExtensive experiments demonstrate that LAMs are particularly susceptible to adversarial perturbations in key human speech attributes including emotion, age, and gender.\nMoreover, StyleBreak consistently achieves outstanding attack performance with minimal additional queries and outperforms existing baselines across multiple attack paradigms.\nOverall, our work reveals critical alignment vulnerabilities in LAMs exposed by style-aware audio jailbreaks, underscoring the pressing necessity of robust LAM safety alignment before their widespread deployment.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "stylebreak",
                    "before"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The following presents the emotion-driven prompt transformation template and audio jailbreak process in StyleBreak, where &lt;Prompt&gt; denotes the original query <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math>, and &lt;Audio_Query&gt; represents the adversarial audio input <math alttext=\"a_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">a_{p}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "stylebreak",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement a multi-head policy network to adaptively select style configurations for optimizing jailbreak efficiency. The policy takes as input a query representation vector <math alttext=\"d_{q}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>q</mi></msub><annotation encoding=\"application/x-tex\">d_{q}</annotation></semantics></math> and outputs discrete selections for three style attributes: emotion, age, and gender.</p>\n\n",
                "matched_terms": [
                    "query",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Three output heads, implemented as linear layers, project the shared representation into logits corresponding to emotion, age, and gender classes, respectively.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we evaluate the quality of jailbreak prompts using four main metrics. Among them, ARR and ASR measure the success rate of jailbreaks across a set of m input queries, while PV and TS assess the harmfulness and high-toxicity of the generated responses. The templates for TS are shown above.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "arr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within the StyleBreak framework, we define a judge function <math alttext=\"J(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>J</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">J(\\cdot)</annotation></semantics></math> to evaluate the output <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> from the target LAM in response to each adversarial audio input <math alttext=\"a_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">a_{p}</annotation></semantics></math>. This judge function computes the average score across four evaluation mertics, which is formulated as:</p>\n\n",
                "matched_terms": [
                    "each",
                    "stylebreak"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GCG<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup>:</span> We adopt the official implementation of GCG targeting LLaMA-2-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>. For black-box models including Qwen2-Audio, Qwen-Omni, MERaLiON, and Ultravox, we follow the transferable optimization setup based on LLaMA-2-7B suffix tuning and convert the adversarial text into audio using CosyVoice2-0.5B. Notably, &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite> have demonstrated that GCG exhibits strong transferability across various black-box LLMs.</p>\n\n",
                "matched_terms": [
                    "gcg∗",
                    "models",
                    "qwenomni",
                    "qwen2audio",
                    "meralion",
                    "ultravox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SSJ:</span> Following the previous setup&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">1-yang2024audio</span>)</cite>, we mask one harmful and unsafe word in the original malicious text, then spell out the masked word character by character. Each character is synthesized into speech using CosyVoice2-0.5B. The resulting audio is then combined with a prompt containing the masked query to evaluate model responses.</p>\n\n",
                "matched_terms": [
                    "each",
                    "query",
                    "ssj"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#A2.F7\" title=\"Figure 7 &#8227; Appendix B Query-adaptive Policy &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the results of ARR, TS, and PV across four LAMs after varying numbers of query attempts, corroborating our previous analysis of StyleBreak efficiency. Notably, all StyleBreak-enhanced variants achieve ARR approaching 100% after just five query iterations across all models, underscoring the inability of LAMs to detect adversarial intent and highlighting their significant vulnerability to style-aware jailbreak attempts.</p>\n\n",
                "matched_terms": [
                    "models",
                    "query",
                    "iterations",
                    "results",
                    "stylebreak",
                    "arr",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#A6.F8\" title=\"Figure 8 &#8227; Appendix F The Learned Query-adaptive Policy &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, we analyze the policy selection distributions of each group in the style configuration across four LAMs. For emotion, the learned policy consistently avoids selecting disgusted and angry, while showing a strong preference for surprised and happy across all models. Regarding age, the child category is rarely selected, and for gender, the policy tends to favor male over female. These results further support our findings on extralinguistic attribute influence discussed in the experiment section.</p>\n\n",
                "matched_terms": [
                    "results",
                    "models",
                    "over",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A detailed ablation analysis is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#A5.T3\" title=\"Table 3 &#8227; Appendix E Query Iteration Exploration &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, which reports the ARR, PV, and TS results after three query iterations across different modules, further demonstrating the contribution of each component in our method.\nIn terms of ARR, we observe a steady improvement with the incremental addition of each module, confirming the effectiveness of our two-stage transformation pipeline and the query-adaptive policy.\nFor PV and TS, perturbations to paralinguistic and extralinguistic attributes at the audio level&#8212;such as in the configurations <math alttext=\"Vanilla+EPT\" class=\"ltx_Math\" display=\"inline\" id=\"A8.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi></mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">Vanilla+EPT</annotation></semantics></math> and <math alttext=\"+EAG\" class=\"ltx_Math\" display=\"inline\" id=\"A8.p1.m2\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>G</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EAG</annotation></semantics></math>&#8212;lead to a significant increase in high-toxicity outputs. This highlights the increased susceptibility of LAMs to adversarial audio inputs that exploit variations in expressive human speech characteristics.</p>\n\n",
                "matched_terms": [
                    "three",
                    "query",
                    "iterations",
                    "results",
                    "arr",
                    "after",
                    "each"
                ]
            }
        ]
    },
    "Sx4.T2": {
        "source_file": "StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak",
        "caption": "Table 2: Ablation study on ASR (%) under 3 query iterations. The bottom row denotes our StyleBreak approach.",
        "body": "Settings (%)\nQwen2-Audio\nQwen-Omni\nMERaLiON\nUltravox\n\n\nText-only Attacks\n\n\nOrigin query\n1.1\n0.0\n1.5\n1.0\n\n\n   +E​P​T+EPT\n\n8.9\n4.1\n12.1\n9.6\n\n\nAudio-based Attacks\n\n\nVanilla\n10.0\n0.0\n4.0\n4.0\n\n\n   +E​P​T+EPT\n\n15.3\n7.0\n20.5\n5.4\n\n\n   +E​P​T,E​A​G+EPT,EAG\n\n17.2\n9.6\n35.1\n14.8\n\n\n   +E​P​T,E​A​G,Q​P+EPT,EAG,QP\n\n30.5\n22.2\n37.8\n16.9",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Settings (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Qwen2-Audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Qwen-Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MERaLiON</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Ultravox</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Text-only Attacks</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Origin query</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#8195;<math alttext=\"+EPT\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T2.m1\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EPT</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.9</td>\n<td class=\"ltx_td ltx_align_center\">4.1</td>\n<td class=\"ltx_td ltx_align_center\">12.1</td>\n<td class=\"ltx_td ltx_align_center\">9.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Audio-based Attacks</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#8195;<math alttext=\"+EPT\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T2.m2\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EPT</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">15.3</td>\n<td class=\"ltx_td ltx_align_center\">7.0</td>\n<td class=\"ltx_td ltx_align_center\">20.5</td>\n<td class=\"ltx_td ltx_align_center\">5.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">&#8194;&#8202;&#8195;<math alttext=\"+EPT,EAG\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T2.m3\" intent=\":literal\"><semantics><mrow><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><mo>,</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>G</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EPT,EAG</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">17.2</td>\n<td class=\"ltx_td ltx_align_center\">9.6</td>\n<td class=\"ltx_td ltx_align_center\">35.1</td>\n<td class=\"ltx_td ltx_align_center\">14.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">&#8194;&#8202;&#8195;<math alttext=\"+EPT,EAG,QP\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T2.m4\" intent=\":literal\"><semantics><mrow><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><mo>,</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>G</mi></mrow><mo>,</mo><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EPT,EAG,QP</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">30.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">22.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">37.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">16.9</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "row",
            "study",
            "qwen2audio",
            "ultravox",
            "e​p​te​a​gepteag",
            "denotes",
            "asr",
            "bottom",
            "origin",
            "meralion",
            "audiobased",
            "ablation",
            "approach",
            "under",
            "query",
            "iterations",
            "attacks",
            "e​p​te​a​gq​pepteagqp",
            "e​p​tept",
            "settings",
            "stylebreak",
            "qwenomni",
            "our",
            "textonly",
            "vanilla"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Studies.</span>\nWe evaluate the impact of three key modules in StyleBreak: emotion-driven prompt transformation (EPT), style-controlled audio attack generation (EAG), and the query-adaptive policy (QP). To this end, we design the following variants: <math alttext=\"+EPT\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx3.p3.m1\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EPT</annotation></semantics></math> applies emotional rewriting to alter the semantic content of original queries; <math alttext=\"+EAG\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx3.p3.m2\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>G</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EAG</annotation></semantics></math> introduces paralinguistic and extralinguistic perturbations to synthesize adversarial audio; <math alttext=\"+QP\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx3.p3.m3\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+QP</annotation></semantics></math> replaces random style selection with a learned policy for adaptive configuration.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.T2\" title=\"Table 2 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the full StyleBreak consistently outperforms all variants, confirming the complementary benefits of each module. Moreover, audio-based attacks achieve markedly higher ASR than their text-only counterparts, underscoring the heightened vulnerability of LAMs to audio modality.\nAdditional analyses on policy selection distributions are presented in Appendices C.2 and C.3.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large Audio-language Models (LAMs) have recently enabled powerful speech-based interactions by coupling audio encoders with Large Language Models (LLMs).\nHowever, the security of LAMs under adversarial attacks remains underexplored, especially through audio jailbreaks that craft malicious audio prompts to bypass alignment. Existing efforts primarily rely on converting text-based attacks into speech or applying shallow signal-level perturbations, overlooking the impact of human speech&#8217;s expressive variations on LAM alignment robustness. To address this gap, we propose StyleBreak, a novel style-aware audio jailbreak framework that systematically investigates how diverse human speech attributes affect LAM alignment robustness. Specifically, StyleBreak employs a two-stage style-aware transformation pipeline that perturbs both textual content and audio to control linguistic, paralinguistic, and extralinguistic attributes. Furthermore, we develop a query-adaptive policy network that automatically searches for adversarial styles to enhance the efficiency of LAM jailbreak exploration. Extensive evaluations demonstrate that LAMs exhibit critical vulnerabilities when exposed to diverse human speech attributes. Moreover, StyleBreak achieves substantial improvements in attack effectiveness and efficiency across multiple attack paradigms, highlighting the urgent need for more robust alignment in LAMs.</p>\n\n",
                "matched_terms": [
                    "stylebreak",
                    "under",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, most existing research focuses on the vulnerabilities of LLMs and Large Vision Models\n(LVMs) under jailbreak, while studies targeting LAMs remain significantly limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">10-liu2024jailbreak</span>)</cite>. Existing efforts typically directly convert text-based jailbreak into speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2-ying2024unveiling</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">8-shen2024voice</span>)</cite> or apply naive audio perturbations such as noise injection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">4-kang2024advwave</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">5-xiao2025tune</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">7-peng2025jalmbench</span>)</cite> and accent conversion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">3-roh2025multilingual</span>)</cite>. These approaches are relatively simplistic, primarily focusing either on text semantic-level or signal-level attacks while overlooking the rich and multifaceted attributes of human speech inputs.</p>\n\n",
                "matched_terms": [
                    "under",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this critical gap, we introduce StyleBreak, a novel style-aware audio jailbreak framework that systematically investigates how the attributes of human speech inputs affect LAM alignment robustness. Specifically, we construct a two-stage style-aware transformation pipeline, which perturbs both the textual content and audio to enable fine-grained control over speech attributes. For text prompt transformation, prompts are rewritten with emotional semantics to simulate linguistic variations. For audio generation, speech is synthesized using a controllable text-to-speech (TTS) system that incorporates fine-grained paralinguistic traits such as emotion, as well as extralinguistic traits including age and gender.\nTo further enhance attack effectiveness, we design a query-adaptive policy network that automatically searches for adversarial style configurations per query, enabling efficient and targeted jailbreak exploration.\nExtensive experiments show that StyleBreak reveals critical LAM vulnerabilities, exposing their lack of robustness to perturbations across three key human speech attributes.\nBy adaptively targeting these weaknesses, StyleBreak achieves strong attack effectiveness and efficiency under various attack paradigms, with attack success rate improvements ranging from 7.1% to 22.3% within only three query iterations.\nGenerally, the contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "stylebreak",
                    "under",
                    "query",
                    "iterations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LAMs</span> typically extend LLMs by incorporating an audio encoder that maps raw speech to semantic representations, enabling LLMs to process audio inputs seamlessly&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">14-chu2024qwen2</span>)</cite>. Recent advances&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">18-xu2025qwen2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">19-ultravox2025</span>)</cite> have developed LAMs as general-purpose frameworks capable of handling a wide range of downstream tasks through appropriately designed audio prompts. However, as they are predominantly provided via APIs or online services, users often have no access to the model&#8217;s internal parameters&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">20-murad2024unveiling</span>)</cite>. Therefore, we focus on LAMs under black-box access settings.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Jailbreak</span> aims to construct strategically crafted inputs to LLMs with the intent to bypass alignment and deceive them into generating objectionable content&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">27-yi2024jailbreak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">28-li2025jailpo</span>)</cite>.\nCurrently, most existing jailbreak research focuses on LLMs, where adversaries craft adversarial text prompts using either handcrafted templates&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">31-wei2023jailbroken</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">32-li2023deepinception</span>)</cite> or automated token-level optimization&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">30-liu2024autodan</span>)</cite> to bypass alignment objectives and elicit objectionable content.\nHowever, there are limited papers focused on the audio Jailbreak.\nOne line of research converts adversarial text prompts into audio using commercial TTS systems such as OpenAI TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2-ying2024unveiling</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">8-shen2024voice</span>)</cite>. Unfortunately, these approaches overlook the semantic and perceptual differences between text and speech, making it difficult to reveal modality-specific vulnerabilities in LAMs.\nAnother line of study introduces low-level perturbations to audio waveforms&#8212;such as background noise injection<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">4-kang2024advwave</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">5-xiao2025tune</span>)</cite>, pitch shifting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">7-peng2025jalmbench</span>)</cite>, or accent conversion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">3-roh2025multilingual</span>)</cite>&#8212;to explore model vulnerabilities. Although these techniques create signal-level variations, they typically lack semantic intent and fail to capture the rich expressive variability of human speech, limiting their effectiveness in evaluating LAMs&#8217; alignment in real-world scenarios.\nUnlike prior work that overlooks speech semantics or uses shallow perturbations, StyleBreak generates expressive adversarial speech through a two-stage transformation and adaptive policy, modeling linguistic, paralinguistic, and extralinguistic cues to expose LAM vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "stylebreak",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Threat Model &amp; Objective.</span> The goal of the adversary is to bypass the safety alignment of a target LAM by crafting harmful queries in diverse human speech attributes, inducing malicious responses rather than refusals.\nFormally, we assume black-box access to a target LAM represented as a function <math alttext=\"M:\\mathcal{A}\\times\\mathcal{T}\\rightarrow\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi></mrow><mo stretchy=\"false\">&#8594;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">M:\\mathcal{A}\\times\\mathcal{T}\\rightarrow\\mathcal{Y}</annotation></semantics></math>, where <math alttext=\"\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><annotation encoding=\"application/x-tex\">\\mathcal{A}</annotation></semantics></math>, <math alttext=\"\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi><annotation encoding=\"application/x-tex\">\\mathcal{T}</annotation></semantics></math>, and <math alttext=\"\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Y}</annotation></semantics></math> denote the audio input space, textual instruction space, and textual response space, respectively. The adversary can query <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m5\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> using audio and/or textual inputs without access to model parameters.\nGiven a set of harmful textual queries <math alttext=\"\\mathcal{Q}=\\{q\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m6\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi>q</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{Q}=\\{q\\}</annotation></semantics></math>, StyleBreak aims to generate adversarial audio prompts <math alttext=\"a_{p}=C(q,x_{ins})\\in\\mathcal{A}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>p</mi></msub><mo>=</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi></mrow><annotation encoding=\"application/x-tex\">a_{p}=C(q,x_{ins})\\in\\mathcal{A}</annotation></semantics></math>, where <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m8\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> is a controllable TTS system and <math alttext=\"x_{ins}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m9\" intent=\":literal\"><semantics><msub><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">x_{ins}</annotation></semantics></math> describes the characteristics of voice. When paired with a fixed textual prompt <math alttext=\"t_{i}\\in\\mathcal{T}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m10\" intent=\":literal\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119983;</mi></mrow><annotation encoding=\"application/x-tex\">t_{i}\\in\\mathcal{T}</annotation></semantics></math> (e.g., &#8220;Answer the question in the audio&#8221;), the goal is to induce an affirmative response <math alttext=\"y=M(a_{p},t_{i})\\in\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m11\" intent=\":literal\"><semantics><mrow><mi>y</mi><mo>=</mo><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mi>p</mi></msub><mo>,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi></mrow><annotation encoding=\"application/x-tex\">y=M(a_{p},t_{i})\\in\\mathcal{Y}</annotation></semantics></math> that aligns with the adversarial intent.</p>\n\n",
                "matched_terms": [
                    "stylebreak",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attack Settings.</span> In this work, we consider two complementary attack scenarios:\n1) Text-only attacks serve as a baseline for prompt-based jailbreaks testing whether original or style-aware text prompts can bypass LAMs&#8217; safety alignment.\n2) Audio-based attacks simulate diverse human speech attributes, exploring how variations in linguistic, paralinguistic, and extralinguistic attributes affect the model&#8217;s alignment robustness. Here, adversarial prompts are delivered via audio, optionally combined with text templates <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p2.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audiobased",
                    "textonly",
                    "settings",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to prior work that primarily focuses on text semantic-level prompts or signal-level perturbations, we introduce StyleBreak, a novel style-aware audio jailbreak framework designed to explore LAM alignment robustness under diverse human speech attributes.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx3.F1\" title=\"Figure 1 &#8227; Overview &#8227; Methodology &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, StyleBreak begins with a two-stage style-aware transformation pipeline, which includes emotion-driven prompt transformation and style-controlled audio attack generation to craft adversarial audio <math alttext=\"a_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">a_{p}</annotation></semantics></math> from origin query <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m2\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> by varying speech styles.\nAs not all style combinations are equally effective at inducing jailbreaks, a query-adaptive policy strategy <math alttext=\"\\pi_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta}</annotation></semantics></math> is introduced to automatically identify effective style configurations for each input query, enabling scalable and efficient jailbreak.\nFinally, the generated stylized adversarial audio is submitted to the target LAM to obtain responses and assess jailbreak performance.</p>\n\n",
                "matched_terms": [
                    "stylebreak",
                    "under",
                    "query",
                    "origin"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Collection.</span> Most existing studies rely on AdvBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>, which contains 520 harmful textual queries. Following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2-ying2024unveiling</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">8-shen2024voice</span>)</cite>, we select 200 representative queries from this benchmark as our origin harmful query set to balance coverage and practicality.\nTo guide the generation of audio with diverse human speech attributes, we define a discrete style configuration space <math alttext=\"\\mathcal{S}=\\mathcal{E}\\times\\mathcal{G}\\times\\mathcal{A}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>g</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\mathcal{E}\\times\\mathcal{G}\\times\\mathcal{A}_{g}</annotation></semantics></math>, where <math alttext=\"e\\in\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>e</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi></mrow><annotation encoding=\"application/x-tex\">e\\in\\mathcal{E}</annotation></semantics></math>, <math alttext=\"g\\in\\mathcal{G}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi></mrow><annotation encoding=\"application/x-tex\">g\\in\\mathcal{G}</annotation></semantics></math>, and <math alttext=\"a_{g}\\in\\mathcal{A}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>g</mi></msub><mo>&#8712;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>g</mi></msub></mrow><annotation encoding=\"application/x-tex\">a_{g}\\in\\mathcal{A}_{g}</annotation></semantics></math> denote emotion, gender, and age group, respectively.\nBased on this, we construct a style reference set <math alttext=\"\\mathcal{X}=\\{x_{ins}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{X}=\\{x_{ins}\\}</annotation></semantics></math> from the GigaSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">33-GigaSpeech2021</span>)</cite>, which provides labeled speech samples annotated with the required attributes (as summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx3.F1\" title=\"Figure 1 &#8227; Overview &#8227; Methodology &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Each style instance <math alttext=\"x_{ins}=(t_{ins},a_{ref})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo>,</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{ins}=(t_{ins},a_{ref})</annotation></semantics></math> consists of a natural language description <math alttext=\"t_{ins}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m7\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">t_{ins}</annotation></semantics></math> (e.g., &#8220;A young male speaker expressing anger&#8221;) and a corresponding reference audio clip <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m8\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> exemplifying the specified style configuration <math alttext=\"(e,g,a_{g})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo>,</mo><mi>g</mi><mo>,</mo><msub><mi>a</mi><mi>g</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(e,g,a_{g})</annotation></semantics></math>.\nFor each unique configuration in <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, we randomly sample 5 diverse reference instances to ensure sufficient coverage and variation during audio generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "query",
                    "origin"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion-Driven Prompt Transformation.</span>\nIn natural conversations, a speaker&#8217;s emotion affects how questions are phrased or understood, leading to linguistic variation. To emulate this, we employ an emotion conditioning approach that rewrites the harmful query <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p3.m1\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> into an emotionally stylized version <math alttext=\"q_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p3.m2\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>e</mi></msub><annotation encoding=\"application/x-tex\">q_{e}</annotation></semantics></math>. Specifically, GPT-4 is prompted with emotion-specific instructions to inject expressive cues (e.g., interjections, emotional modifiers) while preserving intent. This produces multiple stylized textual variants per query, with transformation templates provided in Appendix A.</p>\n\n",
                "matched_terms": [
                    "query",
                    "approach"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by prior work showing that different transformations exhibit varying effectiveness across inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">36-ji2024defending</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">37-DBLP:conf/icml/YangDHSR020</span>)</cite>, we conduct a preliminary study on how style configurations influence jailbreak success. Specifically, we apply diverse style configurations to each query <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p2.m1\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> and measure the resulting attack success rates. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx3.F2\" title=\"Figure 2 &#8227; Style-aware Transformations Pipelines &#8227; Methodology &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, each curve represents the success trend under varying styles for a specific query. The variation in peak positions reveals that jailbreak effectiveness is highly query-specific rather than uniform across queries.</p>\n\n",
                "matched_terms": [
                    "under",
                    "study",
                    "query"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Query-adaptive Policy Strategy.</span>\nBuilding on our observation, we learn a policy network that adaptively chooses style configurations based on the input query, avoiding exhaustive search while preserving effectiveness. Specifically, we introduce a multi-head policy network <math alttext=\"\\pi_{\\theta}:\\mathcal{Q}\\rightarrow\\Delta(\\mathcal{S})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#960;</mi><mi>&#952;</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi><mo stretchy=\"false\">&#8594;</mo><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\theta}:\\mathcal{Q}\\rightarrow\\Delta(\\mathcal{S})</annotation></semantics></math> that maps a harmful query <math alttext=\"q\\in\\mathcal{Q}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p3.m2\" intent=\":literal\"><semantics><mrow><mi>q</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119980;</mi></mrow><annotation encoding=\"application/x-tex\">q\\in\\mathcal{Q}</annotation></semantics></math> to a categorical distribution over the style configuration space <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p3.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, where each head independently predicts the distribution over a specific attribute dimension, and <math alttext=\"\\Delta(\\mathcal{S})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p3.m4\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta(\\mathcal{S})</annotation></semantics></math> denotes the probability simplex over possible configurations.\nThe distribution parameterized by learnable weights <math alttext=\"\\theta\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p3.m5\" intent=\":literal\"><semantics><mi>&#952;</mi><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math> adaptively selects effective style configurations by maximizing the following reward:</p>\n\n",
                "matched_terms": [
                    "our",
                    "query",
                    "denotes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"s=(e,g,a_{g})\\in\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p5.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo>,</mo><mi>g</mi><mo>,</mo><msub><mi>a</mi><mi>g</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi></mrow><annotation encoding=\"application/x-tex\">s=(e,g,a_{g})\\in\\mathcal{S}</annotation></semantics></math> denotes a style configuration. The adversarial audio <math alttext=\"a_{p}^{s}=C(q_{e}^{s},x_{ins}^{s})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p5.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>a</mi><mi>p</mi><mi>s</mi></msubsup><mo>=</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>q</mi><mi>e</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><mi>s</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">a_{p}^{s}=C(q_{e}^{s},x_{ins}^{s})</annotation></semantics></math> is generated based on the configuration <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p5.m3\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> using the predefined controllable TTS model <math alttext=\"C(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p5.m4\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">C(\\cdot)</annotation></semantics></math>, which incorporates the stylized version <math alttext=\"q_{e}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p5.m5\" intent=\":literal\"><semantics><msubsup><mi>q</mi><mi>e</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">q_{e}^{s}</annotation></semantics></math> from <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p5.m6\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> and the corresponding reference style instance <math alttext=\"x_{ins}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p5.m7\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">x_{ins}^{s}</annotation></semantics></math>. The judge function <math alttext=\"J(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p5.m8\" intent=\":literal\"><semantics><mrow><mi>J</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">J(\\cdot)</annotation></semantics></math> evaluates the response of the target LAM <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p5.m9\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math>. It is defined as a weighted aggregation of multiple evaluation metrics provided in the experiment settings section.\nA higher <math alttext=\"J(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p5.m10\" intent=\":literal\"><semantics><mrow><mi>J</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">J(\\cdot)</annotation></semantics></math> value indicates stronger model tendency to respond meaningfully rather than reject, reflecting the adversarial prompt&#8217;s effectiveness in triggering jailbreaks.\nEncouragingly, we investigate not only the StyleBreak&#8217;s efficiency gains but also its effectiveness when combined with other jailbreak strategies in the experiments section.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "denotes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span>\nTo comprehensively evaluate the model alignment robustness under attacks, we adopt four main metrics: Attack Response Rate (ARR), Attack Success Rate (ASR), Toxicity Score (TS), and Policy Violation (PV).\nARR measures the proportion of prompts that receive non-refusal responses, using predefined refusal patterns (e.g., &#8221;I&#8217;m sorry&#8221;)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>.\nASR employs a RoBERTa-based binary classifier&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">39-DBLP:journals/corr/abs-2402-13457</span>)</cite> to assess whether the model directly answers harmful inputs.\nLower ARR and ASR indicate stronger alignment robustness against jailbreak.\nTo further assess the quality of response, we employ a LLaMA3-Guard-based evaluation framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">40-dubey2024llama3herdmodels</span>)</cite>.\nBased on this framework, each response is assigned a vigilance score ranging from 0 to 9, which is subsequently binarized into a TS indicator, where scores above 4 are labeled as high-risk (TS = 1).\nSimilarly, PV denotes whether the response violates predefined safety policies, with PV = 1 indicating a violation.</p>\n\n",
                "matched_terms": [
                    "under",
                    "asr",
                    "denotes",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Models.</span>\nWe consider four open-source LAMs with general capabilities for our major evaluation: Qwen2-Audio-7B-Instruct (Qwen2-Audio)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">14-chu2024qwen2</span>)</cite>, MERaLiON-AudioLLM-Whisper-SEA-LION (MERaLiON)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">16-he2024meralion</span>)</cite>, Ultravox-v0.4.1-Llama-3.1-8B (Ultravox)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">19-ultravox2025</span>)</cite>, and Qwen2.5-Omni-7B (Qwen-Omni)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">18-xu2025qwen2</span>)</cite>. The first three models are selected based on their relatively low ARR reported in VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">41-chen2024voicebench</span>)</cite>, indicating stronger resistance to adversarial prompts. Qwen2.5-Omni serves as a representative state-of-the-art multimodal model with strong general performance. All tested models are safety-aligned to reject harmful instructions and evaluated locally on 2 &#215; A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "qwenomni",
                    "our",
                    "meralion",
                    "ultravox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span>\nTo assess StyleBreak under diverse attack paradigms, we evaluate it with four representative audio jailbreak methods: Vanilla&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2-ying2024unveiling</span>)</cite>, AutoDAN<sup class=\"ltx_sup\">&#8727;</sup>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>, GCG<sup class=\"ltx_sup\">&#8727;</sup>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">30-liu2024autodan</span>)</cite>, and SSJ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">1-yang2024audio</span>)</cite>. Vanilla directly converts the original text queries into speech, while AutoDAN<sup class=\"ltx_sup\">&#8727;</sup> and GCG<sup class=\"ltx_sup\">&#8727;</sup> are text semantic-level attacks that manipulate the textual prompts before audio synthesis. In contrast, SSJ introduces perturbations at the audio level.\nTo ensure fairness and effectiveness under the black-box setting, adversarial examples are first optimized using AutoDAN and GCG on LLaMA2, a well-aligned LLM, and then transferred to the target LAMs.</p>\n\n",
                "matched_terms": [
                    "stylebreak",
                    "under",
                    "vanilla",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets &amp; Settings.</span>\nA 200-query subset of AdvBench, as mentioned in the methodology section, is used to evaluate the impact of speech attributes and to train our adaptive policy, which is further assessed on StyleBreak and other baselines using 50 additional, non-overlapping queries.\nAll evaluations are conducted with default settings and no modifications.\nTo ensure consistency, we employ CosyVoice2-0.5B as the unified TTS model, and each test is repeated five times to mitigate randomness.\nFurther implementation details for policy and evaluation are in Appendix B.</p>\n\n",
                "matched_terms": [
                    "our",
                    "stylebreak",
                    "settings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Attributes.</span>\nWe explore how emotion control in linguistic attributes affects LAMs by altering the textual semantic content of adversarial audio prompts.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) reveals that emotional variations in linguistic attributes lead to significant increases across all jailbreak metrics for all target LAMs. Even the most robust model, Qwen-Omni, shows an average ASR increase from 0% to 9.1%.\nMoreover, specific emotional styles can strongly impact certain models. For instance, on MERaLiON, the surprised variant yields an ASR 8.57% higher than the second-highest, highlighting the nuanced influence of different emotional semantics on LAM alignment robustness.</p>\n\n",
                "matched_terms": [
                    "meralion",
                    "asr",
                    "qwenomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Attributes.</span>\nWe investigate LAM vulnerability to emotional manipulation in paralinguistic attributes by modulating acoustic emotional features in audio prompts with original textual semantic content.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b), emotional variations in paralinguistic attributes significantly increase jailbreak performance across models compared to the Vanilla setting.\nNotably, Ultravox is particularly sensitive to paralinguistic variations, with ASR increasing by 4.6-6.8&#215; over the original input and averaging 21.6% higher than its linguistic counterpart&#8212;likely due to its enhanced performance on emotion-related tasks.\nAlthough less effective than linguistic emotional rewriting which yields 3.9&#215; higher ARR and better conceals intent, paralinguistic emotional control still induces notable jailbreaks, with ASR rising by 9.1% on average. This underscores that even subtle acoustic features can compromise LAM safety alignment.</p>\n\n",
                "matched_terms": [
                    "vanilla",
                    "asr",
                    "ultravox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extralinguistic Attributes.</span>\nTo analyze the impact of extralinguistic attributes, we fix the original textual semantic content and generate adversarial audio prompts by individually varying the age and gender in the style configuration when querying the target LAMs.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c)(d), both age and gender variations show internally consistent trends across all four models and evaluation metrics, respectively.\nFor age, LAMs are most robust to child voices, showing the lowest ASR, while elderly voices yield the highest jailbreak success, with ASR averaging 13.3% higher than that of child voices.\nFor gender, male voices consistently result in higher ASR than female voices, with an average increase of 8.3% across the target LAMs.\nThese findings suggest that LAMs are generally more robust to higher-pitched voices such as those of children and females, but show increased vulnerability to lower-pitched voices such as those of males and the elderly.\nConsistently, among the target LAMs, Qwen2-Audio demonstrates the strongest alignment robustness to extralinguistic variations, while Ultravox remains the most susceptible.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "ultravox",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attack Effectiveness.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.T1\" title=\"Table 1 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that StyleBreak consistently boosts the attack performance across all baselines, with ASR gains ranging from 7.1% to 22.3%, demonstrating strong effectiveness and broad applicability.\nDespite the overall improvements, attack effectiveness varied across methods and models.\nFor signal-level attack, SSJ suffers from low ASR (avg. 4.5%) but high ARR (avg. 57.5%), as LAMs tend to repeat spelled-out prompts rather than provide direct answers.\nHowever, applying StyleBreak on SSJ effectively mitigates this behavior, boosting ASR by 4.7&#215;.\nFor text semantic-level attacks GCG<sup class=\"ltx_sup\">&#8727;</sup> and AutoDAN<sup class=\"ltx_sup\">&#8727;</sup>, although the attack performance is significantly improved after combining with StyleBreak, both the original and StyleBreak-enhanced versions exhibit comparable performance to those of Vanilla except on MERaLiON. We attribute this to limited model capacity to process long audio or semantic loss during text transformation.\nMoreover, models exhibit distinct behaviors. For Ultravox, StyleBreak tends to trigger affirmative replies (e.g., &#8220;Yes, I can help you to&#8230;&#8221;) rather than explicit harmful content, resulting in a notable ARR increase but only modestly affecting other metrics.\nInterestingly, under multi-attribute composite attacks, MERaLiON demonstrates the highest vulnerability, contrary to its robustness under single-attribute perturbations shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. This may stem from MERaLiON&#8217;s stronger generalization in multicultural contexts, which makes it more sensitive to complex style-aware audio prompts.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "under",
                    "vanilla",
                    "attacks",
                    "stylebreak",
                    "meralion",
                    "ultravox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Efficiency.</span>\nIn Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F4\" title=\"Figure 4 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we illustrate the ASR of all baselines with and without StyleBreak in different query iterations to investigate its effects on LAM alignment robustness.\nThe results reflect that integrating StyleBreak rapidly enhances attack success with minimal additional queries, confirming its effectiveness.\nNotably, Vanilla and SSJ initially fail to improve ASR through repeated queries alone but achieve 30.5% and 40.5% gains respectively after applying StyleBreak within just 10 iterations. In addition, an appropriate number of queries can achieve satisfactory attack coverage at an acceptable cost. Detailed results on additional metrics are available in Appendix C.1.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "query",
                    "iterations",
                    "stylebreak",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representations of Attacks.</span>\nAs Qwen-Omni does not provide embedding representations, we visualize the internal representations on the other three LAMs to further explore LAMs&#8217; robustness.\nTo analyze how these models encode different types of inputs, we use the final layer&#8217;s last hidden state to represent each input query, capturing the model&#8217;s latent response&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">42-gong2025figstep</span>)</cite>. Then, t-SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">43-JMLR:v9:vandermaaten08a</span>)</cite> is applied to reduce these high-dimensional embeddings to two dimensions for visualization.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F5\" title=\"Figure 5 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the representation visualization of benign and harmful queries across text and audio modalities, along with StyleBreak based on Vanilla.\nQuery transformation for benign queries is conducted following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">7-peng2025jalmbench</span>)</cite> with details found in Appendix D.\nThe results indicate that the representations of the same content across text and audio modalities show large discrepancies, with Qwen2-Audio exhibiting the smallest cross-modal representation gap, demonstrating better multimodal alignment.\nIn terms of modality,\nwhile LAMs exhibit some ability to distinguish benign from harmful inputs in the text modality, this capability is significantly weaker in the audio modality, where the two types of queries often overlap.\nMoreover, StyleBreak effectively triggers model biases, inducing substantial semantic perturbations relative to the other two audio query types. This finding highlights human speech attributes as a potent factor for revealing LAM vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "query",
                    "qwen2audio",
                    "attacks",
                    "stylebreak",
                    "qwenomni",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Experiments on Advanced Models.</span>\nWe conduct experiments on two advanced commercial LAMs, GPT-4o and Gemini-2.5-flash. The style configurations for our method are directly transferred from the policy trained on Qwen2-Audio, without any fine-tuning on the target models. As shown in Figure 6, StyleBreak consistently improves attack performance across all evaluated baselines. Notably, even on the most robust GPT-4o, ASR increases by 2.1%<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx4.p2.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>9.5% after applying StyleBreak, demonstrating the generalization ability and effectiveness of the learned policy. Furthermore, we observe a significant increase in TS after applying StyleBreak by 4.7% and 9.7% on average across the two models, further highlighting that human speech attributes perturbations can substantially compromise LAM safety alignment.</p>\n\n",
                "matched_terms": [
                    "our",
                    "stylebreak",
                    "asr",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we expose a critical and previously underestimated threat: LAMs are inherently more vulnerable to jailbreak when exposed to audio prompts with perturbed human speech attributes.\nTo investigate this threat, we propose StyleBreak, a novel style-aware audio jailbreak framework that integrates a two-stage transformation pipeline and a query-adaptive policy to generate adversarial audio with controllable linguistic, paralinguistic, and extralinguistic attributes.\nExtensive experiments demonstrate that LAMs are particularly susceptible to adversarial perturbations in key human speech attributes including emotion, age, and gender.\nMoreover, StyleBreak consistently achieves outstanding attack performance with minimal additional queries and outperforms existing baselines across multiple attack paradigms.\nOverall, our work reveals critical alignment vulnerabilities in LAMs exposed by style-aware audio jailbreaks, underscoring the pressing necessity of robust LAM safety alignment before their widespread deployment.</p>\n\n",
                "matched_terms": [
                    "our",
                    "stylebreak"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The following presents the emotion-driven prompt transformation template and audio jailbreak process in StyleBreak, where &lt;Prompt&gt; denotes the original query <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math>, and &lt;Audio_Query&gt; represents the adversarial audio input <math alttext=\"a_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">a_{p}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "stylebreak",
                    "query",
                    "denotes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GCG<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup>:</span> We adopt the official implementation of GCG targeting LLaMA-2-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>. For black-box models including Qwen2-Audio, Qwen-Omni, MERaLiON, and Ultravox, we follow the transferable optimization setup based on LLaMA-2-7B suffix tuning and convert the adversarial text into audio using CosyVoice2-0.5B. Notably, &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite> have demonstrated that GCG exhibits strong transferability across various black-box LLMs.</p>\n\n",
                "matched_terms": [
                    "meralion",
                    "qwenomni",
                    "ultravox",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#A2.F7\" title=\"Figure 7 &#8227; Appendix B Query-adaptive Policy &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the results of ARR, TS, and PV across four LAMs after varying numbers of query attempts, corroborating our previous analysis of StyleBreak efficiency. Notably, all StyleBreak-enhanced variants achieve ARR approaching 100% after just five query iterations across all models, underscoring the inability of LAMs to detect adversarial intent and highlighting their significant vulnerability to style-aware jailbreak attempts.</p>\n\n",
                "matched_terms": [
                    "our",
                    "stylebreak",
                    "query",
                    "iterations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A detailed ablation analysis is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#A5.T3\" title=\"Table 3 &#8227; Appendix E Query Iteration Exploration &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, which reports the ARR, PV, and TS results after three query iterations across different modules, further demonstrating the contribution of each component in our method.\nIn terms of ARR, we observe a steady improvement with the incremental addition of each module, confirming the effectiveness of our two-stage transformation pipeline and the query-adaptive policy.\nFor PV and TS, perturbations to paralinguistic and extralinguistic attributes at the audio level&#8212;such as in the configurations <math alttext=\"Vanilla+EPT\" class=\"ltx_Math\" display=\"inline\" id=\"A8.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi></mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">Vanilla+EPT</annotation></semantics></math> and <math alttext=\"+EAG\" class=\"ltx_Math\" display=\"inline\" id=\"A8.p1.m2\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>G</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EAG</annotation></semantics></math>&#8212;lead to a significant increase in high-toxicity outputs. This highlights the increased susceptibility of LAMs to adversarial audio inputs that exploit variations in expressive human speech characteristics.</p>\n\n",
                "matched_terms": [
                    "our",
                    "query",
                    "ablation",
                    "iterations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">StyleBreak for better LLM alignment.</span>\nThe ultimate goal of this work is to identify the blind spots in LAM alignment and enhance the alignment process accordingly. To this end, StyleBreak systematically analyzes how controllable speech attributes (age, emotion, gender) affect LAM alignment, providing an automated approach to scan target LLMs and collect datasets for future alignment improvement. The generated jailbreak prompts can also be used for further adversarial training to strengthen model robustness.</p>\n\n",
                "matched_terms": [
                    "stylebreak",
                    "approach"
                ]
            }
        ]
    },
    "A5.T3": {
        "source_file": "StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak",
        "caption": "Table 3: Ablation study with ARR, PV and TS.",
        "body": "Settings (%)\nQwen2-audio\nQwen-Omni\nMERaLiON\nUltravox\n\n\nARR\nPV\nTS\nARR\nPV\nTS\nARR\nPV\nTS\nARR\nPV\nTS\n\n\nText-only Attacks\n\n\nOrigin query\n2.50\n1.47\n0.06\n0.38\n0.19\n0.00\n3.00\n3.00\n2.50\n55.00\n3.85\n1.60\n\n\n+ EPT\n37.96\n5.25\n7.25\n31.84\n2.63\n5.26\n47.57\n4.47\n4.85\n79.01\n4.46\n8.22\n\n\nAudio-based Attacks\n\n\nVanilla\n58.00\n10.00\n24.00\n16.00\n0.00\n0.00\n34.00\n2.00\n8.00\n96.00\n6.00\n0.00\n\n\n+EPT\n76.32\n11.68\n25.15\n59.56\n1.66\n6.57\n87.80\n20.07\n23.31\n92.69\n5.21\n7.46\n\n\n+EPT, EAG\n92.05\n13.58\n44.70\n83.77\n2.14\n11.59\n97.68\n26.48\n50.00\n100.00\n3.28\n9.60\n\n\n+EPT, EAG, QP\n98.01\n20.20\n47.02\n86.75\n7.64\n20.86\n97.68\n28.17\n51.32\n100.00\n10.26\n20.86",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\">Settings (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">Qwen2-audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">Qwen-Omni</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">MERaLiON</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">Ultravox</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ARR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">PV</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">TS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ARR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">PV</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">TS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ARR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">PV</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">TS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ARR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">PV</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">TS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"13\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Text-only Attacks</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Origin query</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">55.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">+ EPT</td>\n<td class=\"ltx_td ltx_align_center\">37.96</td>\n<td class=\"ltx_td ltx_align_center\">5.25</td>\n<td class=\"ltx_td ltx_align_center\">7.25</td>\n<td class=\"ltx_td ltx_align_center\">31.84</td>\n<td class=\"ltx_td ltx_align_center\">2.63</td>\n<td class=\"ltx_td ltx_align_center\">5.26</td>\n<td class=\"ltx_td ltx_align_center\">47.57</td>\n<td class=\"ltx_td ltx_align_center\">4.47</td>\n<td class=\"ltx_td ltx_align_center\">4.85</td>\n<td class=\"ltx_td ltx_align_center\">79.01</td>\n<td class=\"ltx_td ltx_align_center\">4.46</td>\n<td class=\"ltx_td ltx_align_center\">8.22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"13\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Audio-based Attacks</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">16.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">96.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">+EPT</td>\n<td class=\"ltx_td ltx_align_center\">76.32</td>\n<td class=\"ltx_td ltx_align_center\">11.68</td>\n<td class=\"ltx_td ltx_align_center\">25.15</td>\n<td class=\"ltx_td ltx_align_center\">59.56</td>\n<td class=\"ltx_td ltx_align_center\">1.66</td>\n<td class=\"ltx_td ltx_align_center\">6.57</td>\n<td class=\"ltx_td ltx_align_center\">87.80</td>\n<td class=\"ltx_td ltx_align_center\">20.07</td>\n<td class=\"ltx_td ltx_align_center\">23.31</td>\n<td class=\"ltx_td ltx_align_center\">92.69</td>\n<td class=\"ltx_td ltx_align_center\">5.21</td>\n<td class=\"ltx_td ltx_align_center\">7.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">+EPT, EAG</td>\n<td class=\"ltx_td ltx_align_center\">92.05</td>\n<td class=\"ltx_td ltx_align_center\">13.58</td>\n<td class=\"ltx_td ltx_align_center\">44.70</td>\n<td class=\"ltx_td ltx_align_center\">83.77</td>\n<td class=\"ltx_td ltx_align_center\">2.14</td>\n<td class=\"ltx_td ltx_align_center\">11.59</td>\n<td class=\"ltx_td ltx_align_center\">97.68</td>\n<td class=\"ltx_td ltx_align_center\">26.48</td>\n<td class=\"ltx_td ltx_align_center\">50.00</td>\n<td class=\"ltx_td ltx_align_center\">100.00</td>\n<td class=\"ltx_td ltx_align_center\">3.28</td>\n<td class=\"ltx_td ltx_align_center\">9.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">+EPT, EAG, QP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">98.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">47.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">86.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">7.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">97.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">28.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">51.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">100.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">10.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20.86</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "audiobased",
            "origin",
            "textonly",
            "ablation",
            "settings",
            "query",
            "study",
            "vanilla",
            "qwenomni",
            "attacks",
            "eag",
            "arr",
            "qwen2audio",
            "ept",
            "meralion",
            "ultravox"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">A detailed ablation analysis is provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#A5.T3\" title=\"Table 3 &#8227; Appendix E Query Iteration Exploration &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, which reports the ARR, PV, and TS results after three query iterations across different modules, further demonstrating the contribution of each component in our method.\nIn terms of ARR, we observe a steady improvement with the incremental addition of each module, confirming the effectiveness of our two-stage transformation pipeline and the query-adaptive policy.\nFor PV and TS, perturbations to paralinguistic and extralinguistic attributes at the audio level&#8212;such as in the configurations <math alttext=\"Vanilla+EPT\" class=\"ltx_Math\" display=\"inline\" id=\"A8.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>V</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi></mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">Vanilla+EPT</annotation></semantics></math> and <math alttext=\"+EAG\" class=\"ltx_Math\" display=\"inline\" id=\"A8.p1.m2\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>G</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EAG</annotation></semantics></math>&#8212;lead to a significant increase in high-toxicity outputs. This highlights the increased susceptibility of LAMs to adversarial audio inputs that exploit variations in expressive human speech characteristics.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attack Settings.</span> In this work, we consider two complementary attack scenarios:\n1) Text-only attacks serve as a baseline for prompt-based jailbreaks testing whether original or style-aware text prompts can bypass LAMs&#8217; safety alignment.\n2) Audio-based attacks simulate diverse human speech attributes, exploring how variations in linguistic, paralinguistic, and extralinguistic attributes affect the model&#8217;s alignment robustness. Here, adversarial prompts are delivered via audio, optionally combined with text templates <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p2.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audiobased",
                    "textonly",
                    "settings",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to prior work that primarily focuses on text semantic-level prompts or signal-level perturbations, we introduce StyleBreak, a novel style-aware audio jailbreak framework designed to explore LAM alignment robustness under diverse human speech attributes.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx3.F1\" title=\"Figure 1 &#8227; Overview &#8227; Methodology &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, StyleBreak begins with a two-stage style-aware transformation pipeline, which includes emotion-driven prompt transformation and style-controlled audio attack generation to craft adversarial audio <math alttext=\"a_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m1\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">a_{p}</annotation></semantics></math> from origin query <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m2\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> by varying speech styles.\nAs not all style combinations are equally effective at inducing jailbreaks, a query-adaptive policy strategy <math alttext=\"\\pi_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta}</annotation></semantics></math> is introduced to automatically identify effective style configurations for each input query, enabling scalable and efficient jailbreak.\nFinally, the generated stylized adversarial audio is submitted to the target LAM to obtain responses and assess jailbreak performance.</p>\n\n",
                "matched_terms": [
                    "query",
                    "origin"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Collection.</span> Most existing studies rely on AdvBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>, which contains 520 harmful textual queries. Following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2-ying2024unveiling</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">8-shen2024voice</span>)</cite>, we select 200 representative queries from this benchmark as our origin harmful query set to balance coverage and practicality.\nTo guide the generation of audio with diverse human speech attributes, we define a discrete style configuration space <math alttext=\"\\mathcal{S}=\\mathcal{E}\\times\\mathcal{G}\\times\\mathcal{A}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>g</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\mathcal{E}\\times\\mathcal{G}\\times\\mathcal{A}_{g}</annotation></semantics></math>, where <math alttext=\"e\\in\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>e</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi></mrow><annotation encoding=\"application/x-tex\">e\\in\\mathcal{E}</annotation></semantics></math>, <math alttext=\"g\\in\\mathcal{G}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119970;</mi></mrow><annotation encoding=\"application/x-tex\">g\\in\\mathcal{G}</annotation></semantics></math>, and <math alttext=\"a_{g}\\in\\mathcal{A}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>g</mi></msub><mo>&#8712;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mi>g</mi></msub></mrow><annotation encoding=\"application/x-tex\">a_{g}\\in\\mathcal{A}_{g}</annotation></semantics></math> denote emotion, gender, and age group, respectively.\nBased on this, we construct a style reference set <math alttext=\"\\mathcal{X}=\\{x_{ins}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{X}=\\{x_{ins}\\}</annotation></semantics></math> from the GigaSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">33-GigaSpeech2021</span>)</cite>, which provides labeled speech samples annotated with the required attributes (as summarized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx3.F1\" title=\"Figure 1 &#8227; Overview &#8227; Methodology &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Each style instance <math alttext=\"x_{ins}=(t_{ins},a_{ref})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo>,</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{ins}=(t_{ins},a_{ref})</annotation></semantics></math> consists of a natural language description <math alttext=\"t_{ins}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m7\" intent=\":literal\"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">t_{ins}</annotation></semantics></math> (e.g., &#8220;A young male speaker expressing anger&#8221;) and a corresponding reference audio clip <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m8\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> exemplifying the specified style configuration <math alttext=\"(e,g,a_{g})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>e</mi><mo>,</mo><mi>g</mi><mo>,</mo><msub><mi>a</mi><mi>g</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(e,g,a_{g})</annotation></semantics></math>.\nFor each unique configuration in <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx3.p2.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, we randomly sample 5 diverse reference instances to ensure sufficient coverage and variation during audio generation.</p>\n\n",
                "matched_terms": [
                    "query",
                    "origin"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by prior work showing that different transformations exhibit varying effectiveness across inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">36-ji2024defending</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">37-DBLP:conf/icml/YangDHSR020</span>)</cite>, we conduct a preliminary study on how style configurations influence jailbreak success. Specifically, we apply diverse style configurations to each query <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p2.m1\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> and measure the resulting attack success rates. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx3.F2\" title=\"Figure 2 &#8227; Style-aware Transformations Pipelines &#8227; Methodology &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, each curve represents the success trend under varying styles for a specific query. The variation in peak positions reveals that jailbreak effectiveness is highly query-specific rather than uniform across queries.</p>\n\n",
                "matched_terms": [
                    "query",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span>\nTo comprehensively evaluate the model alignment robustness under attacks, we adopt four main metrics: Attack Response Rate (ARR), Attack Success Rate (ASR), Toxicity Score (TS), and Policy Violation (PV).\nARR measures the proportion of prompts that receive non-refusal responses, using predefined refusal patterns (e.g., &#8221;I&#8217;m sorry&#8221;)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>.\nASR employs a RoBERTa-based binary classifier&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">39-DBLP:journals/corr/abs-2402-13457</span>)</cite> to assess whether the model directly answers harmful inputs.\nLower ARR and ASR indicate stronger alignment robustness against jailbreak.\nTo further assess the quality of response, we employ a LLaMA3-Guard-based evaluation framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">40-dubey2024llama3herdmodels</span>)</cite>.\nBased on this framework, each response is assigned a vigilance score ranging from 0 to 9, which is subsequently binarized into a TS indicator, where scores above 4 are labeled as high-risk (TS = 1).\nSimilarly, PV denotes whether the response violates predefined safety policies, with PV = 1 indicating a violation.</p>\n\n",
                "matched_terms": [
                    "arr",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Models.</span>\nWe consider four open-source LAMs with general capabilities for our major evaluation: Qwen2-Audio-7B-Instruct (Qwen2-Audio)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">14-chu2024qwen2</span>)</cite>, MERaLiON-AudioLLM-Whisper-SEA-LION (MERaLiON)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">16-he2024meralion</span>)</cite>, Ultravox-v0.4.1-Llama-3.1-8B (Ultravox)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">19-ultravox2025</span>)</cite>, and Qwen2.5-Omni-7B (Qwen-Omni)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">18-xu2025qwen2</span>)</cite>. The first three models are selected based on their relatively low ARR reported in VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">41-chen2024voicebench</span>)</cite>, indicating stronger resistance to adversarial prompts. Qwen2.5-Omni serves as a representative state-of-the-art multimodal model with strong general performance. All tested models are safety-aligned to reject harmful instructions and evaluated locally on 2 &#215; A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "qwenomni",
                    "arr",
                    "qwen2audio",
                    "meralion",
                    "ultravox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span>\nTo assess StyleBreak under diverse attack paradigms, we evaluate it with four representative audio jailbreak methods: Vanilla&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2-ying2024unveiling</span>)</cite>, AutoDAN<sup class=\"ltx_sup\">&#8727;</sup>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>, GCG<sup class=\"ltx_sup\">&#8727;</sup>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">30-liu2024autodan</span>)</cite>, and SSJ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">1-yang2024audio</span>)</cite>. Vanilla directly converts the original text queries into speech, while AutoDAN<sup class=\"ltx_sup\">&#8727;</sup> and GCG<sup class=\"ltx_sup\">&#8727;</sup> are text semantic-level attacks that manipulate the textual prompts before audio synthesis. In contrast, SSJ introduces perturbations at the audio level.\nTo ensure fairness and effectiveness under the black-box setting, adversarial examples are first optimized using AutoDAN and GCG on LLaMA2, a well-aligned LLM, and then transferred to the target LAMs.</p>\n\n",
                "matched_terms": [
                    "vanilla",
                    "attacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Attributes.</span>\nWe explore how emotion control in linguistic attributes affects LAMs by altering the textual semantic content of adversarial audio prompts.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) reveals that emotional variations in linguistic attributes lead to significant increases across all jailbreak metrics for all target LAMs. Even the most robust model, Qwen-Omni, shows an average ASR increase from 0% to 9.1%.\nMoreover, specific emotional styles can strongly impact certain models. For instance, on MERaLiON, the surprised variant yields an ASR 8.57% higher than the second-highest, highlighting the nuanced influence of different emotional semantics on LAM alignment robustness.</p>\n\n",
                "matched_terms": [
                    "meralion",
                    "qwenomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Attributes.</span>\nWe investigate LAM vulnerability to emotional manipulation in paralinguistic attributes by modulating acoustic emotional features in audio prompts with original textual semantic content.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b), emotional variations in paralinguistic attributes significantly increase jailbreak performance across models compared to the Vanilla setting.\nNotably, Ultravox is particularly sensitive to paralinguistic variations, with ASR increasing by 4.6-6.8&#215; over the original input and averaging 21.6% higher than its linguistic counterpart&#8212;likely due to its enhanced performance on emotion-related tasks.\nAlthough less effective than linguistic emotional rewriting which yields 3.9&#215; higher ARR and better conceals intent, paralinguistic emotional control still induces notable jailbreaks, with ASR rising by 9.1% on average. This underscores that even subtle acoustic features can compromise LAM safety alignment.</p>\n\n",
                "matched_terms": [
                    "ultravox",
                    "vanilla",
                    "arr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extralinguistic Attributes.</span>\nTo analyze the impact of extralinguistic attributes, we fix the original textual semantic content and generate adversarial audio prompts by individually varying the age and gender in the style configuration when querying the target LAMs.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c)(d), both age and gender variations show internally consistent trends across all four models and evaluation metrics, respectively.\nFor age, LAMs are most robust to child voices, showing the lowest ASR, while elderly voices yield the highest jailbreak success, with ASR averaging 13.3% higher than that of child voices.\nFor gender, male voices consistently result in higher ASR than female voices, with an average increase of 8.3% across the target LAMs.\nThese findings suggest that LAMs are generally more robust to higher-pitched voices such as those of children and females, but show increased vulnerability to lower-pitched voices such as those of males and the elderly.\nConsistently, among the target LAMs, Qwen2-Audio demonstrates the strongest alignment robustness to extralinguistic variations, while Ultravox remains the most susceptible.</p>\n\n",
                "matched_terms": [
                    "ultravox",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attack Effectiveness.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.T1\" title=\"Table 1 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that StyleBreak consistently boosts the attack performance across all baselines, with ASR gains ranging from 7.1% to 22.3%, demonstrating strong effectiveness and broad applicability.\nDespite the overall improvements, attack effectiveness varied across methods and models.\nFor signal-level attack, SSJ suffers from low ASR (avg. 4.5%) but high ARR (avg. 57.5%), as LAMs tend to repeat spelled-out prompts rather than provide direct answers.\nHowever, applying StyleBreak on SSJ effectively mitigates this behavior, boosting ASR by 4.7&#215;.\nFor text semantic-level attacks GCG<sup class=\"ltx_sup\">&#8727;</sup> and AutoDAN<sup class=\"ltx_sup\">&#8727;</sup>, although the attack performance is significantly improved after combining with StyleBreak, both the original and StyleBreak-enhanced versions exhibit comparable performance to those of Vanilla except on MERaLiON. We attribute this to limited model capacity to process long audio or semantic loss during text transformation.\nMoreover, models exhibit distinct behaviors. For Ultravox, StyleBreak tends to trigger affirmative replies (e.g., &#8220;Yes, I can help you to&#8230;&#8221;) rather than explicit harmful content, resulting in a notable ARR increase but only modestly affecting other metrics.\nInterestingly, under multi-attribute composite attacks, MERaLiON demonstrates the highest vulnerability, contrary to its robustness under single-attribute perturbations shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. This may stem from MERaLiON&#8217;s stronger generalization in multicultural contexts, which makes it more sensitive to complex style-aware audio prompts.</p>\n\n",
                "matched_terms": [
                    "vanilla",
                    "attacks",
                    "arr",
                    "meralion",
                    "ultravox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Efficiency.</span>\nIn Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F4\" title=\"Figure 4 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we illustrate the ASR of all baselines with and without StyleBreak in different query iterations to investigate its effects on LAM alignment robustness.\nThe results reflect that integrating StyleBreak rapidly enhances attack success with minimal additional queries, confirming its effectiveness.\nNotably, Vanilla and SSJ initially fail to improve ASR through repeated queries alone but achieve 30.5% and 40.5% gains respectively after applying StyleBreak within just 10 iterations. In addition, an appropriate number of queries can achieve satisfactory attack coverage at an acceptable cost. Detailed results on additional metrics are available in Appendix C.1.</p>\n\n",
                "matched_terms": [
                    "query",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ablation Studies.</span>\nWe evaluate the impact of three key modules in StyleBreak: emotion-driven prompt transformation (EPT), style-controlled audio attack generation (EAG), and the query-adaptive policy (QP). To this end, we design the following variants: <math alttext=\"+EPT\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx3.p3.m1\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EPT</annotation></semantics></math> applies emotional rewriting to alter the semantic content of original queries; <math alttext=\"+EAG\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx3.p3.m2\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>G</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+EAG</annotation></semantics></math> introduces paralinguistic and extralinguistic perturbations to synthesize adversarial audio; <math alttext=\"+QP\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx3.p3.m3\" intent=\":literal\"><semantics><mrow><mo>+</mo><mrow><mi>Q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow></mrow><annotation encoding=\"application/x-tex\">+QP</annotation></semantics></math> replaces random style selection with a learned policy for adaptive configuration.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.T2\" title=\"Table 2 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the full StyleBreak consistently outperforms all variants, confirming the complementary benefits of each module. Moreover, audio-based attacks achieve markedly higher ASR than their text-only counterparts, underscoring the heightened vulnerability of LAMs to audio modality.\nAdditional analyses on policy selection distributions are presented in Appendices C.2 and C.3.</p>\n\n",
                "matched_terms": [
                    "audiobased",
                    "ablation",
                    "attacks",
                    "eag",
                    "ept",
                    "textonly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representations of Attacks.</span>\nAs Qwen-Omni does not provide embedding representations, we visualize the internal representations on the other three LAMs to further explore LAMs&#8217; robustness.\nTo analyze how these models encode different types of inputs, we use the final layer&#8217;s last hidden state to represent each input query, capturing the model&#8217;s latent response&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">42-gong2025figstep</span>)</cite>. Then, t-SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">43-JMLR:v9:vandermaaten08a</span>)</cite> is applied to reduce these high-dimensional embeddings to two dimensions for visualization.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F5\" title=\"Figure 5 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the representation visualization of benign and harmful queries across text and audio modalities, along with StyleBreak based on Vanilla.\nQuery transformation for benign queries is conducted following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">7-peng2025jalmbench</span>)</cite> with details found in Appendix D.\nThe results indicate that the representations of the same content across text and audio modalities show large discrepancies, with Qwen2-Audio exhibiting the smallest cross-modal representation gap, demonstrating better multimodal alignment.\nIn terms of modality,\nwhile LAMs exhibit some ability to distinguish benign from harmful inputs in the text modality, this capability is significantly weaker in the audio modality, where the two types of queries often overlap.\nMoreover, StyleBreak effectively triggers model biases, inducing substantial semantic perturbations relative to the other two audio query types. This finding highlights human speech attributes as a potent factor for revealing LAM vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "query",
                    "attacks",
                    "qwenomni",
                    "qwen2audio",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GCG<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup>:</span> We adopt the official implementation of GCG targeting LLaMA-2-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>. For black-box models including Qwen2-Audio, Qwen-Omni, MERaLiON, and Ultravox, we follow the transferable optimization setup based on LLaMA-2-7B suffix tuning and convert the adversarial text into audio using CosyVoice2-0.5B. Notably, &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite> have demonstrated that GCG exhibits strong transferability across various black-box LLMs.</p>\n\n",
                "matched_terms": [
                    "meralion",
                    "qwen2audio",
                    "ultravox",
                    "qwenomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#A2.F7\" title=\"Figure 7 &#8227; Appendix B Query-adaptive Policy &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the results of ARR, TS, and PV across four LAMs after varying numbers of query attempts, corroborating our previous analysis of StyleBreak efficiency. Notably, all StyleBreak-enhanced variants achieve ARR approaching 100% after just five query iterations across all models, underscoring the inability of LAMs to detect adversarial intent and highlighting their significant vulnerability to style-aware jailbreak attempts.</p>\n\n",
                "matched_terms": [
                    "query",
                    "arr"
                ]
            }
        ]
    },
    "A6.T4": {
        "source_file": "StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak",
        "caption": "Table 4: ASR Comparison under vanilla and StyleBreak-enhanced settings via Voxinstruct.",
        "body": "(%)\nQwen2_Audio\nQwen_Omni\nMERaLiON\nUltravox\n\n\nVanilla\n0.0\n0.0\n4.2\n0.0\n\n\nVanilla+Ours\n20.8\n16.7\n23.0\n10.4",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">(%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Qwen2_Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Qwen_Omni</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">MERaLiON</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Ultravox</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Vanilla+Ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">16.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">23.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">10.4</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "vanillaours",
            "stylebreakenhanced",
            "settings",
            "under",
            "asr",
            "comparison",
            "ultravox",
            "qwenomni",
            "qwen2audio",
            "via",
            "voxinstruct",
            "meralion",
            "vanilla"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For generalization, we replace CosyVoice2 with VoxInstruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">44-zhou2024voxinstruct</span>)</cite> and observe consistent ASR gains across 4 LAMs, confirming cross-TTS robustness as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#A6.T4\" title=\"Table 4 &#8227; Appendix F The Learned Query-adaptive Policy &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LAMs</span> typically extend LLMs by incorporating an audio encoder that maps raw speech to semantic representations, enabling LLMs to process audio inputs seamlessly&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">14-chu2024qwen2</span>)</cite>. Recent advances&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">18-xu2025qwen2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">19-ultravox2025</span>)</cite> have developed LAMs as general-purpose frameworks capable of handling a wide range of downstream tasks through appropriately designed audio prompts. However, as they are predominantly provided via APIs or online services, users often have no access to the model&#8217;s internal parameters&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">20-murad2024unveiling</span>)</cite>. Therefore, we focus on LAMs under black-box access settings.</p>\n\n",
                "matched_terms": [
                    "via",
                    "settings",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attack Settings.</span> In this work, we consider two complementary attack scenarios:\n1) Text-only attacks serve as a baseline for prompt-based jailbreaks testing whether original or style-aware text prompts can bypass LAMs&#8217; safety alignment.\n2) Audio-based attacks simulate diverse human speech attributes, exploring how variations in linguistic, paralinguistic, and extralinguistic attributes affect the model&#8217;s alignment robustness. Here, adversarial prompts are delivered via audio, optionally combined with text templates <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p2.m1\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "via",
                    "settings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics.</span>\nTo comprehensively evaluate the model alignment robustness under attacks, we adopt four main metrics: Attack Response Rate (ARR), Attack Success Rate (ASR), Toxicity Score (TS), and Policy Violation (PV).\nARR measures the proportion of prompts that receive non-refusal responses, using predefined refusal patterns (e.g., &#8221;I&#8217;m sorry&#8221;)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>.\nASR employs a RoBERTa-based binary classifier&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">39-DBLP:journals/corr/abs-2402-13457</span>)</cite> to assess whether the model directly answers harmful inputs.\nLower ARR and ASR indicate stronger alignment robustness against jailbreak.\nTo further assess the quality of response, we employ a LLaMA3-Guard-based evaluation framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">40-dubey2024llama3herdmodels</span>)</cite>.\nBased on this framework, each response is assigned a vigilance score ranging from 0 to 9, which is subsequently binarized into a TS indicator, where scores above 4 are labeled as high-risk (TS = 1).\nSimilarly, PV denotes whether the response violates predefined safety policies, with PV = 1 indicating a violation.</p>\n\n",
                "matched_terms": [
                    "under",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Models.</span>\nWe consider four open-source LAMs with general capabilities for our major evaluation: Qwen2-Audio-7B-Instruct (Qwen2-Audio)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">14-chu2024qwen2</span>)</cite>, MERaLiON-AudioLLM-Whisper-SEA-LION (MERaLiON)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">16-he2024meralion</span>)</cite>, Ultravox-v0.4.1-Llama-3.1-8B (Ultravox)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">19-ultravox2025</span>)</cite>, and Qwen2.5-Omni-7B (Qwen-Omni)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">18-xu2025qwen2</span>)</cite>. The first three models are selected based on their relatively low ARR reported in VoiceBench&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">41-chen2024voicebench</span>)</cite>, indicating stronger resistance to adversarial prompts. Qwen2.5-Omni serves as a representative state-of-the-art multimodal model with strong general performance. All tested models are safety-aligned to reject harmful instructions and evaluated locally on 2 &#215; A100 GPUs.</p>\n\n",
                "matched_terms": [
                    "meralion",
                    "qwen2audio",
                    "ultravox",
                    "qwenomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span>\nTo assess StyleBreak under diverse attack paradigms, we evaluate it with four representative audio jailbreak methods: Vanilla&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2-ying2024unveiling</span>)</cite>, AutoDAN<sup class=\"ltx_sup\">&#8727;</sup>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>, GCG<sup class=\"ltx_sup\">&#8727;</sup>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">30-liu2024autodan</span>)</cite>, and SSJ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">1-yang2024audio</span>)</cite>. Vanilla directly converts the original text queries into speech, while AutoDAN<sup class=\"ltx_sup\">&#8727;</sup> and GCG<sup class=\"ltx_sup\">&#8727;</sup> are text semantic-level attacks that manipulate the textual prompts before audio synthesis. In contrast, SSJ introduces perturbations at the audio level.\nTo ensure fairness and effectiveness under the black-box setting, adversarial examples are first optimized using AutoDAN and GCG on LLaMA2, a well-aligned LLM, and then transferred to the target LAMs.</p>\n\n",
                "matched_terms": [
                    "under",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Linguistic Attributes.</span>\nWe explore how emotion control in linguistic attributes affects LAMs by altering the textual semantic content of adversarial audio prompts.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) reveals that emotional variations in linguistic attributes lead to significant increases across all jailbreak metrics for all target LAMs. Even the most robust model, Qwen-Omni, shows an average ASR increase from 0% to 9.1%.\nMoreover, specific emotional styles can strongly impact certain models. For instance, on MERaLiON, the surprised variant yields an ASR 8.57% higher than the second-highest, highlighting the nuanced influence of different emotional semantics on LAM alignment robustness.</p>\n\n",
                "matched_terms": [
                    "meralion",
                    "asr",
                    "qwenomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Paralinguistic Attributes.</span>\nWe investigate LAM vulnerability to emotional manipulation in paralinguistic attributes by modulating acoustic emotional features in audio prompts with original textual semantic content.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b), emotional variations in paralinguistic attributes significantly increase jailbreak performance across models compared to the Vanilla setting.\nNotably, Ultravox is particularly sensitive to paralinguistic variations, with ASR increasing by 4.6-6.8&#215; over the original input and averaging 21.6% higher than its linguistic counterpart&#8212;likely due to its enhanced performance on emotion-related tasks.\nAlthough less effective than linguistic emotional rewriting which yields 3.9&#215; higher ARR and better conceals intent, paralinguistic emotional control still induces notable jailbreaks, with ASR rising by 9.1% on average. This underscores that even subtle acoustic features can compromise LAM safety alignment.</p>\n\n",
                "matched_terms": [
                    "vanilla",
                    "asr",
                    "ultravox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extralinguistic Attributes.</span>\nTo analyze the impact of extralinguistic attributes, we fix the original textual semantic content and generate adversarial audio prompts by individually varying the age and gender in the style configuration when querying the target LAMs.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c)(d), both age and gender variations show internally consistent trends across all four models and evaluation metrics, respectively.\nFor age, LAMs are most robust to child voices, showing the lowest ASR, while elderly voices yield the highest jailbreak success, with ASR averaging 13.3% higher than that of child voices.\nFor gender, male voices consistently result in higher ASR than female voices, with an average increase of 8.3% across the target LAMs.\nThese findings suggest that LAMs are generally more robust to higher-pitched voices such as those of children and females, but show increased vulnerability to lower-pitched voices such as those of males and the elderly.\nConsistently, among the target LAMs, Qwen2-Audio demonstrates the strongest alignment robustness to extralinguistic variations, while Ultravox remains the most susceptible.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "ultravox",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attack Effectiveness.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.T1\" title=\"Table 1 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that StyleBreak consistently boosts the attack performance across all baselines, with ASR gains ranging from 7.1% to 22.3%, demonstrating strong effectiveness and broad applicability.\nDespite the overall improvements, attack effectiveness varied across methods and models.\nFor signal-level attack, SSJ suffers from low ASR (avg. 4.5%) but high ARR (avg. 57.5%), as LAMs tend to repeat spelled-out prompts rather than provide direct answers.\nHowever, applying StyleBreak on SSJ effectively mitigates this behavior, boosting ASR by 4.7&#215;.\nFor text semantic-level attacks GCG<sup class=\"ltx_sup\">&#8727;</sup> and AutoDAN<sup class=\"ltx_sup\">&#8727;</sup>, although the attack performance is significantly improved after combining with StyleBreak, both the original and StyleBreak-enhanced versions exhibit comparable performance to those of Vanilla except on MERaLiON. We attribute this to limited model capacity to process long audio or semantic loss during text transformation.\nMoreover, models exhibit distinct behaviors. For Ultravox, StyleBreak tends to trigger affirmative replies (e.g., &#8220;Yes, I can help you to&#8230;&#8221;) rather than explicit harmful content, resulting in a notable ARR increase but only modestly affecting other metrics.\nInterestingly, under multi-attribute composite attacks, MERaLiON demonstrates the highest vulnerability, contrary to its robustness under single-attribute perturbations shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F3\" title=\"Figure 3 &#8227; Experiments Settings &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. This may stem from MERaLiON&#8217;s stronger generalization in multicultural contexts, which makes it more sensitive to complex style-aware audio prompts.</p>\n\n",
                "matched_terms": [
                    "stylebreakenhanced",
                    "under",
                    "asr",
                    "vanilla",
                    "meralion",
                    "ultravox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Efficiency.</span>\nIn Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F4\" title=\"Figure 4 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we illustrate the ASR of all baselines with and without StyleBreak in different query iterations to investigate its effects on LAM alignment robustness.\nThe results reflect that integrating StyleBreak rapidly enhances attack success with minimal additional queries, confirming its effectiveness.\nNotably, Vanilla and SSJ initially fail to improve ASR through repeated queries alone but achieve 30.5% and 40.5% gains respectively after applying StyleBreak within just 10 iterations. In addition, an appropriate number of queries can achieve satisfactory attack coverage at an acceptable cost. Detailed results on additional metrics are available in Appendix C.1.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "vanilla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Representations of Attacks.</span>\nAs Qwen-Omni does not provide embedding representations, we visualize the internal representations on the other three LAMs to further explore LAMs&#8217; robustness.\nTo analyze how these models encode different types of inputs, we use the final layer&#8217;s last hidden state to represent each input query, capturing the model&#8217;s latent response&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">42-gong2025figstep</span>)</cite>. Then, t-SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">43-JMLR:v9:vandermaaten08a</span>)</cite> is applied to reduce these high-dimensional embeddings to two dimensions for visualization.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10692v1#Sx4.F5\" title=\"Figure 5 &#8227; StyleBreak Performance &#8227; Experiments &#8227; StyleBreak: Revealing Alignment Vulnerabilities in Large Audio-Language Models via Style-Aware Audio Jailbreak\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the representation visualization of benign and harmful queries across text and audio modalities, along with StyleBreak based on Vanilla.\nQuery transformation for benign queries is conducted following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">7-peng2025jalmbench</span>)</cite> with details found in Appendix D.\nThe results indicate that the representations of the same content across text and audio modalities show large discrepancies, with Qwen2-Audio exhibiting the smallest cross-modal representation gap, demonstrating better multimodal alignment.\nIn terms of modality,\nwhile LAMs exhibit some ability to distinguish benign from harmful inputs in the text modality, this capability is significantly weaker in the audio modality, where the two types of queries often overlap.\nMoreover, StyleBreak effectively triggers model biases, inducing substantial semantic perturbations relative to the other two audio query types. This finding highlights human speech attributes as a potent factor for revealing LAM vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "vanilla",
                    "qwenomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Experiments on Advanced Models.</span>\nWe conduct experiments on two advanced commercial LAMs, GPT-4o and Gemini-2.5-flash. The style configurations for our method are directly transferred from the policy trained on Qwen2-Audio, without any fine-tuning on the target models. As shown in Figure 6, StyleBreak consistently improves attack performance across all evaluated baselines. Notably, even on the most robust GPT-4o, ASR increases by 2.1%<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx4.p2.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>9.5% after applying StyleBreak, demonstrating the generalization ability and effectiveness of the learned policy. Furthermore, we observe a significant increase in TS after applying StyleBreak by 4.7% and 9.7% on average across the two models, further highlighting that human speech attributes perturbations can substantially compromise LAM safety alignment.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GCG<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup>:</span> We adopt the official implementation of GCG targeting LLaMA-2-7B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite>. For black-box models including Qwen2-Audio, Qwen-Omni, MERaLiON, and Ultravox, we follow the transferable optimization setup based on LLaMA-2-7B suffix tuning and convert the adversarial text into audio using CosyVoice2-0.5B. Notably, &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">29-zou2023universal</span>)</cite> have demonstrated that GCG exhibits strong transferability across various black-box LLMs.</p>\n\n",
                "matched_terms": [
                    "meralion",
                    "qwen2audio",
                    "ultravox",
                    "qwenomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AutoDAN<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup>:</span> We follow the official implementation outlined in research&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">30-liu2024autodan</span>)</cite> for LLaMA-2-7B. In black-box settings, we adopt a transferable configuration by reusing optimized prompts from LLaMA-2 and converting them into adversarial audio via CosyVoice2-0.5B.</p>\n\n",
                "matched_terms": [
                    "via",
                    "settings"
                ]
            }
        ]
    }
}