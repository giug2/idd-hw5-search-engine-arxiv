{
    "S3.T1": {
        "source_file": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap",
        "caption": "Table 1: Average cosine similarity scores for various embedding pairs on AudioCaps. For the CLAP, no transformation is applied, so e^ℳ=eℳ\\hat{e}^{\\mathcal{M}}\\!=\\!e^{\\mathcal{M}}. We report et⋅e^t←ℳe^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow\\mathcal{M}}, where e^t←ℳ\\hat{e}^{t\\leftarrow\\mathcal{M}} denotes a text-like embedding generated from modality ℳ\\mathcal{M}. Transformations are obtained via C3 [10], DB (Diffusion-Bridge) [20], DG (DiffGap) [22], and our DL (Diffusion-Link). Here, ∼\\sim and ≁\\not\\sim indicate matched and non-matched pairs, respectively.",
        "body": "Comparison Pair\nCosine Similarity\n\n\n\nCLAP\nC3\nDB\nDG\nDL\n\n\n\n\n\n\n(a) et⋅e^t←a​(∼)e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow a}(\\sim)\n\n\n0.4860.486\n0.5470.547\n0.5280.528\n0.1100.110\n0.688\n\n\n\n\n(b) et⋅e^t←t​(∼)e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow t}(\\sim)\n\n\n1.000\n1.0001.000\n0.9990.999\n0.3340.334\n0.9450.945\n\n\n\n\n(c) et⋅e^t←a​(≁)e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow a}(\\not\\sim)\n\n\n0.0300.030\n0.0920.092\n0.000\n0.0070.007\n0.000\n\n\n\n\n(d) et⋅e^t←t​(≁)e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow t}(\\not\\sim)\n\n\n0.0980.098\n0.1580.158\n0.0020.002\n0.0430.043\n0.001",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Comparison Pair</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Cosine Similarity</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column\" style=\"padding:-0.2pt 5.0pt;\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">CLAP</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">C3</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">DB</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">DG</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">DL</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding:-0.2pt 5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">(a) </span><math alttext=\"e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow a}(\\sim)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m13\" intent=\":literal\"><semantics><mrow><mrow><msup><mi mathsize=\"0.800em\">e</mi><mi mathsize=\"0.800em\">t</mi></msup><mo lspace=\"0.052em\" mathsize=\"0.800em\" rspace=\"0.052em\">&#8901;</mo><msup><mover accent=\"true\"><mi mathsize=\"0.800em\">e</mi><mo mathsize=\"0.800em\">^</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\" stretchy=\"false\">&#8592;</mo><mi mathsize=\"0.800em\">a</mi></mrow></msup></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><mo lspace=\"0em\" mathsize=\"0.800em\" rspace=\"0em\">&#8764;</mo><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow a}(\\sim)</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.486\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m14\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.486</mn><annotation encoding=\"application/x-tex\">0.486</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.547\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m15\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.547</mn><annotation encoding=\"application/x-tex\">0.547</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.528\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m16\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.528</mn><annotation encoding=\"application/x-tex\">0.528</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.110\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m17\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.110</mn><annotation encoding=\"application/x-tex\">0.110</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.688</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:-0.2pt 5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">(b) </span><math alttext=\"e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow t}(\\sim)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m18\" intent=\":literal\"><semantics><mrow><mrow><msup><mi mathsize=\"0.800em\">e</mi><mi mathsize=\"0.800em\">t</mi></msup><mo lspace=\"0.052em\" mathsize=\"0.800em\" rspace=\"0.052em\">&#8901;</mo><msup><mover accent=\"true\"><mi mathsize=\"0.800em\">e</mi><mo mathsize=\"0.800em\">^</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\" stretchy=\"false\">&#8592;</mo><mi mathsize=\"0.800em\">t</mi></mrow></msup></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><mo lspace=\"0em\" mathsize=\"0.800em\" rspace=\"0em\">&#8764;</mo><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow t}(\\sim)</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">1.000</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"1.000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m19\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">1.000</mn><annotation encoding=\"application/x-tex\">1.000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m20\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.999</mn><annotation encoding=\"application/x-tex\">0.999</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.334\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m21\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.334</mn><annotation encoding=\"application/x-tex\">0.334</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.945\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m22\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.945</mn><annotation encoding=\"application/x-tex\">0.945</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\" style=\"padding:-0.2pt 5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">(c) </span><math alttext=\"e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow a}(\\not\\sim)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m23\" intent=\":literal\"><semantics><mrow><mrow><msup><mi mathsize=\"0.800em\">e</mi><mi mathsize=\"0.800em\">t</mi></msup><mo lspace=\"0.052em\" mathsize=\"0.800em\" rspace=\"0.052em\">&#8901;</mo><msup><mover accent=\"true\"><mi mathsize=\"0.800em\">e</mi><mo mathsize=\"0.800em\">^</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\" stretchy=\"false\">&#8592;</mo><mi mathsize=\"0.800em\">a</mi></mrow></msup></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><mo lspace=\"0em\" mathsize=\"0.800em\" rspace=\"0em\">&#8764;&#824;</mo><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow a}(\\not\\sim)</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.030\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m24\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.030</mn><annotation encoding=\"application/x-tex\">0.030</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.092\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m25\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.092</mn><annotation encoding=\"application/x-tex\">0.092</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.000</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.007\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m26\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.007</mn><annotation encoding=\"application/x-tex\">0.007</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.000</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding:-0.2pt 5.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">(d) </span><math alttext=\"e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow t}(\\not\\sim)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m27\" intent=\":literal\"><semantics><mrow><mrow><msup><mi mathsize=\"0.800em\">e</mi><mi mathsize=\"0.800em\">t</mi></msup><mo lspace=\"0.052em\" mathsize=\"0.800em\" rspace=\"0.052em\">&#8901;</mo><msup><mover accent=\"true\"><mi mathsize=\"0.800em\">e</mi><mo mathsize=\"0.800em\">^</mo></mover><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\" stretchy=\"false\">&#8592;</mo><mi mathsize=\"0.800em\">t</mi></mrow></msup></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo maxsize=\"0.800em\" minsize=\"0.800em\">(</mo><mo lspace=\"0em\" mathsize=\"0.800em\" rspace=\"0em\">&#8764;&#824;</mo><mo maxsize=\"0.800em\" minsize=\"0.800em\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">e^{t}\\!\\cdot\\!\\hat{e}^{t\\leftarrow t}(\\not\\sim)</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.098\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m28\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.098</mn><annotation encoding=\"application/x-tex\">0.098</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.158\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m29\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.158</mn><annotation encoding=\"application/x-tex\">0.158</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.002\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m30\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.002</mn><annotation encoding=\"application/x-tex\">0.002</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:-0.2pt 5.0pt;\"><math alttext=\"0.043\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m31\" intent=\":literal\"><semantics><mn mathsize=\"0.800em\">0.043</mn><annotation encoding=\"application/x-tex\">0.043</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding:-0.2pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.001</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "matched",
            "transformation",
            "et⋅et←a​≁etcdothatetleftarrow",
            "via",
            "et⋅et←a​∼etcdothatetleftarrow",
            "modality",
            "audiocaps",
            "embedding",
            "our",
            "tsim",
            "applied",
            "diffusionlink",
            "diffusionbridge",
            "where",
            "et←ℳhatetleftarrowmathcalm",
            "here",
            "from",
            "nonmatched",
            "various",
            "diffgap",
            "clap",
            "indicate",
            "cosine",
            "textlike",
            "et⋅et←ℳetcdothatetleftarrowmathcalm",
            "obtained",
            "report",
            "tnotsim",
            "≁notsim",
            "pair",
            "anotsim",
            "ℳmathcalm",
            "denotes",
            "similarity",
            "pairs",
            "average",
            "eℳeℳhatemathcalmemathcalm",
            "∼sim",
            "et⋅et←t​∼etcdothatetleftarrow",
            "transformations",
            "scores",
            "et⋅et←t​≁etcdothatetleftarrow",
            "asim",
            "generated",
            "respectively",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Effectiveness of Diffusion-Link for Modality Bridging.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Diffusion-Link attains the highest cosine similarity on matched audio&#8211;text pairs.\nWhile most approaches improve over CLAP, DiffGap underperforms, because it generates from pure Gaussian noise with the input embedding condition, which weakens information reconstruction.\nBy contrast, Diffusion-Link treat the input embedding as residing at an intermediate reverse step, thereby minimizing information loss and ensuring high-quality generation along the reverse trajectory.\nImportantly, Diffusion-Link also yields the lowest similarity on non-matched pairs, indicating not merely a global contraction of the space but maintaining semantic information.\nFigure &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.2.2 Inference to generate text-like embedding &#8227; 2.2 Modality Gap Bridging via Diffusion-Link &#8227; 2 Method &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> visualizes this effect. Both the generated text-like embeddings from audio and text embeddings all move toward the ground-true text embedding distribution, demonstrating that Diffusion-Link has learned a stable generative modality bridge for the text embedding distribution, regardless of the input modality.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This finding is consistent in AAC results. In Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the similarity score of Diffusion-Bridge is similar to that of Diffusion-Link when </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is between 300 and 400. This suggests that the performance of Diffusion-Bridge corresponds to over-noising of Diffusion-Link, which aligns with the observed </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">semantic information loss</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Furthermore, under the same multimodal LLM system, the AAC results in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> follow the same pattern: attaching Diffusion-Link yields large gains, whereas using Diffusion-Bridge provides only limited improvements. Together, the three tables show that </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">excessive</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> forward noising reduces similarity and weakens bridging, which in turn harms downstream performance; conversely, choosing an appropriate </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> maximizes content preservation in the text-like embedding, strengthens conditioning-distribution alignment for the LLM decoder, and translates into AAC gains.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Contrastive audio&#8211;language pretraining yields powerful joint representations, yet a persistent audio&#8211;text modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, a diffusion-based modality-bridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as a lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows a collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised captioning without external knowledge, with relative gains up to 52.5<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> and 7.5<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers a promising direction beyond knowledge-retrieval-centric designs. Code will be released upon acceptance<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text\" style=\"font-size:111%;\">1</span></span><span class=\"ltx_text\" style=\"font-size:111%;\">Official code: </span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/DevKiHyun/Diffusion-Link\" style=\"font-size:111%;\" title=\"\">https://github.com/DevKiHyun/Diffusion-Link</a></span></span></span>.</span>\n</p>\n\n",
                "matched_terms": [
                    "report",
                    "diffusionlink",
                    "from",
                    "similarity",
                    "modality",
                    "audiocaps",
                    "respectively",
                    "embedding",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large-scale audio&#8211;language models have shown strong multimodal performance across a range of multimodal tasks.\nIn particular, CLAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> maps natural-language descriptions and acoustic signals into a shared embedding space via contrastive learning, achieving state-of-the-art results on various audio&#8211;language multimodal tasks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib3\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">3</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In parallel, advances in LLMs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enable coupling contrastive audio&#8211;language encoders with powerful decoders, already demonstrating compelling audio&#8211;language reasoning and captioning&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "via",
                    "various",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Yet recent studies reveal a structural modality gap in contrastive multimodal encoders. Liang et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib9\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">9</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> quantified the gap and linked its magnitude to zero-shot performance and fairness, while Zhang et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib10\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">10</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> analyzed embedding geometry and showed that gap reduction benefits cross-modal tasks. From an application angle, linking contrastive spaces&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> via mediating modalities enables unpaired transfer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib12\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">12</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and broader alignment across audio&#8211;vision&#8211;text&#8211;3D yields competitive zero-shot results&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib13\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">13</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Taken together, these prior works suggest that addressing the modality gap is essential for improving zero-shot and cross-modal task performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "from",
                    "via",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Diffusion models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have become a standard generative paradigm in various fields, reliably producing high-fidelity samples&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. They learn a forward noising process toward an isotropic Gaussian and a reverse denoising process back to the target distribution. Viewing embedding vector as data, diffusion can learn a trajectory that bridges the embedding distributions between two modalities.\nWe adopt this view and design a reverse process that first moves audio embeddings to a shared isotropic Gaussian waypoint and then maps them into the text-embedding distribution, thereby enabling effective modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "modality",
                    "various"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent embedding-generative works support this view. In speaker recognition, SEED&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib19\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">19</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> applies the forward process to both clean and noisy speaker embeddings and trains the reverse process to regenerate the clean speaker embeddings, introducing cross-sample prediction and demonstrating embedding-level generation. In vision&#8211;language, Diffusion-Bridge&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">20</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> trains only on CLIP text embeddings and injects image embeddings at an intermediate reverse step to convert them into text-like vectors&#8211;an early instance of embedding-space modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "textlike",
                    "modality",
                    "diffusionbridge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion-Link</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which directly bridges the audio&#8211;text modality gap, building on prior works&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The key idea is to (i) use paired audio&#8211;text embeddings from an audio-language multimodal encoder during training to explicitly connect the two distributions, and (ii) achieve modality bridging by enforcing that the reverse process always map to the text embedding distribution. To this end, we gradually inject Gaussian noise into both embeddings in the forward process to send them to a common isotropic Gaussian state, and train with an L2 reconstruction loss so that the reverse process consistently predicts embeddings from the text distribution. Moreover, we add a topology loss that preserves the relative geometry of the text distribution by matching the within-batch cosine similarity structure of the original text and the generated text-like embeddings. At inference, Diffusion-Link outputs a text-like embedding regardless of the input modality. Diffusion-Link is a lightweight network composed of three residual multilayer perceptron (MLP) blocks, and the multimodal encoder is frozen during training. For practical validation, we attach Diffusion-Link after multimodal encoder as a plug-in and combine it with a LLM-based decoder to evaluate audio captioning. To our knowledge, this is the first attempt to apply diffusion-based modality bridging to audio captioning.</span>\n</p>\n\n",
                "matched_terms": [
                    "textlike",
                    "generated",
                    "diffusionlink",
                    "from",
                    "similarity",
                    "modality",
                    "cosine",
                    "embedding",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We verify consistent gains on the AudioCaps&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib21\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">21</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset along two axes: modality-gap analysis and LLM-based downstream tasks. On similarity and geometric criteria, Diffusion-Link increases the similarity of paired audio&#8211;text samples while decreasing that of unpaired, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">achieving the largest gap reduction</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> over prior methods. Visualizations further show a clear collective migration of audio embeddings toward the text-embedding distribution after the diffusion process. In Automatic Audio Captioning (AAC), attaching Diffusion-Link as a plug-in to the same multimodal LLM baseline yields relative improvements of up to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in zero-shot audio captioning and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in fully supervised audio captioning, reaching </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">state-of-the-art in both cases</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without external knowledge. Because many existing systems, especially in zero-shot, rely on external knowledge such as retrieval-augmented generation (RAG), these results establish Diffusion-Link as a new powerful solution that achieves consistent gains on the same multimodal LLM system while shifting the source of performance from knowledge retrieval to modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "diffusionlink",
                    "from",
                    "similarity",
                    "modality",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we describe the proposed framework (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S0.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). We denote by </span>\n  <math alttext=\"\\mathbf{e}^{a}_{0},\\mathbf{e}^{t}_{0}\\in\\mathbb{R}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119838;</mi>\n            <mn mathsize=\"0.900em\">0</mn>\n            <mi mathsize=\"0.900em\">a</mi>\n          </msubsup>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msubsup>\n            <mi mathsize=\"0.900em\">&#119838;</mi>\n            <mn mathsize=\"0.900em\">0</mn>\n            <mi mathsize=\"0.900em\">t</mi>\n          </msubsup>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mi mathsize=\"0.900em\">d</mi>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{e}^{a}_{0},\\mathbf{e}^{t}_{0}\\in\\mathbb{R}^{d}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the paired audio and text embeddings obtained from a multimodal encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib1\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">1</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For brevity, we use </span>\n  <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8499;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to indicate the modality, with </span>\n  <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8499;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> representing audio </span>\n  <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">a</mi>\n      <annotation encoding=\"application/x-tex\">a</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and text </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "obtained",
                    "ℳmathcalm",
                    "from",
                    "modality",
                    "indicate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mu_{\\theta}(\\mathbf{z}_{s},s)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#956;</mi>\n          <mi mathsize=\"0.900em\">&#952;</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119859;</mi>\n            <mi mathsize=\"0.900em\">s</mi>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mu_{\\theta}(\\mathbf{z}_{s},s)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is parameterized by a neural denoiser. The denoiser </span>\n  <math alttext=\"\\phi_{\\theta}(\\cdot,s)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#981;</mi>\n          <mi mathsize=\"0.900em\">&#952;</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\phi_{\\theta}(\\cdot,s)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is trained to predict the sample </span>\n  <math alttext=\"\\mathbf{z}_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119859;</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{z}_{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> at </span>\n  <math alttext=\"s=0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s=0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> via the objective</span>\n</p>\n\n",
                "matched_terms": [
                    "via",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We apply the same forward process (</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S2.E1\" style=\"font-size:90%;\" title=\"In 2.1 Background on Diffusion Probabilistic Models &#8227; 2 Method &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) to each modality </span>\n  <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8499;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "ℳmathcalm",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where the first term enforces high-fidelity reconstruction of text-like embeddings, while the second term encourages audio embeddings toward the text distribution.</span>\n</p>\n\n",
                "matched_terms": [
                    "textlike",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Furthermore, we introduce a batch-level topology loss to preserve the relative geometry of the text distribution.\nLet </span>\n  <math alttext=\"\\mathbf{X}\\!=\\![\\mathbf{e}^{t}_{0,i}]_{i\\in\\mathcal{B}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\">=</mo>\n        <msub>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n            <msubsup>\n              <mi mathsize=\"0.900em\">&#119838;</mi>\n              <mrow>\n                <mn mathsize=\"0.900em\">0</mn>\n                <mo mathsize=\"0.900em\">,</mo>\n                <mi mathsize=\"0.900em\">i</mi>\n              </mrow>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msubsup>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n          </mrow>\n          <mrow>\n            <mi mathsize=\"0.900em\">i</mi>\n            <mo mathsize=\"0.900em\">&#8712;</mo>\n            <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8492;</mi>\n          </mrow>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}\\!=\\![\\mathbf{e}^{t}_{0,i}]_{i\\in\\mathcal{B}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and\n</span>\n  <math alttext=\"\\hat{\\mathbf{X}}\\!=\\![\\hat{\\mathbf{e}}^{t}_{i}]_{i\\in\\mathcal{B}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\">=</mo>\n        <msub>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n            <msubsup>\n              <mover accent=\"true\">\n                <mi mathsize=\"0.900em\">&#119838;</mi>\n                <mo mathsize=\"0.900em\">^</mo>\n              </mover>\n              <mi mathsize=\"0.900em\">i</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msubsup>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n          </mrow>\n          <mrow>\n            <mi mathsize=\"0.900em\">i</mi>\n            <mo mathsize=\"0.900em\">&#8712;</mo>\n            <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8492;</mi>\n          </mrow>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\hat{\\mathbf{X}}\\!=\\![\\hat{\\mathbf{e}}^{t}_{i}]_{i\\in\\mathcal{B}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\ndenote the text and text-like embedding matrices.\nRow-wise </span>\n  <math alttext=\"\\ell_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8467;</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\ell_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-normalized matrices </span>\n  <math alttext=\"\\mathbf{X}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\hat{\\mathbf{X}}^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119831;</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{\\mathbf{X}}^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are obtained from </span>\n  <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119831;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\hat{\\mathbf{X}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m7\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">&#119831;</mi>\n        <mo mathsize=\"0.900em\">^</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\hat{\\mathbf{X}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively, yielding similarity matrices\n</span>\n  <math alttext=\"\\mathbf{S}_{xx}=\\mathbf{X}^{\\prime}\\mathbf{X}^{\\prime\\top}\\in\\mathbb{R}^{\\mathcal{B}\\times\\mathcal{B}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119826;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">x</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">x</mi>\n          </mrow>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <msup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mrow>\n              <mo mathsize=\"1.420em\">&#8242;</mo>\n              <mo lspace=\"0.222em\">&#8291;</mo>\n              <mo mathsize=\"0.900em\">&#8868;</mo>\n            </mrow>\n          </msup>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8492;</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8492;</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{S}_{xx}=\\mathbf{X}^{\\prime}\\mathbf{X}^{\\prime\\top}\\in\\mathbb{R}^{\\mathcal{B}\\times\\mathcal{B}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and\n</span>\n  <math alttext=\"\\mathbf{S}_{x\\hat{x}}=\\mathbf{X}^{\\prime}\\hat{\\mathbf{X}}^{\\prime\\top}\\in\\mathbb{R}^{\\mathcal{B}\\times\\mathcal{B}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119826;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">x</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mover accent=\"true\">\n              <mi mathsize=\"0.900em\">x</mi>\n              <mo mathsize=\"0.900em\">^</mo>\n            </mover>\n          </mrow>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <msup>\n            <mi mathsize=\"0.900em\">&#119831;</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <msup>\n            <mover accent=\"true\">\n              <mi mathsize=\"0.900em\">&#119831;</mi>\n              <mo mathsize=\"0.900em\">^</mo>\n            </mover>\n            <mrow>\n              <mo mathsize=\"1.420em\">&#8242;</mo>\n              <mo lspace=\"0.222em\">&#8291;</mo>\n              <mo mathsize=\"0.900em\">&#8868;</mo>\n            </mrow>\n          </msup>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8492;</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8492;</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{S}_{x\\hat{x}}=\\mathbf{X}^{\\prime}\\hat{\\mathbf{X}}^{\\prime\\top}\\in\\mathbb{R}^{\\mathcal{B}\\times\\mathcal{B}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nand the topology loss is the squared Frobenius distance:</span>\n</p>\n\n",
                "matched_terms": [
                    "textlike",
                    "obtained",
                    "similarity",
                    "from",
                    "respectively",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The output </span>\n  <math alttext=\"\\hat{\\mathbf{e}}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119838;</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{\\mathbf{e}}^{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">text-like</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> embedding.</span>\n</p>\n\n",
                "matched_terms": [
                    "textlike",
                    "embedding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given a text-like embedding </span>\n  <math alttext=\"\\hat{\\mathbf{e}}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119838;</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{\\mathbf{e}}^{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a projection head maps it to a soft-prefix vector </span>\n  <math alttext=\"\\mathbf{p}\\in\\mathbb{R}^{mh}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119849;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">m</mi>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mi mathsize=\"0.900em\">h</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{p}\\in\\mathbb{R}^{mh}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Here </span>\n  <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">m</mi>\n      <annotation encoding=\"application/x-tex\">m</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the number of learnable soft tokens and </span>\n  <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">h</mi>\n      <annotation encoding=\"application/x-tex\">h</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the decoder hidden size. We then </span>\n  <math alttext=\"\\mathbf{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119849;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{p}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into soft-prefix tokens sequence </span>\n  <math alttext=\"\\mathbf{p}\\in\\mathbb{R}^{m\\times h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119849;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">m</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">h</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{p}\\in\\mathbb{R}^{m\\times h}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and feed this sequence to the decoder. If we optionally prepend a fixed instruction prompt of </span>\n  <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">n</mi>\n      <annotation encoding=\"application/x-tex\">n</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> tokens, the resulting input becomes </span>\n  <math alttext=\"\\mathbf{p}\\in\\mathbb{R}^{(m+n)\\times h}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119849;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mrow>\n                <mi mathsize=\"0.900em\">m</mi>\n                <mo mathsize=\"0.900em\">+</mo>\n                <mi mathsize=\"0.900em\">n</mi>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" rspace=\"0.055em\">)</mo>\n            </mrow>\n            <mo mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mi mathsize=\"0.900em\">h</mi>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{p}\\in\\mathbb{R}^{(m+n)\\times h}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "denotes",
                    "textlike",
                    "embedding",
                    "here"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\psi\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#968;</mi>\n      <annotation encoding=\"application/x-tex\">\\psi</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the LLM decoder&#8217;s learnable parameters and </span>\n  <math alttext=\"\\mathbf{w}=(w_{1},\\dots,w_{L})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119856;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">w</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">w</mi>\n            <mi mathsize=\"0.900em\">L</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{w}=(w_{1},\\dots,w_{L})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> means the target caption tokens.\nAt inference, given audio data only with optional instruction prompt, and decode target caption.\nWhen the LLM decoder is trained under the text-only training, this evaluation corresponds to zero-shot captioning.</span>\n</p>\n\n",
                "matched_terms": [
                    "denotes",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For training and evaluation, we conduct all experiments on AudioCaps&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib21\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">21</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a corpus of ten-second audio clips paired with human-written captions. We use 48,595 training clips and 944 test clips. The </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">train</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> split provides one caption per clip, whereas the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> split provides up to five. All audio is resampled to 48kHz. For audio preprocessing, we compute STFTs with a 1,024 window size and a 480 hop length, and then form mel-spectrograms with 64 mel-bins. We train on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">train</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> split and report results on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> split.</span>\n</p>\n\n",
                "matched_terms": [
                    "report",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For audio-language multimodal encoder, we use the LAION-CLAP pretrained model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib2\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">2</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and keep it frozen.\nFollowing prior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">20</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we apply the same normalization process to the output embeddings of CLAP. For Diffusion-Link, we adopt three residual MLP blocks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib31\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">31</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We train Diffusion-Link with the Adam&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib32\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">32</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer and a batch size of </span>\n  <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">128</mn>\n      <annotation encoding=\"application/x-tex\">128</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The base learning rate is set to </span>\n  <math alttext=\"1{\\times}10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1{\\times}10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and follows a step-decay schedule, multiplying the rate by </span>\n  <math alttext=\"0.97\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.97</mn>\n      <annotation encoding=\"application/x-tex\">0.97</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> every </span>\n  <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">200</mn>\n      <annotation encoding=\"application/x-tex\">200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps. We employ an exponential moving average (EMA) of the model parameters with a decay of </span>\n  <math alttext=\"0.995\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.995</mn>\n      <annotation encoding=\"application/x-tex\">0.995</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and use the EMA weights for inference. We adopt a cosine noise schedule with a total of </span>\n  <math alttext=\"T{=}1000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">T</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1000</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">T{=}1000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> timesteps.\nAt inference, we employ DDIM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib15\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">15</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sampling with 5 iteration steps. Before denoising, we apply a shallow forward noising to </span>\n  <math alttext=\"s_{\\ast}{=}100\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo mathsize=\"0.900em\">&#8727;</mo>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">100</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}{=}100</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and then run the reverse procoess. For LLM-based text decoder, we adopt LLaMA2(7B)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib5\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">5</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the LLM decoder. In the text-only training, we employ a linear layer with soft prefix tokens </span>\n  <math alttext=\"m{=}1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m{=}1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the project head and prepend a short instruction prompt; in the fully supervised training, we use 2 linear layers with </span>\n  <math alttext=\"m{=}10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">10</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m{=}10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the project head and no hard prompt. We fine-tune project head and the LLM using LoRA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib33\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">33</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. LLM training uses AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib34\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">34</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer with batch size </span>\n  <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">4</mn>\n      <annotation encoding=\"application/x-tex\">4</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for </span>\n  <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m11\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">50</mn>\n      <annotation encoding=\"application/x-tex\">50</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> epochs: the learning rate warms up over the first </span>\n  <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m12\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2</mn>\n      <annotation encoding=\"application/x-tex\">2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> epochs with max learning rate </span>\n  <math alttext=\"5{\\times}10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m13\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">6</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">5{\\times}10^{-6}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, then use a cosine decay. We also train a baseline multimodal LLM system to verify the effectiveness of Diffusion-Link, we adopt same setting but detach only Diffusion-Link module. For evaluation, we adopt the metrics for modality gap analyzing, including cosine simiarity and visualization using UMAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib35\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">35</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For AAC, we use the metrics, METEOR (ME)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib36\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">36</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CIDEr (CD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib37\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">37</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SPICE (SP)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib38\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">38</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and SPIDEr (SD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib39\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">39</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "diffusionlink",
                    "modality",
                    "clap",
                    "average",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares a range of AAC systems. In contrast to many prior methods that leverage longer audio representations or external knowledge (e.g., RAG), our multimodal LLM system captures input audio feature with only a single </span>\n  <math alttext=\"1{\\times}D\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mi mathsize=\"0.900em\">D</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1{\\times}D</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> text-like embedding produced by Diffusion-Link, and achieves SOTA in both zero-shot and fully supervised captioning </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">without external knowledge</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Notably, considering that most prior zero-shot models rely heavily on external knowledge, outperforming them without any external knowledge demonstrates the significant efficiency of Diffusion-Link.</span>\n</p>\n\n",
                "matched_terms": [
                    "textlike",
                    "diffusionlink",
                    "embedding",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">According to Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our baseline LLM-based AAC system is not competitive relative to prior AAC systems in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, applying Diffusion-Link markedly improves the same backbone. In zero-shot audio captioning, we observe a </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.5<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> relative increase in CIDEr together with substantial gains on the other metrics. These dramatic gains demonstrate that Diffusion-Link is the key factor and reaffirm the primacy of </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">modality-gap reduction</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> over using longer audio representations or external knowledge. Moreover, in fully supervised audio captioning we observe up to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.3<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> relative improvement, underscoring our method&#8217;s applicability.</span>\n</p>\n\n",
                "matched_terms": [
                    "diffusionlink",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct ablations to analyze how the depth of forward noising affects modality bridging and high-quality generation. According to Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, increasing the inference forward timestep </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from shallow levels initially keeps similarity quite stable; beyond a threshold, the similarity drops sharply as </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> increases. This indicates that over-noising pushes representations deeper into the common Gaussian space and </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">erases information</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, thereby degrading semantic preservation in the reconstructed text-like embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "textlike",
                    "from",
                    "similarity",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduced </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion-Link</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a lightweight residual MLP diffusion module that bridges audio embeddings to the text embedding distribution without keeping the multimodal encoder frozen. The method aligns the conditioning input by increasing matched similarity and decreasing mismatched similarity. On AAC, it improves the same multimodal LLM baseline by </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.3%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without external knowledge for zero-shot and fully supervised AAC, respectively. This plug-in-play approach of Diffusion-Link is expected to generalize beyond audio captioning and enable effective zero-shot performance in a variety of multimodal LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "matched",
                    "diffusionlink",
                    "similarity",
                    "respectively",
                    "embedding"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap",
        "caption": "Table 2: Average cosine similarity scores for various inference forward timestep s∗s_{\\ast} during diffusion process on AudioCaps.",
        "body": "Diffusion-Link\n\nInference forward timestep s∗s_{\\ast}\n\n\n\n100\n200\n300\n400\n500\n\n\nCosine Similarity\n0.688\n0.654\n0.596\n0.510\n0.404",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Diffusion-Link</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Inference forward timestep </span><math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">s</mi><mo mathsize=\"0.900em\">&#8727;</mo></msub><annotation encoding=\"application/x-tex\">s_{\\ast}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">200</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">300</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">400</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">500</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Cosine Similarity</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.688</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.654</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.596</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.510</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.404</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "diffusionlink",
            "forward",
            "diffusion",
            "scores",
            "similarity",
            "during",
            "inference",
            "various",
            "process",
            "audiocaps",
            "average",
            "cosine",
            "timestep",
            "s∗sast"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct ablations to analyze how the depth of forward noising affects modality bridging and high-quality generation. According to Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, increasing the inference forward timestep </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from shallow levels initially keeps similarity quite stable; beyond a threshold, the similarity drops sharply as </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> increases. This indicates that over-noising pushes representations deeper into the common Gaussian space and </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">erases information</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, thereby degrading semantic preservation in the reconstructed text-like embeddings.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Contrastive audio&#8211;language pretraining yields powerful joint representations, yet a persistent audio&#8211;text modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, a diffusion-based modality-bridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as a lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows a collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised captioning without external knowledge, with relative gains up to 52.5<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> and 7.5<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers a promising direction beyond knowledge-retrieval-centric designs. Code will be released upon acceptance<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text\" style=\"font-size:111%;\">1</span></span><span class=\"ltx_text\" style=\"font-size:111%;\">Official code: </span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/DevKiHyun/Diffusion-Link\" style=\"font-size:111%;\" title=\"\">https://github.com/DevKiHyun/Diffusion-Link</a></span></span></span>.</span>\n</p>\n\n",
                "matched_terms": [
                    "diffusionlink",
                    "similarity",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Diffusion models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have become a standard generative paradigm in various fields, reliably producing high-fidelity samples&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. They learn a forward noising process toward an isotropic Gaussian and a reverse denoising process back to the target distribution. Viewing embedding vector as data, diffusion can learn a trajectory that bridges the embedding distributions between two modalities.\nWe adopt this view and design a reverse process that first moves audio embeddings to a shared isotropic Gaussian waypoint and then maps them into the text-embedding distribution, thereby enabling effective modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "forward",
                    "diffusion",
                    "various",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent embedding-generative works support this view. In speaker recognition, SEED&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib19\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">19</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> applies the forward process to both clean and noisy speaker embeddings and trains the reverse process to regenerate the clean speaker embeddings, introducing cross-sample prediction and demonstrating embedding-level generation. In vision&#8211;language, Diffusion-Bridge&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">20</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> trains only on CLIP text embeddings and injects image embeddings at an intermediate reverse step to convert them into text-like vectors&#8211;an early instance of embedding-space modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "forward",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion-Link</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which directly bridges the audio&#8211;text modality gap, building on prior works&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The key idea is to (i) use paired audio&#8211;text embeddings from an audio-language multimodal encoder during training to explicitly connect the two distributions, and (ii) achieve modality bridging by enforcing that the reverse process always map to the text embedding distribution. To this end, we gradually inject Gaussian noise into both embeddings in the forward process to send them to a common isotropic Gaussian state, and train with an L2 reconstruction loss so that the reverse process consistently predicts embeddings from the text distribution. Moreover, we add a topology loss that preserves the relative geometry of the text distribution by matching the within-batch cosine similarity structure of the original text and the generated text-like embeddings. At inference, Diffusion-Link outputs a text-like embedding regardless of the input modality. Diffusion-Link is a lightweight network composed of three residual multilayer perceptron (MLP) blocks, and the multimodal encoder is frozen during training. For practical validation, we attach Diffusion-Link after multimodal encoder as a plug-in and combine it with a LLM-based decoder to evaluate audio captioning. To our knowledge, this is the first attempt to apply diffusion-based modality bridging to audio captioning.</span>\n</p>\n\n",
                "matched_terms": [
                    "diffusionlink",
                    "forward",
                    "similarity",
                    "during",
                    "inference",
                    "process",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We verify consistent gains on the AudioCaps&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib21\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">21</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset along two axes: modality-gap analysis and LLM-based downstream tasks. On similarity and geometric criteria, Diffusion-Link increases the similarity of paired audio&#8211;text samples while decreasing that of unpaired, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">achieving the largest gap reduction</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> over prior methods. Visualizations further show a clear collective migration of audio embeddings toward the text-embedding distribution after the diffusion process. In Automatic Audio Captioning (AAC), attaching Diffusion-Link as a plug-in to the same multimodal LLM baseline yields relative improvements of up to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in zero-shot audio captioning and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in fully supervised audio captioning, reaching </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">state-of-the-art in both cases</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without external knowledge. Because many existing systems, especially in zero-shot, rely on external knowledge such as retrieval-augmented generation (RAG), these results establish Diffusion-Link as a new powerful solution that achieves consistent gains on the same multimodal LLM system while shifting the source of performance from knowledge retrieval to modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "diffusionlink",
                    "diffusion",
                    "similarity",
                    "process",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The forward diffusion process progressively corrupts a given sample </span>\n  <math alttext=\"\\mathbf{z}_{0}\\sim q(\\mathbf{z}_{0})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119859;</mi>\n          <mn mathsize=\"0.900em\">0</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8764;</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">q</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">&#119859;</mi>\n              <mn mathsize=\"0.900em\">0</mn>\n            </msub>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{z}_{0}\\sim q(\\mathbf{z}_{0})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> at each timestep </span>\n  <math alttext=\"s=1,\\dots,T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">T</mi>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s=1,\\dots,T</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "forward",
                    "timestep",
                    "diffusion",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The reverse diffusion process gradually denoises </span>\n  <math alttext=\"\\mathbf{z}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119859;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{z}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> back toward the data distribution at each timestep </span>\n  <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">s</mi>\n      <annotation encoding=\"application/x-tex\">s</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "timestep",
                    "diffusion",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We apply the same forward process (</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S2.E1\" style=\"font-size:90%;\" title=\"In 2.1 Background on Diffusion Probabilistic Models &#8227; 2 Method &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) to each modality </span>\n  <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8499;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "forward",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">At inference, given </span>\n  <math alttext=\"\\mathbf{e}^{\\mathcal{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">&#119838;</mi>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8499;</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{e}^{\\mathcal{M}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we optionally apply forward noising at step </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with </span>\n  <math alttext=\"\\bm{\\epsilon}\\!\\sim\\!\\mathcal{N}(\\mathbf{0},\\mathbf{I})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_mathvariant_bold-italic\" mathsize=\"0.900em\" mathvariant=\"bold-italic\">&#1013;</mi>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\">&#8764;</mo>\n        <mrow>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119977;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mn mathsize=\"0.900em\">&#120782;</mn>\n            <mo mathsize=\"0.900em\">,</mo>\n            <mi mathsize=\"0.900em\">&#119816;</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bm{\\epsilon}\\!\\sim\\!\\mathcal{N}(\\mathbf{0},\\mathbf{I})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and then run the learned reverse trajectory to </span>\n  <math alttext=\"s\\!=\\!0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo lspace=\"0.108em\" mathsize=\"0.900em\" rspace=\"0.108em\">=</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s\\!=\\!0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using DDIM sampler&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib15\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">15</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "forward",
                    "s∗sast",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For audio-language multimodal encoder, we use the LAION-CLAP pretrained model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib2\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">2</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and keep it frozen.\nFollowing prior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">20</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we apply the same normalization process to the output embeddings of CLAP. For Diffusion-Link, we adopt three residual MLP blocks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib31\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">31</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We train Diffusion-Link with the Adam&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib32\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">32</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer and a batch size of </span>\n  <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">128</mn>\n      <annotation encoding=\"application/x-tex\">128</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The base learning rate is set to </span>\n  <math alttext=\"1{\\times}10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1{\\times}10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and follows a step-decay schedule, multiplying the rate by </span>\n  <math alttext=\"0.97\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.97</mn>\n      <annotation encoding=\"application/x-tex\">0.97</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> every </span>\n  <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">200</mn>\n      <annotation encoding=\"application/x-tex\">200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps. We employ an exponential moving average (EMA) of the model parameters with a decay of </span>\n  <math alttext=\"0.995\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.995</mn>\n      <annotation encoding=\"application/x-tex\">0.995</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and use the EMA weights for inference. We adopt a cosine noise schedule with a total of </span>\n  <math alttext=\"T{=}1000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">T</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1000</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">T{=}1000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> timesteps.\nAt inference, we employ DDIM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib15\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">15</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sampling with 5 iteration steps. Before denoising, we apply a shallow forward noising to </span>\n  <math alttext=\"s_{\\ast}{=}100\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo mathsize=\"0.900em\">&#8727;</mo>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">100</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}{=}100</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and then run the reverse procoess. For LLM-based text decoder, we adopt LLaMA2(7B)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib5\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">5</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the LLM decoder. In the text-only training, we employ a linear layer with soft prefix tokens </span>\n  <math alttext=\"m{=}1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m{=}1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the project head and prepend a short instruction prompt; in the fully supervised training, we use 2 linear layers with </span>\n  <math alttext=\"m{=}10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">10</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m{=}10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the project head and no hard prompt. We fine-tune project head and the LLM using LoRA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib33\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">33</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. LLM training uses AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib34\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">34</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer with batch size </span>\n  <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">4</mn>\n      <annotation encoding=\"application/x-tex\">4</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for </span>\n  <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m11\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">50</mn>\n      <annotation encoding=\"application/x-tex\">50</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> epochs: the learning rate warms up over the first </span>\n  <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m12\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2</mn>\n      <annotation encoding=\"application/x-tex\">2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> epochs with max learning rate </span>\n  <math alttext=\"5{\\times}10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m13\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">6</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">5{\\times}10^{-6}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, then use a cosine decay. We also train a baseline multimodal LLM system to verify the effectiveness of Diffusion-Link, we adopt same setting but detach only Diffusion-Link module. For evaluation, we adopt the metrics for modality gap analyzing, including cosine simiarity and visualization using UMAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib35\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">35</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For AAC, we use the metrics, METEOR (ME)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib36\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">36</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CIDEr (CD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib37\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">37</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SPICE (SP)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib38\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">38</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and SPIDEr (SD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib39\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">39</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "diffusionlink",
                    "forward",
                    "inference",
                    "process",
                    "average",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Effectiveness of Diffusion-Link for Modality Bridging.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Diffusion-Link attains the highest cosine similarity on matched audio&#8211;text pairs.\nWhile most approaches improve over CLAP, DiffGap underperforms, because it generates from pure Gaussian noise with the input embedding condition, which weakens information reconstruction.\nBy contrast, Diffusion-Link treat the input embedding as residing at an intermediate reverse step, thereby minimizing information loss and ensuring high-quality generation along the reverse trajectory.\nImportantly, Diffusion-Link also yields the lowest similarity on non-matched pairs, indicating not merely a global contraction of the space but maintaining semantic information.\nFigure &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.2.2 Inference to generate text-like embedding &#8227; 2.2 Modality Gap Bridging via Diffusion-Link &#8227; 2 Method &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> visualizes this effect. Both the generated text-like embeddings from audio and text embeddings all move toward the ground-true text embedding distribution, demonstrating that Diffusion-Link has learned a stable generative modality bridge for the text embedding distribution, regardless of the input modality.</span>\n</p>\n\n",
                "matched_terms": [
                    "diffusionlink",
                    "similarity",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This finding is consistent in AAC results. In Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the similarity score of Diffusion-Bridge is similar to that of Diffusion-Link when </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is between 300 and 400. This suggests that the performance of Diffusion-Bridge corresponds to over-noising of Diffusion-Link, which aligns with the observed </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">semantic information loss</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Furthermore, under the same multimodal LLM system, the AAC results in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> follow the same pattern: attaching Diffusion-Link yields large gains, whereas using Diffusion-Bridge provides only limited improvements. Together, the three tables show that </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">excessive</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> forward noising reduces similarity and weakens bridging, which in turn harms downstream performance; conversely, choosing an appropriate </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> maximizes content preservation in the text-like embedding, strengthens conditioning-distribution alignment for the LLM decoder, and translates into AAC gains.</span>\n</p>\n\n",
                "matched_terms": [
                    "diffusionlink",
                    "forward",
                    "similarity",
                    "s∗sast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduced </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion-Link</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a lightweight residual MLP diffusion module that bridges audio embeddings to the text embedding distribution without keeping the multimodal encoder frozen. The method aligns the conditioning input by increasing matched similarity and decreasing mismatched similarity. On AAC, it improves the same multimodal LLM baseline by </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.3%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without external knowledge for zero-shot and fully supervised AAC, respectively. This plug-in-play approach of Diffusion-Link is expected to generalize beyond audio captioning and enable effective zero-shot performance in a variety of multimodal LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "diffusionlink",
                    "similarity",
                    "diffusion"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap",
        "caption": "Table 3: Performance comparison of AAC models on AudioCaps. External knowledge # is the number of non-audio samples used by the LLM at test time. For a fair comparison on the embedding-level modality-gap problem, † results use only embedding-level RAG without external kk-caption selection.",
        "body": "External\n\n\nknowledge #",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:-0.2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">External</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:-0.2pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">knowledge #</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "llm",
            "modalitygap",
            "audiocaps",
            "knowledge",
            "used",
            "fair",
            "external",
            "test",
            "results",
            "nonaudio",
            "rag",
            "embeddinglevel",
            "without",
            "number",
            "performance",
            "samples",
            "only",
            "problem",
            "aac",
            "time",
            "models",
            "kkcaption",
            "selection",
            "use",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares a range of AAC systems. In contrast to many prior methods that leverage longer audio representations or external knowledge (e.g., RAG), our multimodal LLM system captures input audio feature with only a single </span>\n  <math alttext=\"1{\\times}D\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mi mathsize=\"0.900em\">D</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1{\\times}D</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> text-like embedding produced by Diffusion-Link, and achieves SOTA in both zero-shot and fully supervised captioning </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">without external knowledge</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Notably, considering that most prior zero-shot models rely heavily on external knowledge, outperforming them without any external knowledge demonstrates the significant efficiency of Diffusion-Link.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">According to Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our baseline LLM-based AAC system is not competitive relative to prior AAC systems in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, applying Diffusion-Link markedly improves the same backbone. In zero-shot audio captioning, we observe a </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.5<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> relative increase in CIDEr together with substantial gains on the other metrics. These dramatic gains demonstrate that Diffusion-Link is the key factor and reaffirm the primacy of </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">modality-gap reduction</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> over using longer audio representations or external knowledge. Moreover, in fully supervised audio captioning we observe up to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.3<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> relative improvement, underscoring our method&#8217;s applicability.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Contrastive audio&#8211;language pretraining yields powerful joint representations, yet a persistent audio&#8211;text modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, a diffusion-based modality-bridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as a lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows a collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised captioning without external knowledge, with relative gains up to 52.5<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> and 7.5<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers a promising direction beyond knowledge-retrieval-centric designs. Code will be released upon acceptance<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text\" style=\"font-size:111%;\">1</span></span><span class=\"ltx_text\" style=\"font-size:111%;\">Official code: </span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/DevKiHyun/Diffusion-Link\" style=\"font-size:111%;\" title=\"\">https://github.com/DevKiHyun/Diffusion-Link</a></span></span></span>.</span>\n</p>\n\n",
                "matched_terms": [
                    "llm",
                    "aac",
                    "external",
                    "modalitygap",
                    "models",
                    "without",
                    "audiocaps",
                    "knowledge",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large-scale audio&#8211;language models have shown strong multimodal performance across a range of multimodal tasks.\nIn particular, CLAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> maps natural-language descriptions and acoustic signals into a shared embedding space via contrastive learning, achieving state-of-the-art results on various audio&#8211;language multimodal tasks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib3\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">3</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In parallel, advances in LLMs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enable coupling contrastive audio&#8211;language encoders with powerful decoders, already demonstrating compelling audio&#8211;language reasoning and captioning&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Yet recent studies reveal a structural modality gap in contrastive multimodal encoders. Liang et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib9\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">9</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> quantified the gap and linked its magnitude to zero-shot performance and fairness, while Zhang et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib10\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">10</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> analyzed embedding geometry and showed that gap reduction benefits cross-modal tasks. From an application angle, linking contrastive spaces&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> via mediating modalities enables unpaired transfer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib12\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">12</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and broader alignment across audio&#8211;vision&#8211;text&#8211;3D yields competitive zero-shot results&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib13\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">13</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Taken together, these prior works suggest that addressing the modality gap is essential for improving zero-shot and cross-modal task performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Diffusion models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have become a standard generative paradigm in various fields, reliably producing high-fidelity samples&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. They learn a forward noising process toward an isotropic Gaussian and a reverse denoising process back to the target distribution. Viewing embedding vector as data, diffusion can learn a trajectory that bridges the embedding distributions between two modalities.\nWe adopt this view and design a reverse process that first moves audio embeddings to a shared isotropic Gaussian waypoint and then maps them into the text-embedding distribution, thereby enabling effective modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent embedding-generative works support this view. In speaker recognition, SEED&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib19\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">19</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> applies the forward process to both clean and noisy speaker embeddings and trains the reverse process to regenerate the clean speaker embeddings, introducing cross-sample prediction and demonstrating embedding-level generation. In vision&#8211;language, Diffusion-Bridge&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">20</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> trains only on CLIP text embeddings and injects image embeddings at an intermediate reverse step to convert them into text-like vectors&#8211;an early instance of embedding-space modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "only",
                    "embeddinglevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion-Link</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which directly bridges the audio&#8211;text modality gap, building on prior works&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The key idea is to (i) use paired audio&#8211;text embeddings from an audio-language multimodal encoder during training to explicitly connect the two distributions, and (ii) achieve modality bridging by enforcing that the reverse process always map to the text embedding distribution. To this end, we gradually inject Gaussian noise into both embeddings in the forward process to send them to a common isotropic Gaussian state, and train with an L2 reconstruction loss so that the reverse process consistently predicts embeddings from the text distribution. Moreover, we add a topology loss that preserves the relative geometry of the text distribution by matching the within-batch cosine similarity structure of the original text and the generated text-like embeddings. At inference, Diffusion-Link outputs a text-like embedding regardless of the input modality. Diffusion-Link is a lightweight network composed of three residual multilayer perceptron (MLP) blocks, and the multimodal encoder is frozen during training. For practical validation, we attach Diffusion-Link after multimodal encoder as a plug-in and combine it with a LLM-based decoder to evaluate audio captioning. To our knowledge, this is the first attempt to apply diffusion-based modality bridging to audio captioning.</span>\n</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We verify consistent gains on the AudioCaps&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib21\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">21</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset along two axes: modality-gap analysis and LLM-based downstream tasks. On similarity and geometric criteria, Diffusion-Link increases the similarity of paired audio&#8211;text samples while decreasing that of unpaired, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">achieving the largest gap reduction</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> over prior methods. Visualizations further show a clear collective migration of audio embeddings toward the text-embedding distribution after the diffusion process. In Automatic Audio Captioning (AAC), attaching Diffusion-Link as a plug-in to the same multimodal LLM baseline yields relative improvements of up to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in zero-shot audio captioning and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in fully supervised audio captioning, reaching </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">state-of-the-art in both cases</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without external knowledge. Because many existing systems, especially in zero-shot, rely on external knowledge such as retrieval-augmented generation (RAG), these results establish Diffusion-Link as a new powerful solution that achieves consistent gains on the same multimodal LLM system while shifting the source of performance from knowledge retrieval to modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "llm",
                    "aac",
                    "external",
                    "rag",
                    "modalitygap",
                    "without",
                    "audiocaps",
                    "knowledge",
                    "performance",
                    "samples",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We consider two training options for our LLM decoder framework:\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(i) text-only training</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: using text-driven </span>\n  <math alttext=\"\\hat{\\mathbf{e}}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119838;</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{\\mathbf{e}}^{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with an instruction prompt.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(ii) fully supervised training</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: using audio-driven </span>\n  <math alttext=\"\\hat{\\mathbf{e}}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119838;</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{\\mathbf{e}}^{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and we not use an instruction prompt.\nWe train the LLM decoder with the standard autoregressive cross-entropy objective</span>\n</p>\n\n",
                "matched_terms": [
                    "llm",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\psi\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#968;</mi>\n      <annotation encoding=\"application/x-tex\">\\psi</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the LLM decoder&#8217;s learnable parameters and </span>\n  <math alttext=\"\\mathbf{w}=(w_{1},\\dots,w_{L})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119856;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">w</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">w</mi>\n            <mi mathsize=\"0.900em\">L</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{w}=(w_{1},\\dots,w_{L})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> means the target caption tokens.\nAt inference, given audio data only with optional instruction prompt, and decode target caption.\nWhen the LLM decoder is trained under the text-only training, this evaluation corresponds to zero-shot captioning.</span>\n</p>\n\n",
                "matched_terms": [
                    "llm",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For training and evaluation, we conduct all experiments on AudioCaps&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib21\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">21</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a corpus of ten-second audio clips paired with human-written captions. We use 48,595 training clips and 944 test clips. The </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">train</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> split provides one caption per clip, whereas the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> split provides up to five. All audio is resampled to 48kHz. For audio preprocessing, we compute STFTs with a 1,024 window size and a 480 hop length, and then form mel-spectrograms with 64 mel-bins. We train on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">train</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> split and report results on the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">test</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> split.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "use",
                    "test",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For audio-language multimodal encoder, we use the LAION-CLAP pretrained model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib2\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">2</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and keep it frozen.\nFollowing prior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">20</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we apply the same normalization process to the output embeddings of CLAP. For Diffusion-Link, we adopt three residual MLP blocks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib31\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">31</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We train Diffusion-Link with the Adam&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib32\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">32</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer and a batch size of </span>\n  <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">128</mn>\n      <annotation encoding=\"application/x-tex\">128</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The base learning rate is set to </span>\n  <math alttext=\"1{\\times}10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1{\\times}10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and follows a step-decay schedule, multiplying the rate by </span>\n  <math alttext=\"0.97\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.97</mn>\n      <annotation encoding=\"application/x-tex\">0.97</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> every </span>\n  <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">200</mn>\n      <annotation encoding=\"application/x-tex\">200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps. We employ an exponential moving average (EMA) of the model parameters with a decay of </span>\n  <math alttext=\"0.995\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.995</mn>\n      <annotation encoding=\"application/x-tex\">0.995</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and use the EMA weights for inference. We adopt a cosine noise schedule with a total of </span>\n  <math alttext=\"T{=}1000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">T</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1000</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">T{=}1000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> timesteps.\nAt inference, we employ DDIM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib15\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">15</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sampling with 5 iteration steps. Before denoising, we apply a shallow forward noising to </span>\n  <math alttext=\"s_{\\ast}{=}100\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo mathsize=\"0.900em\">&#8727;</mo>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">100</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}{=}100</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and then run the reverse procoess. For LLM-based text decoder, we adopt LLaMA2(7B)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib5\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">5</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the LLM decoder. In the text-only training, we employ a linear layer with soft prefix tokens </span>\n  <math alttext=\"m{=}1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m{=}1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the project head and prepend a short instruction prompt; in the fully supervised training, we use 2 linear layers with </span>\n  <math alttext=\"m{=}10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">10</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m{=}10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the project head and no hard prompt. We fine-tune project head and the LLM using LoRA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib33\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">33</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. LLM training uses AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib34\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">34</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer with batch size </span>\n  <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">4</mn>\n      <annotation encoding=\"application/x-tex\">4</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for </span>\n  <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m11\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">50</mn>\n      <annotation encoding=\"application/x-tex\">50</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> epochs: the learning rate warms up over the first </span>\n  <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m12\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2</mn>\n      <annotation encoding=\"application/x-tex\">2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> epochs with max learning rate </span>\n  <math alttext=\"5{\\times}10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m13\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">6</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">5{\\times}10^{-6}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, then use a cosine decay. We also train a baseline multimodal LLM system to verify the effectiveness of Diffusion-Link, we adopt same setting but detach only Diffusion-Link module. For evaluation, we adopt the metrics for modality gap analyzing, including cosine simiarity and visualization using UMAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib35\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">35</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For AAC, we use the metrics, METEOR (ME)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib36\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">36</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CIDEr (CD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib37\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">37</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SPICE (SP)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib38\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">38</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and SPIDEr (SD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib39\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">39</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "llm",
                    "only",
                    "use",
                    "aac"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This finding is consistent in AAC results. In Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the similarity score of Diffusion-Bridge is similar to that of Diffusion-Link when </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is between 300 and 400. This suggests that the performance of Diffusion-Bridge corresponds to over-noising of Diffusion-Link, which aligns with the observed </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">semantic information loss</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Furthermore, under the same multimodal LLM system, the AAC results in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> follow the same pattern: attaching Diffusion-Link yields large gains, whereas using Diffusion-Bridge provides only limited improvements. Together, the three tables show that </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">excessive</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> forward noising reduces similarity and weakens bridging, which in turn harms downstream performance; conversely, choosing an appropriate </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> maximizes content preservation in the text-like embedding, strengthens conditioning-distribution alignment for the LLM decoder, and translates into AAC gains.</span>\n</p>\n\n",
                "matched_terms": [
                    "llm",
                    "aac",
                    "performance",
                    "only",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduced </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion-Link</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a lightweight residual MLP diffusion module that bridges audio embeddings to the text embedding distribution without keeping the multimodal encoder frozen. The method aligns the conditioning input by increasing matched similarity and decreasing mismatched similarity. On AAC, it improves the same multimodal LLM baseline by </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.3%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without external knowledge for zero-shot and fully supervised AAC, respectively. This plug-in-play approach of Diffusion-Link is expected to generalize beyond audio captioning and enable effective zero-shot performance in a variety of multimodal LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "llm",
                    "aac",
                    "external",
                    "without",
                    "knowledge",
                    "performance"
                ]
            }
        ]
    },
    "S3.T4": {
        "source_file": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap",
        "caption": "Table 4: Ablation study to analyze the effectiveness of diffusion-based modality bridging method.",
        "body": "Method\n\n\nME↑\\uparrow\nCD↑\\uparrow\nSP↑\\uparrow\nSD↑\\uparrow\n\n\n\n\nZero-shot Captioning\n\n\n\n\n\n\n\n\n\n\nBaseline (CLAP & LLaMa2-7B)\n\n\n21.221.2\n48.048.0\n14.414.4\n31.231.2\n\n\n\n\n+ Diffusion-Bridge [20]\n\n\n23.323.3\n62.662.6\n16.516.5\n39.539.5\n\n\n\n\n+ Diffusion-Link (Ours)\n\n\n24.2\n73.2\n17.5\n45.4\n\n\n\n\nFully Supervised Captioning\n\n\n\n\n\n\nBaseline (CLAP & LLaMa2-7B)\n\n\n25.025.0\n76.976.9\n18.618.6\n47.747.7\n\n\n\n\n+ Diffusion-Bridge [20]\n\n\n25.225.2\n77.177.1\n18.018.0\n47.447.4\n\n\n\n\n+ Diffusion-Link (Ours)\n\n\n25.6\n82.5\n18.9\n50.7",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_tt\" style=\"padding:-0.55pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ME<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CD<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SP<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_tt\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SD<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.55pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Zero-shot Captioning</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.55pt 3.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.55pt 3.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.55pt 3.0pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding:-0.55pt 3.0pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row\" style=\"padding:-0.55pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Baseline (CLAP &amp; LLaMa2-7B)</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"21.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m5\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">21.2</mn><annotation encoding=\"application/x-tex\">21.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"48.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m6\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">48.0</mn><annotation encoding=\"application/x-tex\">48.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"14.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m7\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">14.4</mn><annotation encoding=\"application/x-tex\">14.4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"31.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m8\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">31.2</mn><annotation encoding=\"application/x-tex\">31.2</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row\" style=\"padding:-0.55pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ Diffusion-Bridge&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"23.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m9\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">23.3</mn><annotation encoding=\"application/x-tex\">23.3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"62.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m10\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">62.6</mn><annotation encoding=\"application/x-tex\">62.6</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"16.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m11\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">16.5</mn><annotation encoding=\"application/x-tex\">16.5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"39.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m12\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">39.5</mn><annotation encoding=\"application/x-tex\">39.5</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row\" style=\"padding:-0.55pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion-Link (Ours)</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">24.2</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.2</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">17.5</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">45.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_row ltx_border_t\" colspan=\"5\" style=\"padding-bottom:-1.0pt;padding:-0.55pt 3.0pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:113.9pt;height:7.9pt;vertical-align:-1.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-0.6pt,0.0pt) scale(0.99,0.99) ;\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Fully Supervised Captioning</span></p>\n</span></div>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row\" style=\"padding:-0.55pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">Baseline (CLAP &amp; LLaMa2-7B)</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"25.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m13\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">25.0</mn><annotation encoding=\"application/x-tex\">25.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"76.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m14\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">76.9</mn><annotation encoding=\"application/x-tex\">76.9</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"18.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m15\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">18.6</mn><annotation encoding=\"application/x-tex\">18.6</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"47.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m16\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">47.7</mn><annotation encoding=\"application/x-tex\">47.7</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row\" style=\"padding:-0.55pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ Diffusion-Bridge&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"25.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m17\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">25.2</mn><annotation encoding=\"application/x-tex\">25.2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"77.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m18\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">77.1</mn><annotation encoding=\"application/x-tex\">77.1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"18.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m19\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">18.0</mn><annotation encoding=\"application/x-tex\">18.0</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding:-0.55pt 3.0pt;\"><math alttext=\"47.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m20\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">47.4</mn><annotation encoding=\"application/x-tex\">47.4</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb\" style=\"padding:-0.55pt 3.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion-Link (Ours)</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">25.6</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">82.5</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">18.9</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding:-0.55pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">50.7</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ablation",
            "supervised",
            "ours",
            "modality",
            "bridging",
            "sd↑uparrow",
            "analyze",
            "diffusionlink",
            "diffusionbridge",
            "zeroshot",
            "fully",
            "baseline",
            "clap",
            "effectiveness",
            "llama27b",
            "diffusionbased",
            "captioning",
            "study",
            "cd↑uparrow",
            "method",
            "me↑uparrow",
            "sp↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">According to Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our baseline LLM-based AAC system is not competitive relative to prior AAC systems in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, applying Diffusion-Link markedly improves the same backbone. In zero-shot audio captioning, we observe a </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.5<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> relative increase in CIDEr together with substantial gains on the other metrics. These dramatic gains demonstrate that Diffusion-Link is the key factor and reaffirm the primacy of </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">modality-gap reduction</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> over using longer audio representations or external knowledge. Moreover, in fully supervised audio captioning we observe up to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.3<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> relative improvement, underscoring our method&#8217;s applicability.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This finding is consistent in AAC results. In Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the similarity score of Diffusion-Bridge is similar to that of Diffusion-Link when </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is between 300 and 400. This suggests that the performance of Diffusion-Bridge corresponds to over-noising of Diffusion-Link, which aligns with the observed </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">semantic information loss</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Furthermore, under the same multimodal LLM system, the AAC results in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> follow the same pattern: attaching Diffusion-Link yields large gains, whereas using Diffusion-Bridge provides only limited improvements. Together, the three tables show that </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">excessive</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> forward noising reduces similarity and weakens bridging, which in turn harms downstream performance; conversely, choosing an appropriate </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> maximizes content preservation in the text-like embedding, strengthens conditioning-distribution alignment for the LLM decoder, and translates into AAC gains.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Contrastive audio&#8211;language pretraining yields powerful joint representations, yet a persistent audio&#8211;text modality gap limits the benefits of coupling multimodal encoders with large language models (LLMs). We present Diffusion-Link, a diffusion-based modality-bridging module that generatively maps audio embeddings into the text-embedding distribution. The module is trained at the output embedding from the frozen multimodal encoder and implemented as a lightweight network with three residual MLP blocks. To assess the effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on Automatic Audio Captioning (AAC); to our knowledge, this is the first application of diffusion-based modality bridging to AAC. We report two results. (1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link reduces the modality gap the most among prior diffusion-based methods and shows a collective migration of audio embeddings toward the text distribution. (2) Downstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline achieves state-of-the-art on AudioCaps in both zero-shot and fully supervised captioning without external knowledge, with relative gains up to 52.5<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> and 7.5<math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>, respectively. These findings show that closing the modality gap is pivotal for effective coupling between multimodal encoders and LLMs, and diffusion-based modality bridging offers a promising direction beyond knowledge-retrieval-centric designs. Code will be released upon acceptance<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text\" style=\"font-size:111%;\">1</span></span><span class=\"ltx_text\" style=\"font-size:111%;\">Official code: </span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/DevKiHyun/Diffusion-Link\" style=\"font-size:111%;\" title=\"\">https://github.com/DevKiHyun/Diffusion-Link</a></span></span></span>.</span>\n</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "diffusionlink",
                    "supervised",
                    "zeroshot",
                    "fully",
                    "modality",
                    "baseline",
                    "bridging",
                    "diffusionbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\ndiffusion probabilistic model, modality gap, large language model, audio captioning, multimodal representation learning</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large-scale audio&#8211;language models have shown strong multimodal performance across a range of multimodal tasks.\nIn particular, CLAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> maps natural-language descriptions and acoustic signals into a shared embedding space via contrastive learning, achieving state-of-the-art results on various audio&#8211;language multimodal tasks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib3\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">3</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In parallel, advances in LLMs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> enable coupling contrastive audio&#8211;language encoders with powerful decoders, already demonstrating compelling audio&#8211;language reasoning and captioning&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Yet recent studies reveal a structural modality gap in contrastive multimodal encoders. Liang et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib9\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">9</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> quantified the gap and linked its magnitude to zero-shot performance and fairness, while Zhang et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib10\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">10</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> analyzed embedding geometry and showed that gap reduction benefits cross-modal tasks. From an application angle, linking contrastive spaces&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> via mediating modalities enables unpaired transfer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib12\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">12</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and broader alignment across audio&#8211;vision&#8211;text&#8211;3D yields competitive zero-shot results&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib13\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">13</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Taken together, these prior works suggest that addressing the modality gap is essential for improving zero-shot and cross-modal task performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "modality",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Diffusion models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have become a standard generative paradigm in various fields, reliably producing high-fidelity samples&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. They learn a forward noising process toward an isotropic Gaussian and a reverse denoising process back to the target distribution. Viewing embedding vector as data, diffusion can learn a trajectory that bridges the embedding distributions between two modalities.\nWe adopt this view and design a reverse process that first moves audio embeddings to a shared isotropic Gaussian waypoint and then maps them into the text-embedding distribution, thereby enabling effective modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "modality",
                    "bridging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent embedding-generative works support this view. In speaker recognition, SEED&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib19\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">19</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> applies the forward process to both clean and noisy speaker embeddings and trains the reverse process to regenerate the clean speaker embeddings, introducing cross-sample prediction and demonstrating embedding-level generation. In vision&#8211;language, Diffusion-Bridge&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">20</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> trains only on CLIP text embeddings and injects image embeddings at an intermediate reverse step to convert them into text-like vectors&#8211;an early instance of embedding-space modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "modality",
                    "diffusionbridge",
                    "bridging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion-Link</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which directly bridges the audio&#8211;text modality gap, building on prior works&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The key idea is to (i) use paired audio&#8211;text embeddings from an audio-language multimodal encoder during training to explicitly connect the two distributions, and (ii) achieve modality bridging by enforcing that the reverse process always map to the text embedding distribution. To this end, we gradually inject Gaussian noise into both embeddings in the forward process to send them to a common isotropic Gaussian state, and train with an L2 reconstruction loss so that the reverse process consistently predicts embeddings from the text distribution. Moreover, we add a topology loss that preserves the relative geometry of the text distribution by matching the within-batch cosine similarity structure of the original text and the generated text-like embeddings. At inference, Diffusion-Link outputs a text-like embedding regardless of the input modality. Diffusion-Link is a lightweight network composed of three residual multilayer perceptron (MLP) blocks, and the multimodal encoder is frozen during training. For practical validation, we attach Diffusion-Link after multimodal encoder as a plug-in and combine it with a LLM-based decoder to evaluate audio captioning. To our knowledge, this is the first attempt to apply diffusion-based modality bridging to audio captioning.</span>\n</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "diffusionlink",
                    "modality",
                    "bridging",
                    "diffusionbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We verify consistent gains on the AudioCaps&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib21\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">21</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset along two axes: modality-gap analysis and LLM-based downstream tasks. On similarity and geometric criteria, Diffusion-Link increases the similarity of paired audio&#8211;text samples while decreasing that of unpaired, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">achieving the largest gap reduction</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> over prior methods. Visualizations further show a clear collective migration of audio embeddings toward the text-embedding distribution after the diffusion process. In Automatic Audio Captioning (AAC), attaching Diffusion-Link as a plug-in to the same multimodal LLM baseline yields relative improvements of up to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in zero-shot audio captioning and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in fully supervised audio captioning, reaching </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">state-of-the-art in both cases</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without external knowledge. Because many existing systems, especially in zero-shot, rely on external knowledge such as retrieval-augmented generation (RAG), these results establish Diffusion-Link as a new powerful solution that achieves consistent gains on the same multimodal LLM system while shifting the source of performance from knowledge retrieval to modality bridging.</span>\n</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "diffusionlink",
                    "supervised",
                    "zeroshot",
                    "fully",
                    "modality",
                    "baseline",
                    "bridging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We consider two training options for our LLM decoder framework:\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(i) text-only training</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: using text-driven </span>\n  <math alttext=\"\\hat{\\mathbf{e}}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119838;</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{\\mathbf{e}}^{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with an instruction prompt.\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(ii) fully supervised training</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: using audio-driven </span>\n  <math alttext=\"\\hat{\\mathbf{e}}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119838;</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\hat{\\mathbf{e}}^{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and we not use an instruction prompt.\nWe train the LLM decoder with the standard autoregressive cross-entropy objective</span>\n</p>\n\n",
                "matched_terms": [
                    "fully",
                    "supervised"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\psi\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#968;</mi>\n      <annotation encoding=\"application/x-tex\">\\psi</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the LLM decoder&#8217;s learnable parameters and </span>\n  <math alttext=\"\\mathbf{w}=(w_{1},\\dots,w_{L})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119856;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">w</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">w</mi>\n            <mi mathsize=\"0.900em\">L</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{w}=(w_{1},\\dots,w_{L})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> means the target caption tokens.\nAt inference, given audio data only with optional instruction prompt, and decode target caption.\nWhen the LLM decoder is trained under the text-only training, this evaluation corresponds to zero-shot captioning.</span>\n</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For audio-language multimodal encoder, we use the LAION-CLAP pretrained model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib2\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">2</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and keep it frozen.\nFollowing prior work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib20\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">20</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we apply the same normalization process to the output embeddings of CLAP. For Diffusion-Link, we adopt three residual MLP blocks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib31\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">31</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We train Diffusion-Link with the Adam&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib32\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">32</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer and a batch size of </span>\n  <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">128</mn>\n      <annotation encoding=\"application/x-tex\">128</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The base learning rate is set to </span>\n  <math alttext=\"1{\\times}10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1{\\times}10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and follows a step-decay schedule, multiplying the rate by </span>\n  <math alttext=\"0.97\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.97</mn>\n      <annotation encoding=\"application/x-tex\">0.97</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> every </span>\n  <math alttext=\"200\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">200</mn>\n      <annotation encoding=\"application/x-tex\">200</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps. We employ an exponential moving average (EMA) of the model parameters with a decay of </span>\n  <math alttext=\"0.995\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.995</mn>\n      <annotation encoding=\"application/x-tex\">0.995</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and use the EMA weights for inference. We adopt a cosine noise schedule with a total of </span>\n  <math alttext=\"T{=}1000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">T</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1000</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">T{=}1000</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> timesteps.\nAt inference, we employ DDIM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib15\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">15</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sampling with 5 iteration steps. Before denoising, we apply a shallow forward noising to </span>\n  <math alttext=\"s_{\\ast}{=}100\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo mathsize=\"0.900em\">&#8727;</mo>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">100</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}{=}100</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and then run the reverse procoess. For LLM-based text decoder, we adopt LLaMA2(7B)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib5\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">5</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the LLM decoder. In the text-only training, we employ a linear layer with soft prefix tokens </span>\n  <math alttext=\"m{=}1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m{=}1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the project head and prepend a short instruction prompt; in the fully supervised training, we use 2 linear layers with </span>\n  <math alttext=\"m{=}10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">10</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m{=}10</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the project head and no hard prompt. We fine-tune project head and the LLM using LoRA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib33\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">33</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. LLM training uses AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib34\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">34</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer with batch size </span>\n  <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">4</mn>\n      <annotation encoding=\"application/x-tex\">4</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for </span>\n  <math alttext=\"50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m11\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">50</mn>\n      <annotation encoding=\"application/x-tex\">50</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> epochs: the learning rate warms up over the first </span>\n  <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m12\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2</mn>\n      <annotation encoding=\"application/x-tex\">2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> epochs with max learning rate </span>\n  <math alttext=\"5{\\times}10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m13\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">6</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">5{\\times}10^{-6}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, then use a cosine decay. We also train a baseline multimodal LLM system to verify the effectiveness of Diffusion-Link, we adopt same setting but detach only Diffusion-Link module. For evaluation, we adopt the metrics for modality gap analyzing, including cosine simiarity and visualization using UMAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib35\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">35</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For AAC, we use the metrics, METEOR (ME)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib36\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">36</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, CIDEr (CD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib37\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">37</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SPICE (SP)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib38\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">38</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and SPIDEr (SD)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#bib.bib39\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">39</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "llama27b",
                    "diffusionlink",
                    "supervised",
                    "fully",
                    "modality",
                    "baseline",
                    "clap",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Effectiveness of Diffusion-Link for Modality Bridging.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Diffusion-Link attains the highest cosine similarity on matched audio&#8211;text pairs.\nWhile most approaches improve over CLAP, DiffGap underperforms, because it generates from pure Gaussian noise with the input embedding condition, which weakens information reconstruction.\nBy contrast, Diffusion-Link treat the input embedding as residing at an intermediate reverse step, thereby minimizing information loss and ensuring high-quality generation along the reverse trajectory.\nImportantly, Diffusion-Link also yields the lowest similarity on non-matched pairs, indicating not merely a global contraction of the space but maintaining semantic information.\nFigure &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.2.2 Inference to generate text-like embedding &#8227; 2.2 Modality Gap Bridging via Diffusion-Link &#8227; 2 Method &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> visualizes this effect. Both the generated text-like embeddings from audio and text embeddings all move toward the ground-true text embedding distribution, demonstrating that Diffusion-Link has learned a stable generative modality bridge for the text embedding distribution, regardless of the input modality.</span>\n</p>\n\n",
                "matched_terms": [
                    "diffusionlink",
                    "modality",
                    "bridging",
                    "clap",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares a range of AAC systems. In contrast to many prior methods that leverage longer audio representations or external knowledge (e.g., RAG), our multimodal LLM system captures input audio feature with only a single </span>\n  <math alttext=\"1{\\times}D\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mi mathsize=\"0.900em\">D</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1{\\times}D</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> text-like embedding produced by Diffusion-Link, and achieves SOTA in both zero-shot and fully supervised captioning </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">without external knowledge</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Notably, considering that most prior zero-shot models rely heavily on external knowledge, outperforming them without any external knowledge demonstrates the significant efficiency of Diffusion-Link.</span>\n</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "diffusionlink",
                    "supervised",
                    "zeroshot",
                    "fully"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct ablations to analyze how the depth of forward noising affects modality bridging and high-quality generation. According to Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11330v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1 Datasets &#8227; 3 Experimental Settings &#8227; Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, increasing the inference forward timestep </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from shallow levels initially keeps similarity quite stable; beyond a threshold, the similarity drops sharply as </span>\n  <math alttext=\"s_{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8727;</mo>\n      </msub>\n      <annotation encoding=\"application/x-tex\">s_{\\ast}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> increases. This indicates that over-noising pushes representations deeper into the common Gaussian space and </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">erases information</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, thereby degrading semantic preservation in the reconstructed text-like embeddings.</span>\n</p>\n\n",
                "matched_terms": [
                    "modality",
                    "bridging",
                    "analyze"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduced </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Diffusion-Link</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a lightweight residual MLP diffusion module that bridges audio embeddings to the text embedding distribution without keeping the multimodal encoder frozen. The method aligns the conditioning input by increasing matched similarity and decreasing mismatched similarity. On AAC, it improves the same multimodal LLM baseline by </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">52.5%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.3%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> without external knowledge for zero-shot and fully supervised AAC, respectively. This plug-in-play approach of Diffusion-Link is expected to generalize beyond audio captioning and enable effective zero-shot performance in a variety of multimodal LLMs.</span>\n</p>\n\n",
                "matched_terms": [
                    "captioning",
                    "diffusionlink",
                    "supervised",
                    "zeroshot",
                    "fully",
                    "baseline",
                    "method"
                ]
            }
        ]
    }
}