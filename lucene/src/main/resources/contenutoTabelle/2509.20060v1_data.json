{
    "S4.T1": {
        "source_file": "Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens",
        "caption": "Table 1: Reconstruction performance of AR-based and DDM-based TASTE models on LibriSpeech. ”No-VQ” denotes models without vector quantization, while ”4L-RVQ” denotes models with a 4-layer RVQ quantizer.",
        "body": "Model\ntest-clean\ntest-other\n\n\n\nWER (%) ↓\\downarrow\n\n\nUT-MOS ↑\\uparrow\n\n\nSpkSim-W ↑\\uparrow\n\n\nWER (%) ↓\\downarrow\n\n\nUT-MOS ↑\\uparrow\n\n\nSpkSim-W ↑\\uparrow\n\n\n\n\n\nOriginal\n1.80\n4.09\n1.00\n3.80\n3.50\n1.00\n\n\nS3-Oracle\n2.69\n4.15\n0.96\n6.74\n3.71\n0.96\n\n\nAR-No-VQ\n2.81\n4.15\n0.96\n8.15\n3.73\n0.95\n\n\nDDM-No-VQ\n2.99\n4.23\n0.96\n7.50\n3.85\n0.96\n\n\nAR-4L-RVQ\n7.60\n3.82\n0.95\n16.50\n3.33\n0.93\n\n\nDDM-4L-RVQ\n5.10\n4.27\n0.94\n10.84\n3.97\n0.94",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">test-clean</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">test-other</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WER (%) </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">UT-MOS </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-W </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WER (%) </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">UT-MOS </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-W </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Original</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">S3-Oracle</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.74</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.96</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">AR-No-VQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.95</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">DDM-No-VQ</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.99</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.23</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.85</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.96</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">AR-4L-RVQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.93</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">DDM-4L-RVQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">10.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.94</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "quantization",
            "ar4lrvq",
            "librispeech",
            "↓downarrow",
            "utmos",
            "testclean",
            "arbased",
            "taste",
            "ddmbased",
            "4layer",
            "wer",
            "original",
            "model",
            "spksimw",
            "without",
            "arnovq",
            "denotes",
            "performance",
            "↑uparrow",
            "rvq",
            "reconstruction",
            "testother",
            "quantizer",
            "models",
            "”4lrvq”",
            "”novq”",
            "ddmnovq",
            "while",
            "s3oracle",
            "ddm4lrvq",
            "vector"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To compare AR- and DDM-based TASTE models, we evaluate both against ground-truth signals on the LibriSpeech test sets. Results are shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 AR and DDM Baselines &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We report performance for the original speech (Original), reconstructions from ground-truth S3 sequences (S3-Oracle), models without vector quantization (-No-VQ) from the first training stage, and models with a 4-layer RVQ module (-4L-RVQ), following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with 512 tokens of 256 dimensions per layer.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 AR and DDM Baselines &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, both AR- and DDM-based models perform well without vector quantization (rows 3&#8211;4), showing only slight WER increases and no loss in quality or speaker similarity compared to </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">S3-Oracle</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, adding RVQ causes significant degradation for the AR-based model: WER jumps from 2.81% to 7.60% and UT-MOS drops from 4.15 to 3.82 on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (row 5). In contrast, the DDM-based model shows only a modest WER rise (from 2.99% to 5.10%) while maintaining high UT-MOS, yielding much better reconstruction quality. Similar trends are seen on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-other</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper introduces a discrete diffusion model (DDM) framework for text-aligned speech tokenization and reconstruction. By replacing the auto-regressive speech decoder with a discrete diffusion counterpart, our model achieves significantly better reconstruction quality, stronger ASR performance, and faster inference. We provide a comprehensive analysis of applying DDMs to speech reconstruction, examining sampler choices, inference steps, and robustness to length-scale estimation errors. Furthermore, we improve the original TASTE by systematically comparing vector quantization modules, showing that FSQ yields up to a 35% relative WER reduction and +0.14 UT-MOS improvement over RVQ for AR models, while also enhancing DDM performance. Our model generates speech in just 10 denoising steps and even supports single-step generation with only minor quality degradation.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "quantization",
                    "performance",
                    "models",
                    "reconstruction",
                    "utmos",
                    "while",
                    "wer",
                    "original",
                    "vector",
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nDiscrete Diffusion Model, Speech Tokenization, Speech Reconstruction</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In recent years, </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">discrete diffusion models</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (DDMs) have emerged as a powerful generative modeling framework for language and sequence modeling. Originating from the Discrete Denoising Diffusion Probabilistic Models (D3PM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, this line of research has advanced rapidly through extensions such as the Masked Diffusion Language Model (MDLM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Discrete Flow Matching&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ReMDM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Edit-Flow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Unlike traditional autoregressive (AR) models that require strictly sequential decoding, DDMs perform inference through iterative but inherently parallelizable denoising steps. This property enables efficient sampling, flexible trade-offs between speed and quality, and greater controllability in generation. Such advantages have driven their adoption in large-scale language models including LLaDA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Mercury&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we present the first comprehensive study of DDMs for speech tokenization and reconstruction. Our approach builds on the TASTE framework&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which aligns speech tokens with transcription tokens to produce highly efficient, text-aligned, low-bitrate speech representations well-suited for spoken language models. However, the original TASTE relies on an AR speech decoder to predict S3 token sequences&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> before time-domain signal reconstruction, resulting in inefficiency and suboptimal generation. To address this limitation, we propose to replace the AR decoder with a DDM-based decoder and conduct a thorough analysis of its effectiveness on speech reconstruction tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "models",
                    "taste",
                    "original",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In summary, this work makes three main contributions. First, we present the first application of DDMs to speech tokenization and reconstruction, achieving higher reconstruction quality and significantly faster inference than the AR-based TASTE baseline. Second, we enhance TASTE&#8217;s performance through a systematic analysis of quantizer design. Finally, we provide a comprehensive evaluation of inference settings&#8212;including sampler choice, number of steps, and robustness to length estimation errors&#8212;offering insights and guidance for deploying DDMs in speech applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "quantizer",
                    "reconstruction",
                    "arbased",
                    "performance",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The foundation of discrete diffusion models can be traced back to D3PM introduced by Austin </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">et al.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which extended the continuous diffusion framework to categorical state spaces and provided a principled way of adapting diffusion models to discrete domains. A widely studied special case of D3PM is the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">masking</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">absorbing state</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> discrete diffusion process. Its forward process </span>\n  <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">q</mi>\n      <annotation encoding=\"application/x-tex\">q</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> can be formally defined as follows. Let </span>\n  <math alttext=\"\\mathbf{x}\\in\\{0,1\\}^{|V|}\\subset\\Delta^{|V|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n            <mn mathsize=\"0.900em\">0</mn>\n            <mo mathsize=\"0.900em\">,</mo>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n          </mrow>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n            <mi mathsize=\"0.900em\">V</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n          </mrow>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8834;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n            <mi mathsize=\"0.900em\">V</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\{0,1\\}^{|V|}\\subset\\Delta^{|V|}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denote a one-hot vector representing a clean token, where </span>\n  <math alttext=\"|V|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n        <mi mathsize=\"0.900em\">V</mi>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">|V|</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the vocabulary size and </span>\n  <math alttext=\"\\Delta^{|V|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n          <mi mathsize=\"0.900em\">V</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\Delta^{|V|}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the probability simplex over this vocabulary. Given a time step </span>\n  <math alttext=\"t\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t\\in[0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the marginal distribution of an intermediate latent variable </span>\n  <math alttext=\"\\mathbf{z}_{t}\\in\\{0,1\\}^{|V|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119859;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n            <mn mathsize=\"0.900em\">0</mn>\n            <mo mathsize=\"0.900em\">,</mo>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n          </mrow>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n            <mi mathsize=\"0.900em\">V</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{z}_{t}\\in\\{0,1\\}^{|V|}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "vector"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">TASTE&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a recently proposed framework for </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">text-aligned speech tokenization</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, designed to enable more effective joint speech and text modeling for spoken language model applications. Its key idea is to align speech tokens with their corresponding text tokens during tokenization, thereby creating a one-to-one correspondence between text and speech representations. This framework addresses the common </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">token length mismatch problem</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between speech and text modalities&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which has been a major challenge for large spoken language models.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 TASTE: Text-Aligned Speech Tokenization &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(a), TASTE consists of two main components: a </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TASTE tokenizer</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> which produces text-aligned speech tokens and an AR </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">speech decoder</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> which reconstructs speech from the transcript and the aligned TASTE tokens.\nThe tokenizer consists of an encoder, an aggregator, and a quantizer. The encoder, initialized from the Whisper encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, extracts acoustic features that are processed by an </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">aggregator</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, initialized from the Whisper decoder, to align the acoustic features with the corresponding text tokens through cross-attention. Finally, a quantizer based on residual vector quantization (RVQ)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> discretizes the aligned acoustic representations into the TASTE speech tokens.</span>\n</p>\n\n",
                "matched_terms": [
                    "quantizer",
                    "quantization",
                    "vector",
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For reconstruction, TASTE employs a Transformer-based speech decoder that conditions on both the transcript and the continuous TASTE embedding vectors (rather than their discrete indices). The decoder autoregressively predicts S3 tokens from CosyVoice&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which serve as intermediate representations between TASTE tokens and waveforms. These predicted S3 tokens are then converted into mel-spectrograms using a pretrained flow-matching S3-to-mel converter, followed by waveform generation with a HiFi-GAN vocoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The overall training objective combines two losses: (1) a cross-entropy loss between the predicted and ground-truth S3 tokens, and (2) a commitment loss from the RVQ quantizer to encourage stable and consistent token assignments.</span>\n</p>\n\n",
                "matched_terms": [
                    "reconstruction",
                    "quantizer",
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While TASTE has demonstrated its ability to achieve high-quality speech reconstruction at extremely low bitrates and straightforward adaptation to joint speech&#8211;text modeling, it also suffers from a major drawback. Unlike most speech and audio tokenizers that support parallel decoding for reconstruction, TASTE relies on an AR speech decoder due to the dynamic time alignment between its tokens and speech frames. This requirement makes the decoding process rather slow, particularly for long speech signals. To address this limitation, we propose replacing the AR-based speech decoder with a discrete diffusion decoder. Interestingly, we find that this modification not only accelerates decoding but also improves reconstructed speech quality, as demonstrated in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4\" style=\"font-size:90%;\" title=\"4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "taste",
                    "while",
                    "reconstruction",
                    "arbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We follow the original structure in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 TASTE: Text-Aligned Speech Tokenization &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(a) to implement the AR-based TASTE baseline with several modifications. First, we replace the pretrained </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">whisper-large-v3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> ASR model</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">canary-180m-flash<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text ltx_font_serif\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib24\" title=\"\">24</a><span class=\"ltx_text ltx_font_serif\">]</span></cite></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from NVIDIA NeMo</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for both encoder and aggregator initialization. Second, we use the final-layer encoder outputs for both keys and values in the aggregator&#8217;s cross-attention. Third, we proposed to use finite scalar quantization (FSQ)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as an alternative to RVQ, which improves performance (see Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS2\" style=\"font-size:90%;\" title=\"4.2 Effect of Vector Quantization &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Fourth, we adopt an encoder&#8211;decoder architecture instead of a decoder-only design. Finally, we discard the speaker embedding input, finding it unnecessary for reconstruction quality. This is consistent with findings reported in CosyVoice2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "quantization",
                    "reconstruction",
                    "arbased",
                    "original",
                    "performance",
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The proposed DDM-based TASTE model shares the same architecture as the AR baseline, differing only in decoder input (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 TASTE: Text-Aligned Speech Tokenization &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(b)): instead of conditioning on previous S3 tokens </span>\n  <math alttext=\"S_{1:n-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mrow>\n            <mi mathsize=\"0.900em\">n</mi>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">1</mn>\n          </mrow>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{1:n-1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the DDM decoder takes </span>\n  <math alttext=\"S_{\\text{mask}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mtext mathsize=\"0.900em\">mask</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{\\text{mask}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a partially masked version of the target sequence </span>\n  <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">S</mi>\n      <annotation encoding=\"application/x-tex\">S</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We follow the MDLM framework</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for training but use a modified objective compared to&#160;(</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.E3\" style=\"font-size:90%;\" title=\"In 2.1 Discrete Diffusion Models &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">):</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "model",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">which is more stable and lead to consistently better results in our experiments. The overall training loss consists of the the cross entropy loss&#160;(</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.E4\" style=\"font-size:90%;\" title=\"In 2.3 Proposed Model &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and the commitment loss for the RVQ quantizer, or just the cross entropy loss&#160;(</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.E4\" style=\"font-size:90%;\" title=\"In 2.3 Proposed Model &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) for the FSQ quantizer.</span>\n</p>\n\n",
                "matched_terms": [
                    "quantizer",
                    "rvq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Unlike the original TASTE work trained on Emilia and LibriTTS, our models use the Granary English-only dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which contains approximately 275k hours of speech. Note that Granary is much larger but also noisier, enabling us to evaluate TASTE&#8217;s robustness on ASR-style data. For evaluation, we use the LibriSpeech </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-other</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sets to assess performance under clean and noisy conditions, respectively.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "librispeech",
                    "testclean",
                    "original",
                    "performance",
                    "testother",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We employ a variety of metrics to evaluate speech reconstruction. Word Error Rate (WER) is computed by transcribing the reconstructed speech using NVIDIA&#8217;s FastConformer-Transducer-Large ASR model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Perceptual quality is measured with </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Squim-PESQ</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Squim-SISDR</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, along with </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">WV-MOS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">UT-MOS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Speaker similarity is measured by extracting embeddings with WavLM</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and TitaNet&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and computing cosine similarities, reported as </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-W</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively.</span>\n</p>\n\n",
                "matched_terms": [
                    "spksimw",
                    "model",
                    "reconstruction",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Training is conducted on 32 NVIDIA A100 GPUs, with each GPU processing an average batch equivalent to roughly 375 seconds of audio. Following the two-stage training procedure of the original TASTE&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we first pre-train the model without the quantizer block for 100k steps. In the second stage, we add either the FSQ or RVQ quantizer and freeze the encoder and continue training for an additional 150k steps. The total number of trainable parameters is approximately 316M. We use the Adam optimizer in both training stages, with the learning rate linearly warmed up to </span>\n  <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> over the first 5,000 steps and subsequently decayed to </span>\n  <math alttext=\"10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">6</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-6}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using cosine annealing. Dropout rates between 0% and 20% were explored, where 10% proved most effective for AR-based TASTE, while 0% yielded the best results for DDM-based TASTE.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "model",
                    "quantizer",
                    "without",
                    "while",
                    "arbased",
                    "original",
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For inference with DDM-based TASTE, we adopt the confidence-based Top-K (Conf-TopK) sampler from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the default strategy, with the number of inference steps set to 50 unless otherwise noted. In this sampling approach, the unmasking order of tokens plays a crucial role: at each step, the model uses its own output logits to estimate confidence scores for all masked positions, then selects the </span>\n  <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">K</mi>\n      <annotation encoding=\"application/x-tex\">K</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> tokens with the highest confidence to unmask first&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The comparisons between samplers is shown in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS3\" style=\"font-size:90%;\" title=\"4.3 Effect of Sampler Choice &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We further assume that the number of S3 tokens to be predicted is known during inference. Although this information is unavailable in practice, it can be estimated using a global length predictor conditioned on the input text tokens. The effect of inaccurate length estimation is analyzed in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS5\" style=\"font-size:90%;\" title=\"4.5 Effect of Length Estimation Errors &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "model",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we present the speech reconstruction results of our AR- and DDM-based TASTE models. We encourage readers to explore the audio samples on our demo page</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://kuray107.github.io/DDMs_on_taste26_examples/demo\" title=\"\">https://kuray107.github.io/DDMs_on_taste26_examples/demo</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">to complement the quantitative results, particularly for Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS4\" style=\"font-size:90%;\" title=\"4.4 Effect of Number of Inference Steps &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS5\" style=\"font-size:90%;\" title=\"4.5 Effect of Length Estimation Errors &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "models",
                    "reconstruction",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Beyond reconstruction quality, DDM-based TASTE is also significantly more computationally efficient, requiring at most 50 inference steps compared to roughly 370 steps on average for AR-based TASTE. On the </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> set, our DDM-based model reconstructs an utterance in about 1.65 seconds, making it </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.3<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> faster</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> than the AR baseline, which requires an average of 5.48 seconds.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "model",
                    "reconstruction",
                    "testclean",
                    "arbased",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To study the impact of vector quantization, we trained AR and DDM variants with different VQ configurations, comparing RVQ and FSQ modules with 2, 4, or 8 layers, yielding six variants per model. Results in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 AR and DDM Baselines &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> show that across all configurations, DDM-based TASTE consistently outperforms its AR counterparts, particularly in WER, demonstrating superior efficiency and reconstruction quality. FSQ also clearly outperforms RVQ, especially for AR models, reducing WER by about 35% on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and 31% on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-other</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while improving UT-MOS by +0.14 and +0.31, respectively. For DDM, RVQ already performs strongly, so FSQ provides smaller but consistent WER gains. Lastly, while using fewer VQ layers degrades AR performance across all metrics, DDM remains robust, though speaker similarity drops noticeably.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "model",
                    "quantization",
                    "models",
                    "reconstruction",
                    "utmos",
                    "testclean",
                    "testother",
                    "while",
                    "wer",
                    "performance",
                    "vector",
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Next, to evaluate the impact of sampler choice on DDM inference, we compare the default confidence-based TopK (Conf-TopK) sampler with four alternatives: Ancestral&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, confidence-based TopP (Conf-TopP)</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and two ReMDM variants (ReMDM-Cap and ReMDM-Loop)</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which allow non-masked tokens to be remasked during the denoising process. Results with the DDM-4L-FSQ TASTE model are shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.3 Proposed Model &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where confidence-based samplers (Conf-TopK and Conf-TopP) achieve the best performance, while Ancestral and ReMDM perform worse&#8212;despite often being reported to produce more diverse outputs in unconditional text generation</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This is likely because confidence-based greedy sampling introduces a strong bias toward highly probable tokens, which can hurt tasks like unconditional text generation where diversity is crucial and context is limited. In our setting, however, speech reconstruction is strongly conditioned on text and TASTE embeddings, making diversity less important. This strong conditioning makes confidence-based sampling more effective, consistently producing higher-quality reconstructions than more stochastic methods.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "while",
                    "taste",
                    "performance",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As discussed in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.SS1\" style=\"font-size:90%;\" title=\"2.1 Discrete Diffusion Models &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">2.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a major advantage of DDMs over AR models is the flexibility to adjust the number of inference steps, trading off efficiency and generation quality. To evaluate this, we tested the DDM-4L-FSQ model with 1, 10, 25, 50, and 100 steps. The Conf-TopK sampler is used in all cases except for single-step inference, where the sampler choice has no effect. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.3 Effect of Sampler Choice &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the model achieves strong performance with as few as 10 steps, showing no degradation across metrics. In fact, 10 steps yield the lowest WER, underscoring DDM&#8217;s ability to provide both efficiency and high-quality reconstruction. Even single-step inference remains usable, with only modest degradations in WER (from 4.00% to 5.14%) and UT-MOS (from 4.30 to 3.81), demonstrating that DDMs can deliver significantly faster speech reconstruction than AR baselines while maintaining high quality.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "utmos",
                    "while",
                    "wer",
                    "performance",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Finally, we revisit the assumption in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Training and Inference Setup &#8227; 3 Experimental Setup &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that the model has access to the global S3 token length during inference, which in practice requires an auxiliary length predictor. To analyze its impact, we evaluate the DDM-4L-FSQ model under length scales ranging from 70% to 130% of the original sequence, measuring insertion, deletion, and substitution rates. Results are shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.5 Effect of Length Estimation Errors &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. When the inference length is shorter than the ground truth, the model maintains the same speaking rate and truncates the sequence tail, causing higher deletion and substitution rates, while insertion remains stable. In contrast, when the length is longer, it simply appends silence tokens with minimal ASR degradation. These results suggest that overestimation is relatively safe, whereas underestimation leads to severe errors. A promising future direction is to extend the DDM to predict an end-of-sequence (EOS) token</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, eliminating the need for an external length predictor.</span>\n</p>\n\n",
                "matched_terms": [
                    "original",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we presented the first comprehensive study of applying discrete diffusion to speech tokenization and reconstruction within the TASTE framework. The proposed discrete diffusion-based decoder achieves both higher reconstruction quality and substantially faster inference than the AR baseline, requiring only ten denoising steps without performance degradation and even supporting single-step generation with minor degradation. Experiments further demonstrated FSQ outperforms RVQ in quantizer design, confidence-based samplers are better suited for speech reconstruction than more stochastic alternatives, and DDMs remain robust under varying inference lengths. These results establish DDM-based TASTE as a more efficient and flexible alternative to the AR baseline, offering practical benefits for future speech applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "quantizer",
                    "without",
                    "reconstruction",
                    "performance",
                    "rvq",
                    "taste"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens",
        "caption": "Table 2: Comparison of AR- and DDM-based TASTE models with different vector quantization settings (2/4/8 layers of RVQ or FSQ). DDM-based TASTE consistently outperforms AR counterparts across all metrics, and FSQ provides clear gains over RVQ.",
        "body": "AR\nBitrate\ntest-clean\ntest-other\n\n\n\nWER (%) ↓\\downarrow\n\n\nUT-MOS ↑\\uparrow\n\n\nSQUIM-PESQ ↑\\uparrow\n\n\nSpkSim-T ↑\\uparrow\n\n\nSpkSim-W ↑\\uparrow\n\n\nWER (%) ↓\\downarrow\n\n\nUT-MOS ↑\\uparrow\n\n\nSQUIM-PESQ ↑\\uparrow\n\n\nSpkSim-T ↑\\uparrow\n\n\nSpkSim-W ↑\\uparrow\n\n\n\n2L-RVQ\n95\n10.07\n3.70\n3.53\n0.64\n0.93\n16.22\n3.20\n3.23\n0.60\n0.92\n\n\n2L-FSQ\n95\n6.36\n3.93\n3.55\n0.62\n0.93\n12.20\n3.50\n3.25\n0.58\n0.91\n\n\n4L-RVQ\n190\n7.60\n3.82\n3.56\n0.68\n0.95\n16.50\n3.33\n3.23\n0.64\n0.93\n\n\n4L-FSQ\n190\n4.87\n4.07\n3.65\n0.66\n0.94\n10.70\n3.67\n3.37\n0.63\n0.93\n\n\n8L-RVQ\n380\n7.94\n3.92\n3.59\n0.71\n0.95\n16.47\n3.39\n3.25\n0.67\n0.94\n\n\n8L-FSQ\n380\n5.14\n4.13\n3.70\n0.71\n0.95\n10.94\n3.71\n3.40\n0.68\n0.95\n\n\nDDM\nBitrate\ntest-clean\ntest-other\n\n\n\nWER (%) ↓\\downarrow\n\n\nUT-MOS ↑\\uparrow\n\n\nSQUIM-PESQ ↑\\uparrow\n\n\nSpkSim-T ↑\\uparrow\n\n\nSpkSim-W ↑\\uparrow\n\n\nWER (%) ↓\\downarrow\n\n\nUT-MOS ↑\\uparrow\n\n\nSQUIM-PESQ ↑\\uparrow\n\n\nSpkSim-T ↑\\uparrow\n\n\nSpkSim-W ↑\\uparrow\n\n\n\n2L-RVQ\n95\n5.18\n4.32\n3.84\n0.63\n0.93\n8.50\n4.08\n3.65\n0.59\n0.91\n\n\n2L-FSQ\n95\n3.95\n4.31\n3.84\n0.62\n0.92\n7.51\n4.08\n3.66\n0.56\n0.90\n\n\n4L-RVQ\n190\n5.10\n4.27\n3.80\n0.68\n0.94\n10.84\n3.97\n3.58\n0.65\n0.94\n\n\n4L-FSQ\n190\n4.00\n4.30\n3.82\n0.67\n0.95\n8.62\n4.00\n3.60\n0.64\n0.94\n\n\n8L-RVQ\n380\n4.20\n4.24\n3.78\n0.71\n0.95\n9.39\n3.89\n3.54\n0.69\n0.94\n\n\n8L-FSQ\n380\n3.99\n4.14\n3.79\n0.71\n0.95\n9.08\n3.90\n3.54\n0.69\n0.95",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Bitrate</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">test-clean</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">test-other</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WER (%) </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">UT-MOS </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SQUIM-PESQ </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-T </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-W </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WER (%) </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">UT-MOS </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SQUIM-PESQ </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-T </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-W </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2L-RVQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">95</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.92</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2L-FSQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">95</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.36</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.93</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.55</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.25</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.58</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.91</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4L-RVQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">190</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.60</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.33</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.23</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.64</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.93</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4L-FSQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">190</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.87</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.66</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">10.70</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.37</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.93</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8L-RVQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">380</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.92</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.59</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.47</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.39</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.25</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8L-FSQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">380</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.14</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.70</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.95</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DDM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Bitrate</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">test-clean</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">test-other</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WER (%) </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">UT-MOS </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SQUIM-PESQ </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-T </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-W </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WER (%) </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">UT-MOS </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SQUIM-PESQ </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-T </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-W </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2L-RVQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">95</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.91</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2L-FSQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">95</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.95</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.31</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.84</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.92</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.51</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.66</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.90</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4L-RVQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">190</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.10</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.27</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.80</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.84</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.97</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.58</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4L-FSQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">190</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.30</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.60</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.64</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8L-RVQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">380</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.24</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.78</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.39</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.89</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.54</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8L-FSQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">380</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.99</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.95</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "fsq",
            "over",
            "quantization",
            "↓downarrow",
            "consistently",
            "testclean",
            "utmos",
            "squimpesq",
            "8lfsq",
            "gains",
            "taste",
            "ddmbased",
            "ddm",
            "across",
            "all",
            "metrics",
            "2lrvq",
            "4lrvq",
            "counterparts",
            "wer",
            "clear",
            "layers",
            "4lfsq",
            "settings",
            "spksimw",
            "8lrvq",
            "↑uparrow",
            "2lfsq",
            "rvq",
            "testother",
            "spksimt",
            "bitrate",
            "outperforms",
            "models",
            "different",
            "comparison",
            "vector",
            "provides"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To study the impact of vector quantization, we trained AR and DDM variants with different VQ configurations, comparing RVQ and FSQ modules with 2, 4, or 8 layers, yielding six variants per model. Results in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 AR and DDM Baselines &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> show that across all configurations, DDM-based TASTE consistently outperforms its AR counterparts, particularly in WER, demonstrating superior efficiency and reconstruction quality. FSQ also clearly outperforms RVQ, especially for AR models, reducing WER by about 35% on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and 31% on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-other</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while improving UT-MOS by +0.14 and +0.31, respectively. For DDM, RVQ already performs strongly, so FSQ provides smaller but consistent WER gains. Lastly, while using fewer VQ layers degrades AR performance across all metrics, DDM remains robust, though speaker similarity drops noticeably.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper introduces a discrete diffusion model (DDM) framework for text-aligned speech tokenization and reconstruction. By replacing the auto-regressive speech decoder with a discrete diffusion counterpart, our model achieves significantly better reconstruction quality, stronger ASR performance, and faster inference. We provide a comprehensive analysis of applying DDMs to speech reconstruction, examining sampler choices, inference steps, and robustness to length-scale estimation errors. Furthermore, we improve the original TASTE by systematically comparing vector quantization modules, showing that FSQ yields up to a 35% relative WER reduction and +0.14 UT-MOS improvement over RVQ for AR models, while also enhancing DDM performance. Our model generates speech in just 10 denoising steps and even supports single-step generation with only minor quality degradation.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddm",
                    "fsq",
                    "over",
                    "quantization",
                    "models",
                    "utmos",
                    "wer",
                    "vector",
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we present the first comprehensive study of DDMs for speech tokenization and reconstruction. Our approach builds on the TASTE framework&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which aligns speech tokens with transcription tokens to produce highly efficient, text-aligned, low-bitrate speech representations well-suited for spoken language models. However, the original TASTE relies on an AR speech decoder to predict S3 token sequences&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> before time-domain signal reconstruction, resulting in inefficiency and suboptimal generation. To address this limitation, we propose to replace the AR decoder with a DDM-based decoder and conduct a thorough analysis of its effectiveness on speech reconstruction tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "models",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The foundation of discrete diffusion models can be traced back to D3PM introduced by Austin </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">et al.</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which extended the continuous diffusion framework to categorical state spaces and provided a principled way of adapting diffusion models to discrete domains. A widely studied special case of D3PM is the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">masking</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">absorbing state</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> discrete diffusion process. Its forward process </span>\n  <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">q</mi>\n      <annotation encoding=\"application/x-tex\">q</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> can be formally defined as follows. Let </span>\n  <math alttext=\"\\mathbf{x}\\in\\{0,1\\}^{|V|}\\subset\\Delta^{|V|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n            <mn mathsize=\"0.900em\">0</mn>\n            <mo mathsize=\"0.900em\">,</mo>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n          </mrow>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n            <mi mathsize=\"0.900em\">V</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n          </mrow>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8834;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n            <mi mathsize=\"0.900em\">V</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}\\in\\{0,1\\}^{|V|}\\subset\\Delta^{|V|}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denote a one-hot vector representing a clean token, where </span>\n  <math alttext=\"|V|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n        <mi mathsize=\"0.900em\">V</mi>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">|V|</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the vocabulary size and </span>\n  <math alttext=\"\\Delta^{|V|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n          <mi mathsize=\"0.900em\">V</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\Delta^{|V|}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the probability simplex over this vocabulary. Given a time step </span>\n  <math alttext=\"t\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t\\in[0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the marginal distribution of an intermediate latent variable </span>\n  <math alttext=\"\\mathbf{z}_{t}\\in\\{0,1\\}^{|V|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119859;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n            <mn mathsize=\"0.900em\">0</mn>\n            <mo mathsize=\"0.900em\">,</mo>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n          </mrow>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n            <mi mathsize=\"0.900em\">V</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{z}_{t}\\in\\{0,1\\}^{|V|}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is</span>\n</p>\n\n",
                "matched_terms": [
                    "vector",
                    "models",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mathsf{Cat}\\left(.;\\mathbf{p}\\right)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#120226;&#120250;&#120269;</mi>\n        <mrow>\n          <mo>(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0.167em\">.</mo>\n          <mo mathsize=\"0.900em\">;</mo>\n          <mi mathsize=\"0.900em\">&#119849;</mi>\n          <mo>)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathsf{Cat}\\left(.;\\mathbf{p}\\right)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the categorical distribution over </span>\n  <math alttext=\"|V|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n        <mi mathsize=\"0.900em\">V</mi>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">|V|</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> classes with probabilities </span>\n  <math alttext=\"\\mathbf{p}\\in\\Delta^{|V|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119849;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n            <mi mathsize=\"0.900em\">V</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{p}\\in\\Delta^{|V|}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\mathbf{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119846;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathbf{m}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a one-hot vector corresponding to a special </span>\n  <math alttext=\"[\\text{MASK}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m11\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mtext mathsize=\"0.900em\">MASK</mtext>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[\\text{MASK}]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> token, and </span>\n  <math alttext=\"\\alpha_{t}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m12\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#945;</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha_{t}\\in[0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a strictly decreasing schedule with </span>\n  <math alttext=\"\\alpha_{0}\\approx 1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m13\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#945;</mi>\n          <mn mathsize=\"0.900em\">0</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8776;</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha_{0}\\approx 1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\alpha_{1}\\approx 0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m14\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#945;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8776;</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha_{1}\\approx 0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For sequences of </span>\n  <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m15\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">L</mi>\n      <annotation encoding=\"application/x-tex\">L</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> tokens, we denote clean and noisy sequences as </span>\n  <math alttext=\"\\mathbf{x}^{(1:L)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m16\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n            <mi mathsize=\"0.900em\">L</mi>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}^{(1:L)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{z}_{t}^{(1:L)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m17\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119859;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n            <mi mathsize=\"0.900em\">L</mi>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{z}_{t}^{(1:L)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with individual tokens represented as </span>\n  <math alttext=\"\\mathbf{x}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m18\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">&#119857;</mi>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8467;</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{x}^{(\\ell)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{z}_{t}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m19\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">&#119859;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8467;</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\mathbf{z}_{t}^{(\\ell)}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for </span>\n  <math alttext=\"\\ell\\in\\{1,\\ldots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m20\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8467;</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">L</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\ell\\in\\{1,\\ldots,L\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "vector",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">TASTE&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a recently proposed framework for </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">text-aligned speech tokenization</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, designed to enable more effective joint speech and text modeling for spoken language model applications. Its key idea is to align speech tokens with their corresponding text tokens during tokenization, thereby creating a one-to-one correspondence between text and speech representations. This framework addresses the common </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">token length mismatch problem</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between speech and text modalities&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which has been a major challenge for large spoken language models.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 TASTE: Text-Aligned Speech Tokenization &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(a), TASTE consists of two main components: a </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TASTE tokenizer</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> which produces text-aligned speech tokens and an AR </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">speech decoder</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> which reconstructs speech from the transcript and the aligned TASTE tokens.\nThe tokenizer consists of an encoder, an aggregator, and a quantizer. The encoder, initialized from the Whisper encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, extracts acoustic features that are processed by an </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">aggregator</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, initialized from the Whisper decoder, to align the acoustic features with the corresponding text tokens through cross-attention. Finally, a quantizer based on residual vector quantization (RVQ)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> discretizes the aligned acoustic representations into the TASTE speech tokens.</span>\n</p>\n\n",
                "matched_terms": [
                    "vector",
                    "quantization",
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For reconstruction, TASTE employs a Transformer-based speech decoder that conditions on both the transcript and the continuous TASTE embedding vectors (rather than their discrete indices). The decoder autoregressively predicts S3 tokens from CosyVoice&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which serve as intermediate representations between TASTE tokens and waveforms. These predicted S3 tokens are then converted into mel-spectrograms using a pretrained flow-matching S3-to-mel converter, followed by waveform generation with a HiFi-GAN vocoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The overall training objective combines two losses: (1) a cross-entropy loss between the predicted and ground-truth S3 tokens, and (2) a commitment loss from the RVQ quantizer to encourage stable and consistent token assignments.</span>\n</p>\n\n",
                "matched_terms": [
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We follow the original structure in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 TASTE: Text-Aligned Speech Tokenization &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(a) to implement the AR-based TASTE baseline with several modifications. First, we replace the pretrained </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">whisper-large-v3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> ASR model</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">canary-180m-flash<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text ltx_font_serif\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib24\" title=\"\">24</a><span class=\"ltx_text ltx_font_serif\">]</span></cite></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from NVIDIA NeMo</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for both encoder and aggregator initialization. Second, we use the final-layer encoder outputs for both keys and values in the aggregator&#8217;s cross-attention. Third, we proposed to use finite scalar quantization (FSQ)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as an alternative to RVQ, which improves performance (see Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS2\" style=\"font-size:90%;\" title=\"4.2 Effect of Vector Quantization &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Fourth, we adopt an encoder&#8211;decoder architecture instead of a decoder-only design. Finally, we discard the speaker embedding input, finding it unnecessary for reconstruction quality. This is consistent with findings reported in CosyVoice2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "quantization",
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The proposed DDM-based TASTE model shares the same architecture as the AR baseline, differing only in decoder input (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 TASTE: Text-Aligned Speech Tokenization &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(b)): instead of conditioning on previous S3 tokens </span>\n  <math alttext=\"S_{1:n-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mrow>\n            <mi mathsize=\"0.900em\">n</mi>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">1</mn>\n          </mrow>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{1:n-1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the DDM decoder takes </span>\n  <math alttext=\"S_{\\text{mask}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mtext mathsize=\"0.900em\">mask</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{\\text{mask}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a partially masked version of the target sequence </span>\n  <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">S</mi>\n      <annotation encoding=\"application/x-tex\">S</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We follow the MDLM framework</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for training but use a modified objective compared to&#160;(</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.E3\" style=\"font-size:90%;\" title=\"In 2.1 Discrete Diffusion Models &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">):</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "ddm",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">which is more stable and lead to consistently better results in our experiments. The overall training loss consists of the the cross entropy loss&#160;(</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.E4\" style=\"font-size:90%;\" title=\"In 2.3 Proposed Model &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and the commitment loss for the RVQ quantizer, or just the cross entropy loss&#160;(</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.E4\" style=\"font-size:90%;\" title=\"In 2.3 Proposed Model &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) for the FSQ quantizer.</span>\n</p>\n\n",
                "matched_terms": [
                    "fsq",
                    "consistently",
                    "rvq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Unlike the original TASTE work trained on Emilia and LibriTTS, our models use the Granary English-only dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which contains approximately 275k hours of speech. Note that Granary is much larger but also noisier, enabling us to evaluate TASTE&#8217;s robustness on ASR-style data. For evaluation, we use the LibriSpeech </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-other</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sets to assess performance under clean and noisy conditions, respectively.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "testclean",
                    "testother",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We employ a variety of metrics to evaluate speech reconstruction. Word Error Rate (WER) is computed by transcribing the reconstructed speech using NVIDIA&#8217;s FastConformer-Transducer-Large ASR model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Perceptual quality is measured with </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Squim-PESQ</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Squim-SISDR</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, along with </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">WV-MOS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">UT-MOS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Speaker similarity is measured by extracting embeddings with WavLM</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and TitaNet&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and computing cosine similarities, reported as </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-W</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively.</span>\n</p>\n\n",
                "matched_terms": [
                    "spksimt",
                    "spksimw",
                    "metrics",
                    "squimpesq",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Training is conducted on 32 NVIDIA A100 GPUs, with each GPU processing an average batch equivalent to roughly 375 seconds of audio. Following the two-stage training procedure of the original TASTE&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we first pre-train the model without the quantizer block for 100k steps. In the second stage, we add either the FSQ or RVQ quantizer and freeze the encoder and continue training for an additional 150k steps. The total number of trainable parameters is approximately 316M. We use the Adam optimizer in both training stages, with the learning rate linearly warmed up to </span>\n  <math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">5</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">5\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> over the first 5,000 steps and subsequently decayed to </span>\n  <math alttext=\"10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">6</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-6}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using cosine annealing. Dropout rates between 0% and 20% were explored, where 10% proved most effective for AR-based TASTE, while 0% yielded the best results for DDM-based TASTE.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "fsq",
                    "over",
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For inference with DDM-based TASTE, we adopt the confidence-based Top-K (Conf-TopK) sampler from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the default strategy, with the number of inference steps set to 50 unless otherwise noted. In this sampling approach, the unmasking order of tokens plays a crucial role: at each step, the model uses its own output logits to estimate confidence scores for all masked positions, then selects the </span>\n  <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">K</mi>\n      <annotation encoding=\"application/x-tex\">K</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> tokens with the highest confidence to unmask first&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The comparisons between samplers is shown in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS3\" style=\"font-size:90%;\" title=\"4.3 Effect of Sampler Choice &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We further assume that the number of S3 tokens to be predicted is known during inference. Although this information is unavailable in practice, it can be estimated using a global length predictor conditioned on the input text tokens. The effect of inaccurate length estimation is analyzed in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS5\" style=\"font-size:90%;\" title=\"4.5 Effect of Length Estimation Errors &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "all",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we present the speech reconstruction results of our AR- and DDM-based TASTE models. We encourage readers to explore the audio samples on our demo page</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://kuray107.github.io/DDMs_on_taste26_examples/demo\" title=\"\">https://kuray107.github.io/DDMs_on_taste26_examples/demo</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">to complement the quantitative results, particularly for Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS4\" style=\"font-size:90%;\" title=\"4.4 Effect of Number of Inference Steps &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS5\" style=\"font-size:90%;\" title=\"4.5 Effect of Length Estimation Errors &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "models",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To compare AR- and DDM-based TASTE models, we evaluate both against ground-truth signals on the LibriSpeech test sets. Results are shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 AR and DDM Baselines &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We report performance for the original speech (Original), reconstructions from ground-truth S3 sequences (S3-Oracle), models without vector quantization (-No-VQ) from the first training stage, and models with a 4-layer RVQ module (-4L-RVQ), following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with 512 tokens of 256 dimensions per layer.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "quantization",
                    "models",
                    "4lrvq",
                    "vector",
                    "rvq",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 AR and DDM Baselines &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, both AR- and DDM-based models perform well without vector quantization (rows 3&#8211;4), showing only slight WER increases and no loss in quality or speaker similarity compared to </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">S3-Oracle</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, adding RVQ causes significant degradation for the AR-based model: WER jumps from 2.81% to 7.60% and UT-MOS drops from 4.15 to 3.82 on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (row 5). In contrast, the DDM-based model shows only a modest WER rise (from 2.99% to 5.10%) while maintaining high UT-MOS, yielding much better reconstruction quality. Similar trends are seen on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-other</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "quantization",
                    "models",
                    "utmos",
                    "testclean",
                    "testother",
                    "wer",
                    "vector",
                    "rvq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Beyond reconstruction quality, DDM-based TASTE is also significantly more computationally efficient, requiring at most 50 inference steps compared to roughly 370 steps on average for AR-based TASTE. On the </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> set, our DDM-based model reconstructs an utterance in about 1.65 seconds, making it </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.3<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> faster</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> than the AR baseline, which requires an average of 5.48 seconds.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "testclean",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Next, to evaluate the impact of sampler choice on DDM inference, we compare the default confidence-based TopK (Conf-TopK) sampler with four alternatives: Ancestral&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, confidence-based TopP (Conf-TopP)</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and two ReMDM variants (ReMDM-Cap and ReMDM-Loop)</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which allow non-masked tokens to be remasked during the denoising process. Results with the DDM-4L-FSQ TASTE model are shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.3 Proposed Model &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where confidence-based samplers (Conf-TopK and Conf-TopP) achieve the best performance, while Ancestral and ReMDM perform worse&#8212;despite often being reported to produce more diverse outputs in unconditional text generation</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This is likely because confidence-based greedy sampling introduces a strong bias toward highly probable tokens, which can hurt tasks like unconditional text generation where diversity is crucial and context is limited. In our setting, however, speech reconstruction is strongly conditioned on text and TASTE embeddings, making diversity less important. This strong conditioning makes confidence-based sampling more effective, consistently producing higher-quality reconstructions than more stochastic methods.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddm",
                    "consistently",
                    "taste"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As discussed in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.SS1\" style=\"font-size:90%;\" title=\"2.1 Discrete Diffusion Models &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">2.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a major advantage of DDMs over AR models is the flexibility to adjust the number of inference steps, trading off efficiency and generation quality. To evaluate this, we tested the DDM-4L-FSQ model with 1, 10, 25, 50, and 100 steps. The Conf-TopK sampler is used in all cases except for single-step inference, where the sampler choice has no effect. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.3 Effect of Sampler Choice &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the model achieves strong performance with as few as 10 steps, showing no degradation across metrics. In fact, 10 steps yield the lowest WER, underscoring DDM&#8217;s ability to provide both efficiency and high-quality reconstruction. Even single-step inference remains usable, with only modest degradations in WER (from 4.00% to 5.14%) and UT-MOS (from 4.30 to 3.81), demonstrating that DDMs can deliver significantly faster speech reconstruction than AR baselines while maintaining high quality.</span>\n</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "models",
                    "all",
                    "metrics",
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we presented the first comprehensive study of applying discrete diffusion to speech tokenization and reconstruction within the TASTE framework. The proposed discrete diffusion-based decoder achieves both higher reconstruction quality and substantially faster inference than the AR baseline, requiring only ten denoising steps without performance degradation and even supporting single-step generation with minor degradation. Experiments further demonstrated FSQ outperforms RVQ in quantizer design, confidence-based samplers are better suited for speech reconstruction than more stochastic alternatives, and DDMs remain robust under varying inference lengths. These results establish DDM-based TASTE as a more efficient and flexible alternative to the AR baseline, offering practical benefits for future speech applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddmbased",
                    "fsq",
                    "outperforms",
                    "rvq",
                    "taste"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens",
        "caption": "Table 3: Reconstruction performance of DDM with different numbers of inference steps on test-clean.",
        "body": "# Steps\nInference Time (s)\n\nWER (%) ↓\\downarrow\n\n\nUTMOS ↑\\uparrow\n\n\nSQUIM-PESQ ↑\\uparrow\n\n\nSpkSim-W ↑\\uparrow\n\n\n\n1\n1.09\n5.14\n3.81\n3.39\n0.95\n\n\n\n\n10\n1.18\n3.70\n4.28\n3.80\n0.95\n\n\n25\n1.36\n3.83\n4.29\n3.81\n0.95\n\n\n50\n1.65\n4.00\n4.30\n3.82\n0.95\n\n\n100\n2.29\n4.01\n4.30\n3.82\n0.94",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\"># Steps</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Inference Time (s)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER (%)</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UTMOS</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SQUIM-PESQ</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SpkSim-W</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.09</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.14</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.81</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.39</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.95</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.95</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.36</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.29</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.81</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.95</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">50</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.30</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.95</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">100</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.94</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ddm",
            "steps",
            "time",
            "spksimw",
            "↑uparrow",
            "different",
            "↓downarrow",
            "reconstruction",
            "utmos",
            "testclean",
            "inference",
            "squimpesq",
            "wer",
            "performance",
            "numbers"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As discussed in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.SS1\" style=\"font-size:90%;\" title=\"2.1 Discrete Diffusion Models &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">2.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a major advantage of DDMs over AR models is the flexibility to adjust the number of inference steps, trading off efficiency and generation quality. To evaluate this, we tested the DDM-4L-FSQ model with 1, 10, 25, 50, and 100 steps. The Conf-TopK sampler is used in all cases except for single-step inference, where the sampler choice has no effect. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.3 Effect of Sampler Choice &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the model achieves strong performance with as few as 10 steps, showing no degradation across metrics. In fact, 10 steps yield the lowest WER, underscoring DDM&#8217;s ability to provide both efficiency and high-quality reconstruction. Even single-step inference remains usable, with only modest degradations in WER (from 4.00% to 5.14%) and UT-MOS (from 4.30 to 3.81), demonstrating that DDMs can deliver significantly faster speech reconstruction than AR baselines while maintaining high quality.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This paper introduces a discrete diffusion model (DDM) framework for text-aligned speech tokenization and reconstruction. By replacing the auto-regressive speech decoder with a discrete diffusion counterpart, our model achieves significantly better reconstruction quality, stronger ASR performance, and faster inference. We provide a comprehensive analysis of applying DDMs to speech reconstruction, examining sampler choices, inference steps, and robustness to length-scale estimation errors. Furthermore, we improve the original TASTE by systematically comparing vector quantization modules, showing that FSQ yields up to a 35% relative WER reduction and +0.14 UT-MOS improvement over RVQ for AR models, while also enhancing DDM performance. Our model generates speech in just 10 denoising steps and even supports single-step generation with only minor quality degradation.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddm",
                    "steps",
                    "utmos",
                    "inference",
                    "wer",
                    "performance",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In recent years, </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">discrete diffusion models</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (DDMs) have emerged as a powerful generative modeling framework for language and sequence modeling. Originating from the Discrete Denoising Diffusion Probabilistic Models (D3PM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, this line of research has advanced rapidly through extensions such as the Masked Diffusion Language Model (MDLM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Discrete Flow Matching&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ReMDM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Edit-Flow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Unlike traditional autoregressive (AR) models that require strictly sequential decoding, DDMs perform inference through iterative but inherently parallelizable denoising steps. This property enables efficient sampling, flexible trade-offs between speed and quality, and greater controllability in generation. Such advantages have driven their adoption in large-scale language models including LLaDA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Mercury&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "steps",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In summary, this work makes three main contributions. First, we present the first application of DDMs to speech tokenization and reconstruction, achieving higher reconstruction quality and significantly faster inference than the AR-based TASTE baseline. Second, we enhance TASTE&#8217;s performance through a systematic analysis of quantizer design. Finally, we provide a comprehensive evaluation of inference settings&#8212;including sampler choice, number of steps, and robustness to length estimation errors&#8212;offering insights and guidance for deploying DDMs in speech applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "steps",
                    "inference",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While TASTE has demonstrated its ability to achieve high-quality speech reconstruction at extremely low bitrates and straightforward adaptation to joint speech&#8211;text modeling, it also suffers from a major drawback. Unlike most speech and audio tokenizers that support parallel decoding for reconstruction, TASTE relies on an AR speech decoder due to the dynamic time alignment between its tokens and speech frames. This requirement makes the decoding process rather slow, particularly for long speech signals. To address this limitation, we propose replacing the AR-based speech decoder with a discrete diffusion decoder. Interestingly, we find that this modification not only accelerates decoding but also improves reconstructed speech quality, as demonstrated in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4\" style=\"font-size:90%;\" title=\"4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "time",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We follow the original structure in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 TASTE: Text-Aligned Speech Tokenization &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(a) to implement the AR-based TASTE baseline with several modifications. First, we replace the pretrained </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">whisper-large-v3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> ASR model</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">canary-180m-flash<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text ltx_font_serif\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib24\" title=\"\">24</a><span class=\"ltx_text ltx_font_serif\">]</span></cite></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from NVIDIA NeMo</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for both encoder and aggregator initialization. Second, we use the final-layer encoder outputs for both keys and values in the aggregator&#8217;s cross-attention. Third, we proposed to use finite scalar quantization (FSQ)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as an alternative to RVQ, which improves performance (see Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS2\" style=\"font-size:90%;\" title=\"4.2 Effect of Vector Quantization &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Fourth, we adopt an encoder&#8211;decoder architecture instead of a decoder-only design. Finally, we discard the speaker embedding input, finding it unnecessary for reconstruction quality. This is consistent with findings reported in CosyVoice2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Unlike the original TASTE work trained on Emilia and LibriTTS, our models use the Granary English-only dataset&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which contains approximately 275k hours of speech. Note that Granary is much larger but also noisier, enabling us to evaluate TASTE&#8217;s robustness on ASR-style data. For evaluation, we use the LibriSpeech </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-other</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> sets to assess performance under clean and noisy conditions, respectively.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "testclean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We employ a variety of metrics to evaluate speech reconstruction. Word Error Rate (WER) is computed by transcribing the reconstructed speech using NVIDIA&#8217;s FastConformer-Transducer-Large ASR model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Perceptual quality is measured with </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Squim-PESQ</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Squim-SISDR</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, along with </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">WV-MOS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">UT-MOS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Speaker similarity is measured by extracting embeddings with WavLM</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and TitaNet&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and computing cosine similarities, reported as </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-W</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">SpkSim-T</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively.</span>\n</p>\n\n",
                "matched_terms": [
                    "spksimw",
                    "squimpesq",
                    "wer",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For inference with DDM-based TASTE, we adopt the confidence-based Top-K (Conf-TopK) sampler from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the default strategy, with the number of inference steps set to 50 unless otherwise noted. In this sampling approach, the unmasking order of tokens plays a crucial role: at each step, the model uses its own output logits to estimate confidence scores for all masked positions, then selects the </span>\n  <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">K</mi>\n      <annotation encoding=\"application/x-tex\">K</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> tokens with the highest confidence to unmask first&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The comparisons between samplers is shown in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS3\" style=\"font-size:90%;\" title=\"4.3 Effect of Sampler Choice &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We further assume that the number of S3 tokens to be predicted is known during inference. Although this information is unavailable in practice, it can be estimated using a global length predictor conditioned on the input text tokens. The effect of inaccurate length estimation is analyzed in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.SS5\" style=\"font-size:90%;\" title=\"4.5 Effect of Length Estimation Errors &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">4.5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "steps",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.1 AR and DDM Baselines &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, both AR- and DDM-based models perform well without vector quantization (rows 3&#8211;4), showing only slight WER increases and no loss in quality or speaker similarity compared to </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">S3-Oracle</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, adding RVQ causes significant degradation for the AR-based model: WER jumps from 2.81% to 7.60% and UT-MOS drops from 4.15 to 3.82 on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (row 5). In contrast, the DDM-based model shows only a modest WER rise (from 2.99% to 5.10%) while maintaining high UT-MOS, yielding much better reconstruction quality. Similar trends are seen on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-other</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "testclean",
                    "wer",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Beyond reconstruction quality, DDM-based TASTE is also significantly more computationally efficient, requiring at most 50 inference steps compared to roughly 370 steps on average for AR-based TASTE. On the </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> set, our DDM-based model reconstructs an utterance in about 1.65 seconds, making it </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.3<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> faster</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> than the AR baseline, which requires an average of 5.48 seconds.</span>\n</p>\n\n",
                "matched_terms": [
                    "steps",
                    "testclean",
                    "inference",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To study the impact of vector quantization, we trained AR and DDM variants with different VQ configurations, comparing RVQ and FSQ modules with 2, 4, or 8 layers, yielding six variants per model. Results in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 AR and DDM Baselines &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> show that across all configurations, DDM-based TASTE consistently outperforms its AR counterparts, particularly in WER, demonstrating superior efficiency and reconstruction quality. FSQ also clearly outperforms RVQ, especially for AR models, reducing WER by about 35% on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and 31% on </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">test-other</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while improving UT-MOS by +0.14 and +0.31, respectively. For DDM, RVQ already performs strongly, so FSQ provides smaller but consistent WER gains. Lastly, while using fewer VQ layers degrades AR performance across all metrics, DDM remains robust, though speaker similarity drops noticeably.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddm",
                    "different",
                    "utmos",
                    "testclean",
                    "wer",
                    "performance",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Next, to evaluate the impact of sampler choice on DDM inference, we compare the default confidence-based TopK (Conf-TopK) sampler with four alternatives: Ancestral&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, confidence-based TopP (Conf-TopP)</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and two ReMDM variants (ReMDM-Cap and ReMDM-Loop)</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which allow non-masked tokens to be remasked during the denoising process. Results with the DDM-4L-FSQ TASTE model are shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.3 Proposed Model &#8227; 2 Approach &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where confidence-based samplers (Conf-TopK and Conf-TopP) achieve the best performance, while Ancestral and ReMDM perform worse&#8212;despite often being reported to produce more diverse outputs in unconditional text generation</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This is likely because confidence-based greedy sampling introduces a strong bias toward highly probable tokens, which can hurt tasks like unconditional text generation where diversity is crucial and context is limited. In our setting, however, speech reconstruction is strongly conditioned on text and TASTE embeddings, making diversity less important. This strong conditioning makes confidence-based sampling more effective, consistently producing higher-quality reconstructions than more stochastic methods.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddm",
                    "performance",
                    "inference",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Finally, we revisit the assumption in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Training and Inference Setup &#8227; 3 Experimental Setup &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that the model has access to the global S3 token length during inference, which in practice requires an auxiliary length predictor. To analyze its impact, we evaluate the DDM-4L-FSQ model under length scales ranging from 70% to 130% of the original sequence, measuring insertion, deletion, and substitution rates. Results are shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#S4.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 4.5 Effect of Length Estimation Errors &#8227; 4 Results &#8227; Discrete Diffusion for Generative Modeling of Text-Aligned Speech Tokens\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. When the inference length is shorter than the ground truth, the model maintains the same speaking rate and truncates the sequence tail, causing higher deletion and substitution rates, while insertion remains stable. In contrast, when the length is longer, it simply appends silence tokens with minimal ASR degradation. These results suggest that overestimation is relatively safe, whereas underestimation leads to severe errors. A promising future direction is to extend the DDM to predict an end-of-sequence (EOS) token</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20060v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, eliminating the need for an external length predictor.</span>\n</p>\n\n",
                "matched_terms": [
                    "ddm",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we presented the first comprehensive study of applying discrete diffusion to speech tokenization and reconstruction within the TASTE framework. The proposed discrete diffusion-based decoder achieves both higher reconstruction quality and substantially faster inference than the AR baseline, requiring only ten denoising steps without performance degradation and even supporting single-step generation with minor degradation. Experiments further demonstrated FSQ outperforms RVQ in quantizer design, confidence-based samplers are better suited for speech reconstruction than more stochastic alternatives, and DDMs remain robust under varying inference lengths. These results establish DDM-based TASTE as a more efficient and flexible alternative to the AR baseline, offering practical benefits for future speech applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "steps",
                    "inference",
                    "reconstruction"
                ]
            }
        ]
    }
}