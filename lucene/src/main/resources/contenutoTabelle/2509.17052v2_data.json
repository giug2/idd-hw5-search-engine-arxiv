{
    "S3.T1": {
        "caption": "Table 1: List of datasets used in Sidon training. “SF,” “Lang.,” and “Dur.’ stands for sampling frequency of speech, language, and durations, respectively.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">Name</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">SF</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Lang.</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:80%;\">Dur. [h]</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">HiFi-CAPTAIN&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib24\" title=\"\">24</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">48k</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">ja, en</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">EARS&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib25\" title=\"\">25</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">48k</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">en</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">EXPRESSO&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib26\" title=\"\">26</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">48k</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">en</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">40</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">JSUT&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib27\" title=\"\">27</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">48k</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">ja</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Bible-TTS&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib28\" title=\"\">28</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">48k</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">akan, ewe, hausa, lingala, yoruba</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">80</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">VCTK&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib29\" title=\"\">29</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">48k</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">en</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">44</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">JVS&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib27\" title=\"\">27</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">24k</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">ja</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">FLEURS-R&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">24k</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">102 langs</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.3k</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">LibriTTS-R&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">24k</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">en</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">585</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">Total</span></td>\n<td class=\"ltx_td ltx_border_bb ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:71.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">104</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">2,219</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "dur",
            "48k",
            "lingala",
            "fleursr",
            "durations",
            "hausa",
            "ewe",
            "expresso",
            "jsut",
            "list",
            "ears",
            "sampling",
            "langs",
            "frequency",
            "stands",
            "training",
            "lang",
            "13k",
            "used",
            "“sf”",
            "language",
            "yoruba",
            "akan",
            "speech",
            "name",
            "librittsr",
            "datasets",
            "respectively",
            "bibletts",
            "total",
            "hificaptain",
            "vctk",
            "24k",
            "“dur’",
            "jvs",
            "sidon",
            "“lang”"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In order to achieve speech restoration with high fidelity and good generalization,\ncollecting enough amount of data with good recording quality is pivotal. Therefore,\nwe collected multiple publicly available datasets. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.3 Dataset collection &#8227; 3 Sidon &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nshows the list of the collected datasets. In Total, we collected 2,219\nhours of speech across 104 languages.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For clean speech dataset, we used corpora listed on Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.3 Dataset collection &#8227; 3 Sidon &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWhen the training, valid, and test sets were specified, the split were followed.\nOtherwise, all samples were used as the training set.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large-scale text-to-speech (TTS) systems are limited by the scarcity of clean,\nmultilingual recordings. We introduce <span class=\"ltx_text ltx_font_bold\">Sidon</span>, a fast, open-source\nspeech restoration model that converts noisy in-the-wild speech into\nstudio-quality speech and scales to dozens of languages. Sidon consists of\ntwo models: w2v-BERT 2.0 finetuned feature predictor to cleanse features from noisy speech\nand vocoder trained to synthesize restored speech from the cleansed features.\nSidon achieves restoration performance comparable to Miipher: Google&#8217;s internal speech restoration model with\nthe aim of dataset cleansing for speech synthesis. Sidon is also computationally\nefficient, running up to 500&#8201;<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> faster than real time on a\nsingle GPU. We further show that training a TTS model using a Sidon-cleansed automatic speech\nrecognition corpus improves the quality of synthetic\nspeech in a zero-shot setting. Code and model are released to\nfacilitate reproducible dataset cleansing for the research community.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in deep learning have demonstrated remarkable\nprogress across a variety of domains, largely enabled by scaling model\ncapacity and dataset size. Similar trends can be observed in speech synthesis,\nwhere data quantity and quality play a critical role to improve the quality\nof synthetic speech. In fact, recent text-to-speech (TTS) models are trained on tens of\nthousand hours of speech corpus&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and for spoken language\nmodeling, the number becomes even larger as they are trained on million\nhours of speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, unlike text or image domains, expanding\ntraining data by crawling in-the-wild speech samples for speech synthesis is\nparticularly challenging due to the presence of noise, artifacts, and recording\nenvironment.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these issues, speech restoration&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> aims to map noisy, in-the-wild speech to clean, studio-quality speech. The task typically combines speech enhancement, dereverberation, super-resolution, and codec-artifact removal. Notably, Miipher&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Miipher-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have proven effective at converting real-world recordings into studio-quality speech, and have been adopted in datasets such as LibriTTS-R&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and FLEURS-R&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, their closed-source nature restricts accessibility and limits applicability to new datasets. Open-source alternatives exist, but they are often monolingual and trained under limited noise conditions on relatively small datasets, which hinders generalization and reduces their utility for large-scale dataset restoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "fleursr",
                    "datasets",
                    "librittsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we present </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sidon</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an open-source multilingual speech\nrestoration model designed for large-scale dataset cleansing. Sidon is\ncomputationally efficient, enabling practical use for cleansing large corpora,\nand supports diverse languages beyond English.\nWe conduct three set of experiments: (i) speech restoration on English data,\n(ii) speech restoration across 100 languages, and (iii) English TTS training\nusing Sidon-processed datasets. Results show that Sidon achieves performance\ncomparable to Miipher, and Sidon effectively improves the quality of\nTTS training data, thereby enhancing downstream speech synthesis quality.\nSpeech samples, demo and code are available at our project page</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/spaces/Wataru/SidonSamples\" title=\"\">https://hf.co/spaces/Wataru/SidonSamples</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "datasets",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech restoration, also known as universal speech enhancement, aims to recover clean speech from signals degraded by noise, reverberation, bandwidth limitations, and other artifacts. VoiceFixer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced a two-stage analysis&#8211;synthesis pipeline that predicts intermediate representations from degraded speech and uses a neural vocoder to synthesize high-fidelity audio; it restores speech corrupted by noise, reverberation, clipping, and also upscales low-bandwidth signals to 44.1&#160;kHz. Several work explored the generative modeling approach for speech restoration&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and demonstrated its effectiveness for improving the perceptual quality of restored speech. However, these work have focused on English, limited noise conditions, and relatively small training sets, yielding insufficient generalization for large-scale dataset cleansing.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To improve generalization, Miipher was proposed. Miipher integrates self-supervised learning (SSL) model representations from w2v-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and linguistic conditioning from PnG-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to predict clean features that condition a neural vocoder, enabling robust restoration across diverse degradations and facilitating training high-quality TTS models from cleansed ASR datasets&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Its successor, Miipher-2, leverages the Universal Speech Model (USM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a frozen feature extractor and trains parallel adapters&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to predict clean USM representations from noisy inputs, improving cross-lingual generalization and scaling to million-hour corpora without explicit text or speaker conditioning&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Crucially, these systems are trained at scale (e.g., Miipher on 2,680 hours of English speech; Miipher-2 on 3,195 hours of multilingual speech), which contributes to strong generalization required for universal dataset cleansing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Nevertheless, both models are closed-source, and their full training data and recipes are unavailable, limiting applicability to new datasets and hindering reproducibility.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite these advances, most open-source methods remain English-centric, assume restricted degradation types, or are trained on comparatively small corpora, and they rarely exploit multilingual self-supervised encoders. Our work addresses these gaps by proposing Sidon: a open source speech restoration model trained on diverse noise conditions on large multilingual training dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S3.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 3 Sidon &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the overall architecture of Sidon. Sidon is\nbased on a parametric resynthesis framework used in previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given a noisy waveform, feature predictor estimates an SSL feature extracted from\nclean speech. In order to achieve generalized speech restoration across many\nlanguages, it is pivotal to use an SSL model trained on large datasets from multiple\nlanguages. Therefore, we use w2v-BERT 2.0&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a feature predictor.\nThis model is trained on 4.5M hours of speech in 143 languages, making it suitable\nfor the multilingual speech restoration. Specifically, the feature predictor\nis initialized with the parameters of pretrained w2v-BERT 2.0. Then, an output\nlinear layer in each feed forward network of Conformer blocks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nare updated using low rank adaptation (LoRA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> similar to </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis strategy not only reduces training cost, but also prevents the catastrophic\nforgetting of the pretrained knowledge obtained by SSL. For the clean SSL\nmodel feature extraction, we extract the 8th layer hidden state from w2v-BERT 2.0.\nIn previous research, it is reported that earlier layer of an SSL model contains\nacoustic features while the latter is more focused on semantic features&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIn speech restoration, keeping the acoustic information such as speaker identity\nand prosody is also important. Therefore, we choose the 8th layer for the target\nlayer.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the vocoder trainings in the second and third stages, only 48&#160;kHz datasets\nwere used as mixing with 24&#160;kHz data would hurt the fidelity of the trained models\nwhile all the corpora listed\nare used for training feature predictor.</span>\n</p>\n\n",
                "matched_terms": [
                    "used",
                    "datasets",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the first stage, feature predictor is trained to minimize the mean squared\nerror (MSE) loss between the predicted and target SSL features. For vocoder training\nin the second and third stages, we used a combination of MSE loss between\ngenerated and target mel-spectrograms, adversarial loss, and feature\nmatching loss following the original HiFi-GAN paper&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Sidon follows similar architecture to Miipher-2 apart from three main differences.\nThe first difference is the SSL model. Miipher-2 uses the 13th layer hidden state of 2B parameter USM model. However, this USM model is closed source. Therefore, Sidon uses the 8th layer hidden state of open-sourced 600M parameter w2v-BERT 2.0 model.\nThe second difference is that Sidon produces 48&#160;kHz full-fidelity speech using HiFi-GAN with snake activation as its vocoder while Miipher-2 produces 24&#160;kHz speech using WaveFiT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nLastly, Miipher-2 was trained on the 3,195 hours of Google internal data while Sidon is trained on 2,219 hours of curated high-quality public datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To validate the speech restoration performance and multilingual ability of Sidon,\nwe performed following experiments: (i) speech restoration in English and multilingual settings, (ii) dataset cleansing for zero-shot TTS model training, (iii) and inference speed comparison with different batch sizes.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Data preparation:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To train a generalized speech restoration model, diversity in degradation is important as we never know what kind of artifact present in the in-the-wild data. Therefore, we used a degradation simulation pipeline following previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis pipeline applies degradation in following order: reverberation,\nbackground noise, band limitation, clipping, codec, and packet loss. Each degradations\nwas applied with a probability of 50%.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Reverberation: We used pyroomacoustics&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nfor simulating room impulse responses (RIRs). Specifically, random RT60 and rectangular cuboid room dimensions were drawn from </span>\n  <math alttext=\"\\mathcal{U}(0.1,2.0)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119984;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mn mathsize=\"0.900em\">0.1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">2.0</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{U}(0.1,2.0)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> seconds and </span>\n  <math alttext=\"\\mathcal{U}(2,20)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119984;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mn mathsize=\"0.900em\">2</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">20</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{U}(2,20)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;m respectively. Based on the drawn RT60 and room dimensions, wall absorption and maximum order of the image-source method&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> were calculated using sabine&#8217;s equation. Then, RIRs were simulated.</span>\n</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Band limitation: The input speech was randomly resampled at {8,\n16, 22.05, 24, 44.1, 48}&#160;kHz sampling rate before being converted\nback to the original sampling rate.</span>\n</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DNN optimization:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We used AdamW optimizer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib38\" title=\"\">38</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <math alttext=\"\\beta_{1}=0.9,\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#946;</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">0.9</mn>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#946;</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">0.999</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{1}=0.9,\\beta_{2}=0.999</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) with learning rate of </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with weight decay\nof </span>\n  <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.01</mn>\n      <annotation encoding=\"application/x-tex\">0.01</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Learning rate exponential decay was applied when training the\nvocoder to stabilize training with </span>\n  <math alttext=\"\\gamma=0.9998\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#947;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9998</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\gamma=0.9998</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The feature predictor was\ntrained for 400k steps (four days) with batch size of 256, and the vocoder\nwas pretrained for 140k steps (two days) and finetuned for 280k steps (four days)\nwith batch size of 32. All trainings were conducted on eight NVIDIA H200 GPUs. The number of parameters were 198M (five million trainable parameters) for the feature predictor and 52.4M for the vocoder. In total, Sidon has 250M parameters.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "total",
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate speech restoration capability of Sidon, we compare Sidon against\nMiipher on two different settings: English and multilingual speech\nrestoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the English setting, we used &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets of LibriTTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib39\" title=\"\">39</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nEven though we do not have direct access to the Google&#8217;s internal Miipher\nmodel, restored samples on &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets are available as\nLibriTTS-R. We used these restored samples for evaluation.\nPlease note that Miipher is a text-conditioned model while Sidon is not. Therefore, Miipher uses ground-truth transcript upon restoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "used",
                    "librittsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the multilingual setting, we used the test set of FLEURS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib40\" title=\"\">40</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSimilar to LibriTTS-R, FLEURS-R contains Miipher-restored samples in the test\nset. Note that Miipher used for restoring FLEURS in our experiment differed\nfrom the one used in LibriTTS-R and it had identical architecture to Miipher-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nHowever, details of the model used in FLEURS-R such as training dataset are not disclosed in any\npapers. We refer to this model as &#8220;Miipher-2&#8221; in this paper for convenience.\nWe noticed that FLEURS-R lacks approximately 10% of samples from FLEURS.\nTherefore, we only used test set samples included in FLEURS-R.\nUnlike Miipher, Miipher-2 is not a text-conditioned model. Therefore, we can expect fair comparison between Sidon and Miipher-2 regarding the text conditioning.\nEvaluation was conducted on 100 languages selected from 102 languages in FLEURS as we\ncould not find any publicly available ASR models supporting Filipino (fil_ph) and Oriya (or_in).</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "sidon",
                    "used",
                    "fleursr",
                    "librittsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We used four metrics for evaluation. For spoken content preservation, we\nused word error rate (WER) in the English setting and character error rate (CER) in\nthe multilingual setting. These metrics measure distance between transcribed results\nby an ASR model against the ground-truth scripts. For the ASR model, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">mms-1B-all</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib41\" title=\"\">41</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available\non HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/facebook/mms-1b-all\" title=\"\">https://hf.co/facebook/mms-1b-all</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which covers\n1,162 languages with its model size of 1B parameter. For restoration quality,\nwe used NISQA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib42\" title=\"\">42</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and DNSMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib43\" title=\"\">43</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> quality predictors.\nThese are commonly used for quantifying the quality of restored speech. Specifically,\nwe used overall quality scores predicted by NISQA and DNSMOS. For speaker preservation,\nwe used cosine similarity of speaker embeddings (SpkSim) those extracted from input\nnoisy speech and the restored speech. For speaker embedding extraction, we\nused </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">wavlm-base-plus-sv</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available on HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/microsoft/wavlm-base-plus-sv\" title=\"\">https://hf.co/microsoft/wavlm-base-plus-sv</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To investigate the effectiveness of Sidon as a preprocessing tool for TTS model training, we trained a TTS model on a Sidon-cleansed ASR corpus.\nFor the corpus to be cleansed, we used TED-LIUM release 3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib44\" title=\"\">44</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is a 452 hours of speech dataset designed for ASR studies. It consists of TED talks recorded mostly in a hall. Therefore it contains reverberation and noise.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For TTS model, we used F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib45\" title=\"\">45</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is a zero-shot TTS\nmodel based on flow matching&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib46\" title=\"\">46</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For comparison, we\ncompared Sidon with two conventional open-source speech restoration / separation models\nfor cleansing TED-LIUM release 3: VoiceFixer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Demucs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib47\" title=\"\">47</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. VoiceFixer is an open-source speech restoration model, and Demucs is an open-source music separation model capable of separating speech from background music; both have been used for dataset cleansing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib49\" title=\"\">49</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe compared &#8220;Original&#8221; (i.e., unprocessed) TED-LIUM release 3 to\ninvestigate the effectiveness of dataset cleansing.\nUpon training of F5-TTS, we resampled all samples to 24&#160;kHz and performed training\nusing the base model configuration of F5-TTS. The training was performed for 750k steps on eight H200 GPUs.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "used",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To efficiently cleanse large-scale in-the-wild corpora, the cleansing pipeline must be computationally lightweight.\nWe therefore measured the real-time factor (RTF) of Sidon on an NVIDIA H200 using batch sizes\n</span>\n  <math alttext=\"\\{1,2,4,8\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">4</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">8</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\{1,2,4,8\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">bfloat16</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe measurements were taken with 16&#160;kHz 30 seconds speech input.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T2.st1\" style=\"font-size:90%;\" title=\"In Table 2 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">2(a)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T2.st2\" style=\"font-size:90%;\" title=\"In Table 2 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">2(b)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> show the result of speech\nrestoration for &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets in the English setting,\nrespectively.\nThe results show that Sidon achieves strong performance on &#8220;test-clean&#8221; and better\nperformance on &#8220;test-other&#8221; in all metrics except for WER. Please note that Miipher is a text-conditioned model while Sidon is not. Therefore, Miipher has an advantage in\nWER as it can utilize textual information. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nshows the result of multilingual speech restoration.\nIn terms of sound quality, a comparison of DNSMOS and NISQA scores between the noisy inputs and the Sidon outputs shows that Sidon consistently improves the sound quality in any tested language, exhibiting its success in speech restoration.\nOn average, Sidon\noutperforms Miipher-2 in CER and DNSMOS, is comparable in SpkSim, and is slightly worse in NISQA.\nTherefore, Sidon&#8217;s performance is comparable to Miipher-2 despite relying on the smaller SSL model.\nThis may be due to the use of a more comprehensive degradation simulation pipeline with six different degradations whereas Miipher-2 only simulates reverberation, background noise and codec artifacts.\nAdditionally, please note that NISQA might not be well-suited for multilingual evaluation as its trained on subjective evaluation results from English speech only.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "sidon",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.2 ASR dataset cleansing for TTS &#8227; 5 Results &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the result of TTS model training with Sidon-cleansed dataset. The result shows that Sidon outperforms other conventional open-source restoration / separation models\nand the original noisy data in terms of the quality of synthetic speech. This indicates that Sidon can effectively\nimprove the quality of training data, thereby enhancing the performance of\ndownstream TTS task.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S5.T5\" style=\"font-size:90%;\" title=\"Table 5 &#8227; 5.3 Inference speed evaluation result &#8227; 5 Results &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes Sidon&#8217;s inference efficiency (RTF) as a function of batch size. With batch size&#160;8 on a single GPU, Sidon runs approximately 500</span>\n  <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#215;</mo>\n      <annotation encoding=\"application/x-tex\">\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> faster than real time implying that restoring a 1M-hour corpus would take about 2000 hours on one GPU&#8212;substantially reducing the cost of constructing large-scale studio quality speech datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we presented Sidon, an open-source multilingual speech restoration\nmodel designed for large-scale dataset cleansing. Sidon is computationally\nefficient, enabling practical use in large corpus cleansing, and supports\ndiverse languages beyond English. Experimental results demonstrated that Sidon\nachieves performance comparable to closed-source models like Miipher and effectively\nimproves the quality of training data, thereby enhancing downstream speech\nsynthesis quality. We believe that Sidon will be a valuable tool for the speech\nsynthesis community, facilitating the development of high-quality, large-scale\nspeech datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "datasets",
                    "training"
                ]
            }
        ]
    },
    "S4.T2.st1": {
        "caption": "(a) Results for “test-clean” set.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">WER</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">SpkSim</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">NISQA</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">DNSMOS</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">Noisy</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.040</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.093 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.017</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">3.179 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st1.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.008</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">Miipher</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.047</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.942</span></td>\n<td class=\"ltx_td ltx_align_right\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.688 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.010</span>\n</td>\n<td class=\"ltx_td ltx_align_right\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">3.134 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st1.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.009</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">Sidon (ours)</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.045</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.971</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.790</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st1.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.010</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.303</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st1.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.007</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "set",
            "wer↓downarrow",
            "ours",
            "dnsmos↑uparrow",
            "nisqa↑uparrow",
            "miipher",
            "sidon",
            "spksim↑uparrow",
            "±pm",
            "results",
            "“testclean”",
            "noisy"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T2.st1\" style=\"font-size:90%;\" title=\"In Table 2 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">2(a)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T2.st2\" style=\"font-size:90%;\" title=\"In Table 2 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">2(b)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> show the result of speech\nrestoration for &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets in the English setting,\nrespectively.\nThe results show that Sidon achieves strong performance on &#8220;test-clean&#8221; and better\nperformance on &#8220;test-other&#8221; in all metrics except for WER. Please note that Miipher is a text-conditioned model while Sidon is not. Therefore, Miipher has an advantage in\nWER as it can utilize textual information. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nshows the result of multilingual speech restoration.\nIn terms of sound quality, a comparison of DNSMOS and NISQA scores between the noisy inputs and the Sidon outputs shows that Sidon consistently improves the sound quality in any tested language, exhibiting its success in speech restoration.\nOn average, Sidon\noutperforms Miipher-2 in CER and DNSMOS, is comparable in SpkSim, and is slightly worse in NISQA.\nTherefore, Sidon&#8217;s performance is comparable to Miipher-2 despite relying on the smaller SSL model.\nThis may be due to the use of a more comprehensive degradation simulation pipeline with six different degradations whereas Miipher-2 only simulates reverberation, background noise and codec artifacts.\nAdditionally, please note that NISQA might not be well-suited for multilingual evaluation as its trained on subjective evaluation results from English speech only.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large-scale text-to-speech (TTS) systems are limited by the scarcity of clean,\nmultilingual recordings. We introduce <span class=\"ltx_text ltx_font_bold\">Sidon</span>, a fast, open-source\nspeech restoration model that converts noisy in-the-wild speech into\nstudio-quality speech and scales to dozens of languages. Sidon consists of\ntwo models: w2v-BERT 2.0 finetuned feature predictor to cleanse features from noisy speech\nand vocoder trained to synthesize restored speech from the cleansed features.\nSidon achieves restoration performance comparable to Miipher: Google&#8217;s internal speech restoration model with\nthe aim of dataset cleansing for speech synthesis. Sidon is also computationally\nefficient, running up to 500&#8201;<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> faster than real time on a\nsingle GPU. We further show that training a TTS model using a Sidon-cleansed automatic speech\nrecognition corpus improves the quality of synthetic\nspeech in a zero-shot setting. Code and model are released to\nfacilitate reproducible dataset cleansing for the research community.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "noisy",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these issues, speech restoration&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> aims to map noisy, in-the-wild speech to clean, studio-quality speech. The task typically combines speech enhancement, dereverberation, super-resolution, and codec-artifact removal. Notably, Miipher&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Miipher-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have proven effective at converting real-world recordings into studio-quality speech, and have been adopted in datasets such as LibriTTS-R&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and FLEURS-R&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, their closed-source nature restricts accessibility and limits applicability to new datasets. Open-source alternatives exist, but they are often monolingual and trained under limited noise conditions on relatively small datasets, which hinders generalization and reduces their utility for large-scale dataset restoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we present </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sidon</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an open-source multilingual speech\nrestoration model designed for large-scale dataset cleansing. Sidon is\ncomputationally efficient, enabling practical use for cleansing large corpora,\nand supports diverse languages beyond English.\nWe conduct three set of experiments: (i) speech restoration on English data,\n(ii) speech restoration across 100 languages, and (iii) English TTS training\nusing Sidon-processed datasets. Results show that Sidon achieves performance\ncomparable to Miipher, and Sidon effectively improves the quality of\nTTS training data, thereby enhancing downstream speech synthesis quality.\nSpeech samples, demo and code are available at our project page</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/spaces/Wataru/SidonSamples\" title=\"\">https://hf.co/spaces/Wataru/SidonSamples</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "sidon",
                    "results",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To improve generalization, Miipher was proposed. Miipher integrates self-supervised learning (SSL) model representations from w2v-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and linguistic conditioning from PnG-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to predict clean features that condition a neural vocoder, enabling robust restoration across diverse degradations and facilitating training high-quality TTS models from cleansed ASR datasets&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Its successor, Miipher-2, leverages the Universal Speech Model (USM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a frozen feature extractor and trains parallel adapters&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to predict clean USM representations from noisy inputs, improving cross-lingual generalization and scaling to million-hour corpora without explicit text or speaker conditioning&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Crucially, these systems are trained at scale (e.g., Miipher on 2,680 hours of English speech; Miipher-2 on 3,195 hours of multilingual speech), which contributes to strong generalization required for universal dataset cleansing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Nevertheless, both models are closed-source, and their full training data and recipes are unavailable, limiting applicability to new datasets and hindering reproducibility.</span>\n</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate speech restoration capability of Sidon, we compare Sidon against\nMiipher on two different settings: English and multilingual speech\nrestoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the English setting, we used &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets of LibriTTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib39\" title=\"\">39</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nEven though we do not have direct access to the Google&#8217;s internal Miipher\nmodel, restored samples on &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets are available as\nLibriTTS-R. We used these restored samples for evaluation.\nPlease note that Miipher is a text-conditioned model while Sidon is not. Therefore, Miipher uses ground-truth transcript upon restoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "“testclean”",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the multilingual setting, we used the test set of FLEURS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib40\" title=\"\">40</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSimilar to LibriTTS-R, FLEURS-R contains Miipher-restored samples in the test\nset. Note that Miipher used for restoring FLEURS in our experiment differed\nfrom the one used in LibriTTS-R and it had identical architecture to Miipher-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nHowever, details of the model used in FLEURS-R such as training dataset are not disclosed in any\npapers. We refer to this model as &#8220;Miipher-2&#8221; in this paper for convenience.\nWe noticed that FLEURS-R lacks approximately 10% of samples from FLEURS.\nTherefore, we only used test set samples included in FLEURS-R.\nUnlike Miipher, Miipher-2 is not a text-conditioned model. Therefore, we can expect fair comparison between Sidon and Miipher-2 regarding the text conditioning.\nEvaluation was conducted on 100 languages selected from 102 languages in FLEURS as we\ncould not find any publicly available ASR models supporting Filipino (fil_ph) and Oriya (or_in).</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "sidon",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We used four metrics for evaluation. For spoken content preservation, we\nused word error rate (WER) in the English setting and character error rate (CER) in\nthe multilingual setting. These metrics measure distance between transcribed results\nby an ASR model against the ground-truth scripts. For the ASR model, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">mms-1B-all</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib41\" title=\"\">41</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available\non HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/facebook/mms-1b-all\" title=\"\">https://hf.co/facebook/mms-1b-all</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which covers\n1,162 languages with its model size of 1B parameter. For restoration quality,\nwe used NISQA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib42\" title=\"\">42</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and DNSMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib43\" title=\"\">43</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> quality predictors.\nThese are commonly used for quantifying the quality of restored speech. Specifically,\nwe used overall quality scores predicted by NISQA and DNSMOS. For speaker preservation,\nwe used cosine similarity of speaker embeddings (SpkSim) those extracted from input\nnoisy speech and the restored speech. For speaker embedding extraction, we\nused </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">wavlm-base-plus-sv</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available on HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/microsoft/wavlm-base-plus-sv\" title=\"\">https://hf.co/microsoft/wavlm-base-plus-sv</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.2 ASR dataset cleansing for TTS &#8227; 5 Results &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the result of TTS model training with Sidon-cleansed dataset. The result shows that Sidon outperforms other conventional open-source restoration / separation models\nand the original noisy data in terms of the quality of synthetic speech. This indicates that Sidon can effectively\nimprove the quality of training data, thereby enhancing the performance of\ndownstream TTS task.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we presented Sidon, an open-source multilingual speech restoration\nmodel designed for large-scale dataset cleansing. Sidon is computationally\nefficient, enabling practical use in large corpus cleansing, and supports\ndiverse languages beyond English. Experimental results demonstrated that Sidon\nachieves performance comparable to closed-source models like Miipher and effectively\nimproves the quality of training data, thereby enhancing downstream speech\nsynthesis quality. We believe that Sidon will be a valuable tool for the speech\nsynthesis community, facilitating the development of high-quality, large-scale\nspeech datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "results",
                    "miipher"
                ]
            }
        ]
    },
    "S4.T2.st2": {
        "caption": "(b) Results for “test-other” set",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">WER</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">SpkSim</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">NISQA</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">DNSMOS</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">Noisy</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.079</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">3.623 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.019</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">2.949 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.010</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">Miipher</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.090</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.930</span></td>\n<td class=\"ltx_td ltx_align_right\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">4.597 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.011</span>\n</td>\n<td class=\"ltx_td ltx_align_right\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">3.040 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.010</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">Sidon (ours)</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.095</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.961</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.698</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.011</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.219</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.st2.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\"> 0.008</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "set",
            "wer↓downarrow",
            "ours",
            "dnsmos↑uparrow",
            "nisqa↑uparrow",
            "miipher",
            "spksim↑uparrow",
            "“testother”",
            "±pm",
            "results",
            "sidon",
            "noisy"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T2.st1\" style=\"font-size:90%;\" title=\"In Table 2 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">2(a)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T2.st2\" style=\"font-size:90%;\" title=\"In Table 2 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">2(b)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> show the result of speech\nrestoration for &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets in the English setting,\nrespectively.\nThe results show that Sidon achieves strong performance on &#8220;test-clean&#8221; and better\nperformance on &#8220;test-other&#8221; in all metrics except for WER. Please note that Miipher is a text-conditioned model while Sidon is not. Therefore, Miipher has an advantage in\nWER as it can utilize textual information. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nshows the result of multilingual speech restoration.\nIn terms of sound quality, a comparison of DNSMOS and NISQA scores between the noisy inputs and the Sidon outputs shows that Sidon consistently improves the sound quality in any tested language, exhibiting its success in speech restoration.\nOn average, Sidon\noutperforms Miipher-2 in CER and DNSMOS, is comparable in SpkSim, and is slightly worse in NISQA.\nTherefore, Sidon&#8217;s performance is comparable to Miipher-2 despite relying on the smaller SSL model.\nThis may be due to the use of a more comprehensive degradation simulation pipeline with six different degradations whereas Miipher-2 only simulates reverberation, background noise and codec artifacts.\nAdditionally, please note that NISQA might not be well-suited for multilingual evaluation as its trained on subjective evaluation results from English speech only.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large-scale text-to-speech (TTS) systems are limited by the scarcity of clean,\nmultilingual recordings. We introduce <span class=\"ltx_text ltx_font_bold\">Sidon</span>, a fast, open-source\nspeech restoration model that converts noisy in-the-wild speech into\nstudio-quality speech and scales to dozens of languages. Sidon consists of\ntwo models: w2v-BERT 2.0 finetuned feature predictor to cleanse features from noisy speech\nand vocoder trained to synthesize restored speech from the cleansed features.\nSidon achieves restoration performance comparable to Miipher: Google&#8217;s internal speech restoration model with\nthe aim of dataset cleansing for speech synthesis. Sidon is also computationally\nefficient, running up to 500&#8201;<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> faster than real time on a\nsingle GPU. We further show that training a TTS model using a Sidon-cleansed automatic speech\nrecognition corpus improves the quality of synthetic\nspeech in a zero-shot setting. Code and model are released to\nfacilitate reproducible dataset cleansing for the research community.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "noisy",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these issues, speech restoration&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> aims to map noisy, in-the-wild speech to clean, studio-quality speech. The task typically combines speech enhancement, dereverberation, super-resolution, and codec-artifact removal. Notably, Miipher&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Miipher-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have proven effective at converting real-world recordings into studio-quality speech, and have been adopted in datasets such as LibriTTS-R&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and FLEURS-R&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, their closed-source nature restricts accessibility and limits applicability to new datasets. Open-source alternatives exist, but they are often monolingual and trained under limited noise conditions on relatively small datasets, which hinders generalization and reduces their utility for large-scale dataset restoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we present </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sidon</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an open-source multilingual speech\nrestoration model designed for large-scale dataset cleansing. Sidon is\ncomputationally efficient, enabling practical use for cleansing large corpora,\nand supports diverse languages beyond English.\nWe conduct three set of experiments: (i) speech restoration on English data,\n(ii) speech restoration across 100 languages, and (iii) English TTS training\nusing Sidon-processed datasets. Results show that Sidon achieves performance\ncomparable to Miipher, and Sidon effectively improves the quality of\nTTS training data, thereby enhancing downstream speech synthesis quality.\nSpeech samples, demo and code are available at our project page</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/spaces/Wataru/SidonSamples\" title=\"\">https://hf.co/spaces/Wataru/SidonSamples</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "results",
                    "sidon",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To improve generalization, Miipher was proposed. Miipher integrates self-supervised learning (SSL) model representations from w2v-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and linguistic conditioning from PnG-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to predict clean features that condition a neural vocoder, enabling robust restoration across diverse degradations and facilitating training high-quality TTS models from cleansed ASR datasets&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Its successor, Miipher-2, leverages the Universal Speech Model (USM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a frozen feature extractor and trains parallel adapters&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to predict clean USM representations from noisy inputs, improving cross-lingual generalization and scaling to million-hour corpora without explicit text or speaker conditioning&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Crucially, these systems are trained at scale (e.g., Miipher on 2,680 hours of English speech; Miipher-2 on 3,195 hours of multilingual speech), which contributes to strong generalization required for universal dataset cleansing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Nevertheless, both models are closed-source, and their full training data and recipes are unavailable, limiting applicability to new datasets and hindering reproducibility.</span>\n</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate speech restoration capability of Sidon, we compare Sidon against\nMiipher on two different settings: English and multilingual speech\nrestoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the English setting, we used &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets of LibriTTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib39\" title=\"\">39</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nEven though we do not have direct access to the Google&#8217;s internal Miipher\nmodel, restored samples on &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets are available as\nLibriTTS-R. We used these restored samples for evaluation.\nPlease note that Miipher is a text-conditioned model while Sidon is not. Therefore, Miipher uses ground-truth transcript upon restoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "“testother”",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the multilingual setting, we used the test set of FLEURS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib40\" title=\"\">40</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSimilar to LibriTTS-R, FLEURS-R contains Miipher-restored samples in the test\nset. Note that Miipher used for restoring FLEURS in our experiment differed\nfrom the one used in LibriTTS-R and it had identical architecture to Miipher-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nHowever, details of the model used in FLEURS-R such as training dataset are not disclosed in any\npapers. We refer to this model as &#8220;Miipher-2&#8221; in this paper for convenience.\nWe noticed that FLEURS-R lacks approximately 10% of samples from FLEURS.\nTherefore, we only used test set samples included in FLEURS-R.\nUnlike Miipher, Miipher-2 is not a text-conditioned model. Therefore, we can expect fair comparison between Sidon and Miipher-2 regarding the text conditioning.\nEvaluation was conducted on 100 languages selected from 102 languages in FLEURS as we\ncould not find any publicly available ASR models supporting Filipino (fil_ph) and Oriya (or_in).</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "sidon",
                    "miipher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We used four metrics for evaluation. For spoken content preservation, we\nused word error rate (WER) in the English setting and character error rate (CER) in\nthe multilingual setting. These metrics measure distance between transcribed results\nby an ASR model against the ground-truth scripts. For the ASR model, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">mms-1B-all</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib41\" title=\"\">41</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available\non HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/facebook/mms-1b-all\" title=\"\">https://hf.co/facebook/mms-1b-all</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which covers\n1,162 languages with its model size of 1B parameter. For restoration quality,\nwe used NISQA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib42\" title=\"\">42</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and DNSMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib43\" title=\"\">43</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> quality predictors.\nThese are commonly used for quantifying the quality of restored speech. Specifically,\nwe used overall quality scores predicted by NISQA and DNSMOS. For speaker preservation,\nwe used cosine similarity of speaker embeddings (SpkSim) those extracted from input\nnoisy speech and the restored speech. For speaker embedding extraction, we\nused </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">wavlm-base-plus-sv</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available on HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/microsoft/wavlm-base-plus-sv\" title=\"\">https://hf.co/microsoft/wavlm-base-plus-sv</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.2 ASR dataset cleansing for TTS &#8227; 5 Results &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the result of TTS model training with Sidon-cleansed dataset. The result shows that Sidon outperforms other conventional open-source restoration / separation models\nand the original noisy data in terms of the quality of synthetic speech. This indicates that Sidon can effectively\nimprove the quality of training data, thereby enhancing the performance of\ndownstream TTS task.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we presented Sidon, an open-source multilingual speech restoration\nmodel designed for large-scale dataset cleansing. Sidon is computationally\nefficient, enabling practical use in large corpus cleansing, and supports\ndiverse languages beyond English. Experimental results demonstrated that Sidon\nachieves performance comparable to closed-source models like Miipher and effectively\nimproves the quality of training data, thereby enhancing downstream speech\nsynthesis quality. We believe that Sidon will be a valuable tool for the speech\nsynthesis community, facilitating the development of high-quality, large-scale\nspeech datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "results",
                    "miipher"
                ]
            }
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Evaluation results for speech restoration on the FLEURS test set. Best values among the restored\nspeech are shown in bold. Due to the space constraint, we only show\n10 major languages based on Whisper’s dataset distribution [3] but the full 100 languages results are available online at our project page.\nPlease note that “average” indicates averaged results of “all” 100 languages.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_rr ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">CER</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">DNSMOS</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">NISQA</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">SpkSim</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Lang</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">noisy</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Miipher-2</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Sidon (ours)</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">noisy</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Miipher-2</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Sidon (ours)</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">noisy</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Miipher-2</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Sidon (ours)</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">noisy</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Miipher-2</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Sidon (ours)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">cmn</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.266</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.287</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.280</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.112 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.013</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.397</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.007</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.374 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.006</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.595 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.026</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.373</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.015</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">4.364 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.012</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.984</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.984</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">en</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.051</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.061</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.061</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">2.925 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.053</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.393 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.013</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.466</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.010</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.091 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.104</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">4.459 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.015</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.491</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.016</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.982</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.976</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">es</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.021</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.022</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.022</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.133 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.015</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.336 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.007</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.399</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.006</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.638 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.030</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.546</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.010</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">4.515 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.012</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.978</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.978</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">ru</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.042</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.046</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.043</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.005 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m23\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.018</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.307 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m24\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.012</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.364</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m25\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.009</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.420 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m26\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.041</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.550</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m27\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.013</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">4.490 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m28\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.013</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.984</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.986</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">fr</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.043</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.052</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.048</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.125 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m29\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.018</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.278 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m30\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.014</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.336</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m31\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.011</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.633 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m32\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.029</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.428</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m33\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.015</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">4.386 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m34\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.013</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.986</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.987</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">pt</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.028</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.032</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.038</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">2.951 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m35\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.022</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.399 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m36\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.007</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.450</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m37\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.006</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.364 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m38\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.037</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">4.481 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m39\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.010</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.514</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m40\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.009</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.969</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.964</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">ko</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.173</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.183</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.179</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">2.914 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m41\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.036</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.431 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m42\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.010</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.465</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m43\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.009</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.191 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m44\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.030</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.507</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m45\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.016</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">4.480 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m46\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.016</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.979</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.980</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">ja</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.212</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.225</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.213</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">2.673 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m47\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.045</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.479</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m48\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.006</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.453 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m49\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.006</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">2.913 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m50\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.042</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.562</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m51\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.011</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">4.472 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m52\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.009</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.959</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.958</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">tr</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.040</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.044</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.041</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.069 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m53\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.020</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.417 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m54\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.008</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.449</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m55\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.007</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.365 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m56\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.039</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.556</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m57\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.009</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">4.524 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m58\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.009</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.973</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.973</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">pl</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.030</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.036</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.033</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">2.912 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m59\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.023</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.274 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m60\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.013</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.333</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m61\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.011</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.544 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m62\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.044</span>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.621</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m63\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.009</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">4.573 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m64\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.009</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.984</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.983</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_rr ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Average</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.084</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.094</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_rr ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.090</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">2.910 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m65\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.003</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.352 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m66\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.001</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_rr ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.393</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m67\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.001</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">3.252 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m68\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.005</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.475</span><span class=\"ltx_text\" style=\"font-size:70%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m69\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.002</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_rr ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:70%;\">4.420 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m70\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:70%;\"> 0.002</span>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.979</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.979</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "full",
            "whisper’s",
            "restored",
            "evaluation",
            "“average”",
            "restoration",
            "our",
            "cer↓downarrow",
            "based",
            "nisqa↑uparrow",
            "sidon",
            "miipher2",
            "fleurs",
            "average",
            "show",
            "lang",
            "test",
            "constraint",
            "distribution",
            "project",
            "page",
            "due",
            "major",
            "dnsmos↑uparrow",
            "spksim↑uparrow",
            "bold",
            "averaged",
            "note",
            "cmn",
            "results",
            "indicates",
            "speech",
            "available",
            "only",
            "online",
            "values",
            "set",
            "among",
            "languages",
            "please",
            "ours",
            "“all”",
            "best",
            "±pm",
            "space",
            "dataset",
            "noisy"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T2.st1\" style=\"font-size:90%;\" title=\"In Table 2 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">2(a)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T2.st2\" style=\"font-size:90%;\" title=\"In Table 2 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">2(b)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> show the result of speech\nrestoration for &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets in the English setting,\nrespectively.\nThe results show that Sidon achieves strong performance on &#8220;test-clean&#8221; and better\nperformance on &#8220;test-other&#8221; in all metrics except for WER. Please note that Miipher is a text-conditioned model while Sidon is not. Therefore, Miipher has an advantage in\nWER as it can utilize textual information. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nshows the result of multilingual speech restoration.\nIn terms of sound quality, a comparison of DNSMOS and NISQA scores between the noisy inputs and the Sidon outputs shows that Sidon consistently improves the sound quality in any tested language, exhibiting its success in speech restoration.\nOn average, Sidon\noutperforms Miipher-2 in CER and DNSMOS, is comparable in SpkSim, and is slightly worse in NISQA.\nTherefore, Sidon&#8217;s performance is comparable to Miipher-2 despite relying on the smaller SSL model.\nThis may be due to the use of a more comprehensive degradation simulation pipeline with six different degradations whereas Miipher-2 only simulates reverberation, background noise and codec artifacts.\nAdditionally, please note that NISQA might not be well-suited for multilingual evaluation as its trained on subjective evaluation results from English speech only.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large-scale text-to-speech (TTS) systems are limited by the scarcity of clean,\nmultilingual recordings. We introduce <span class=\"ltx_text ltx_font_bold\">Sidon</span>, a fast, open-source\nspeech restoration model that converts noisy in-the-wild speech into\nstudio-quality speech and scales to dozens of languages. Sidon consists of\ntwo models: w2v-BERT 2.0 finetuned feature predictor to cleanse features from noisy speech\nand vocoder trained to synthesize restored speech from the cleansed features.\nSidon achieves restoration performance comparable to Miipher: Google&#8217;s internal speech restoration model with\nthe aim of dataset cleansing for speech synthesis. Sidon is also computationally\nefficient, running up to 500&#8201;<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> faster than real time on a\nsingle GPU. We further show that training a TTS model using a Sidon-cleansed automatic speech\nrecognition corpus improves the quality of synthetic\nspeech in a zero-shot setting. Code and model are released to\nfacilitate reproducible dataset cleansing for the research community.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "dataset",
                    "restored",
                    "show",
                    "speech",
                    "sidon",
                    "noisy",
                    "restoration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nspeech restoration, self-supervised learning model, multilingual, TTS, vocoder</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "restoration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in deep learning have demonstrated remarkable\nprogress across a variety of domains, largely enabled by scaling model\ncapacity and dataset size. Similar trends can be observed in speech synthesis,\nwhere data quantity and quality play a critical role to improve the quality\nof synthetic speech. In fact, recent text-to-speech (TTS) models are trained on tens of\nthousand hours of speech corpus&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and for spoken language\nmodeling, the number becomes even larger as they are trained on million\nhours of speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, unlike text or image domains, expanding\ntraining data by crawling in-the-wild speech samples for speech synthesis is\nparticularly challenging due to the presence of noise, artifacts, and recording\nenvironment.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "due"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these issues, speech restoration&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> aims to map noisy, in-the-wild speech to clean, studio-quality speech. The task typically combines speech enhancement, dereverberation, super-resolution, and codec-artifact removal. Notably, Miipher&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Miipher-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have proven effective at converting real-world recordings into studio-quality speech, and have been adopted in datasets such as LibriTTS-R&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and FLEURS-R&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, their closed-source nature restricts accessibility and limits applicability to new datasets. Open-source alternatives exist, but they are often monolingual and trained under limited noise conditions on relatively small datasets, which hinders generalization and reduces their utility for large-scale dataset restoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "miipher2",
                    "speech",
                    "dataset",
                    "noisy",
                    "restoration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we present </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sidon</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an open-source multilingual speech\nrestoration model designed for large-scale dataset cleansing. Sidon is\ncomputationally efficient, enabling practical use for cleansing large corpora,\nand supports diverse languages beyond English.\nWe conduct three set of experiments: (i) speech restoration on English data,\n(ii) speech restoration across 100 languages, and (iii) English TTS training\nusing Sidon-processed datasets. Results show that Sidon achieves performance\ncomparable to Miipher, and Sidon effectively improves the quality of\nTTS training data, thereby enhancing downstream speech synthesis quality.\nSpeech samples, demo and code are available at our project page</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/spaces/Wataru/SidonSamples\" title=\"\">https://hf.co/spaces/Wataru/SidonSamples</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "languages",
                    "dataset",
                    "available",
                    "show",
                    "results",
                    "speech",
                    "sidon",
                    "project",
                    "restoration",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech restoration, also known as universal speech enhancement, aims to recover clean speech from signals degraded by noise, reverberation, bandwidth limitations, and other artifacts. VoiceFixer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced a two-stage analysis&#8211;synthesis pipeline that predicts intermediate representations from degraded speech and uses a neural vocoder to synthesize high-fidelity audio; it restores speech corrupted by noise, reverberation, clipping, and also upscales low-bandwidth signals to 44.1&#160;kHz. Several work explored the generative modeling approach for speech restoration&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and demonstrated its effectiveness for improving the perceptual quality of restored speech. However, these work have focused on English, limited noise conditions, and relatively small training sets, yielding insufficient generalization for large-scale dataset cleansing.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "restoration",
                    "restored"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To improve generalization, Miipher was proposed. Miipher integrates self-supervised learning (SSL) model representations from w2v-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and linguistic conditioning from PnG-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to predict clean features that condition a neural vocoder, enabling robust restoration across diverse degradations and facilitating training high-quality TTS models from cleansed ASR datasets&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Its successor, Miipher-2, leverages the Universal Speech Model (USM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a frozen feature extractor and trains parallel adapters&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to predict clean USM representations from noisy inputs, improving cross-lingual generalization and scaling to million-hour corpora without explicit text or speaker conditioning&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Crucially, these systems are trained at scale (e.g., Miipher on 2,680 hours of English speech; Miipher-2 on 3,195 hours of multilingual speech), which contributes to strong generalization required for universal dataset cleansing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Nevertheless, both models are closed-source, and their full training data and recipes are unavailable, limiting applicability to new datasets and hindering reproducibility.</span>\n</p>\n\n",
                "matched_terms": [
                    "full",
                    "miipher2",
                    "speech",
                    "dataset",
                    "noisy",
                    "restoration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite these advances, most open-source methods remain English-centric, assume restricted degradation types, or are trained on comparatively small corpora, and they rarely exploit multilingual self-supervised encoders. Our work addresses these gaps by proposing Sidon: a open source speech restoration model trained on diverse noise conditions on large multilingual training dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "speech",
                    "sidon",
                    "restoration",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S3.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 3 Sidon &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the overall architecture of Sidon. Sidon is\nbased on a parametric resynthesis framework used in previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given a noisy waveform, feature predictor estimates an SSL feature extracted from\nclean speech. In order to achieve generalized speech restoration across many\nlanguages, it is pivotal to use an SSL model trained on large datasets from multiple\nlanguages. Therefore, we use w2v-BERT 2.0&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a feature predictor.\nThis model is trained on 4.5M hours of speech in 143 languages, making it suitable\nfor the multilingual speech restoration. Specifically, the feature predictor\nis initialized with the parameters of pretrained w2v-BERT 2.0. Then, an output\nlinear layer in each feed forward network of Conformer blocks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nare updated using low rank adaptation (LoRA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> similar to </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis strategy not only reduces training cost, but also prevents the catastrophic\nforgetting of the pretrained knowledge obtained by SSL. For the clean SSL\nmodel feature extraction, we extract the 8th layer hidden state from w2v-BERT 2.0.\nIn previous research, it is reported that earlier layer of an SSL model contains\nacoustic features while the latter is more focused on semantic features&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIn speech restoration, keeping the acoustic information such as speaker identity\nand prosody is also important. Therefore, we choose the 8th layer for the target\nlayer.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "speech",
                    "noisy",
                    "only",
                    "restoration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The vocoder converts the predicted clean feature back to a waveform to produce\nrestored speech. For this purpose, we use HiFi-GAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nextended with snake activation&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> similar to the Descript Audio Codec\n(DAC)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> decoder as its model structure.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "restored"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In order to achieve speech restoration with high fidelity and good generalization,\ncollecting enough amount of data with good recording quality is pivotal. Therefore,\nwe collected multiple publicly available datasets. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.3 Dataset collection &#8227; 3 Sidon &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nshows the list of the collected datasets. In Total, we collected 2,219\nhours of speech across 104 languages.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "available",
                    "restoration",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Sidon follows similar architecture to Miipher-2 apart from three main differences.\nThe first difference is the SSL model. Miipher-2 uses the 13th layer hidden state of 2B parameter USM model. However, this USM model is closed source. Therefore, Sidon uses the 8th layer hidden state of open-sourced 600M parameter w2v-BERT 2.0 model.\nThe second difference is that Sidon produces 48&#160;kHz full-fidelity speech using HiFi-GAN with snake activation as its vocoder while Miipher-2 produces 24&#160;kHz speech using WaveFiT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nLastly, Miipher-2 was trained on the 3,195 hours of Google internal data while Sidon is trained on 2,219 hours of curated high-quality public datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "miipher2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To validate the speech restoration performance and multilingual ability of Sidon,\nwe performed following experiments: (i) speech restoration in English and multilingual settings, (ii) dataset cleansing for zero-shot TTS model training, (iii) and inference speed comparison with different batch sizes.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "restoration",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For clean speech dataset, we used corpora listed on Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.3 Dataset collection &#8227; 3 Sidon &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWhen the training, valid, and test sets were specified, the split were followed.\nOtherwise, all samples were used as the training set.</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "dataset",
                    "speech",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Data preparation:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To train a generalized speech restoration model, diversity in degradation is important as we never know what kind of artifact present in the in-the-wild data. Therefore, we used a degradation simulation pipeline following previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis pipeline applies degradation in following order: reverberation,\nbackground noise, band limitation, clipping, codec, and packet loss. Each degradations\nwas applied with a probability of 50%.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "restoration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The noising pipeline was applied four times. As a result,\nwe obtained roughly 9,000 hours of paired (clean, noisy) speech data.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DNN architecture:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For feature predictor, we set the LoRA adapter\nsetting to </span>\n  <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">16</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha=16</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, LoRA dropout of 0.1 and rank of </span>\n  <math alttext=\"64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">64</mn>\n      <annotation encoding=\"application/x-tex\">64</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For the\nvocoder, we set the upsampling rates to </span>\n  <math alttext=\"\\{8,5,4,3,2\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n        <mn mathsize=\"0.900em\">8</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">5</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">4</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\{8,5,4,3,2\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, resulting in a 960 times\nupsampling for converting 50&#160;Hz w2v-BERT 2.0 features into 48&#160;kHz full fidelity speech\nand input channels to 1,536 to match the dimension of SSL feature. For other\nhyperparameters regarding the vocoder, we followed the same values as the\nprevious work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "full",
                    "set",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate speech restoration capability of Sidon, we compare Sidon against\nMiipher on two different settings: English and multilingual speech\nrestoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon",
                    "restoration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the English setting, we used &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets of LibriTTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib39\" title=\"\">39</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nEven though we do not have direct access to the Google&#8217;s internal Miipher\nmodel, restored samples on &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets are available as\nLibriTTS-R. We used these restored samples for evaluation.\nPlease note that Miipher is a text-conditioned model while Sidon is not. Therefore, Miipher uses ground-truth transcript upon restoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "please",
                    "sidon",
                    "note",
                    "restored",
                    "evaluation",
                    "available",
                    "restoration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the multilingual setting, we used the test set of FLEURS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib40\" title=\"\">40</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSimilar to LibriTTS-R, FLEURS-R contains Miipher-restored samples in the test\nset. Note that Miipher used for restoring FLEURS in our experiment differed\nfrom the one used in LibriTTS-R and it had identical architecture to Miipher-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nHowever, details of the model used in FLEURS-R such as training dataset are not disclosed in any\npapers. We refer to this model as &#8220;Miipher-2&#8221; in this paper for convenience.\nWe noticed that FLEURS-R lacks approximately 10% of samples from FLEURS.\nTherefore, we only used test set samples included in FLEURS-R.\nUnlike Miipher, Miipher-2 is not a text-conditioned model. Therefore, we can expect fair comparison between Sidon and Miipher-2 regarding the text conditioning.\nEvaluation was conducted on 100 languages selected from 102 languages in FLEURS as we\ncould not find any publicly available ASR models supporting Filipino (fil_ph) and Oriya (or_in).</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "languages",
                    "miipher2",
                    "fleurs",
                    "dataset",
                    "note",
                    "available",
                    "evaluation",
                    "test",
                    "sidon",
                    "only",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We used four metrics for evaluation. For spoken content preservation, we\nused word error rate (WER) in the English setting and character error rate (CER) in\nthe multilingual setting. These metrics measure distance between transcribed results\nby an ASR model against the ground-truth scripts. For the ASR model, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">mms-1B-all</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib41\" title=\"\">41</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available\non HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/facebook/mms-1b-all\" title=\"\">https://hf.co/facebook/mms-1b-all</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which covers\n1,162 languages with its model size of 1B parameter. For restoration quality,\nwe used NISQA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib42\" title=\"\">42</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and DNSMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib43\" title=\"\">43</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> quality predictors.\nThese are commonly used for quantifying the quality of restored speech. Specifically,\nwe used overall quality scores predicted by NISQA and DNSMOS. For speaker preservation,\nwe used cosine similarity of speaker embeddings (SpkSim) those extracted from input\nnoisy speech and the restored speech. For speaker embedding extraction, we\nused </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">wavlm-base-plus-sv</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available on HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/microsoft/wavlm-base-plus-sv\" title=\"\">https://hf.co/microsoft/wavlm-base-plus-sv</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "restored",
                    "evaluation",
                    "results",
                    "speech",
                    "available",
                    "noisy",
                    "restoration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To investigate the effectiveness of Sidon as a preprocessing tool for TTS model training, we trained a TTS model on a Sidon-cleansed ASR corpus.\nFor the corpus to be cleansed, we used TED-LIUM release 3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib44\" title=\"\">44</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is a 452 hours of speech dataset designed for ASR studies. It consists of TED talks recorded mostly in a hall. Therefore it contains reverberation and noise.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "sidon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For TTS model, we used F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib45\" title=\"\">45</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is a zero-shot TTS\nmodel based on flow matching&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib46\" title=\"\">46</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For comparison, we\ncompared Sidon with two conventional open-source speech restoration / separation models\nfor cleansing TED-LIUM release 3: VoiceFixer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Demucs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib47\" title=\"\">47</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. VoiceFixer is an open-source speech restoration model, and Demucs is an open-source music separation model capable of separating speech from background music; both have been used for dataset cleansing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib49\" title=\"\">49</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe compared &#8220;Original&#8221; (i.e., unprocessed) TED-LIUM release 3 to\ninvestigate the effectiveness of dataset cleansing.\nUpon training of F5-TTS, we resampled all samples to 24&#160;kHz and performed training\nusing the base model configuration of F5-TTS. The training was performed for 750k steps on eight H200 GPUs.</span>\n</p>\n\n",
                "matched_terms": [
                    "based",
                    "dataset",
                    "speech",
                    "sidon",
                    "restoration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For evaluation, we performed a five-scale mean opinion score (MOS) test on the sound\nquality following previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The number of raters was 120\nand each raters evaluated 16 samples. To prepare test samples,\nwe used the initial 40% of each utterance as the speaker prompt and the other\n60% were synthesized and used for evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To efficiently cleanse large-scale in-the-wild corpora, the cleansing pipeline must be computationally lightweight.\nWe therefore measured the real-time factor (RTF) of Sidon on an NVIDIA H200 using batch sizes\n</span>\n  <math alttext=\"\\{1,2,4,8\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">4</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">8</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\{1,2,4,8\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">bfloat16</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe measurements were taken with 16&#160;kHz 30 seconds speech input.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.2 ASR dataset cleansing for TTS &#8227; 5 Results &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the result of TTS model training with Sidon-cleansed dataset. The result shows that Sidon outperforms other conventional open-source restoration / separation models\nand the original noisy data in terms of the quality of synthetic speech. This indicates that Sidon can effectively\nimprove the quality of training data, thereby enhancing the performance of\ndownstream TTS task.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "indicates",
                    "speech",
                    "dataset",
                    "noisy",
                    "restoration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S5.T5\" style=\"font-size:90%;\" title=\"Table 5 &#8227; 5.3 Inference speed evaluation result &#8227; 5 Results &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes Sidon&#8217;s inference efficiency (RTF) as a function of batch size. With batch size&#160;8 on a single GPU, Sidon runs approximately 500</span>\n  <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#215;</mo>\n      <annotation encoding=\"application/x-tex\">\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> faster than real time implying that restoring a 1M-hour corpus would take about 2000 hours on one GPU&#8212;substantially reducing the cost of constructing large-scale studio quality speech datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sidon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we presented Sidon, an open-source multilingual speech restoration\nmodel designed for large-scale dataset cleansing. Sidon is computationally\nefficient, enabling practical use in large corpus cleansing, and supports\ndiverse languages beyond English. Experimental results demonstrated that Sidon\nachieves performance comparable to closed-source models like Miipher and effectively\nimproves the quality of training data, thereby enhancing downstream speech\nsynthesis quality. We believe that Sidon will be a valuable tool for the speech\nsynthesis community, facilitating the development of high-quality, large-scale\nspeech datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "languages",
                    "dataset",
                    "results",
                    "speech",
                    "sidon",
                    "restoration"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Result of MOS test of TTS model trained on TED-LIUM Release 3 with each preprocess models. Values after with 95% confidence intervals. Best result shown in bold. There are statistical significance (pp-value << 0.05) between all models except for “Original” and “Demucs.”",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Preprocess model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MOS</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Original (noisy)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.254 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.089</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Demucs&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib47\" title=\"\">47</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.265 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.086</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">VoiceFixer&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.771 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.102</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Sidon (ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.248</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.109</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "original",
            "tedlium",
            "each",
            "mos",
            "tts",
            "after",
            "except",
            "release",
            "test",
            "between",
            "“original”",
            "demucs",
            "trained",
            "“demucs”",
            "result",
            "bold",
            "ppvalue",
            "confidence",
            "values",
            "voicefixer",
            "ours",
            "intervals",
            "model",
            "preprocess",
            "best",
            "all",
            "±pm",
            "significance",
            "there",
            "mos↑uparrow",
            "sidon",
            "noisy",
            "statistical"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S5.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 5.2 ASR dataset cleansing for TTS &#8227; 5 Results &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the result of TTS model training with Sidon-cleansed dataset. The result shows that Sidon outperforms other conventional open-source restoration / separation models\nand the original noisy data in terms of the quality of synthetic speech. This indicates that Sidon can effectively\nimprove the quality of training data, thereby enhancing the performance of\ndownstream TTS task.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large-scale text-to-speech (TTS) systems are limited by the scarcity of clean,\nmultilingual recordings. We introduce <span class=\"ltx_text ltx_font_bold\">Sidon</span>, a fast, open-source\nspeech restoration model that converts noisy in-the-wild speech into\nstudio-quality speech and scales to dozens of languages. Sidon consists of\ntwo models: w2v-BERT 2.0 finetuned feature predictor to cleanse features from noisy speech\nand vocoder trained to synthesize restored speech from the cleansed features.\nSidon achieves restoration performance comparable to Miipher: Google&#8217;s internal speech restoration model with\nthe aim of dataset cleansing for speech synthesis. Sidon is also computationally\nefficient, running up to 500&#8201;<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> faster than real time on a\nsingle GPU. We further show that training a TTS model using a Sidon-cleansed automatic speech\nrecognition corpus improves the quality of synthetic\nspeech in a zero-shot setting. Code and model are released to\nfacilitate reproducible dataset cleansing for the research community.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "model",
                    "sidon",
                    "noisy",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nspeech restoration, self-supervised learning model, multilingual, TTS, vocoder</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in deep learning have demonstrated remarkable\nprogress across a variety of domains, largely enabled by scaling model\ncapacity and dataset size. Similar trends can be observed in speech synthesis,\nwhere data quantity and quality play a critical role to improve the quality\nof synthetic speech. In fact, recent text-to-speech (TTS) models are trained on tens of\nthousand hours of speech corpus&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and for spoken language\nmodeling, the number becomes even larger as they are trained on million\nhours of speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, unlike text or image domains, expanding\ntraining data by crawling in-the-wild speech samples for speech synthesis is\nparticularly challenging due to the presence of noise, artifacts, and recording\nenvironment.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "trained",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these issues, speech restoration&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> aims to map noisy, in-the-wild speech to clean, studio-quality speech. The task typically combines speech enhancement, dereverberation, super-resolution, and codec-artifact removal. Notably, Miipher&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Miipher-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have proven effective at converting real-world recordings into studio-quality speech, and have been adopted in datasets such as LibriTTS-R&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and FLEURS-R&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, their closed-source nature restricts accessibility and limits applicability to new datasets. Open-source alternatives exist, but they are often monolingual and trained under limited noise conditions on relatively small datasets, which hinders generalization and reduces their utility for large-scale dataset restoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we present </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sidon</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, an open-source multilingual speech\nrestoration model designed for large-scale dataset cleansing. Sidon is\ncomputationally efficient, enabling practical use for cleansing large corpora,\nand supports diverse languages beyond English.\nWe conduct three set of experiments: (i) speech restoration on English data,\n(ii) speech restoration across 100 languages, and (iii) English TTS training\nusing Sidon-processed datasets. Results show that Sidon achieves performance\ncomparable to Miipher, and Sidon effectively improves the quality of\nTTS training data, thereby enhancing downstream speech synthesis quality.\nSpeech samples, demo and code are available at our project page</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/spaces/Wataru/SidonSamples\" title=\"\">https://hf.co/spaces/Wataru/SidonSamples</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To improve generalization, Miipher was proposed. Miipher integrates self-supervised learning (SSL) model representations from w2v-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and linguistic conditioning from PnG-BERT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to predict clean features that condition a neural vocoder, enabling robust restoration across diverse degradations and facilitating training high-quality TTS models from cleansed ASR datasets&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Its successor, Miipher-2, leverages the Universal Speech Model (USM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a frozen feature extractor and trains parallel adapters&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to predict clean USM representations from noisy inputs, improving cross-lingual generalization and scaling to million-hour corpora without explicit text or speaker conditioning&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Crucially, these systems are trained at scale (e.g., Miipher on 2,680 hours of English speech; Miipher-2 on 3,195 hours of multilingual speech), which contributes to strong generalization required for universal dataset cleansing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Nevertheless, both models are closed-source, and their full training data and recipes are unavailable, limiting applicability to new datasets and hindering reproducibility.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "model",
                    "noisy",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite these advances, most open-source methods remain English-centric, assume restricted degradation types, or are trained on comparatively small corpora, and they rarely exploit multilingual self-supervised encoders. Our work addresses these gaps by proposing Sidon: a open source speech restoration model trained on diverse noise conditions on large multilingual training dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "sidon",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given a noisy waveform, feature predictor estimates an SSL feature extracted from\nclean speech. In order to achieve generalized speech restoration across many\nlanguages, it is pivotal to use an SSL model trained on large datasets from multiple\nlanguages. Therefore, we use w2v-BERT 2.0&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a feature predictor.\nThis model is trained on 4.5M hours of speech in 143 languages, making it suitable\nfor the multilingual speech restoration. Specifically, the feature predictor\nis initialized with the parameters of pretrained w2v-BERT 2.0. Then, an output\nlinear layer in each feed forward network of Conformer blocks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nare updated using low rank adaptation (LoRA)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> similar to </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis strategy not only reduces training cost, but also prevents the catastrophic\nforgetting of the pretrained knowledge obtained by SSL. For the clean SSL\nmodel feature extraction, we extract the 8th layer hidden state from w2v-BERT 2.0.\nIn previous research, it is reported that earlier layer of an SSL model contains\nacoustic features while the latter is more focused on semantic features&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIn speech restoration, keeping the acoustic information such as speaker identity\nand prosody is also important. Therefore, we choose the 8th layer for the target\nlayer.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "trained",
                    "each",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the vocoder trainings in the second and third stages, only 48&#160;kHz datasets\nwere used as mixing with 24&#160;kHz data would hurt the fidelity of the trained models\nwhile all the corpora listed\nare used for training feature predictor.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "trained",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the first stage, feature predictor is trained to minimize the mean squared\nerror (MSE) loss between the predicted and target SSL features. For vocoder training\nin the second and third stages, we used a combination of MSE loss between\ngenerated and target mel-spectrograms, adversarial loss, and feature\nmatching loss following the original HiFi-GAN paper&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "trained",
                    "original",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Sidon follows similar architecture to Miipher-2 apart from three main differences.\nThe first difference is the SSL model. Miipher-2 uses the 13th layer hidden state of 2B parameter USM model. However, this USM model is closed source. Therefore, Sidon uses the 8th layer hidden state of open-sourced 600M parameter w2v-BERT 2.0 model.\nThe second difference is that Sidon produces 48&#160;kHz full-fidelity speech using HiFi-GAN with snake activation as its vocoder while Miipher-2 produces 24&#160;kHz speech using WaveFiT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nLastly, Miipher-2 was trained on the 3,195 hours of Google internal data while Sidon is trained on 2,219 hours of curated high-quality public datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "sidon",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To validate the speech restoration performance and multilingual ability of Sidon,\nwe performed following experiments: (i) speech restoration in English and multilingual settings, (ii) dataset cleansing for zero-shot TTS model training, (iii) and inference speed comparison with different batch sizes.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For clean speech dataset, we used corpora listed on Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.3 Dataset collection &#8227; 3 Sidon &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWhen the training, valid, and test sets were specified, the split were followed.\nOtherwise, all samples were used as the training set.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Data preparation:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To train a generalized speech restoration model, diversity in degradation is important as we never know what kind of artifact present in the in-the-wild data. Therefore, we used a degradation simulation pipeline following previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThis pipeline applies degradation in following order: reverberation,\nbackground noise, band limitation, clipping, codec, and packet loss. Each degradations\nwas applied with a probability of 50%.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Clipping: The input speech was randomly clipped by setting its new\nminimum value to the value corresponding to a quantile uniformly\nchosen between the 0th and 10th percentiles, and its new maximum value\nto the value corresponding to a quantile uniformly chosen between the\n90th and 100th percentiles of the original signal.</span>\n</p>\n\n",
                "matched_terms": [
                    "original",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The noising pipeline was applied four times. As a result,\nwe obtained roughly 9,000 hours of paired (clean, noisy) speech data.</span>\n</p>\n\n",
                "matched_terms": [
                    "result",
                    "noisy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DNN optimization:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We used AdamW optimizer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib38\" title=\"\">38</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <math alttext=\"\\beta_{1}=0.9,\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#946;</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">0.9</mn>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#946;</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">0.999</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{1}=0.9,\\beta_{2}=0.999</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) with learning rate of </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with weight decay\nof </span>\n  <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.01</mn>\n      <annotation encoding=\"application/x-tex\">0.01</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Learning rate exponential decay was applied when training the\nvocoder to stabilize training with </span>\n  <math alttext=\"\\gamma=0.9998\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#947;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9998</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\gamma=0.9998</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The feature predictor was\ntrained for 400k steps (four days) with batch size of 256, and the vocoder\nwas pretrained for 140k steps (two days) and finetuned for 280k steps (four days)\nwith batch size of 32. All trainings were conducted on eight NVIDIA H200 GPUs. The number of parameters were 198M (five million trainable parameters) for the feature predictor and 52.4M for the vocoder. In total, Sidon has 250M parameters.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "all",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the English setting, we used &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets of LibriTTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib39\" title=\"\">39</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nEven though we do not have direct access to the Google&#8217;s internal Miipher\nmodel, restored samples on &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets are available as\nLibriTTS-R. We used these restored samples for evaluation.\nPlease note that Miipher is a text-conditioned model while Sidon is not. Therefore, Miipher uses ground-truth transcript upon restoration.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the multilingual setting, we used the test set of FLEURS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib40\" title=\"\">40</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSimilar to LibriTTS-R, FLEURS-R contains Miipher-restored samples in the test\nset. Note that Miipher used for restoring FLEURS in our experiment differed\nfrom the one used in LibriTTS-R and it had identical architecture to Miipher-2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nHowever, details of the model used in FLEURS-R such as training dataset are not disclosed in any\npapers. We refer to this model as &#8220;Miipher-2&#8221; in this paper for convenience.\nWe noticed that FLEURS-R lacks approximately 10% of samples from FLEURS.\nTherefore, we only used test set samples included in FLEURS-R.\nUnlike Miipher, Miipher-2 is not a text-conditioned model. Therefore, we can expect fair comparison between Sidon and Miipher-2 regarding the text conditioning.\nEvaluation was conducted on 100 languages selected from 102 languages in FLEURS as we\ncould not find any publicly available ASR models supporting Filipino (fil_ph) and Oriya (or_in).</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "test",
                    "between",
                    "sidon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We used four metrics for evaluation. For spoken content preservation, we\nused word error rate (WER) in the English setting and character error rate (CER) in\nthe multilingual setting. These metrics measure distance between transcribed results\nby an ASR model against the ground-truth scripts. For the ASR model, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">mms-1B-all</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib41\" title=\"\">41</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available\non HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/facebook/mms-1b-all\" title=\"\">https://hf.co/facebook/mms-1b-all</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which covers\n1,162 languages with its model size of 1B parameter. For restoration quality,\nwe used NISQA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib42\" title=\"\">42</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and DNSMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib43\" title=\"\">43</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> quality predictors.\nThese are commonly used for quantifying the quality of restored speech. Specifically,\nwe used overall quality scores predicted by NISQA and DNSMOS. For speaker preservation,\nwe used cosine similarity of speaker embeddings (SpkSim) those extracted from input\nnoisy speech and the restored speech. For speaker embedding extraction, we\nused </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">wavlm-base-plus-sv</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available on HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/microsoft/wavlm-base-plus-sv\" title=\"\">https://hf.co/microsoft/wavlm-base-plus-sv</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "model",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To investigate the effectiveness of Sidon as a preprocessing tool for TTS model training, we trained a TTS model on a Sidon-cleansed ASR corpus.\nFor the corpus to be cleansed, we used TED-LIUM release 3&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib44\" title=\"\">44</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is a 452 hours of speech dataset designed for ASR studies. It consists of TED talks recorded mostly in a hall. Therefore it contains reverberation and noise.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "tedlium",
                    "model",
                    "release",
                    "sidon",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For TTS model, we used F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib45\" title=\"\">45</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is a zero-shot TTS\nmodel based on flow matching&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib46\" title=\"\">46</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For comparison, we\ncompared Sidon with two conventional open-source speech restoration / separation models\nfor cleansing TED-LIUM release 3: VoiceFixer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Demucs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib47\" title=\"\">47</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. VoiceFixer is an open-source speech restoration model, and Demucs is an open-source music separation model capable of separating speech from background music; both have been used for dataset cleansing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib49\" title=\"\">49</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe compared &#8220;Original&#8221; (i.e., unprocessed) TED-LIUM release 3 to\ninvestigate the effectiveness of dataset cleansing.\nUpon training of F5-TTS, we resampled all samples to 24&#160;kHz and performed training\nusing the base model configuration of F5-TTS. The training was performed for 750k steps on eight H200 GPUs.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts",
                    "tedlium",
                    "model",
                    "all",
                    "release",
                    "demucs",
                    "sidon",
                    "“original”",
                    "voicefixer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For evaluation, we performed a five-scale mean opinion score (MOS) test on the sound\nquality following previous work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The number of raters was 120\nand each raters evaluated 16 samples. To prepare test samples,\nwe used the initial 40% of each utterance as the speaker prompt and the other\n60% were synthesized and used for evaluation.</span>\n</p>\n\n",
                "matched_terms": [
                    "mos",
                    "each",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T2.st1\" style=\"font-size:90%;\" title=\"In Table 2 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">2(a)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T2.st2\" style=\"font-size:90%;\" title=\"In Table 2 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">2(b)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> show the result of speech\nrestoration for &#8220;test-clean&#8221; and &#8220;test-other&#8221; subsets in the English setting,\nrespectively.\nThe results show that Sidon achieves strong performance on &#8220;test-clean&#8221; and better\nperformance on &#8220;test-other&#8221; in all metrics except for WER. Please note that Miipher is a text-conditioned model while Sidon is not. Therefore, Miipher has an advantage in\nWER as it can utilize textual information. Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.1 Experimental conditions &#8227; 4 Experiment &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nshows the result of multilingual speech restoration.\nIn terms of sound quality, a comparison of DNSMOS and NISQA scores between the noisy inputs and the Sidon outputs shows that Sidon consistently improves the sound quality in any tested language, exhibiting its success in speech restoration.\nOn average, Sidon\noutperforms Miipher-2 in CER and DNSMOS, is comparable in SpkSim, and is slightly worse in NISQA.\nTherefore, Sidon&#8217;s performance is comparable to Miipher-2 despite relying on the smaller SSL model.\nThis may be due to the use of a more comprehensive degradation simulation pipeline with six different degradations whereas Miipher-2 only simulates reverberation, background noise and codec artifacts.\nAdditionally, please note that NISQA might not be well-suited for multilingual evaluation as its trained on subjective evaluation results from English speech only.</span>\n</p>\n\n",
                "matched_terms": [
                    "result",
                    "model",
                    "all",
                    "except",
                    "between",
                    "sidon",
                    "noisy",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we presented Sidon, an open-source multilingual speech restoration\nmodel designed for large-scale dataset cleansing. Sidon is computationally\nefficient, enabling practical use in large corpus cleansing, and supports\ndiverse languages beyond English. Experimental results demonstrated that Sidon\nachieves performance comparable to closed-source models like Miipher and effectively\nimproves the quality of training data, thereby enhancing downstream speech\nsynthesis quality. We believe that Sidon will be a valuable tool for the speech\nsynthesis community, facilitating the development of high-quality, large-scale\nspeech datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "sidon",
                    "models",
                    "model"
                ]
            }
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Batch scaling of elapsed time and RTF on GPU (NVIDIA\nH200). Results measured with 30 seconds 16 kHz audio input.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Batch size</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">RTF</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.002\\,260\\,235\\,5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m1\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.002&#8201;260&#8201;235&#8201;5</mn><annotation encoding=\"application/x-tex\">0.002\\,260\\,235\\,5</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></th>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.002\\,096\\,504\\,7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m2\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.002&#8201;096&#8201;504&#8201;7</mn><annotation encoding=\"application/x-tex\">0.002\\,096\\,504\\,7</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></th>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.002\\,050\\,022\\,2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m3\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.002&#8201;050&#8201;022&#8201;2</mn><annotation encoding=\"application/x-tex\">0.002\\,050\\,022\\,2</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.001\\,998\\,892\\,6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m4\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.001&#8201;998&#8201;892&#8201;6</mn><annotation encoding=\"application/x-tex\">0.001\\,998\\,892\\,6</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "time",
            "h200",
            "scaling",
            "seconds",
            "audio",
            "measured",
            "batch",
            "size",
            "khz",
            "elapsed",
            "results",
            "nvidia",
            "input",
            "gpu",
            "rtf"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#S5.T5\" style=\"font-size:90%;\" title=\"Table 5 &#8227; 5.3 Inference speed evaluation result &#8227; 5 Results &#8227; Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing\">\n    <span class=\"ltx_text ltx_ref_tag\">5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes Sidon&#8217;s inference efficiency (RTF) as a function of batch size. With batch size&#160;8 on a single GPU, Sidon runs approximately 500</span>\n  <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#215;</mo>\n      <annotation encoding=\"application/x-tex\">\\times</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> faster than real time implying that restoring a 1M-hour corpus would take about 2000 hours on one GPU&#8212;substantially reducing the cost of constructing large-scale studio quality speech datasets.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large-scale text-to-speech (TTS) systems are limited by the scarcity of clean,\nmultilingual recordings. We introduce <span class=\"ltx_text ltx_font_bold\">Sidon</span>, a fast, open-source\nspeech restoration model that converts noisy in-the-wild speech into\nstudio-quality speech and scales to dozens of languages. Sidon consists of\ntwo models: w2v-BERT 2.0 finetuned feature predictor to cleanse features from noisy speech\nand vocoder trained to synthesize restored speech from the cleansed features.\nSidon achieves restoration performance comparable to Miipher: Google&#8217;s internal speech restoration model with\nthe aim of dataset cleansing for speech synthesis. Sidon is also computationally\nefficient, running up to 500&#8201;<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> faster than real time on a\nsingle GPU. We further show that training a TTS model using a Sidon-cleansed automatic speech\nrecognition corpus improves the quality of synthetic\nspeech in a zero-shot setting. Code and model are released to\nfacilitate reproducible dataset cleansing for the research community.</span>\n</p>\n\n",
                "matched_terms": [
                    "time",
                    "gpu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in deep learning have demonstrated remarkable\nprogress across a variety of domains, largely enabled by scaling model\ncapacity and dataset size. Similar trends can be observed in speech synthesis,\nwhere data quantity and quality play a critical role to improve the quality\nof synthetic speech. In fact, recent text-to-speech (TTS) models are trained on tens of\nthousand hours of speech corpus&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and for spoken language\nmodeling, the number becomes even larger as they are trained on million\nhours of speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, unlike text or image domains, expanding\ntraining data by crawling in-the-wild speech samples for speech synthesis is\nparticularly challenging due to the presence of noise, artifacts, and recording\nenvironment.</span>\n</p>\n\n",
                "matched_terms": [
                    "size",
                    "scaling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Speech restoration, also known as universal speech enhancement, aims to recover clean speech from signals degraded by noise, reverberation, bandwidth limitations, and other artifacts. VoiceFixer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced a two-stage analysis&#8211;synthesis pipeline that predicts intermediate representations from degraded speech and uses a neural vocoder to synthesize high-fidelity audio; it restores speech corrupted by noise, reverberation, clipping, and also upscales low-bandwidth signals to 44.1&#160;kHz. Several work explored the generative modeling approach for speech restoration&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and demonstrated its effectiveness for improving the perceptual quality of restored speech. However, these work have focused on English, limited noise conditions, and relatively small training sets, yielding insufficient generalization for large-scale dataset cleansing.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "khz"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Band limitation: The input speech was randomly resampled at {8,\n16, 22.05, 24, 44.1, 48}&#160;kHz sampling rate before being converted\nback to the original sampling rate.</span>\n</p>\n\n",
                "matched_terms": [
                    "input",
                    "khz"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DNN architecture:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For feature predictor, we set the LoRA adapter\nsetting to </span>\n  <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">16</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha=16</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, LoRA dropout of 0.1 and rank of </span>\n  <math alttext=\"64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">64</mn>\n      <annotation encoding=\"application/x-tex\">64</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For the\nvocoder, we set the upsampling rates to </span>\n  <math alttext=\"\\{8,5,4,3,2\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n        <mn mathsize=\"0.900em\">8</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">5</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">4</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\{8,5,4,3,2\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, resulting in a 960 times\nupsampling for converting 50&#160;Hz w2v-BERT 2.0 features into 48&#160;kHz full fidelity speech\nand input channels to 1,536 to match the dimension of SSL feature. For other\nhyperparameters regarding the vocoder, we followed the same values as the\nprevious work&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "input",
                    "khz"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DNN optimization:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We used AdamW optimizer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib38\" title=\"\">38</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <math alttext=\"\\beta_{1}=0.9,\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#946;</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">0.9</mn>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">&#946;</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">0.999</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{1}=0.9,\\beta_{2}=0.999</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) with learning rate of </span>\n  <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with weight decay\nof </span>\n  <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.01</mn>\n      <annotation encoding=\"application/x-tex\">0.01</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Learning rate exponential decay was applied when training the\nvocoder to stabilize training with </span>\n  <math alttext=\"\\gamma=0.9998\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#947;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9998</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\gamma=0.9998</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The feature predictor was\ntrained for 400k steps (four days) with batch size of 256, and the vocoder\nwas pretrained for 140k steps (two days) and finetuned for 280k steps (four days)\nwith batch size of 32. All trainings were conducted on eight NVIDIA H200 GPUs. The number of parameters were 198M (five million trainable parameters) for the feature predictor and 52.4M for the vocoder. In total, Sidon has 250M parameters.</span>\n</p>\n\n",
                "matched_terms": [
                    "batch",
                    "h200",
                    "size",
                    "nvidia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We used four metrics for evaluation. For spoken content preservation, we\nused word error rate (WER) in the English setting and character error rate (CER) in\nthe multilingual setting. These metrics measure distance between transcribed results\nby an ASR model against the ground-truth scripts. For the ASR model, </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">mms-1B-all</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib41\" title=\"\">41</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available\non HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/facebook/mms-1b-all\" title=\"\">https://hf.co/facebook/mms-1b-all</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which covers\n1,162 languages with its model size of 1B parameter. For restoration quality,\nwe used NISQA&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib42\" title=\"\">42</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and DNSMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib43\" title=\"\">43</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> quality predictors.\nThese are commonly used for quantifying the quality of restored speech. Specifically,\nwe used overall quality scores predicted by NISQA and DNSMOS. For speaker preservation,\nwe used cosine similarity of speaker embeddings (SpkSim) those extracted from input\nnoisy speech and the restored speech. For speaker embedding extraction, we\nused </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">wavlm-base-plus-sv</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> available on HuggingFace</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">3</sup>\n        <span class=\"ltx_tag ltx_tag_note\">3</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/microsoft/wavlm-base-plus-sv\" title=\"\">https://hf.co/microsoft/wavlm-base-plus-sv</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "size",
                    "input",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For TTS model, we used F5-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib45\" title=\"\">45</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is a zero-shot TTS\nmodel based on flow matching&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib46\" title=\"\">46</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For comparison, we\ncompared Sidon with two conventional open-source speech restoration / separation models\nfor cleansing TED-LIUM release 3: VoiceFixer&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Demucs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib47\" title=\"\">47</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. VoiceFixer is an open-source speech restoration model, and Demucs is an open-source music separation model capable of separating speech from background music; both have been used for dataset cleansing&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17052v2#bib.bib49\" title=\"\">49</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe compared &#8220;Original&#8221; (i.e., unprocessed) TED-LIUM release 3 to\ninvestigate the effectiveness of dataset cleansing.\nUpon training of F5-TTS, we resampled all samples to 24&#160;kHz and performed training\nusing the base model configuration of F5-TTS. The training was performed for 750k steps on eight H200 GPUs.</span>\n</p>\n\n",
                "matched_terms": [
                    "h200",
                    "khz"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To efficiently cleanse large-scale in-the-wild corpora, the cleansing pipeline must be computationally lightweight.\nWe therefore measured the real-time factor (RTF) of Sidon on an NVIDIA H200 using batch sizes\n</span>\n  <math alttext=\"\\{1,2,4,8\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">4</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mn mathsize=\"0.900em\">8</mn>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\{1,2,4,8\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">bfloat16</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe measurements were taken with 16&#160;kHz 30 seconds speech input.</span>\n</p>\n\n",
                "matched_terms": [
                    "h200",
                    "seconds",
                    "nvidia",
                    "batch",
                    "khz",
                    "measured",
                    "input",
                    "rtf"
                ]
            }
        ]
    }
}