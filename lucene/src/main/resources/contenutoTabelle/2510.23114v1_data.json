{
    "S4.T1": {
        "source_file": "Flexing in 73 Languages: A Single Small Model for Multilingual Inflection",
        "caption": "Table 1: SIGMORPHON 2022 (Feature Overlap):\nComparison across all 16 development languages that provided both large training dataset (7k examples) and test data under the feature overlap evaluation condition.\nReported exact-match accuracy (%).\n‡ marks datasets with less than 100 test items. The systems are: CLUZH [26], Flexica [19], OSU [5], TüM [15], UBC [27], and baselines [12], Sou is Sourada et al. (2024) [20].",
        "body": "Submitted systems\nBaselines\nSou\nOURS\n\n\nlang\nCLUZH\nFlexica\nOSU\nTüM\nUBC\nNeural\nNonNeur\nLSTM\nTRM\nmono\nmulti\n\n\nang\n76.6\n64.4\n73.7\n71.9\n74.1\n73.4\n68.7\n76.3\n75.5\n73.4\n77.3\n\n\nara\n81.7\n65.5\n78.7\n78.5\n65.5\n81.9\n50.8\n79.2\n82.6\n81.0\n79.6\n\n\nasm‡\n\n83.3\n75.0\n75.0\n91.7\n83.3\n83.3\n83.3\n83.3\n83.3\n83.3\n83.3\n\n\ngot\n92.9\n41.4\n94.1\n91.7\n91.7\n93.5\n87.6\n92.3\n92.3\n91.1\n91.7\n\n\nhun\n93.5\n62.9\n93.1\n92.8\n91.5\n94.4\n73.1\n92.8\n94.4\n93.2\n93.2\n\n\nkat\n96.7\n95.7\n96.7\n96.7\n96.7\n97.3\n96.7\n97.3\n97.8\n96.7\n97.3\n\n\nkhk‡\n\n94.1\n47.1\n94.1\n94.1\n88.2\n94.1\n88.2\n100.0\n94.1\n94.1\n94.1\n\n\nkor‡\n\n71.1\n55.4\n50.6\n56.6\n60.2\n62.7\n59.0\n49.4\n62.7\n63.9\n62.7\n\n\nkrl\n87.5\n69.8\n85.9\n57.8\n85.4\n57.8\n20.8\n89.1\n85.9\n85.9\n86.5\n\n\nlud\n87.3\n92.0\n92.9\n93.4\n88.2\n94.3\n93.4\n89.2\n92.0\n91.5\n93.4\n\n\nnon‡\n\n85.2\n77.0\n85.2\n80.3\n90.2\n88.5\n80.3\n83.6\n88.5\n90.2\n86.9\n\n\npol\n96.1\n85.9\n94.9\n74.0\n95.7\n74.4\n86.3\n96.1\n95.6\n95.7\n95.5\n\n\npoma\n76.1\n54.5\n70.1\n69.4\n73.3\n74.1\n47.8\n75.2\n76.3\n73.9\n72.8\n\n\nslk\n93.5\n90.0\n92.2\n70.4\n95.7\n71.1\n92.4\n95.2\n95.7\n94.4\n95.2\n\n\ntur\n93.7\n57.9\n95.2\n80.2\n92.9\n79.4\n66.7\n95.2\n92.9\n91.3\n95.2\n\n\nvep\n71.5\n58.8\n70.0\n57.5\n68.8\n59.2\n60.4\n70.7\n68.8\n67.8\n70.2\n\n\navg\n86.3\n68.3\n83.9\n78.6\n83.8\n80.0\n72.2\n85.3\n86.2\n85.5\n85.9",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\">Submitted systems</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">Baselines</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">Sou</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">OURS</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">lang</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">CLUZH</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Flexica</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">OSU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">T&#252;M</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">UBC</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Neural</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">NonNeur</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">LSTM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">TRM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">mono</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">multi</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">ang</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">76.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">73.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">71.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">74.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">73.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">76.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">73.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">77.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">ara</td>\n<td class=\"ltx_td ltx_align_center\">81.7</td>\n<td class=\"ltx_td ltx_align_center\">65.5</td>\n<td class=\"ltx_td ltx_align_center\">78.7</td>\n<td class=\"ltx_td ltx_align_center\">78.5</td>\n<td class=\"ltx_td ltx_align_center\">65.5</td>\n<td class=\"ltx_td ltx_align_center\">81.9</td>\n<td class=\"ltx_td ltx_align_center\">50.8</td>\n<td class=\"ltx_td ltx_align_center\">79.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">82.6</span></td>\n<td class=\"ltx_td ltx_align_center\">81.0</td>\n<td class=\"ltx_td ltx_align_center\">79.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">asm<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8225;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">83.3</td>\n<td class=\"ltx_td ltx_align_center\">75.0</td>\n<td class=\"ltx_td ltx_align_center\">75.0</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">91.7</span></td>\n<td class=\"ltx_td ltx_align_center\">83.3</td>\n<td class=\"ltx_td ltx_align_center\">83.3</td>\n<td class=\"ltx_td ltx_align_center\">83.3</td>\n<td class=\"ltx_td ltx_align_center\">83.3</td>\n<td class=\"ltx_td ltx_align_center\">83.3</td>\n<td class=\"ltx_td ltx_align_center\">83.3</td>\n<td class=\"ltx_td ltx_align_center\">83.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">got</td>\n<td class=\"ltx_td ltx_align_center\">92.9</td>\n<td class=\"ltx_td ltx_align_center\">41.4</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">94.1</span></td>\n<td class=\"ltx_td ltx_align_center\">91.7</td>\n<td class=\"ltx_td ltx_align_center\">91.7</td>\n<td class=\"ltx_td ltx_align_center\">93.5</td>\n<td class=\"ltx_td ltx_align_center\">87.6</td>\n<td class=\"ltx_td ltx_align_center\">92.3</td>\n<td class=\"ltx_td ltx_align_center\">92.3</td>\n<td class=\"ltx_td ltx_align_center\">91.1</td>\n<td class=\"ltx_td ltx_align_center\">91.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">hun</td>\n<td class=\"ltx_td ltx_align_center\">93.5</td>\n<td class=\"ltx_td ltx_align_center\">62.9</td>\n<td class=\"ltx_td ltx_align_center\">93.1</td>\n<td class=\"ltx_td ltx_align_center\">92.8</td>\n<td class=\"ltx_td ltx_align_center\">91.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">94.4</span></td>\n<td class=\"ltx_td ltx_align_center\">73.1</td>\n<td class=\"ltx_td ltx_align_center\">92.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">94.4</span></td>\n<td class=\"ltx_td ltx_align_center\">93.2</td>\n<td class=\"ltx_td ltx_align_center\">93.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">kat</td>\n<td class=\"ltx_td ltx_align_center\">96.7</td>\n<td class=\"ltx_td ltx_align_center\">95.7</td>\n<td class=\"ltx_td ltx_align_center\">96.7</td>\n<td class=\"ltx_td ltx_align_center\">96.7</td>\n<td class=\"ltx_td ltx_align_center\">96.7</td>\n<td class=\"ltx_td ltx_align_center\">97.3</td>\n<td class=\"ltx_td ltx_align_center\">96.7</td>\n<td class=\"ltx_td ltx_align_center\">97.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">97.8</span></td>\n<td class=\"ltx_td ltx_align_center\">96.7</td>\n<td class=\"ltx_td ltx_align_center\">97.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">khk<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8225;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">94.1</td>\n<td class=\"ltx_td ltx_align_center\">47.1</td>\n<td class=\"ltx_td ltx_align_center\">94.1</td>\n<td class=\"ltx_td ltx_align_center\">94.1</td>\n<td class=\"ltx_td ltx_align_center\">88.2</td>\n<td class=\"ltx_td ltx_align_center\">94.1</td>\n<td class=\"ltx_td ltx_align_center\">88.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">100.0</span></td>\n<td class=\"ltx_td ltx_align_center\">94.1</td>\n<td class=\"ltx_td ltx_align_center\">94.1</td>\n<td class=\"ltx_td ltx_align_center\">94.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">kor<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8225;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">71.1</span></td>\n<td class=\"ltx_td ltx_align_center\">55.4</td>\n<td class=\"ltx_td ltx_align_center\">50.6</td>\n<td class=\"ltx_td ltx_align_center\">56.6</td>\n<td class=\"ltx_td ltx_align_center\">60.2</td>\n<td class=\"ltx_td ltx_align_center\">62.7</td>\n<td class=\"ltx_td ltx_align_center\">59.0</td>\n<td class=\"ltx_td ltx_align_center\">49.4</td>\n<td class=\"ltx_td ltx_align_center\">62.7</td>\n<td class=\"ltx_td ltx_align_center\">63.9</td>\n<td class=\"ltx_td ltx_align_center\">62.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">krl</td>\n<td class=\"ltx_td ltx_align_center\">87.5</td>\n<td class=\"ltx_td ltx_align_center\">69.8</td>\n<td class=\"ltx_td ltx_align_center\">85.9</td>\n<td class=\"ltx_td ltx_align_center\">57.8</td>\n<td class=\"ltx_td ltx_align_center\">85.4</td>\n<td class=\"ltx_td ltx_align_center\">57.8</td>\n<td class=\"ltx_td ltx_align_center\">20.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">89.1</span></td>\n<td class=\"ltx_td ltx_align_center\">85.9</td>\n<td class=\"ltx_td ltx_align_center\">85.9</td>\n<td class=\"ltx_td ltx_align_center\">86.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">lud</td>\n<td class=\"ltx_td ltx_align_center\">87.3</td>\n<td class=\"ltx_td ltx_align_center\">92.0</td>\n<td class=\"ltx_td ltx_align_center\">92.9</td>\n<td class=\"ltx_td ltx_align_center\">93.4</td>\n<td class=\"ltx_td ltx_align_center\">88.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">94.3</span></td>\n<td class=\"ltx_td ltx_align_center\">93.4</td>\n<td class=\"ltx_td ltx_align_center\">89.2</td>\n<td class=\"ltx_td ltx_align_center\">92.0</td>\n<td class=\"ltx_td ltx_align_center\">91.5</td>\n<td class=\"ltx_td ltx_align_center\">93.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">non<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8225;</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">85.2</td>\n<td class=\"ltx_td ltx_align_center\">77.0</td>\n<td class=\"ltx_td ltx_align_center\">85.2</td>\n<td class=\"ltx_td ltx_align_center\">80.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">90.2</span></td>\n<td class=\"ltx_td ltx_align_center\">88.5</td>\n<td class=\"ltx_td ltx_align_center\">80.3</td>\n<td class=\"ltx_td ltx_align_center\">83.6</td>\n<td class=\"ltx_td ltx_align_center\">88.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">90.2</span></td>\n<td class=\"ltx_td ltx_align_center\">86.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">pol</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">96.1</span></td>\n<td class=\"ltx_td ltx_align_center\">85.9</td>\n<td class=\"ltx_td ltx_align_center\">94.9</td>\n<td class=\"ltx_td ltx_align_center\">74.0</td>\n<td class=\"ltx_td ltx_align_center\">95.7</td>\n<td class=\"ltx_td ltx_align_center\">74.4</td>\n<td class=\"ltx_td ltx_align_center\">86.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">96.1</span></td>\n<td class=\"ltx_td ltx_align_center\">95.6</td>\n<td class=\"ltx_td ltx_align_center\">95.7</td>\n<td class=\"ltx_td ltx_align_center\">95.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">poma</td>\n<td class=\"ltx_td ltx_align_center\">76.1</td>\n<td class=\"ltx_td ltx_align_center\">54.5</td>\n<td class=\"ltx_td ltx_align_center\">70.1</td>\n<td class=\"ltx_td ltx_align_center\">69.4</td>\n<td class=\"ltx_td ltx_align_center\">73.3</td>\n<td class=\"ltx_td ltx_align_center\">74.1</td>\n<td class=\"ltx_td ltx_align_center\">47.8</td>\n<td class=\"ltx_td ltx_align_center\">75.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">76.3</span></td>\n<td class=\"ltx_td ltx_align_center\">73.9</td>\n<td class=\"ltx_td ltx_align_center\">72.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">slk</td>\n<td class=\"ltx_td ltx_align_center\">93.5</td>\n<td class=\"ltx_td ltx_align_center\">90.0</td>\n<td class=\"ltx_td ltx_align_center\">92.2</td>\n<td class=\"ltx_td ltx_align_center\">70.4</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.7</span></td>\n<td class=\"ltx_td ltx_align_center\">71.1</td>\n<td class=\"ltx_td ltx_align_center\">92.4</td>\n<td class=\"ltx_td ltx_align_center\">95.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.7</span></td>\n<td class=\"ltx_td ltx_align_center\">94.4</td>\n<td class=\"ltx_td ltx_align_center\">95.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">tur</td>\n<td class=\"ltx_td ltx_align_center\">93.7</td>\n<td class=\"ltx_td ltx_align_center\">57.9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.2</span></td>\n<td class=\"ltx_td ltx_align_center\">80.2</td>\n<td class=\"ltx_td ltx_align_center\">92.9</td>\n<td class=\"ltx_td ltx_align_center\">79.4</td>\n<td class=\"ltx_td ltx_align_center\">66.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.2</span></td>\n<td class=\"ltx_td ltx_align_center\">92.9</td>\n<td class=\"ltx_td ltx_align_center\">91.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">vep</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">71.5</span></td>\n<td class=\"ltx_td ltx_align_center\">58.8</td>\n<td class=\"ltx_td ltx_align_center\">70.0</td>\n<td class=\"ltx_td ltx_align_center\">57.5</td>\n<td class=\"ltx_td ltx_align_center\">68.8</td>\n<td class=\"ltx_td ltx_align_center\">59.2</td>\n<td class=\"ltx_td ltx_align_center\">60.4</td>\n<td class=\"ltx_td ltx_align_center\">70.7</td>\n<td class=\"ltx_td ltx_align_center\">68.8</td>\n<td class=\"ltx_td ltx_align_center\">67.8</td>\n<td class=\"ltx_td ltx_align_center\">70.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">avg</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">86.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">68.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">83.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">78.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">83.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">80.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">72.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">85.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">86.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">85.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">85.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "baselines",
            "both",
            "ara",
            "submitted",
            "sou",
            "data",
            "reported",
            "development",
            "exactmatch",
            "datasets",
            "lstm",
            "ang",
            "krl",
            "tur",
            "avg",
            "trm",
            "than",
            "tüm",
            "kat",
            "condition",
            "hun",
            "kor‡",
            "evaluation",
            "cluzh",
            "accuracy",
            "lang",
            "feature",
            "languages",
            "provided",
            "items",
            "mono",
            "nonneur",
            "ubc",
            "osu",
            "systems",
            "under",
            "poma",
            "comparison",
            "multi",
            "got",
            "sigmorphon",
            "examples",
            "large",
            "overlap",
            "training",
            "asm‡",
            "sourada",
            "less",
            "test",
            "dataset",
            "flexica",
            "slk",
            "ours",
            "across",
            "neural",
            "khk‡",
            "all",
            "pol",
            "non‡",
            "vep",
            "lud",
            "marks"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our monolingual and multilingual systems achieve competitive performance on a global inflection benchmark, specifically the SIGMORPHON Inflection Shared Tasks from 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> and 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite>, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T1\" title=\"Table 1 &#8227; 4.0.2 Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T2\" title=\"Table 2 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, respectively:\n<span class=\"ltx_text ltx_font_smallcaps\">mono</span> ranks 4th in the 2022 data and 6th in the 2023 data, while <span class=\"ltx_text ltx_font_smallcaps\">multi</span> attains 3rd place in 2022 and 1st place in 2023, based on the average across languages.\nFor clarity, we note that the models were trained from scratch exclusively on the SIGMORPHON benchmark data.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present a compact, single-model approach to multilingual inflection, the task of generating inflected word forms from base lemmas to express grammatical categories. Our model, trained jointly on data from 73 languages, is lightweight, robust to unseen words, and outperforms monolingual baselines in most languages. This demonstrates the effectiveness of multilingual modeling for inflection and highlights its practical benefits: simplifying deployment by eliminating the need to manage and retrain dozens of separate monolingual models.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "languages",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the standard SIGMORPHON shared task benchmarks, we evaluate our monolingual and multilingual models on 73 Universal Dependencies (UD) treebanks, extracting lemma-tag-form triples and their frequency counts. To ensure realistic data splits, we introduce a novel frequency-weighted, lemma-disjoint train-dev-test resampling procedure.</p>\n\n",
                "matched_terms": [
                    "sigmorphon",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work addresses the lack of an open-source, general-purpose, multilingual morphological inflection system capable of handling unseen words across a wide range of languages, including Czech. All code is publicly released at: <a class=\"ltx_ref ltx_url\" href=\"https://github.com/tomsouri/multilingual-inflection\" style=\"--ltx-fg-color:#0000FF;\" title=\"\">https://github.com/tomsouri/multilingual-inflection</a>.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the field&#8217;s high level of activity, driven mainly by the SIGMORPHON shared tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib24\" title=\"\">24</a>]</cite>,\nthere is still no open-source, lightweight, neural morphological inflection generator for unknown (out-of-vocabulary, OOV) words in Czech, let alone one supporting multiple languages, available within the Czech academic environment.\nMorphoDiTa&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib23\" title=\"\">23</a>]</cite> relies on a morphological dictionary for Czech and lacks an out-of-vocabulary (OOV) guesser for generation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Although MorphoDiTa includes OOV heuristics for morphological analysis, the inverse process of generation, it does not support OOV generation.</span></span></span> UDPipe <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib22\" title=\"\">22</a>]</cite> does not support inflection generation at all. And although Sourada et al. (2024) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib20\" title=\"\">20</a>]</cite> provide an OOV guesser, it is limited to Czech nouns.</p>\n\n",
                "matched_terms": [
                    "neural",
                    "all",
                    "languages",
                    "sigmorphon",
                    "sourada"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our multilingual system, trained jointly on 73 languages, is extremely lightweight, consisting of a single model with 5.69M trainable parameters, capable of performing inflection across all 73 languages. The model handles all parts of speech, inflects unknown words (with evaluation conducted entirely on words unseen during training), and crucially, outperforms both the baseline systems and separately trained monolingual models in most languages. Furthermore, it offers practical advantages for deployment: a single multilingual model eliminates the need to manage dozens of individual models in memory and significantly simplifies maintenance, avoiding the overhead associated with retraining 73 separate models.</p>\n\n",
                "matched_terms": [
                    "both",
                    "across",
                    "systems",
                    "all",
                    "evaluation",
                    "languages",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Morphological inflection has seen considerable research interest in the recent years, mainly thanks to the SIGMORPHON shared tasks held from 2016 to 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib24\" title=\"\">24</a>]</cite>.\nThis period was marked by major advancements in model architectures, particularly recurrent neural networks and Transformers.</p>\n\n",
                "matched_terms": [
                    "sigmorphon",
                    "neural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the SIGMORPHON 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> and the SIGMORPHON 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite> shared task benchmarks for evaluating the strength of our models.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Hebrew was excluded from SIGMORPHON 2023 due to filename inconsistencies that prevented reliable comparison.</span></span></span>\nFurthermore, with the goal of future deployment and multilingual applicability, we select 73 languages\nof the Universal Dependencies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib16\" title=\"\">16</a>]</cite> corpora<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The selection contains 32 Indo-European languages written in the Latin script and 41 languages without restriction on language family or script. For languages with multiple corpora in UD, we chose one corpus per language based on historical preferences, practical considerations, and the defaults in the widely adopted UDPipe tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib21\" title=\"\">21</a>]</cite>. We prioritized large, canonical corpora, excluded those with narrow or specialized domains, and favored manually over semi-automatically annotated corpora. Corpora lacking lemma annotations were excluded.</span></span></span> and lexicalize them by extracting unique lemma-tag-form triples along with their occurrence counts for training and evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Unlike UniMorph, the Universal Dependencies data can be more prone to noise. We rely on a neural network to mitigate its impact.</span></span></span></p>\n\n",
                "matched_terms": [
                    "neural",
                    "comparison",
                    "data",
                    "languages",
                    "sigmorphon",
                    "large",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Strategies for data splitting in the inflection field differ in the degree of overlap control between train-test splits, from an uncontrolled overlap (where lemma-tag-form may overlap randomly) to varying degrees of control over lemma and/or morphological feature overlap. The former has been the traditional approach for decades but has recently come under critique for hindering evaluation of models&#8217; generalization abilities.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "under",
                    "feature",
                    "data",
                    "overlap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the very least, a <span class=\"ltx_text ltx_font_italic\">lemma-disjoint</span> option\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib8\" title=\"\">8</a>]</cite> is recently advisable, and SIGMORPHON 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> probed this setting in its <span class=\"ltx_text ltx_font_italic\">feature overlap</span> setting: A test pair&#8217;s feature set is attested in training, but its lemma is novel. The latest SIGMORPHON shared task installation completely moved to the <span class=\"ltx_text ltx_font_italic\">lemma-disjoint</span> setting in 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite>. We use the official train-dev-test split for both the SIGMORPHON benchmarks.</p>\n\n",
                "matched_terms": [
                    "both",
                    "feature",
                    "sigmorphon",
                    "overlap",
                    "training",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the lack of standardized practice for sampling with corpus frequencies, we design\na new train-dev-test split technique for the UD datasets that satisfies both recent methodological standards (being <span class=\"ltx_text ltx_font_italic\">lemma-disjoint</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib8\" title=\"\">8</a>]</cite>) and practical requirements for real-world deployment by ensuring a realistic distribution of items with varying corpus frequencies through <span class=\"ltx_text ltx_font_italic\">frequency-weighted sampling</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib13\" title=\"\">13</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "datasets",
                    "items"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike previous work on multilingual inflection which usually used the SIGMORPHON shared task data where the train set for all languages was of the same size, we need to deal with the problem of mixing corpora of substantially different sizes. If we na&#239;vely pooled the training datasets, the model may be overly influenced by the high-resource languages. To address this issue, we adapt a corpus upsampling method by Wang et al. (2020) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib25\" title=\"\">25</a>]</cite>.\nFor better control over sampling into training batches, we also adapt a temperature value <math alttext=\"\\tau{}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau{}\\in[0,1]</annotation></semantics></math> (our <math alttext=\"\\tau=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\tau=0.5</annotation></semantics></math>) to smooth the distribution of training data from datasets of different sizes, see van der Goot et al. (2021) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib9\" title=\"\">9</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "datasets",
                    "data",
                    "languages",
                    "sigmorphon",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two-part Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T3\" title=\"Table 3 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>&#160;and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T4\" title=\"Table 4 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the results on 73 corpora from UD. The overall best-performing system is <span class=\"ltx_text ltx_font_smallcaps\">multi</span>, which was trained jointly on all 73 languages. Among these languages, it is outperformed in 3 cases by the simple <span class=\"ltx_text ltx_font_smallcaps\">copy</span> baseline, and in 7 cases by the monolingual models (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>), each trained separately for a single language.\nOn macro average across all languages, the <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model improves over the <span class=\"ltx_text ltx_font_smallcaps\">mono</span> models by 4.69%.</p>\n\n",
                "matched_terms": [
                    "across",
                    "all",
                    "multi",
                    "languages",
                    "mono"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focused on the task of automatic morphological inflection in a multilingual setting, with the aim of enabling future deployment in an open-source tool or web service. Trained jointly on 73 languages, our multilingual model is remarkably lightweight (5.69M trainable parameters), handles unseen words well, works across parts of speech, and importantly, outperforms monolingual models in most languages. We designed and implemented a novel frequency-weighted, lemma-disjoint train-dev-test split method, which combines recent evaluation practices (lemma-disjoint splits) with a realistic frequency distribution. This work establishes a foundation for deploying a practical and multilingual inflection system.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "languages",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our <span class=\"ltx_text ltx_font_smallcaps\">mono</span> and <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model, hyperparameters tuned on UD\ndevelopment data are: layers (encoder/decoder) = <math alttext=\"3/3\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m1\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo>/</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3/3</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>) and <math alttext=\"4/4\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo>/</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">4/4</annotation></semantics></math>&#160;(<span class=\"ltx_text ltx_font_smallcaps\">multi</span>),\nlayer dimension = <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m3\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math>, feed-forward dimension = <math alttext=\"64\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m4\" intent=\":literal\"><semantics><mn>64</mn><annotation encoding=\"application/x-tex\">64</annotation></semantics></math>, attention heads = <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m5\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, dropout = <math alttext=\"0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m6\" intent=\":literal\"><semantics><mn>0.15</mn><annotation encoding=\"application/x-tex\">0.15</annotation></semantics></math>, attention dropout = <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m7\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>, activation dropout = <math alttext=\"0.35\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m8\" intent=\":literal\"><semantics><mn>0.35</mn><annotation encoding=\"application/x-tex\">0.35</annotation></semantics></math>, and layer drop = <math alttext=\"0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m9\" intent=\":literal\"><semantics><mn>0.2</mn><annotation encoding=\"application/x-tex\">0.2</annotation></semantics></math>. We use gradient clipping (max norm 1.0), L2 regularization (0.01), and train with Adam\nusing cosine learning rate decay, initial LR&#160;=&#160;0.001, batch size = <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m10\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>) and <math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m11\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">multi</span>).\nCheckpoints are selected by dev set performance: on the target language for <span class=\"ltx_text ltx_font_smallcaps\">mono</span>, or macro-averaged across languages for <span class=\"ltx_text ltx_font_smallcaps\">multi</span>, over up to <math alttext=\"960\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m12\" intent=\":literal\"><semantics><mn>960</mn><annotation encoding=\"application/x-tex\">960</annotation></semantics></math> epochs.\nThe <span class=\"ltx_text ltx_font_smallcaps\">mono</span> model has 3.12M-3.69M parameters,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>varying by vocabulary size</span></span></span> while the <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model has 5.69M parameters.</p>\n\n",
                "matched_terms": [
                    "across",
                    "multi",
                    "data",
                    "development",
                    "languages",
                    "mono"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Flexing in 73 Languages: A Single Small Model for Multilingual Inflection",
        "caption": "Table 2: SIGMORPHON 2023 (Lemma Disjoint): exact-match accuracy (%), with systems AZ1-4 [14], TÜB [6], IL1-4 [1], baselines [7].\nAs multilingual training wasn’t clearly defined in 2023, we highlight (*) the best systems excluding multi (†).",
        "body": "Submitted systems and official baselines\nOURS\n\n\nlang\nAZ3\nAZ1\nnon-neur\nAZ2\nAZ4\nTÜB\nneur\nIL1\nIL2\nIL3\nIL4\nmono\nmulti†\n\n\nafb\n34.5\n30.8\n30.8\n52.7\n52.7\n75.8\n80.1\n80.7\n82.2\n84.1\n84.6\n80.3\n79.6\n\n\namh\n59.9\n65.4\n65.4\n74.0\n74.0\n83.8\n82.2\n88.9\n90.6*\n88.9\n88.6\n88.6\n90.8\n\n\narz\n75.7\n77.2\n77.9\n80.8\n80.8\n87.6\n89.6*\n89.2\n88.7\n89.1\n88.7\n88.5\n89.9\n\n\nbel\n46.2\n68.1\n68.1\n64.5\n64.5\n56.3\n74.5\n73.5\n74.7\n72.9\n72.9\n75.4*\n75.9\n\n\ndan\n64.8\n89.5\n89.5\n87.4\n87.4\n85.7\n88.8\n88.8\n89.5\n86.5\n87.5\n86.0\n89.4\n\n\ndeu\n59.9\n79.8\n79.8\n77.9\n77.9\n74.5\n83.7*\n79.7\n79.7\n80.2\n79.7\n80.5\n84.8\n\n\neng\n67.0\n96.6*\n96.6*\n96.2\n96.2\n96.0\n95.1\n95.6\n95.9\n94.6\n95.0\n94.5\n97.5\n\n\nfin\n48.2\n80.8\n80.8\n80.6\n80.6\n67.6\n85.4\n79.2\n80.6\n85.7\n86.1*\n84.1\n88.0\n\n\nfra\n76.7\n77.7*\n77.7*\n76.3\n76.3\n67.9\n73.3\n69.3\n74.7\n71.7\n72.9\n77.2\n80.0\n\n\ngrc\n40.4\n52.6\n52.6\n54.8\n54.8\n36.7\n54.0\n48.9\n53.7\n56.0*\n56.0*\n49.7\n61.7\n\n\nhun\n45.9\n74.7\n74.7\n74.7\n74.7\n75.9\n80.5\n76.3\n79.8\n84.3\n85.0*\n84.6\n88.9\n\n\nhye\n88.9\n86.3\n86.3\n86.2\n88.9\n85.9\n91.0\n88.4\n91.5\n94.4\n94.3\n95.1*\n98.4\n\n\nita\n78.0\n75.0\n75.0\n63.6\n78.0\n84.7\n94.1\n95.8\n97.2*\n92.1\n92.2\n95.8\n97.8\n\n\njap\n67.0\n64.1\n64.1\n64.1\n67.0\n95.3\n26.3\n92.8\n94.2\n94.9\n94.9\n36.2\n48.9\n\n\nkat\n71.7\n82.0\n82.0\n82.1\n82.1\n70.5\n84.5\n84.1\n84.7\n81.3\n82.9\n87.1\n85.4\n\n\nklr\n27.8\n54.5\n54.5\n53.1\n53.1\n96.4\n99.5\n99.4\n99.4\n99.4\n99.4\n99.1\n99.2\n\n\nmkd\n64.9\n91.6\n91.6\n90.8\n90.8\n86.7\n93.8\n91.9\n92.4\n92.1\n92.4\n92.9\n93.2\n\n\nnav\n23.7\n35.8\n35.8\n41.8\n41.8\n53.6\n52.1\n54.0\n55.1\n55.1\n55.6*\n52.6\n60.0\n\n\nrus\n66.8\n86.0\n86.0\n85.6\n85.6\n82.1\n90.5*\n87.4\n87.3\n84.2\n85.5\n87.8\n91.9\n\n\nsan\n47.0\n62.2\n62.2\n62.1\n62.1\n54.5\n66.3\n63.3\n69.1*\n67.7\n65.9\n68.6\n73.4\n\n\nsme\n30.1\n56.0\n56.0\n49.7\n49.7\n58.5\n74.8*\n69.9\n71.8\n67.4\n67.3\n73.0\n83.1\n\n\nspa\n86.3\n87.8\n87.8\n87.4\n87.4\n88.7\n93.6\n90.9\n91.4\n93.8\n93.1\n94.9*\n95.2\n\n\nsqi\n73.8\n19.3\n83.4\n78.1\n78.1\n71.5\n85.9\n87.6\n88.9\n92.0*\n91.6\n89.3\n93.0\n\n\nswa\n56.2\n60.5\n60.5\n65.0\n65.0\n94.7\n93.7\n93.1\n93.1\n96.6\n96.6\n98.4*\n99.2\n\n\ntur\n28.1\n64.6\n64.6\n64.6\n64.6\n81.8\n95.0*\n90.9\n90.8\n90.3\n92.0\n93.8\n95.8\n\n\navg\n57.2\n68.8\n71.4\n71.8\n72.6\n76.5\n81.1\n82.4\n83.9\n83.8\n84.0*\n82.2\n85.6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"11\">Submitted systems and official baselines</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">OURS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">lang</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AZ3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AZ1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">non-neur</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AZ2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AZ4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">T&#220;B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">neur</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">IL1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">IL2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">IL3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">IL4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">mono</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">multi<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_upright\">&#8224;</span></sup></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">afb</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">52.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">52.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">75.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">80.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">80.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">82.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">84.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">84.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">80.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">79.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">amh</th>\n<td class=\"ltx_td ltx_align_center\">59.9</td>\n<td class=\"ltx_td ltx_align_center\">65.4</td>\n<td class=\"ltx_td ltx_align_center\">65.4</td>\n<td class=\"ltx_td ltx_align_center\">74.0</td>\n<td class=\"ltx_td ltx_align_center\">74.0</td>\n<td class=\"ltx_td ltx_align_center\">83.8</td>\n<td class=\"ltx_td ltx_align_center\">82.2</td>\n<td class=\"ltx_td ltx_align_center\">88.9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">90.6<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">88.9</td>\n<td class=\"ltx_td ltx_align_center\">88.6</td>\n<td class=\"ltx_td ltx_align_center\">88.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">90.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">arz</th>\n<td class=\"ltx_td ltx_align_center\">75.7</td>\n<td class=\"ltx_td ltx_align_center\">77.2</td>\n<td class=\"ltx_td ltx_align_center\">77.9</td>\n<td class=\"ltx_td ltx_align_center\">80.8</td>\n<td class=\"ltx_td ltx_align_center\">80.8</td>\n<td class=\"ltx_td ltx_align_center\">87.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">89.6<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">89.2</td>\n<td class=\"ltx_td ltx_align_center\">88.7</td>\n<td class=\"ltx_td ltx_align_center\">89.1</td>\n<td class=\"ltx_td ltx_align_center\">88.7</td>\n<td class=\"ltx_td ltx_align_center\">88.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">89.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">bel</th>\n<td class=\"ltx_td ltx_align_center\">46.2</td>\n<td class=\"ltx_td ltx_align_center\">68.1</td>\n<td class=\"ltx_td ltx_align_center\">68.1</td>\n<td class=\"ltx_td ltx_align_center\">64.5</td>\n<td class=\"ltx_td ltx_align_center\">64.5</td>\n<td class=\"ltx_td ltx_align_center\">56.3</td>\n<td class=\"ltx_td ltx_align_center\">74.5</td>\n<td class=\"ltx_td ltx_align_center\">73.5</td>\n<td class=\"ltx_td ltx_align_center\">74.7</td>\n<td class=\"ltx_td ltx_align_center\">72.9</td>\n<td class=\"ltx_td ltx_align_center\">72.9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">75.4<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">75.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">dan</th>\n<td class=\"ltx_td ltx_align_center\">64.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">89.5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">89.5</span></td>\n<td class=\"ltx_td ltx_align_center\">87.4</td>\n<td class=\"ltx_td ltx_align_center\">87.4</td>\n<td class=\"ltx_td ltx_align_center\">85.7</td>\n<td class=\"ltx_td ltx_align_center\">88.8</td>\n<td class=\"ltx_td ltx_align_center\">88.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">89.5</span></td>\n<td class=\"ltx_td ltx_align_center\">86.5</td>\n<td class=\"ltx_td ltx_align_center\">87.5</td>\n<td class=\"ltx_td ltx_align_center\">86.0</td>\n<td class=\"ltx_td ltx_align_center\">89.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">deu</th>\n<td class=\"ltx_td ltx_align_center\">59.9</td>\n<td class=\"ltx_td ltx_align_center\">79.8</td>\n<td class=\"ltx_td ltx_align_center\">79.8</td>\n<td class=\"ltx_td ltx_align_center\">77.9</td>\n<td class=\"ltx_td ltx_align_center\">77.9</td>\n<td class=\"ltx_td ltx_align_center\">74.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">83.7<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">79.7</td>\n<td class=\"ltx_td ltx_align_center\">79.7</td>\n<td class=\"ltx_td ltx_align_center\">80.2</td>\n<td class=\"ltx_td ltx_align_center\">79.7</td>\n<td class=\"ltx_td ltx_align_center\">80.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">84.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">eng</th>\n<td class=\"ltx_td ltx_align_center\">67.0</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">96.6<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">96.6<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">96.2</td>\n<td class=\"ltx_td ltx_align_center\">96.2</td>\n<td class=\"ltx_td ltx_align_center\">96.0</td>\n<td class=\"ltx_td ltx_align_center\">95.1</td>\n<td class=\"ltx_td ltx_align_center\">95.6</td>\n<td class=\"ltx_td ltx_align_center\">95.9</td>\n<td class=\"ltx_td ltx_align_center\">94.6</td>\n<td class=\"ltx_td ltx_align_center\">95.0</td>\n<td class=\"ltx_td ltx_align_center\">94.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">97.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">fin</th>\n<td class=\"ltx_td ltx_align_center\">48.2</td>\n<td class=\"ltx_td ltx_align_center\">80.8</td>\n<td class=\"ltx_td ltx_align_center\">80.8</td>\n<td class=\"ltx_td ltx_align_center\">80.6</td>\n<td class=\"ltx_td ltx_align_center\">80.6</td>\n<td class=\"ltx_td ltx_align_center\">67.6</td>\n<td class=\"ltx_td ltx_align_center\">85.4</td>\n<td class=\"ltx_td ltx_align_center\">79.2</td>\n<td class=\"ltx_td ltx_align_center\">80.6</td>\n<td class=\"ltx_td ltx_align_center\">85.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">86.1<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">84.1</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">88.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">fra</th>\n<td class=\"ltx_td ltx_align_center\">76.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">77.7<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">77.7<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">76.3</td>\n<td class=\"ltx_td ltx_align_center\">76.3</td>\n<td class=\"ltx_td ltx_align_center\">67.9</td>\n<td class=\"ltx_td ltx_align_center\">73.3</td>\n<td class=\"ltx_td ltx_align_center\">69.3</td>\n<td class=\"ltx_td ltx_align_center\">74.7</td>\n<td class=\"ltx_td ltx_align_center\">71.7</td>\n<td class=\"ltx_td ltx_align_center\">72.9</td>\n<td class=\"ltx_td ltx_align_center\">77.2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">80.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">grc</th>\n<td class=\"ltx_td ltx_align_center\">40.4</td>\n<td class=\"ltx_td ltx_align_center\">52.6</td>\n<td class=\"ltx_td ltx_align_center\">52.6</td>\n<td class=\"ltx_td ltx_align_center\">54.8</td>\n<td class=\"ltx_td ltx_align_center\">54.8</td>\n<td class=\"ltx_td ltx_align_center\">36.7</td>\n<td class=\"ltx_td ltx_align_center\">54.0</td>\n<td class=\"ltx_td ltx_align_center\">48.9</td>\n<td class=\"ltx_td ltx_align_center\">53.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">56.0<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">56.0<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">49.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">61.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">hun</th>\n<td class=\"ltx_td ltx_align_center\">45.9</td>\n<td class=\"ltx_td ltx_align_center\">74.7</td>\n<td class=\"ltx_td ltx_align_center\">74.7</td>\n<td class=\"ltx_td ltx_align_center\">74.7</td>\n<td class=\"ltx_td ltx_align_center\">74.7</td>\n<td class=\"ltx_td ltx_align_center\">75.9</td>\n<td class=\"ltx_td ltx_align_center\">80.5</td>\n<td class=\"ltx_td ltx_align_center\">76.3</td>\n<td class=\"ltx_td ltx_align_center\">79.8</td>\n<td class=\"ltx_td ltx_align_center\">84.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">85.0<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">84.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">88.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">hye</th>\n<td class=\"ltx_td ltx_align_center\">88.9</td>\n<td class=\"ltx_td ltx_align_center\">86.3</td>\n<td class=\"ltx_td ltx_align_center\">86.3</td>\n<td class=\"ltx_td ltx_align_center\">86.2</td>\n<td class=\"ltx_td ltx_align_center\">88.9</td>\n<td class=\"ltx_td ltx_align_center\">85.9</td>\n<td class=\"ltx_td ltx_align_center\">91.0</td>\n<td class=\"ltx_td ltx_align_center\">88.4</td>\n<td class=\"ltx_td ltx_align_center\">91.5</td>\n<td class=\"ltx_td ltx_align_center\">94.4</td>\n<td class=\"ltx_td ltx_align_center\">94.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.1<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">98.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ita</th>\n<td class=\"ltx_td ltx_align_center\">78.0</td>\n<td class=\"ltx_td ltx_align_center\">75.0</td>\n<td class=\"ltx_td ltx_align_center\">75.0</td>\n<td class=\"ltx_td ltx_align_center\">63.6</td>\n<td class=\"ltx_td ltx_align_center\">78.0</td>\n<td class=\"ltx_td ltx_align_center\">84.7</td>\n<td class=\"ltx_td ltx_align_center\">94.1</td>\n<td class=\"ltx_td ltx_align_center\">95.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">97.2<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">92.1</td>\n<td class=\"ltx_td ltx_align_center\">92.2</td>\n<td class=\"ltx_td ltx_align_center\">95.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">97.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">jap</th>\n<td class=\"ltx_td ltx_align_center\">67.0</td>\n<td class=\"ltx_td ltx_align_center\">64.1</td>\n<td class=\"ltx_td ltx_align_center\">64.1</td>\n<td class=\"ltx_td ltx_align_center\">64.1</td>\n<td class=\"ltx_td ltx_align_center\">67.0</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.3</span></td>\n<td class=\"ltx_td ltx_align_center\">26.3</td>\n<td class=\"ltx_td ltx_align_center\">92.8</td>\n<td class=\"ltx_td ltx_align_center\">94.2</td>\n<td class=\"ltx_td ltx_align_center\">94.9</td>\n<td class=\"ltx_td ltx_align_center\">94.9</td>\n<td class=\"ltx_td ltx_align_center\">36.2</td>\n<td class=\"ltx_td ltx_align_center\">48.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">kat</th>\n<td class=\"ltx_td ltx_align_center\">71.7</td>\n<td class=\"ltx_td ltx_align_center\">82.0</td>\n<td class=\"ltx_td ltx_align_center\">82.0</td>\n<td class=\"ltx_td ltx_align_center\">82.1</td>\n<td class=\"ltx_td ltx_align_center\">82.1</td>\n<td class=\"ltx_td ltx_align_center\">70.5</td>\n<td class=\"ltx_td ltx_align_center\">84.5</td>\n<td class=\"ltx_td ltx_align_center\">84.1</td>\n<td class=\"ltx_td ltx_align_center\">84.7</td>\n<td class=\"ltx_td ltx_align_center\">81.3</td>\n<td class=\"ltx_td ltx_align_center\">82.9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">87.1</span></td>\n<td class=\"ltx_td ltx_align_center\">85.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">klr</th>\n<td class=\"ltx_td ltx_align_center\">27.8</td>\n<td class=\"ltx_td ltx_align_center\">54.5</td>\n<td class=\"ltx_td ltx_align_center\">54.5</td>\n<td class=\"ltx_td ltx_align_center\">53.1</td>\n<td class=\"ltx_td ltx_align_center\">53.1</td>\n<td class=\"ltx_td ltx_align_center\">96.4</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">99.5</span></td>\n<td class=\"ltx_td ltx_align_center\">99.4</td>\n<td class=\"ltx_td ltx_align_center\">99.4</td>\n<td class=\"ltx_td ltx_align_center\">99.4</td>\n<td class=\"ltx_td ltx_align_center\">99.4</td>\n<td class=\"ltx_td ltx_align_center\">99.1</td>\n<td class=\"ltx_td ltx_align_center\">99.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">mkd</th>\n<td class=\"ltx_td ltx_align_center\">64.9</td>\n<td class=\"ltx_td ltx_align_center\">91.6</td>\n<td class=\"ltx_td ltx_align_center\">91.6</td>\n<td class=\"ltx_td ltx_align_center\">90.8</td>\n<td class=\"ltx_td ltx_align_center\">90.8</td>\n<td class=\"ltx_td ltx_align_center\">86.7</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">93.8</span></td>\n<td class=\"ltx_td ltx_align_center\">91.9</td>\n<td class=\"ltx_td ltx_align_center\">92.4</td>\n<td class=\"ltx_td ltx_align_center\">92.1</td>\n<td class=\"ltx_td ltx_align_center\">92.4</td>\n<td class=\"ltx_td ltx_align_center\">92.9</td>\n<td class=\"ltx_td ltx_align_center\">93.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">nav</th>\n<td class=\"ltx_td ltx_align_center\">23.7</td>\n<td class=\"ltx_td ltx_align_center\">35.8</td>\n<td class=\"ltx_td ltx_align_center\">35.8</td>\n<td class=\"ltx_td ltx_align_center\">41.8</td>\n<td class=\"ltx_td ltx_align_center\">41.8</td>\n<td class=\"ltx_td ltx_align_center\">53.6</td>\n<td class=\"ltx_td ltx_align_center\">52.1</td>\n<td class=\"ltx_td ltx_align_center\">54.0</td>\n<td class=\"ltx_td ltx_align_center\">55.1</td>\n<td class=\"ltx_td ltx_align_center\">55.1</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">55.6<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">52.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">60.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">rus</th>\n<td class=\"ltx_td ltx_align_center\">66.8</td>\n<td class=\"ltx_td ltx_align_center\">86.0</td>\n<td class=\"ltx_td ltx_align_center\">86.0</td>\n<td class=\"ltx_td ltx_align_center\">85.6</td>\n<td class=\"ltx_td ltx_align_center\">85.6</td>\n<td class=\"ltx_td ltx_align_center\">82.1</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">90.5<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">87.4</td>\n<td class=\"ltx_td ltx_align_center\">87.3</td>\n<td class=\"ltx_td ltx_align_center\">84.2</td>\n<td class=\"ltx_td ltx_align_center\">85.5</td>\n<td class=\"ltx_td ltx_align_center\">87.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">91.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">san</th>\n<td class=\"ltx_td ltx_align_center\">47.0</td>\n<td class=\"ltx_td ltx_align_center\">62.2</td>\n<td class=\"ltx_td ltx_align_center\">62.2</td>\n<td class=\"ltx_td ltx_align_center\">62.1</td>\n<td class=\"ltx_td ltx_align_center\">62.1</td>\n<td class=\"ltx_td ltx_align_center\">54.5</td>\n<td class=\"ltx_td ltx_align_center\">66.3</td>\n<td class=\"ltx_td ltx_align_center\">63.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">69.1<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">67.7</td>\n<td class=\"ltx_td ltx_align_center\">65.9</td>\n<td class=\"ltx_td ltx_align_center\">68.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">73.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">sme</th>\n<td class=\"ltx_td ltx_align_center\">30.1</td>\n<td class=\"ltx_td ltx_align_center\">56.0</td>\n<td class=\"ltx_td ltx_align_center\">56.0</td>\n<td class=\"ltx_td ltx_align_center\">49.7</td>\n<td class=\"ltx_td ltx_align_center\">49.7</td>\n<td class=\"ltx_td ltx_align_center\">58.5</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">74.8<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">69.9</td>\n<td class=\"ltx_td ltx_align_center\">71.8</td>\n<td class=\"ltx_td ltx_align_center\">67.4</td>\n<td class=\"ltx_td ltx_align_center\">67.3</td>\n<td class=\"ltx_td ltx_align_center\">73.0</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">83.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">spa</th>\n<td class=\"ltx_td ltx_align_center\">86.3</td>\n<td class=\"ltx_td ltx_align_center\">87.8</td>\n<td class=\"ltx_td ltx_align_center\">87.8</td>\n<td class=\"ltx_td ltx_align_center\">87.4</td>\n<td class=\"ltx_td ltx_align_center\">87.4</td>\n<td class=\"ltx_td ltx_align_center\">88.7</td>\n<td class=\"ltx_td ltx_align_center\">93.6</td>\n<td class=\"ltx_td ltx_align_center\">90.9</td>\n<td class=\"ltx_td ltx_align_center\">91.4</td>\n<td class=\"ltx_td ltx_align_center\">93.8</td>\n<td class=\"ltx_td ltx_align_center\">93.1</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">94.9<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">sqi</th>\n<td class=\"ltx_td ltx_align_center\">73.8</td>\n<td class=\"ltx_td ltx_align_center\">19.3</td>\n<td class=\"ltx_td ltx_align_center\">83.4</td>\n<td class=\"ltx_td ltx_align_center\">78.1</td>\n<td class=\"ltx_td ltx_align_center\">78.1</td>\n<td class=\"ltx_td ltx_align_center\">71.5</td>\n<td class=\"ltx_td ltx_align_center\">85.9</td>\n<td class=\"ltx_td ltx_align_center\">87.6</td>\n<td class=\"ltx_td ltx_align_center\">88.9</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">92.0<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">91.6</td>\n<td class=\"ltx_td ltx_align_center\">89.3</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">93.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">swa</th>\n<td class=\"ltx_td ltx_align_center\">56.2</td>\n<td class=\"ltx_td ltx_align_center\">60.5</td>\n<td class=\"ltx_td ltx_align_center\">60.5</td>\n<td class=\"ltx_td ltx_align_center\">65.0</td>\n<td class=\"ltx_td ltx_align_center\">65.0</td>\n<td class=\"ltx_td ltx_align_center\">94.7</td>\n<td class=\"ltx_td ltx_align_center\">93.7</td>\n<td class=\"ltx_td ltx_align_center\">93.1</td>\n<td class=\"ltx_td ltx_align_center\">93.1</td>\n<td class=\"ltx_td ltx_align_center\">96.6</td>\n<td class=\"ltx_td ltx_align_center\">96.6</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">98.4<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">99.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">tur</th>\n<td class=\"ltx_td ltx_align_center\">28.1</td>\n<td class=\"ltx_td ltx_align_center\">64.6</td>\n<td class=\"ltx_td ltx_align_center\">64.6</td>\n<td class=\"ltx_td ltx_align_center\">64.6</td>\n<td class=\"ltx_td ltx_align_center\">64.6</td>\n<td class=\"ltx_td ltx_align_center\">81.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.0<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center\">90.9</td>\n<td class=\"ltx_td ltx_align_center\">90.8</td>\n<td class=\"ltx_td ltx_align_center\">90.3</td>\n<td class=\"ltx_td ltx_align_center\">92.0</td>\n<td class=\"ltx_td ltx_align_center\">93.8</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">95.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">avg</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">57.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">68.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">71.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">71.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">72.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">76.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">81.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">82.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">83.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">83.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">84.0<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">*</span></sup></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">82.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">85.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "baselines",
            "clearly",
            "il14",
            "submitted",
            "deu",
            "az3",
            "nav",
            "wasn’t",
            "klr",
            "spa",
            "exactmatch",
            "multilingual",
            "tur",
            "lemma",
            "avg",
            "kat",
            "ita",
            "grc",
            "hun",
            "disjoint",
            "best",
            "arz",
            "accuracy",
            "lang",
            "defined",
            "bel",
            "tüb",
            "mono",
            "nonneur",
            "amh",
            "eng",
            "systems",
            "il4",
            "san",
            "highlight",
            "multi",
            "excluding",
            "sqi",
            "sigmorphon",
            "dan",
            "afb",
            "training",
            "hye",
            "sme",
            "official",
            "fin",
            "neur",
            "swa",
            "ours",
            "il3",
            "az4",
            "mkd",
            "rus",
            "az14",
            "az2",
            "az1",
            "multi†",
            "jap",
            "il1",
            "il2"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our monolingual and multilingual systems achieve competitive performance on a global inflection benchmark, specifically the SIGMORPHON Inflection Shared Tasks from 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> and 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite>, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T1\" title=\"Table 1 &#8227; 4.0.2 Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T2\" title=\"Table 2 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, respectively:\n<span class=\"ltx_text ltx_font_smallcaps\">mono</span> ranks 4th in the 2022 data and 6th in the 2023 data, while <span class=\"ltx_text ltx_font_smallcaps\">multi</span> attains 3rd place in 2022 and 1st place in 2023, based on the average across languages.\nFor clarity, we note that the models were trained from scratch exclusively on the SIGMORPHON benchmark data.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present a compact, single-model approach to multilingual inflection, the task of generating inflected word forms from base lemmas to express grammatical categories. Our model, trained jointly on data from 73 languages, is lightweight, robust to unseen words, and outperforms monolingual baselines in most languages. This demonstrates the effectiveness of multilingual modeling for inflection and highlights its practical benefits: simplifying deployment by eliminating the need to manage and retrain dozens of separate monolingual models.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the standard SIGMORPHON shared task benchmarks, we evaluate our monolingual and multilingual models on 73 Universal Dependencies (UD) treebanks, extracting lemma-tag-form triples and their frequency counts. To ensure realistic data splits, we introduce a novel frequency-weighted, lemma-disjoint train-dev-test resampling procedure.</p>\n\n",
                "matched_terms": [
                    "sigmorphon",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our multilingual system, trained jointly on 73 languages, is extremely lightweight, consisting of a single model with 5.69M trainable parameters, capable of performing inflection across all 73 languages. The model handles all parts of speech, inflects unknown words (with evaluation conducted entirely on words unseen during training), and crucially, outperforms both the baseline systems and separately trained monolingual models in most languages. Furthermore, it offers practical advantages for deployment: a single multilingual model eliminates the need to manage dozens of individual models in memory and significantly simplifies maintenance, avoiding the overhead associated with retraining 73 separate models.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "multilingual",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the SIGMORPHON 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> and the SIGMORPHON 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite> shared task benchmarks for evaluating the strength of our models.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Hebrew was excluded from SIGMORPHON 2023 due to filename inconsistencies that prevented reliable comparison.</span></span></span>\nFurthermore, with the goal of future deployment and multilingual applicability, we select 73 languages\nof the Universal Dependencies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib16\" title=\"\">16</a>]</cite> corpora<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The selection contains 32 Indo-European languages written in the Latin script and 41 languages without restriction on language family or script. For languages with multiple corpora in UD, we chose one corpus per language based on historical preferences, practical considerations, and the defaults in the widely adopted UDPipe tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib21\" title=\"\">21</a>]</cite>. We prioritized large, canonical corpora, excluded those with narrow or specialized domains, and favored manually over semi-automatically annotated corpora. Corpora lacking lemma annotations were excluded.</span></span></span> and lexicalize them by extracting unique lemma-tag-form triples along with their occurrence counts for training and evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Unlike UniMorph, the Universal Dependencies data can be more prone to noise. We rely on a neural network to mitigate its impact.</span></span></span></p>\n\n",
                "matched_terms": [
                    "sigmorphon",
                    "lemma",
                    "multilingual",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the very least, a <span class=\"ltx_text ltx_font_italic\">lemma-disjoint</span> option\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib8\" title=\"\">8</a>]</cite> is recently advisable, and SIGMORPHON 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> probed this setting in its <span class=\"ltx_text ltx_font_italic\">feature overlap</span> setting: A test pair&#8217;s feature set is attested in training, but its lemma is novel. The latest SIGMORPHON shared task installation completely moved to the <span class=\"ltx_text ltx_font_italic\">lemma-disjoint</span> setting in 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite>. We use the official train-dev-test split for both the SIGMORPHON benchmarks.</p>\n\n",
                "matched_terms": [
                    "sigmorphon",
                    "lemma",
                    "official",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike previous work on multilingual inflection which usually used the SIGMORPHON shared task data where the train set for all languages was of the same size, we need to deal with the problem of mixing corpora of substantially different sizes. If we na&#239;vely pooled the training datasets, the model may be overly influenced by the high-resource languages. To address this issue, we adapt a corpus upsampling method by Wang et al. (2020) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib25\" title=\"\">25</a>]</cite>.\nFor better control over sampling into training batches, we also adapt a temperature value <math alttext=\"\\tau{}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau{}\\in[0,1]</annotation></semantics></math> (our <math alttext=\"\\tau=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\tau=0.5</annotation></semantics></math>) to smooth the distribution of training data from datasets of different sizes, see van der Goot et al. (2021) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib9\" title=\"\">9</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "sigmorphon",
                    "multilingual",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two-part Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T3\" title=\"Table 3 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>&#160;and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T4\" title=\"Table 4 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the results on 73 corpora from UD. The overall best-performing system is <span class=\"ltx_text ltx_font_smallcaps\">multi</span>, which was trained jointly on all 73 languages. Among these languages, it is outperformed in 3 cases by the simple <span class=\"ltx_text ltx_font_smallcaps\">copy</span> baseline, and in 7 cases by the monolingual models (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>), each trained separately for a single language.\nOn macro average across all languages, the <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model improves over the <span class=\"ltx_text ltx_font_smallcaps\">mono</span> models by 4.69%.</p>\n\n",
                "matched_terms": [
                    "multi",
                    "mono"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our <span class=\"ltx_text ltx_font_smallcaps\">mono</span> and <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model, hyperparameters tuned on UD\ndevelopment data are: layers (encoder/decoder) = <math alttext=\"3/3\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m1\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo>/</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3/3</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>) and <math alttext=\"4/4\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo>/</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">4/4</annotation></semantics></math>&#160;(<span class=\"ltx_text ltx_font_smallcaps\">multi</span>),\nlayer dimension = <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m3\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math>, feed-forward dimension = <math alttext=\"64\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m4\" intent=\":literal\"><semantics><mn>64</mn><annotation encoding=\"application/x-tex\">64</annotation></semantics></math>, attention heads = <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m5\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, dropout = <math alttext=\"0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m6\" intent=\":literal\"><semantics><mn>0.15</mn><annotation encoding=\"application/x-tex\">0.15</annotation></semantics></math>, attention dropout = <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m7\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>, activation dropout = <math alttext=\"0.35\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m8\" intent=\":literal\"><semantics><mn>0.35</mn><annotation encoding=\"application/x-tex\">0.35</annotation></semantics></math>, and layer drop = <math alttext=\"0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m9\" intent=\":literal\"><semantics><mn>0.2</mn><annotation encoding=\"application/x-tex\">0.2</annotation></semantics></math>. We use gradient clipping (max norm 1.0), L2 regularization (0.01), and train with Adam\nusing cosine learning rate decay, initial LR&#160;=&#160;0.001, batch size = <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m10\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>) and <math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m11\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">multi</span>).\nCheckpoints are selected by dev set performance: on the target language for <span class=\"ltx_text ltx_font_smallcaps\">mono</span>, or macro-averaged across languages for <span class=\"ltx_text ltx_font_smallcaps\">multi</span>, over up to <math alttext=\"960\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m12\" intent=\":literal\"><semantics><mn>960</mn><annotation encoding=\"application/x-tex\">960</annotation></semantics></math> epochs.\nThe <span class=\"ltx_text ltx_font_smallcaps\">mono</span> model has 3.12M-3.69M parameters,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>varying by vocabulary size</span></span></span> while the <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model has 5.69M parameters.</p>\n\n",
                "matched_terms": [
                    "multi",
                    "mono"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Flexing in 73 Languages: A Single Small Model for Multilingual Inflection",
        "caption": "Table 3: UD test accuracy — part 1. The copy baseline copies the input lemma to output. †The copy baseline fails on lemma-to-form transfer in Ancient_Hebrew-PTNK due to special diacritics (e.g., cantillation marks) present in surface forms but absent in standardized lemmas.",
        "body": "size (# lemma-tag-form)\naccuracy (%)\n\n\nlang-corpus\ntrain\ndev\ntest\ncopy\nmono\nmulti\n\n\nAfrikaans-AfriBooms\n1.9K\n2.4K\n2.3K\n69.1869.18\n83.7283.72\n89.41\n\n\nAncient_Greek-PROIEL\n12.8K\n11.7K\n11.2K\n10.6710.67\n53.6353.63\n56.14\n\n\nAncient_Hebrew-PTNK\n4.8K\n2.8K\n2.7K\n\n0.000.00†\n\n1.401.40\n1.661.66\n\n\nArabic-PADT\n14.7K\n12.2K\n11.9K\n24.0824.08\n86.9486.94\n87.17\n\n\nArmenian-ArmTDP\n6.5K\n3.7K\n3.6K\n35.6935.69\n88.0688.06\n91.04\n\n\nBasque-BDT\n13.0K\n7.8K\n7.7K\n39.7439.74\n87.1387.13\n88.42\n\n\nBelarusian-HSE\n21.4K\n19.4K\n19.3K\n40.8840.88\n88.0288.02\n89.15\n\n\nBreton-KEB\n1.2K\n696\n718\n57.6657.66\n64.6264.62\n75.91\n\n\nBulgarian-BTB\n10.0K\n9.1K\n9.0K\n37.9037.90\n90.9490.94\n92.00\n\n\nCatalan-AnCora\n7.4K\n15.6K\n15.4K\n64.7364.73\n93.6293.62\n95.39\n\n\nChinese-GSDSimp\n7.2K\n7.7K\n7.7K\n82.1082.10\n80.8880.88\n86.45\n\n\nClassical_Armenian-CAVaL\n2.6K\n2.9K\n2.6K\n28.5228.52\n55.6455.64\n72.20\n\n\nClassical_Chinese-Kyoto\n3.9K\n6.4K\n6.4K\n13.9913.99\n1.621.62\n46.27\n\n\nCoptic-Scriptorium\n685\n1.5K\n1.3K\n83.18\n54.8654.86\n79.4579.45\n\n\nCroatian-SET\n17.7K\n12.6K\n13.0K\n36.1036.10\n92.1092.10\n93.45\n\n\nCzech-PDT\n47.9K\n62.0K\n62.3K\n37.5337.53\n97.44\n97.3797.37\n\n\nDanish-DDT\n5.8K\n6.6K\n6.7K\n55.6155.61\n89.9289.92\n92.95\n\n\nDutch-Alpino\n7.0K\n11.7K\n11.8K\n55.4855.48\n77.6677.66\n79.16\n\n\nEnglish-EWT\n6.6K\n9.5K\n9.7K\n76.6776.67\n93.6393.63\n94.96\n\n\nErzya-JR\n4.3K\n1.7K\n1.6K\n29.6329.63\n82.2782.27\n86.10\n\n\nEstonian-EDT\n31.7K\n27.8K\n28.2K\n23.2423.24\n88.83\n87.9387.93\n\n\nFinnish-TDT\n26.3K\n15.2K\n15.0K\n23.8823.88\n90.58\n88.8488.84\n\n\nFrench-GSD\n10.4K\n20.0K\n19.6K\n72.5572.55\n95.3695.36\n96.79\n\n\nGalician-TreeGal\n2.4K\n1.7K\n1.7K\n59.6759.67\n94.0994.09\n97.36\n\n\nGeorgian-GLC\n988\n229\n224\n44.6444.64\n68.3068.30\n75.45\n\n\nGerman-GSD\n25.5K\n23.7K\n23.8K\n75.2275.22\n89.1289.12\n89.37\n\n\nGothic-PROIEL\n4.1K\n3.1K\n2.9K\n17.5017.50\n71.1371.13\n79.30\n\n\nGreek-GDT\n5.2K\n4.0K\n3.9K\n34.4534.45\n82.6982.69\n86.18\n\n\nHebrew-HTB\n6.7K\n6.9K\n7.0K\n51.7551.75\n88.7888.78\n90.47\n\n\nHindi-HDTB\n10.4K\n12.3K\n12.1K\n79.6479.64\n92.3192.31\n92.71\n\n\nHungarian-Szeged\n7.4K\n3.4K\n3.4K\n51.3451.34\n92.5592.55\n93.23\n\n\nIcelandic-Modern\n4.6K\n4.5K\n4.5K\n35.7135.71\n70.6270.62\n77.57\n\n\nIndonesian-GSD\n7.3K\n7.4K\n7.6K\n88.5788.57\n89.8989.89\n91.59\n\n\nIrish-IDT\n6.8K\n6.8K\n6.8K\n51.9351.93\n83.7583.75\n87.74\n\n\nItalian-ISDT\n7.8K\n11.9K\n11.7K\n62.9562.95\n95.2695.26\n96.02\n\n\nJapanese-GSDLUW\n8.4K\n11.6K\n11.6K\n74.7774.77\n75.3175.31\n79.74\n\n\nKorean-Kaist\n45.9K\n28.3K\n28.5K\n10.0610.06\n95.9095.90\n96.19",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">size (# lemma-tag-form)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">accuracy (%)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">lang-corpus</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">train</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\">dev</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\">test</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">copy</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">mono</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">multi</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Afrikaans-AfriBooms</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\">2.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\">2.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\"><math alttext=\"69.18\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mn>69.18</mn><annotation encoding=\"application/x-tex\">69.18</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"83.72\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mn>83.72</mn><annotation encoding=\"application/x-tex\">83.72</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">89.41</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Ancient_Greek-PROIEL</td>\n<td class=\"ltx_td ltx_align_right\">12.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"10.67\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mn>10.67</mn><annotation encoding=\"application/x-tex\">10.67</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"53.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mn>53.63</mn><annotation encoding=\"application/x-tex\">53.63</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">56.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Ancient_Hebrew-PTNK</td>\n<td class=\"ltx_td ltx_align_right\">4.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\">\n<math alttext=\"0.00\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mn>0.00</mn><annotation encoding=\"application/x-tex\">0.00</annotation></semantics></math><sup class=\"ltx_sup\">&#8224;</sup>\n</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"1.40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mn>1.40</mn><annotation encoding=\"application/x-tex\">1.40</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"1.66\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m7\" intent=\":literal\"><semantics><mn>1.66</mn><annotation encoding=\"application/x-tex\">1.66</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Arabic-PADT</td>\n<td class=\"ltx_td ltx_align_right\">14.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">12.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"24.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m8\" intent=\":literal\"><semantics><mn>24.08</mn><annotation encoding=\"application/x-tex\">24.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"86.94\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m9\" intent=\":literal\"><semantics><mn>86.94</mn><annotation encoding=\"application/x-tex\">86.94</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">87.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Armenian-ArmTDP</td>\n<td class=\"ltx_td ltx_align_right\">6.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">3.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">3.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"35.69\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m10\" intent=\":literal\"><semantics><mn>35.69</mn><annotation encoding=\"application/x-tex\">35.69</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"88.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m11\" intent=\":literal\"><semantics><mn>88.06</mn><annotation encoding=\"application/x-tex\">88.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">91.04</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Basque-BDT</td>\n<td class=\"ltx_td ltx_align_right\">13.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">7.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">7.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"39.74\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m12\" intent=\":literal\"><semantics><mn>39.74</mn><annotation encoding=\"application/x-tex\">39.74</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"87.13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m13\" intent=\":literal\"><semantics><mn>87.13</mn><annotation encoding=\"application/x-tex\">87.13</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">88.42</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Belarusian-HSE</td>\n<td class=\"ltx_td ltx_align_right\">21.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">19.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">19.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"40.88\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m14\" intent=\":literal\"><semantics><mn>40.88</mn><annotation encoding=\"application/x-tex\">40.88</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"88.02\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m15\" intent=\":literal\"><semantics><mn>88.02</mn><annotation encoding=\"application/x-tex\">88.02</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">89.15</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Breton-KEB</td>\n<td class=\"ltx_td ltx_align_right\">1.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">696</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">718</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"57.66\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m16\" intent=\":literal\"><semantics><mn>57.66</mn><annotation encoding=\"application/x-tex\">57.66</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"64.62\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m17\" intent=\":literal\"><semantics><mn>64.62</mn><annotation encoding=\"application/x-tex\">64.62</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">75.91</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Bulgarian-BTB</td>\n<td class=\"ltx_td ltx_align_right\">10.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">9.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">9.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"37.90\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m18\" intent=\":literal\"><semantics><mn>37.90</mn><annotation encoding=\"application/x-tex\">37.90</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"90.94\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m19\" intent=\":literal\"><semantics><mn>90.94</mn><annotation encoding=\"application/x-tex\">90.94</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">92.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Catalan-AnCora</td>\n<td class=\"ltx_td ltx_align_right\">7.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">15.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">15.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"64.73\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m20\" intent=\":literal\"><semantics><mn>64.73</mn><annotation encoding=\"application/x-tex\">64.73</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"93.62\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m21\" intent=\":literal\"><semantics><mn>93.62</mn><annotation encoding=\"application/x-tex\">93.62</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">95.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Chinese-GSDSimp</td>\n<td class=\"ltx_td ltx_align_right\">7.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">7.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">7.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"82.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m22\" intent=\":literal\"><semantics><mn>82.10</mn><annotation encoding=\"application/x-tex\">82.10</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"80.88\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m23\" intent=\":literal\"><semantics><mn>80.88</mn><annotation encoding=\"application/x-tex\">80.88</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">86.45</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Classical_Armenian-CAVaL</td>\n<td class=\"ltx_td ltx_align_right\">2.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"28.52\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m24\" intent=\":literal\"><semantics><mn>28.52</mn><annotation encoding=\"application/x-tex\">28.52</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"55.64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m25\" intent=\":literal\"><semantics><mn>55.64</mn><annotation encoding=\"application/x-tex\">55.64</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">72.20</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Classical_Chinese-Kyoto</td>\n<td class=\"ltx_td ltx_align_right\">3.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">6.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">6.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"13.99\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m26\" intent=\":literal\"><semantics><mn>13.99</mn><annotation encoding=\"application/x-tex\">13.99</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"1.62\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m27\" intent=\":literal\"><semantics><mn>1.62</mn><annotation encoding=\"application/x-tex\">1.62</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">46.27</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Coptic-Scriptorium</td>\n<td class=\"ltx_td ltx_align_right\">685</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">83.18</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"54.86\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m28\" intent=\":literal\"><semantics><mn>54.86</mn><annotation encoding=\"application/x-tex\">54.86</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"79.45\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m29\" intent=\":literal\"><semantics><mn>79.45</mn><annotation encoding=\"application/x-tex\">79.45</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Croatian-SET</td>\n<td class=\"ltx_td ltx_align_right\">17.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">12.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">13.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"36.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m30\" intent=\":literal\"><semantics><mn>36.10</mn><annotation encoding=\"application/x-tex\">36.10</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"92.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m31\" intent=\":literal\"><semantics><mn>92.10</mn><annotation encoding=\"application/x-tex\">92.10</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">93.45</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Czech-PDT</td>\n<td class=\"ltx_td ltx_align_right\">47.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">62.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">62.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"37.53\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m32\" intent=\":literal\"><semantics><mn>37.53</mn><annotation encoding=\"application/x-tex\">37.53</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">97.44</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"97.37\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m33\" intent=\":literal\"><semantics><mn>97.37</mn><annotation encoding=\"application/x-tex\">97.37</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Danish-DDT</td>\n<td class=\"ltx_td ltx_align_right\">5.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">6.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">6.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"55.61\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m34\" intent=\":literal\"><semantics><mn>55.61</mn><annotation encoding=\"application/x-tex\">55.61</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"89.92\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m35\" intent=\":literal\"><semantics><mn>89.92</mn><annotation encoding=\"application/x-tex\">89.92</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">92.95</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Dutch-Alpino</td>\n<td class=\"ltx_td ltx_align_right\">7.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"55.48\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m36\" intent=\":literal\"><semantics><mn>55.48</mn><annotation encoding=\"application/x-tex\">55.48</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"77.66\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m37\" intent=\":literal\"><semantics><mn>77.66</mn><annotation encoding=\"application/x-tex\">77.66</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">79.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">English-EWT</td>\n<td class=\"ltx_td ltx_align_right\">6.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">9.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">9.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"76.67\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m38\" intent=\":literal\"><semantics><mn>76.67</mn><annotation encoding=\"application/x-tex\">76.67</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"93.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m39\" intent=\":literal\"><semantics><mn>93.63</mn><annotation encoding=\"application/x-tex\">93.63</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">94.96</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Erzya-JR</td>\n<td class=\"ltx_td ltx_align_right\">4.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"29.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m40\" intent=\":literal\"><semantics><mn>29.63</mn><annotation encoding=\"application/x-tex\">29.63</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"82.27\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m41\" intent=\":literal\"><semantics><mn>82.27</mn><annotation encoding=\"application/x-tex\">82.27</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">86.10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Estonian-EDT</td>\n<td class=\"ltx_td ltx_align_right\">31.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">27.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">28.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"23.24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m42\" intent=\":literal\"><semantics><mn>23.24</mn><annotation encoding=\"application/x-tex\">23.24</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">88.83</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"87.93\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m43\" intent=\":literal\"><semantics><mn>87.93</mn><annotation encoding=\"application/x-tex\">87.93</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Finnish-TDT</td>\n<td class=\"ltx_td ltx_align_right\">26.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">15.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">15.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"23.88\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m44\" intent=\":literal\"><semantics><mn>23.88</mn><annotation encoding=\"application/x-tex\">23.88</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">90.58</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"88.84\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m45\" intent=\":literal\"><semantics><mn>88.84</mn><annotation encoding=\"application/x-tex\">88.84</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">French-GSD</td>\n<td class=\"ltx_td ltx_align_right\">10.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">20.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">19.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"72.55\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m46\" intent=\":literal\"><semantics><mn>72.55</mn><annotation encoding=\"application/x-tex\">72.55</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"95.36\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m47\" intent=\":literal\"><semantics><mn>95.36</mn><annotation encoding=\"application/x-tex\">95.36</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">96.79</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Galician-TreeGal</td>\n<td class=\"ltx_td ltx_align_right\">2.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"59.67\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m48\" intent=\":literal\"><semantics><mn>59.67</mn><annotation encoding=\"application/x-tex\">59.67</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"94.09\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m49\" intent=\":literal\"><semantics><mn>94.09</mn><annotation encoding=\"application/x-tex\">94.09</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">97.36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Georgian-GLC</td>\n<td class=\"ltx_td ltx_align_right\">988</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">229</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">224</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"44.64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m50\" intent=\":literal\"><semantics><mn>44.64</mn><annotation encoding=\"application/x-tex\">44.64</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"68.30\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m51\" intent=\":literal\"><semantics><mn>68.30</mn><annotation encoding=\"application/x-tex\">68.30</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">75.45</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">German-GSD</td>\n<td class=\"ltx_td ltx_align_right\">25.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">23.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">23.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"75.22\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m52\" intent=\":literal\"><semantics><mn>75.22</mn><annotation encoding=\"application/x-tex\">75.22</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"89.12\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m53\" intent=\":literal\"><semantics><mn>89.12</mn><annotation encoding=\"application/x-tex\">89.12</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">89.37</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gothic-PROIEL</td>\n<td class=\"ltx_td ltx_align_right\">4.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">3.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"17.50\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m54\" intent=\":literal\"><semantics><mn>17.50</mn><annotation encoding=\"application/x-tex\">17.50</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"71.13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m55\" intent=\":literal\"><semantics><mn>71.13</mn><annotation encoding=\"application/x-tex\">71.13</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">79.30</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Greek-GDT</td>\n<td class=\"ltx_td ltx_align_right\">5.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">4.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">3.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"34.45\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m56\" intent=\":literal\"><semantics><mn>34.45</mn><annotation encoding=\"application/x-tex\">34.45</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"82.69\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m57\" intent=\":literal\"><semantics><mn>82.69</mn><annotation encoding=\"application/x-tex\">82.69</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">86.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hebrew-HTB</td>\n<td class=\"ltx_td ltx_align_right\">6.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">6.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">7.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"51.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m58\" intent=\":literal\"><semantics><mn>51.75</mn><annotation encoding=\"application/x-tex\">51.75</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"88.78\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m59\" intent=\":literal\"><semantics><mn>88.78</mn><annotation encoding=\"application/x-tex\">88.78</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">90.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hindi-HDTB</td>\n<td class=\"ltx_td ltx_align_right\">10.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">12.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">12.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"79.64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m60\" intent=\":literal\"><semantics><mn>79.64</mn><annotation encoding=\"application/x-tex\">79.64</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"92.31\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m61\" intent=\":literal\"><semantics><mn>92.31</mn><annotation encoding=\"application/x-tex\">92.31</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">92.71</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Hungarian-Szeged</td>\n<td class=\"ltx_td ltx_align_right\">7.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">3.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">3.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"51.34\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m62\" intent=\":literal\"><semantics><mn>51.34</mn><annotation encoding=\"application/x-tex\">51.34</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"92.55\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m63\" intent=\":literal\"><semantics><mn>92.55</mn><annotation encoding=\"application/x-tex\">92.55</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">93.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Icelandic-Modern</td>\n<td class=\"ltx_td ltx_align_right\">4.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">4.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">4.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"35.71\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m64\" intent=\":literal\"><semantics><mn>35.71</mn><annotation encoding=\"application/x-tex\">35.71</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"70.62\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m65\" intent=\":literal\"><semantics><mn>70.62</mn><annotation encoding=\"application/x-tex\">70.62</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">77.57</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Indonesian-GSD</td>\n<td class=\"ltx_td ltx_align_right\">7.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">7.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">7.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"88.57\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m66\" intent=\":literal\"><semantics><mn>88.57</mn><annotation encoding=\"application/x-tex\">88.57</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"89.89\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m67\" intent=\":literal\"><semantics><mn>89.89</mn><annotation encoding=\"application/x-tex\">89.89</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">91.59</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Irish-IDT</td>\n<td class=\"ltx_td ltx_align_right\">6.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">6.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">6.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"51.93\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m68\" intent=\":literal\"><semantics><mn>51.93</mn><annotation encoding=\"application/x-tex\">51.93</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"83.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m69\" intent=\":literal\"><semantics><mn>83.75</mn><annotation encoding=\"application/x-tex\">83.75</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">87.74</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Italian-ISDT</td>\n<td class=\"ltx_td ltx_align_right\">7.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"62.95\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m70\" intent=\":literal\"><semantics><mn>62.95</mn><annotation encoding=\"application/x-tex\">62.95</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"95.26\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m71\" intent=\":literal\"><semantics><mn>95.26</mn><annotation encoding=\"application/x-tex\">95.26</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">96.02</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Japanese-GSDLUW</td>\n<td class=\"ltx_td ltx_align_right\">8.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"74.77\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m72\" intent=\":literal\"><semantics><mn>74.77</mn><annotation encoding=\"application/x-tex\">74.77</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"75.31\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m73\" intent=\":literal\"><semantics><mn>75.31</mn><annotation encoding=\"application/x-tex\">75.31</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">79.74</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Korean-Kaist</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">45.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_bb\">28.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_bb\">28.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_bb\"><math alttext=\"10.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m74\" intent=\":literal\"><semantics><mn>10.06</mn><annotation encoding=\"application/x-tex\">10.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><math alttext=\"95.90\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m75\" intent=\":literal\"><semantics><mn>95.90</mn><annotation encoding=\"application/x-tex\">95.90</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">96.19</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "train",
            "78k",
            "126k",
            "basquebdt",
            "belarusianhse",
            "237k",
            "104k",
            "lemma",
            "19k",
            "copticscriptorium",
            "hungarianszeged",
            "48k",
            "transfer",
            "germangsd",
            "japanesegsdluw",
            "greekgdt",
            "52k",
            "194k",
            "72k",
            "englishewt",
            "13k",
            "40k",
            "29k",
            "45k",
            "58k",
            "122k",
            "37k",
            "64k",
            "arabicpadt",
            "lemmatoform",
            "test",
            "16k",
            "erzyajr",
            "chinesegsdsimp",
            "156k",
            "baseline",
            "95k",
            "indonesiangsd",
            "74k",
            "galiciantreegal",
            "68k",
            "due",
            "copies",
            "catalanancora",
            "263k",
            "283k",
            "43k",
            "output",
            "46k",
            "152k",
            "fails",
            "147k",
            "cantillation",
            "hebrewhtb",
            "23k",
            "117k",
            "116k",
            "koreankaist",
            "26k",
            "196k",
            "36k",
            "accuracy",
            "absent",
            "000000†",
            "77k",
            "91k",
            "69k",
            "459k",
            "estonianedt",
            "georgianglc",
            "73k",
            "†the",
            "ancientgreekproiel",
            "119k",
            "langcorpus",
            "multi",
            "278k",
            "620k",
            "forms",
            "112k",
            "66k",
            "128k",
            "lemmatagform",
            "surface",
            "size",
            "121k",
            "154k",
            "hindihdtb",
            "gothicproiel",
            "special",
            "27k",
            "bulgarianbtb",
            "70k",
            "34k",
            "copy",
            "97k",
            "67k",
            "623k",
            "282k",
            "present",
            "finnishtdt",
            "dev",
            "28k",
            "214k",
            "classicalarmeniancaval",
            "frenchgsd",
            "lemmas",
            "afrikaansafribooms",
            "dutchalpino",
            "193k",
            "84k",
            "standardized",
            "39k",
            "76k",
            "input",
            "238k",
            "danishddt",
            "200k",
            "100k",
            "479k",
            "12k",
            "150k",
            "icelandicmodern",
            "31k",
            "90k",
            "130k",
            "177k",
            "irishidt",
            "255k",
            "bretonkeb",
            "15k",
            "285k",
            "diacritics",
            "17k",
            "mono",
            "part",
            "italianisdt",
            "118k",
            "65k",
            "123k",
            "czechpdt",
            "41k",
            "ancienthebrewptnk",
            "croatianset",
            "armenianarmtdp",
            "classicalchinesekyoto",
            "marks",
            "24k",
            "317k"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The two-part Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T3\" title=\"Table 3 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>&#160;and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T4\" title=\"Table 4 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the results on 73 corpora from UD. The overall best-performing system is <span class=\"ltx_text ltx_font_smallcaps\">multi</span>, which was trained jointly on all 73 languages. Among these languages, it is outperformed in 3 cases by the simple <span class=\"ltx_text ltx_font_smallcaps\">copy</span> baseline, and in 7 cases by the monolingual models (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>), each trained separately for a single language.\nOn macro average across all languages, the <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model improves over the <span class=\"ltx_text ltx_font_smallcaps\">mono</span> models by 4.69%.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present a compact, single-model approach to multilingual inflection, the task of generating inflected word forms from base lemmas to express grammatical categories. Our model, trained jointly on data from 73 languages, is lightweight, robust to unseen words, and outperforms monolingual baselines in most languages. This demonstrates the effectiveness of multilingual modeling for inflection and highlights its practical benefits: simplifying deployment by eliminating the need to manage and retrain dozens of separate monolingual models.</p>\n\n",
                "matched_terms": [
                    "present",
                    "forms",
                    "lemmas"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the SIGMORPHON 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> and the SIGMORPHON 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite> shared task benchmarks for evaluating the strength of our models.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Hebrew was excluded from SIGMORPHON 2023 due to filename inconsistencies that prevented reliable comparison.</span></span></span>\nFurthermore, with the goal of future deployment and multilingual applicability, we select 73 languages\nof the Universal Dependencies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib16\" title=\"\">16</a>]</cite> corpora<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The selection contains 32 Indo-European languages written in the Latin script and 41 languages without restriction on language family or script. For languages with multiple corpora in UD, we chose one corpus per language based on historical preferences, practical considerations, and the defaults in the widely adopted UDPipe tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib21\" title=\"\">21</a>]</cite>. We prioritized large, canonical corpora, excluded those with narrow or specialized domains, and favored manually over semi-automatically annotated corpora. Corpora lacking lemma annotations were excluded.</span></span></span> and lexicalize them by extracting unique lemma-tag-form triples along with their occurrence counts for training and evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Unlike UniMorph, the Universal Dependencies data can be more prone to noise. We rely on a neural network to mitigate its impact.</span></span></span></p>\n\n",
                "matched_terms": [
                    "due",
                    "lemma",
                    "lemmatagform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Strategies for data splitting in the inflection field differ in the degree of overlap control between train-test splits, from an uncontrolled overlap (where lemma-tag-form may overlap randomly) to varying degrees of control over lemma and/or morphological feature overlap. The former has been the traditional approach for decades but has recently come under critique for hindering evaluation of models&#8217; generalization abilities.</p>\n\n",
                "matched_terms": [
                    "lemma",
                    "lemmatagform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the very least, a <span class=\"ltx_text ltx_font_italic\">lemma-disjoint</span> option\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib8\" title=\"\">8</a>]</cite> is recently advisable, and SIGMORPHON 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> probed this setting in its <span class=\"ltx_text ltx_font_italic\">feature overlap</span> setting: A test pair&#8217;s feature set is attested in training, but its lemma is novel. The latest SIGMORPHON shared task installation completely moved to the <span class=\"ltx_text ltx_font_italic\">lemma-disjoint</span> setting in 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite>. We use the official train-dev-test split for both the SIGMORPHON benchmarks.</p>\n\n",
                "matched_terms": [
                    "lemma",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another axis for consideration is the manipulation with the lemma-tag-form corpus frequencies during sampling into the train-test splits. Traditionally, the corpus frequencies are neglected (or not even present in the mostly used data source, UniMorph <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib11\" title=\"\">11</a>]</cite>) and the lemma-tag-form triples are sampled uniformly. Kodner et al. (2023) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib13\" title=\"\">13</a>]</cite> argued that uniform sampling leads to an unrealistic train-test split, with train set unnaturally biased towards low-frequency types. They introduced a <span class=\"ltx_text ltx_font_italic\">frequency-weighted sampling</span> method to produce a more realistic train-test distribution.\nHowever, the suggested split is not lemma-disjoint. Also, no further work has experimented with that split.</p>\n\n",
                "matched_terms": [
                    "present",
                    "lemmatagform",
                    "train"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the lack of standardized practice for sampling with corpus frequencies, we design\na new train-dev-test split technique for the UD datasets that satisfies both recent methodological standards (being <span class=\"ltx_text ltx_font_italic\">lemma-disjoint</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib8\" title=\"\">8</a>]</cite>) and practical requirements for real-world deployment by ensuring a realistic distribution of items with varying corpus frequencies through <span class=\"ltx_text ltx_font_italic\">frequency-weighted sampling</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib13\" title=\"\">13</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "standardized",
                    "due"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a Transformer architecture of the encoder-decoder, sequence-to-sequence type, characterized by a small capacity\naccording to state-of-the-art standards in the inflection field.\nThis model is trained from scratch on inflection data, using characters as tokens, with the input consisting of lemma-tag pair and the output being the corresponding inflected form (see Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "output",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a special language ID token as part of the input sequence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib17\" title=\"\">17</a>]</cite> by prepending it to the sequence of morphological tags. This should help the model disambiguate identical lemma strings corresponding to different languages.</p>\n\n",
                "matched_terms": [
                    "part",
                    "lemma",
                    "special",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike previous work on multilingual inflection which usually used the SIGMORPHON shared task data where the train set for all languages was of the same size, we need to deal with the problem of mixing corpora of substantially different sizes. If we na&#239;vely pooled the training datasets, the model may be overly influenced by the high-resource languages. To address this issue, we adapt a corpus upsampling method by Wang et al. (2020) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib25\" title=\"\">25</a>]</cite>.\nFor better control over sampling into training batches, we also adapt a temperature value <math alttext=\"\\tau{}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau{}\\in[0,1]</annotation></semantics></math> (our <math alttext=\"\\tau=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\tau=0.5</annotation></semantics></math>) to smooth the distribution of training data from datasets of different sizes, see van der Goot et al. (2021) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib9\" title=\"\">9</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "train"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our monolingual and multilingual systems achieve competitive performance on a global inflection benchmark, specifically the SIGMORPHON Inflection Shared Tasks from 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> and 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite>, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T1\" title=\"Table 1 &#8227; 4.0.2 Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T2\" title=\"Table 2 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, respectively:\n<span class=\"ltx_text ltx_font_smallcaps\">mono</span> ranks 4th in the 2022 data and 6th in the 2023 data, while <span class=\"ltx_text ltx_font_smallcaps\">multi</span> attains 3rd place in 2022 and 1st place in 2023, based on the average across languages.\nFor clarity, we note that the models were trained from scratch exclusively on the SIGMORPHON benchmark data.</p>\n\n",
                "matched_terms": [
                    "multi",
                    "mono"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our <span class=\"ltx_text ltx_font_smallcaps\">mono</span> and <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model, hyperparameters tuned on UD\ndevelopment data are: layers (encoder/decoder) = <math alttext=\"3/3\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m1\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo>/</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3/3</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>) and <math alttext=\"4/4\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo>/</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">4/4</annotation></semantics></math>&#160;(<span class=\"ltx_text ltx_font_smallcaps\">multi</span>),\nlayer dimension = <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m3\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math>, feed-forward dimension = <math alttext=\"64\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m4\" intent=\":literal\"><semantics><mn>64</mn><annotation encoding=\"application/x-tex\">64</annotation></semantics></math>, attention heads = <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m5\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, dropout = <math alttext=\"0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m6\" intent=\":literal\"><semantics><mn>0.15</mn><annotation encoding=\"application/x-tex\">0.15</annotation></semantics></math>, attention dropout = <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m7\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>, activation dropout = <math alttext=\"0.35\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m8\" intent=\":literal\"><semantics><mn>0.35</mn><annotation encoding=\"application/x-tex\">0.35</annotation></semantics></math>, and layer drop = <math alttext=\"0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m9\" intent=\":literal\"><semantics><mn>0.2</mn><annotation encoding=\"application/x-tex\">0.2</annotation></semantics></math>. We use gradient clipping (max norm 1.0), L2 regularization (0.01), and train with Adam\nusing cosine learning rate decay, initial LR&#160;=&#160;0.001, batch size = <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m10\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>) and <math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m11\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">multi</span>).\nCheckpoints are selected by dev set performance: on the target language for <span class=\"ltx_text ltx_font_smallcaps\">mono</span>, or macro-averaged across languages for <span class=\"ltx_text ltx_font_smallcaps\">multi</span>, over up to <math alttext=\"960\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m12\" intent=\":literal\"><semantics><mn>960</mn><annotation encoding=\"application/x-tex\">960</annotation></semantics></math> epochs.\nThe <span class=\"ltx_text ltx_font_smallcaps\">mono</span> model has 3.12M-3.69M parameters,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>varying by vocabulary size</span></span></span> while the <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model has 5.69M parameters.</p>\n\n",
                "matched_terms": [
                    "train",
                    "dev",
                    "size",
                    "multi",
                    "mono"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "Flexing in 73 Languages: A Single Small Model for Multilingual Inflection",
        "caption": "Table 4: UD test accuracy — part 2. The copy baseline copies the input lemma to output.",
        "body": "size (# lemma-tag-form)\naccuracy (%)\n\n\nlang-corpus\ntrain\ndev\ntest\ncopy\nmono\nmulti\n\n\nKyrgyz-KTMU\n2.9K\n675\n673\n42.2042.20\n64.3464.34\n66.57\n\n\nLatin-ITTB\n5.8K\n9.7K\n9.7K\n14.5814.58\n76.3376.33\n88.94\n\n\nLatvian-LVTB\n22.6K\n18.3K\n18.2K\n27.8127.81\n96.48\n96.1596.15\n\n\nLithuanian-ALKSNIS\n9.3K\n5.0K\n5.0K\n27.7927.79\n92.7492.74\n93.38\n\n\nLow_Saxon-LSDC\n3.5K\n1.9K\n1.8K\n54.4254.42\n54.1454.14\n62.49\n\n\nMaghrebi_Arabic_French-Arabizi\n5.9K\n1.8K\n1.8K\n17.46\n9.549.54\n15.3515.35\n\n\nManx-Cadhan\n762\n1.1K\n1.2K\n65.1565.15\n66.8466.84\n82.03\n\n\nMarathi-UFAL\n759\n302\n302\n47.0247.02\n56.6256.62\n74.17\n\n\nNaija-NSC\n916\n2.4K\n2.3K\n86.4186.41\n70.6470.64\n96.04\n\n\nNorth_Sami-Giella\n4.3K\n2.1K\n2.1K\n33.0033.00\n68.5068.50\n75.96\n\n\nNorwegian-Bokmaal\n7.4K\n14.8K\n14.2K\n53.6253.62\n93.0893.08\n95.26\n\n\nOld_Church_Slavonic-PROIEL\n24.7K\n12.9K\n13.2K\n6.356.35\n24.3424.34\n27.56\n\n\nOld_East_Slavic-TOROT\n28.6K\n16.7K\n16.2K\n7.497.49\n31.0831.08\n33.82\n\n\nOld_French-PROFITEROLE\n19.4K\n2.8K\n3.0K\n19.82\n0.130.13\n18.0518.05\n\n\nOttoman_Turkish-BOUN\n3.0K\n781\n789\n47.6647.66\n78.2078.20\n84.03\n\n\nPersian-PerDT\n12.4K\n14.7K\n14.2K\n67.7067.70\n72.96\n70.7370.73\n\n\nPolish-PDB\n28.3K\n22.6K\n23.1K\n28.4228.42\n93.78\n93.6493.64\n\n\nPomak-Philotis\n3.0K\n2.2K\n2.2K\n20.9820.98\n43.2143.21\n54.99\n\n\nPortuguese-Bosque\n9.7K\n11.6K\n11.7K\n67.6667.66\n96.2196.21\n97.63\n\n\nRomanian-RRT\n11.1K\n11.7K\n11.6K\n42.0042.00\n92.6592.65\n93.60\n\n\nRussian-SynTagRus\n44.4K\n64.1K\n63.2K\n26.3526.35\n93.2893.28\n93.57\n\n\nSanskrit-Vedic\n19.8K\n11.3K\n11.7K\n9.439.43\n75.55\n75.2275.22\n\n\nScottish_Gaelic-ARCOSG\n2.7K\n4.1K\n4.0K\n59.7159.71\n60.6560.65\n68.46\n\n\nSlovak-SNK\n14.1K\n8.1K\n7.8K\n31.8331.83\n94.1394.13\n94.45\n\n\nSlovenian-SSJ\n22.3K\n18.3K\n18.2K\n33.3133.31\n95.3495.34\n96.06\n\n\nSpanish-AnCora\n9.5K\n17.7K\n17.7K\n61.3861.38\n95.1695.16\n96.59\n\n\nSwedish-Talbanken\n4.9K\n5.7K\n5.6K\n48.6348.63\n89.6389.63\n92.51\n\n\nTamil-TTB\n2.3K\n727\n734\n50.5450.54\n68.6668.66\n69.62\n\n\nTurkish-BOUN\n23.4K\n9.1K\n9.2K\n42.8042.80\n81.7781.77\n83.00\n\n\nUkrainian-IU\n16.4K\n9.7K\n9.6K\n32.1132.11\n92.5692.56\n93.81\n\n\nUrdu-UDTB\n9.0K\n6.5K\n6.7K\n82.3282.32\n87.6587.65\n89.05\n\n\nUyghur-UDT\n8.8K\n2.1K\n2.0K\n44.0344.03\n90.3690.36\n93.67\n\n\nVietnamese-VTB\n2.2K\n2.7K\n2.7K\n92.9692.96\n97.7497.74\n99.44\n\n\nWelsh-CCG\n3.0K\n2.8K\n2.8K\n60.1160.11\n84.9184.91\n88.80\n\n\nWestern_Armenian-ArmTDP\n11.1K\n7.5K\n7.5K\n36.3336.33\n91.6391.63\n93.39\n\n\nWolof-WTB\n2.3K\n2.3K\n2.4K\n78.3478.34\n87.3587.35\n89.76\n\n\ntotal size / avg accuracy\n809.6K\n723.3K\n720.2K\n45.2745.27\n76.6776.67\n81.36",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">size (# lemma-tag-form)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">accuracy (%)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">lang-corpus</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">train</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\">dev</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\">test</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">copy</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">mono</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_smallcaps\">multi</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Kyrgyz-KTMU</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">2.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\">675</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_t\">673</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\"><math alttext=\"42.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mn>42.20</mn><annotation encoding=\"application/x-tex\">42.20</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"64.34\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mn>64.34</mn><annotation encoding=\"application/x-tex\">64.34</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">66.57</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Latin-ITTB</td>\n<td class=\"ltx_td ltx_align_right\">5.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">9.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">9.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"14.58\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mn>14.58</mn><annotation encoding=\"application/x-tex\">14.58</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"76.33\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mn>76.33</mn><annotation encoding=\"application/x-tex\">76.33</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">88.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Latvian-LVTB</td>\n<td class=\"ltx_td ltx_align_right\">22.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">18.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">18.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"27.81\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m5\" intent=\":literal\"><semantics><mn>27.81</mn><annotation encoding=\"application/x-tex\">27.81</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">96.48</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"96.15\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m6\" intent=\":literal\"><semantics><mn>96.15</mn><annotation encoding=\"application/x-tex\">96.15</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Lithuanian-ALKSNIS</td>\n<td class=\"ltx_td ltx_align_right\">9.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">5.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">5.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"27.79\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m7\" intent=\":literal\"><semantics><mn>27.79</mn><annotation encoding=\"application/x-tex\">27.79</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"92.74\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m8\" intent=\":literal\"><semantics><mn>92.74</mn><annotation encoding=\"application/x-tex\">92.74</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">93.38</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Low_Saxon-LSDC</td>\n<td class=\"ltx_td ltx_align_right\">3.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"54.42\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m9\" intent=\":literal\"><semantics><mn>54.42</mn><annotation encoding=\"application/x-tex\">54.42</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"54.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m10\" intent=\":literal\"><semantics><mn>54.14</mn><annotation encoding=\"application/x-tex\">54.14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">62.49</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Maghrebi_Arabic_French-Arabizi</td>\n<td class=\"ltx_td ltx_align_right\">5.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">17.46</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"9.54\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m11\" intent=\":literal\"><semantics><mn>9.54</mn><annotation encoding=\"application/x-tex\">9.54</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"15.35\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m12\" intent=\":literal\"><semantics><mn>15.35</mn><annotation encoding=\"application/x-tex\">15.35</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Manx-Cadhan</td>\n<td class=\"ltx_td ltx_align_right\">762</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">1.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"65.15\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m13\" intent=\":literal\"><semantics><mn>65.15</mn><annotation encoding=\"application/x-tex\">65.15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"66.84\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m14\" intent=\":literal\"><semantics><mn>66.84</mn><annotation encoding=\"application/x-tex\">66.84</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">82.03</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Marathi-UFAL</td>\n<td class=\"ltx_td ltx_align_right\">759</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">302</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">302</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"47.02\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m15\" intent=\":literal\"><semantics><mn>47.02</mn><annotation encoding=\"application/x-tex\">47.02</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"56.62\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m16\" intent=\":literal\"><semantics><mn>56.62</mn><annotation encoding=\"application/x-tex\">56.62</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">74.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Naija-NSC</td>\n<td class=\"ltx_td ltx_align_right\">916</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"86.41\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m17\" intent=\":literal\"><semantics><mn>86.41</mn><annotation encoding=\"application/x-tex\">86.41</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"70.64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m18\" intent=\":literal\"><semantics><mn>70.64</mn><annotation encoding=\"application/x-tex\">70.64</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">96.04</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">North_Sami-Giella</td>\n<td class=\"ltx_td ltx_align_right\">4.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"33.00\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m19\" intent=\":literal\"><semantics><mn>33.00</mn><annotation encoding=\"application/x-tex\">33.00</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"68.50\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m20\" intent=\":literal\"><semantics><mn>68.50</mn><annotation encoding=\"application/x-tex\">68.50</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">75.96</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Norwegian-Bokmaal</td>\n<td class=\"ltx_td ltx_align_right\">7.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">14.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">14.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"53.62\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m21\" intent=\":literal\"><semantics><mn>53.62</mn><annotation encoding=\"application/x-tex\">53.62</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"93.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m22\" intent=\":literal\"><semantics><mn>93.08</mn><annotation encoding=\"application/x-tex\">93.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">95.26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Old_Church_Slavonic-PROIEL</td>\n<td class=\"ltx_td ltx_align_right\">24.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">12.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">13.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"6.35\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m23\" intent=\":literal\"><semantics><mn>6.35</mn><annotation encoding=\"application/x-tex\">6.35</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"24.34\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m24\" intent=\":literal\"><semantics><mn>24.34</mn><annotation encoding=\"application/x-tex\">24.34</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">27.56</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Old_East_Slavic-TOROT</td>\n<td class=\"ltx_td ltx_align_right\">28.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">16.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">16.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"7.49\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m25\" intent=\":literal\"><semantics><mn>7.49</mn><annotation encoding=\"application/x-tex\">7.49</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"31.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m26\" intent=\":literal\"><semantics><mn>31.08</mn><annotation encoding=\"application/x-tex\">31.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">33.82</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Old_French-PROFITEROLE</td>\n<td class=\"ltx_td ltx_align_right\">19.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">3.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">19.82</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"0.13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m27\" intent=\":literal\"><semantics><mn>0.13</mn><annotation encoding=\"application/x-tex\">0.13</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"18.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m28\" intent=\":literal\"><semantics><mn>18.05</mn><annotation encoding=\"application/x-tex\">18.05</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Ottoman_Turkish-BOUN</td>\n<td class=\"ltx_td ltx_align_right\">3.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">781</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">789</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"47.66\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m29\" intent=\":literal\"><semantics><mn>47.66</mn><annotation encoding=\"application/x-tex\">47.66</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"78.20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m30\" intent=\":literal\"><semantics><mn>78.20</mn><annotation encoding=\"application/x-tex\">78.20</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">84.03</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Persian-PerDT</td>\n<td class=\"ltx_td ltx_align_right\">12.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">14.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">14.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"67.70\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m31\" intent=\":literal\"><semantics><mn>67.70</mn><annotation encoding=\"application/x-tex\">67.70</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">72.96</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"70.73\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m32\" intent=\":literal\"><semantics><mn>70.73</mn><annotation encoding=\"application/x-tex\">70.73</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Polish-PDB</td>\n<td class=\"ltx_td ltx_align_right\">28.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">22.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">23.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"28.42\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m33\" intent=\":literal\"><semantics><mn>28.42</mn><annotation encoding=\"application/x-tex\">28.42</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">93.78</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"93.64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m34\" intent=\":literal\"><semantics><mn>93.64</mn><annotation encoding=\"application/x-tex\">93.64</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Pomak-Philotis</td>\n<td class=\"ltx_td ltx_align_right\">3.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"20.98\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m35\" intent=\":literal\"><semantics><mn>20.98</mn><annotation encoding=\"application/x-tex\">20.98</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"43.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m36\" intent=\":literal\"><semantics><mn>43.21</mn><annotation encoding=\"application/x-tex\">43.21</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">54.99</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Portuguese-Bosque</td>\n<td class=\"ltx_td ltx_align_right\">9.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"67.66\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m37\" intent=\":literal\"><semantics><mn>67.66</mn><annotation encoding=\"application/x-tex\">67.66</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"96.21\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m38\" intent=\":literal\"><semantics><mn>96.21</mn><annotation encoding=\"application/x-tex\">96.21</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">97.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Romanian-RRT</td>\n<td class=\"ltx_td ltx_align_right\">11.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"42.00\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m39\" intent=\":literal\"><semantics><mn>42.00</mn><annotation encoding=\"application/x-tex\">42.00</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"92.65\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m40\" intent=\":literal\"><semantics><mn>92.65</mn><annotation encoding=\"application/x-tex\">92.65</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">93.60</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Russian-SynTagRus</td>\n<td class=\"ltx_td ltx_align_right\">44.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">64.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">63.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"26.35\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m41\" intent=\":literal\"><semantics><mn>26.35</mn><annotation encoding=\"application/x-tex\">26.35</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"93.28\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m42\" intent=\":literal\"><semantics><mn>93.28</mn><annotation encoding=\"application/x-tex\">93.28</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">93.57</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Sanskrit-Vedic</td>\n<td class=\"ltx_td ltx_align_right\">19.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">11.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"9.43\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m43\" intent=\":literal\"><semantics><mn>9.43</mn><annotation encoding=\"application/x-tex\">9.43</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">75.55</span></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"75.22\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m44\" intent=\":literal\"><semantics><mn>75.22</mn><annotation encoding=\"application/x-tex\">75.22</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Scottish_Gaelic-ARCOSG</td>\n<td class=\"ltx_td ltx_align_right\">2.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">4.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">4.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"59.71\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m45\" intent=\":literal\"><semantics><mn>59.71</mn><annotation encoding=\"application/x-tex\">59.71</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"60.65\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m46\" intent=\":literal\"><semantics><mn>60.65</mn><annotation encoding=\"application/x-tex\">60.65</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">68.46</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Slovak-SNK</td>\n<td class=\"ltx_td ltx_align_right\">14.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">8.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">7.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"31.83\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m47\" intent=\":literal\"><semantics><mn>31.83</mn><annotation encoding=\"application/x-tex\">31.83</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"94.13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m48\" intent=\":literal\"><semantics><mn>94.13</mn><annotation encoding=\"application/x-tex\">94.13</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">94.45</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Slovenian-SSJ</td>\n<td class=\"ltx_td ltx_align_right\">22.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">18.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">18.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"33.31\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m49\" intent=\":literal\"><semantics><mn>33.31</mn><annotation encoding=\"application/x-tex\">33.31</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"95.34\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m50\" intent=\":literal\"><semantics><mn>95.34</mn><annotation encoding=\"application/x-tex\">95.34</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">96.06</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Spanish-AnCora</td>\n<td class=\"ltx_td ltx_align_right\">9.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">17.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">17.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"61.38\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m51\" intent=\":literal\"><semantics><mn>61.38</mn><annotation encoding=\"application/x-tex\">61.38</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"95.16\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m52\" intent=\":literal\"><semantics><mn>95.16</mn><annotation encoding=\"application/x-tex\">95.16</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">96.59</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Swedish-Talbanken</td>\n<td class=\"ltx_td ltx_align_right\">4.9K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">5.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">5.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"48.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m53\" intent=\":literal\"><semantics><mn>48.63</mn><annotation encoding=\"application/x-tex\">48.63</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"89.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m54\" intent=\":literal\"><semantics><mn>89.63</mn><annotation encoding=\"application/x-tex\">89.63</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">92.51</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Tamil-TTB</td>\n<td class=\"ltx_td ltx_align_right\">2.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">727</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">734</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"50.54\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m55\" intent=\":literal\"><semantics><mn>50.54</mn><annotation encoding=\"application/x-tex\">50.54</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"68.66\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m56\" intent=\":literal\"><semantics><mn>68.66</mn><annotation encoding=\"application/x-tex\">68.66</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">69.62</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Turkish-BOUN</td>\n<td class=\"ltx_td ltx_align_right\">23.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">9.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">9.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"42.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m57\" intent=\":literal\"><semantics><mn>42.80</mn><annotation encoding=\"application/x-tex\">42.80</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"81.77\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m58\" intent=\":literal\"><semantics><mn>81.77</mn><annotation encoding=\"application/x-tex\">81.77</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">83.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Ukrainian-IU</td>\n<td class=\"ltx_td ltx_align_right\">16.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">9.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">9.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"32.11\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m59\" intent=\":literal\"><semantics><mn>32.11</mn><annotation encoding=\"application/x-tex\">32.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"92.56\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m60\" intent=\":literal\"><semantics><mn>92.56</mn><annotation encoding=\"application/x-tex\">92.56</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">93.81</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Urdu-UDTB</td>\n<td class=\"ltx_td ltx_align_right\">9.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">6.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">6.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"82.32\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m61\" intent=\":literal\"><semantics><mn>82.32</mn><annotation encoding=\"application/x-tex\">82.32</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"87.65\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m62\" intent=\":literal\"><semantics><mn>87.65</mn><annotation encoding=\"application/x-tex\">87.65</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">89.05</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Uyghur-UDT</td>\n<td class=\"ltx_td ltx_align_right\">8.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"44.03\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m63\" intent=\":literal\"><semantics><mn>44.03</mn><annotation encoding=\"application/x-tex\">44.03</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"90.36\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m64\" intent=\":literal\"><semantics><mn>90.36</mn><annotation encoding=\"application/x-tex\">90.36</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">93.67</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Vietnamese-VTB</td>\n<td class=\"ltx_td ltx_align_right\">2.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.7K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"92.96\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m65\" intent=\":literal\"><semantics><mn>92.96</mn><annotation encoding=\"application/x-tex\">92.96</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"97.74\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m66\" intent=\":literal\"><semantics><mn>97.74</mn><annotation encoding=\"application/x-tex\">97.74</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">99.44</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Welsh-CCG</td>\n<td class=\"ltx_td ltx_align_right\">3.0K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.8K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"60.11\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m67\" intent=\":literal\"><semantics><mn>60.11</mn><annotation encoding=\"application/x-tex\">60.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"84.91\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m68\" intent=\":literal\"><semantics><mn>84.91</mn><annotation encoding=\"application/x-tex\">84.91</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">88.80</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Western_Armenian-ArmTDP</td>\n<td class=\"ltx_td ltx_align_right\">11.1K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">7.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">7.5K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"36.33\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m69\" intent=\":literal\"><semantics><mn>36.33</mn><annotation encoding=\"application/x-tex\">36.33</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"91.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m70\" intent=\":literal\"><semantics><mn>91.63</mn><annotation encoding=\"application/x-tex\">91.63</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">93.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Wolof-WTB</td>\n<td class=\"ltx_td ltx_align_right\">2.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right\">2.4K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\"><math alttext=\"78.34\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m71\" intent=\":literal\"><semantics><mn>78.34</mn><annotation encoding=\"application/x-tex\">78.34</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"87.35\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m72\" intent=\":literal\"><semantics><mn>87.35</mn><annotation encoding=\"application/x-tex\">87.35</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">89.76</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">total size / avg accuracy</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">809.6K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_bb ltx_border_t\">723.3K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_right ltx_border_bb ltx_border_t\">720.2K</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_bb ltx_border_t\"><math alttext=\"45.27\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m73\" intent=\":literal\"><semantics><mn>45.27</mn><annotation encoding=\"application/x-tex\">45.27</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><math alttext=\"76.67\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m74\" intent=\":literal\"><semantics><mn>76.67</mn><annotation encoding=\"application/x-tex\">76.67</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">81.36</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "train",
            "78k",
            "lemma",
            "19k",
            "167k",
            "swedishtalbanken",
            "198k",
            "tamilttb",
            "latvianlvtb",
            "persianperdt",
            "northsamigiella",
            "92k",
            "kyrgyzktmu",
            "194k",
            "40k",
            "29k",
            "58k",
            "8096k",
            "pomakphilotis",
            "test",
            "231k",
            "baseline",
            "95k",
            "74k",
            "copies",
            "226k",
            "283k",
            "43k",
            "output",
            "welshccg",
            "147k",
            "88k",
            "124k",
            "23k",
            "11k",
            "117k",
            "35k",
            "162k",
            "116k",
            "avg",
            "romanianrrt",
            "naijansc",
            "accuracy",
            "maghrebiarabicfrencharabizi",
            "91k",
            "247k",
            "vietnamesevtb",
            "scottishgaelicarcosg",
            "57k",
            "latinittb",
            "langcorpus",
            "spanishancora",
            "multi",
            "59k",
            "75k",
            "slovaksnk",
            "lemmatagform",
            "size",
            "urduudtb",
            "7233k",
            "turkishboun",
            "148k",
            "ottomanturkishboun",
            "22k",
            "27k",
            "copy",
            "97k",
            "67k",
            "444k",
            "234k",
            "93k",
            "dev",
            "28k",
            "21k",
            "lowsaxonlsdc",
            "142k",
            "norwegianbokmaal",
            "129k",
            "uyghurudt",
            "total",
            "lithuanianalksnis",
            "oldchurchslavonicproiel",
            "wolofwtb",
            "oldeastslavictorot",
            "7202k",
            "164k",
            "marathiufal",
            "141k",
            "183k",
            "russiansyntagrus",
            "182k",
            "641k",
            "input",
            "sanskritvedic",
            "ukrainianiu",
            "286k",
            "111k",
            "49k",
            "223k",
            "12k",
            "96k",
            "90k",
            "177k",
            "30k",
            "polishpdb",
            "632k",
            "mono",
            "portuguesebosque",
            "18k",
            "part",
            "westernarmenianarmtdp",
            "81k",
            "slovenianssj",
            "65k",
            "manxcadhan",
            "oldfrenchprofiterole",
            "132k",
            "56k",
            "41k",
            "50k",
            "20k",
            "24k",
            "113k"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The two-part Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T3\" title=\"Table 3 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>&#160;and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T4\" title=\"Table 4 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the results on 73 corpora from UD. The overall best-performing system is <span class=\"ltx_text ltx_font_smallcaps\">multi</span>, which was trained jointly on all 73 languages. Among these languages, it is outperformed in 3 cases by the simple <span class=\"ltx_text ltx_font_smallcaps\">copy</span> baseline, and in 7 cases by the monolingual models (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>), each trained separately for a single language.\nOn macro average across all languages, the <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model improves over the <span class=\"ltx_text ltx_font_smallcaps\">mono</span> models by 4.69%.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We use the SIGMORPHON 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> and the SIGMORPHON 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite> shared task benchmarks for evaluating the strength of our models.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Hebrew was excluded from SIGMORPHON 2023 due to filename inconsistencies that prevented reliable comparison.</span></span></span>\nFurthermore, with the goal of future deployment and multilingual applicability, we select 73 languages\nof the Universal Dependencies <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib16\" title=\"\">16</a>]</cite> corpora<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>The selection contains 32 Indo-European languages written in the Latin script and 41 languages without restriction on language family or script. For languages with multiple corpora in UD, we chose one corpus per language based on historical preferences, practical considerations, and the defaults in the widely adopted UDPipe tool <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib21\" title=\"\">21</a>]</cite>. We prioritized large, canonical corpora, excluded those with narrow or specialized domains, and favored manually over semi-automatically annotated corpora. Corpora lacking lemma annotations were excluded.</span></span></span> and lexicalize them by extracting unique lemma-tag-form triples along with their occurrence counts for training and evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Unlike UniMorph, the Universal Dependencies data can be more prone to noise. We rely on a neural network to mitigate its impact.</span></span></span></p>\n\n",
                "matched_terms": [
                    "lemma",
                    "lemmatagform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Strategies for data splitting in the inflection field differ in the degree of overlap control between train-test splits, from an uncontrolled overlap (where lemma-tag-form may overlap randomly) to varying degrees of control over lemma and/or morphological feature overlap. The former has been the traditional approach for decades but has recently come under critique for hindering evaluation of models&#8217; generalization abilities.</p>\n\n",
                "matched_terms": [
                    "lemma",
                    "lemmatagform"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the very least, a <span class=\"ltx_text ltx_font_italic\">lemma-disjoint</span> option\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib8\" title=\"\">8</a>]</cite> is recently advisable, and SIGMORPHON 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> probed this setting in its <span class=\"ltx_text ltx_font_italic\">feature overlap</span> setting: A test pair&#8217;s feature set is attested in training, but its lemma is novel. The latest SIGMORPHON shared task installation completely moved to the <span class=\"ltx_text ltx_font_italic\">lemma-disjoint</span> setting in 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite>. We use the official train-dev-test split for both the SIGMORPHON benchmarks.</p>\n\n",
                "matched_terms": [
                    "lemma",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another axis for consideration is the manipulation with the lemma-tag-form corpus frequencies during sampling into the train-test splits. Traditionally, the corpus frequencies are neglected (or not even present in the mostly used data source, UniMorph <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib11\" title=\"\">11</a>]</cite>) and the lemma-tag-form triples are sampled uniformly. Kodner et al. (2023) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib13\" title=\"\">13</a>]</cite> argued that uniform sampling leads to an unrealistic train-test split, with train set unnaturally biased towards low-frequency types. They introduced a <span class=\"ltx_text ltx_font_italic\">frequency-weighted sampling</span> method to produce a more realistic train-test distribution.\nHowever, the suggested split is not lemma-disjoint. Also, no further work has experimented with that split.</p>\n\n",
                "matched_terms": [
                    "lemmatagform",
                    "train"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a Transformer architecture of the encoder-decoder, sequence-to-sequence type, characterized by a small capacity\naccording to state-of-the-art standards in the inflection field.\nThis model is trained from scratch on inflection data, using characters as tokens, with the input consisting of lemma-tag pair and the output being the corresponding inflected form (see Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "output",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use a special language ID token as part of the input sequence <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib17\" title=\"\">17</a>]</cite> by prepending it to the sequence of morphological tags. This should help the model disambiguate identical lemma strings corresponding to different languages.</p>\n\n",
                "matched_terms": [
                    "part",
                    "lemma",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike previous work on multilingual inflection which usually used the SIGMORPHON shared task data where the train set for all languages was of the same size, we need to deal with the problem of mixing corpora of substantially different sizes. If we na&#239;vely pooled the training datasets, the model may be overly influenced by the high-resource languages. To address this issue, we adapt a corpus upsampling method by Wang et al. (2020) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib25\" title=\"\">25</a>]</cite>.\nFor better control over sampling into training batches, we also adapt a temperature value <math alttext=\"\\tau{}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau{}\\in[0,1]</annotation></semantics></math> (our <math alttext=\"\\tau=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\tau=0.5</annotation></semantics></math>) to smooth the distribution of training data from datasets of different sizes, see van der Goot et al. (2021) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib9\" title=\"\">9</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "train"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our monolingual and multilingual systems achieve competitive performance on a global inflection benchmark, specifically the SIGMORPHON Inflection Shared Tasks from 2022 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib12\" title=\"\">12</a>]</cite> and 2023 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#bib.bib7\" title=\"\">7</a>]</cite>, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T1\" title=\"Table 1 &#8227; 4.0.2 Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.23114v1#S4.T2\" title=\"Table 2 &#8227; 4.0.3 Multilingual Model &#8227; 4 Methodology &#8227; Flexing in 73 Languages: A Single Small Model for Multilingual Inflection\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, respectively:\n<span class=\"ltx_text ltx_font_smallcaps\">mono</span> ranks 4th in the 2022 data and 6th in the 2023 data, while <span class=\"ltx_text ltx_font_smallcaps\">multi</span> attains 3rd place in 2022 and 1st place in 2023, based on the average across languages.\nFor clarity, we note that the models were trained from scratch exclusively on the SIGMORPHON benchmark data.</p>\n\n",
                "matched_terms": [
                    "multi",
                    "mono"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our <span class=\"ltx_text ltx_font_smallcaps\">mono</span> and <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model, hyperparameters tuned on UD\ndevelopment data are: layers (encoder/decoder) = <math alttext=\"3/3\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m1\" intent=\":literal\"><semantics><mrow><mn>3</mn><mo>/</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3/3</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>) and <math alttext=\"4/4\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m2\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo>/</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">4/4</annotation></semantics></math>&#160;(<span class=\"ltx_text ltx_font_smallcaps\">multi</span>),\nlayer dimension = <math alttext=\"256\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m3\" intent=\":literal\"><semantics><mn>256</mn><annotation encoding=\"application/x-tex\">256</annotation></semantics></math>, feed-forward dimension = <math alttext=\"64\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m4\" intent=\":literal\"><semantics><mn>64</mn><annotation encoding=\"application/x-tex\">64</annotation></semantics></math>, attention heads = <math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m5\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, dropout = <math alttext=\"0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m6\" intent=\":literal\"><semantics><mn>0.15</mn><annotation encoding=\"application/x-tex\">0.15</annotation></semantics></math>, attention dropout = <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m7\" intent=\":literal\"><semantics><mn>0.1</mn><annotation encoding=\"application/x-tex\">0.1</annotation></semantics></math>, activation dropout = <math alttext=\"0.35\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m8\" intent=\":literal\"><semantics><mn>0.35</mn><annotation encoding=\"application/x-tex\">0.35</annotation></semantics></math>, and layer drop = <math alttext=\"0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m9\" intent=\":literal\"><semantics><mn>0.2</mn><annotation encoding=\"application/x-tex\">0.2</annotation></semantics></math>. We use gradient clipping (max norm 1.0), L2 regularization (0.01), and train with Adam\nusing cosine learning rate decay, initial LR&#160;=&#160;0.001, batch size = <math alttext=\"512\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m10\" intent=\":literal\"><semantics><mn>512</mn><annotation encoding=\"application/x-tex\">512</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">mono</span>) and <math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m11\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math> (<span class=\"ltx_text ltx_font_smallcaps\">multi</span>).\nCheckpoints are selected by dev set performance: on the target language for <span class=\"ltx_text ltx_font_smallcaps\">mono</span>, or macro-averaged across languages for <span class=\"ltx_text ltx_font_smallcaps\">multi</span>, over up to <math alttext=\"960\" class=\"ltx_Math\" display=\"inline\" id=\"S7.p1.m12\" intent=\":literal\"><semantics><mn>960</mn><annotation encoding=\"application/x-tex\">960</annotation></semantics></math> epochs.\nThe <span class=\"ltx_text ltx_font_smallcaps\">mono</span> model has 3.12M-3.69M parameters,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>varying by vocabulary size</span></span></span> while the <span class=\"ltx_text ltx_font_smallcaps\">multi</span> model has 5.69M parameters.</p>\n\n",
                "matched_terms": [
                    "train",
                    "dev",
                    "size",
                    "multi",
                    "mono"
                ]
            }
        ]
    }
}