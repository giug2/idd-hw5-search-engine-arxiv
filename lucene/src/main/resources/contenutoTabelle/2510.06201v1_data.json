{
    "S3.T1": {
        "source_file": "TokenChain: A Discrete Speech Chain via Semantic Token Modeling",
        "caption": "Table 1: ASR Epoch-12 CER/WER (%) on LibriSpeech-960.\nd-{c,o}/t-{c,o} denotes dev-{clean,other}/test-{clean,other}.",
        "body": "CER ↓\nWER ↓\n\n\nModel\nd-c\nd-o\nt-c\nt-o\nd-c\nd-o\nt-c\nt-o\n\n\n\nPre-chain (Epoch 0)\n\n4.0\n10.5\n4.0\n10.9\n10.4\n23.1\n10.6\n23.9\n\n\n\n\n\nBaseline (LASRL_{\\text{ASR}} only)\n\n1.6\n5.6\n1.7\n6.0\n4.8\n13.0\n5.0\n13.8\n\n\nST-Argmax\n1.5\n5.3\n1.5\n5.7\n4.4\n12.5\n4.5\n13.2\n\n\nST-Gumbel Anneal\n1.4\n5.3\n1.4\n5.5\n4.2\n12.1\n4.4\n12.8\n\n\nST-Gumbel 1.5\n1.4\n5.3\n1.5\n5.5\n4.2\n12.2\n4.5\n12.8\n\n\nST-Gumbel 1.0\n1.5\n5.3\n1.5\n5.7\n4.5\n12.3\n4.6\n13.1\n\n\nST-Gumbel 0.75\n1.5\n5.3\n1.5\n5.6\n4.4\n12.4\n4.5\n13.1",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding:-0.25pt 2.8pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CER &#8595;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER &#8595;</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">d-c</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">d-o</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">t-c</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">t-o</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">d-c</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">d-o</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">t-c</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">t-o</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\">\n<em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Pre-chain</em><span class=\"ltx_text\" style=\"font-size:90%;\"> (Epoch 0)</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.0</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.5</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.0</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.9</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.4</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.6</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.9</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Baseline (</span><math alttext=\"L_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">L</mi><mtext mathsize=\"0.900em\">ASR</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{ASR}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> only)</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Argmax</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ST-Gumbel Anneal</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">12.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">12.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel 1.5</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel 1.0</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel 0.75</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "model",
            "epoch",
            "cerwer",
            "lasrltextasr",
            "stgumbel",
            "librispeech960",
            "anneal",
            "dcotco",
            "denotes",
            "devcleanothertestcleanother",
            "prechain",
            "baseline",
            "epoch12",
            "wer",
            "cer",
            "only",
            "stargmax",
            "asr"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T1\" title=\"Table 1 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents equal-compute CER/WER at epoch 12, when TokenChain variants have effectively converged and exceed baseline&#8217;s final figures at epoch 20. Across all splits, chain feedback outperforms the <math alttext=\"L_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{ASR}}</annotation></semantics></math>-only baseline, adding to the baseline&#8217;s roughly halved error rate from the pre-chain checkpoint. The strongest variant, ST-Gumbel Anneal, yields further 10&#8211;13% relative gains on clean sets and 5&#8211;9% on other sets. Fixed <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> around 1.5 is competitive, whereas sharper <math alttext=\"\\tau{\\leq}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8804;</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\leq}1.0</annotation></semantics></math> are less effective yet still surpass the baseline.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Machine Speech Chain, simulating the human perception-production loop, proves effective in jointly improving ASR and TTS. We propose TokenChain, a fully discrete speech chain coupling semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic model co-trained with ASR and a masked-generative semantic-to-acoustic model for synthesis only. End-to-end feedback across the text interface is enabled with straight-through argmax/Gumbel&#8211;Softmax and balanced with supervised ASR via dynamic weight averaging. Ablations examine optimal temperature schedules for in- and cross-domain transfer. Evaluation reveals TokenChain surpasses baseline accuracy 2&#8211;6 epochs earlier and yields 5&#8211;13% lower equal-epoch error with stable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by 31% on TED-LIUM with minimal forgetting, showing that chain learning remains effective with token interfaces and models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "baseline",
                    "wer",
                    "only",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, scaling in self-supervised learning have driven gains, and discretizing SSL features emerged as viable inputs. Early unit-based systems required language models to match log-mel baselines <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib12\" title=\"\">12</a>]</cite>; recent work quantizes SSL features into compact codebooks to achieve competitive WER with lower storage and I/O <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib13\" title=\"\">13</a>]</cite>. These results suggest semantic token sequences are suitable carriers for recognition and, by symmetry, promising targets for text-conditioned generation.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TokenChain (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) employs SOTA-competitive architectures, coupling a semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic (T2S) model co-trained with ASR, and a masked-generative semantic-to-acoustic (S2A) module for synthesis only. The ASR&#8211;T2S interface is textual, while feedback is entirely token-based: ST-argmax/Gumbel&#8211;Softmax enable backpropagation, and a semantic-token reconstruction loss is dynamically balanced with ASR cross-entropy. In chained training, TokenChain outperforms baselines, converging 2&#8211;6 epochs earlier with 5&#8211;13% lower equal-epoch error, and under domain adaptation reduces WER by 56% for ASR and 31% for T2S with minimal forgetting, demonstrating that the speech-chain paradigm reinstantiates effectively in the discrete-token era.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "only",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all experiments, the forward pass uses a hard one-hot <math alttext=\"\\hat{\\mathbf{y}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119858;</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{y}}_{t}</annotation></semantics></math>; in the backward pass, gradients propagate through <math alttext=\"{\\mathbf{p}}_{y}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>&#119849;</mi><mi>y</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">{\\mathbf{p}}_{y}^{t}</annotation></semantics></math> for ST-argmax and through <math alttext=\"\\tilde{\\mathbf{p}}_{y}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119849;</mi><mo>~</mo></mover><mi>y</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{p}}_{y}^{t}</annotation></semantics></math> for ST-Gumbel.</p>\n\n",
                "matched_terms": [
                    "stgumbel",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics.</span>\nFor ASR, we evaluate CER and WER. For synthesized audio, we evaluate WER via Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib26\" title=\"\">26</a>]</cite>, speaker similarity (SIM-O) with WavLM&#8211;TDNN2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib27\" title=\"\">27</a>]</cite>, and speech quality using UTMOSv2 (Predicted MOS) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib28\" title=\"\">28</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TokenChain Experiments.</span>\nStarting from LibriSpeech-100 pretrained ASR/T2S, enable chain feedback (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S2.SS4\" title=\"2.4 Discrete Pass-Through and Chain Feedback &#8227; 2 Methods &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>) on LibriSpeech-960 and TED-LIUM v2 for 20 epochs. As a comparison baseline, we train the same setup for 20 epochs with T2S feedback disabled. We compare ST-argmax, ST-Gumbel with temperature\n<math alttext=\"\\tau\\!\\in\\!\\{\\,\\text{anneal }2.0{\\to}0.1\\text{ in 10 epochs},\\allowbreak\\,1.5,\\,1.0,\\,0.75\\,\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8712;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mtext>anneal&#160;</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2.0</mn></mrow><mo stretchy=\"false\">&#8594;</mo><mrow><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>&#160;in 10 epochs</mtext></mrow><mo>,</mo><mn>&#8201;1.5</mn><mo>,</mo><mn>&#8201;1.0</mn><mo>,</mo><mn>&#8201;0.75</mn></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\!\\in\\!\\{\\,\\text{anneal }2.0{\\to}0.1\\text{ in 10 epochs},\\allowbreak\\,1.5,\\,1.0,\\,0.75\\,\\}</annotation></semantics></math>.\nEarly stopping halts training after 3 consecutive epochs without validation improvement.\nThe chain weight <math alttext=\"\\alpha_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mi>e</mi></msub><annotation encoding=\"application/x-tex\">\\alpha_{e}</annotation></semantics></math> follows DWA with warm-ups and capped ramp. In our runs <math alttext=\"(\\alpha_{\\mathrm{w0}},\\alpha_{\\mathrm{w1}},\\alpha_{\\max},e_{\\mathrm{ramp}},T)=(10^{-3},0.05,0.5,6,2)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>w0</mi></msub><mo>,</mo><msub><mi>&#945;</mi><mi>w1</mi></msub><mo>,</mo><msub><mi>&#945;</mi><mi>max</mi></msub><mo>,</mo><msub><mi>e</mi><mi>ramp</mi></msub><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup><mo>,</mo><mn>0.05</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">(\\alpha_{\\mathrm{w0}},\\alpha_{\\mathrm{w1}},\\alpha_{\\max},e_{\\mathrm{ramp}},T)=(10^{-3},0.05,0.5,6,2)</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "stgumbel",
                    "librispeech960",
                    "stargmax",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> evaluates T2S by calculating Whisper-WER, SIM-O, and predicted MOS on audio synthesized with a fixed S2A. Chain feedback improves content robustness while preserving perceptual quality. ST-argmax achieves the lowest Whisper-WER (-11.6% vs. baseline). SIM-O remains within <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.5 of baseline, and MOS is stable, peaking at <math alttext=\"\\tau{=}1.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>1.5</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}1.5</annotation></semantics></math> (+0.06). Sharper temperatures (<math alttext=\"\\tau{\\leq}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8804;</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\leq}1.0</annotation></semantics></math>) degrade WER with minimal change in SIM-O/MOS, indicating that over-discrete interfaces (small <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math>) impede text controllability of the T2S.</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "stargmax",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Convergence Efficiency.</span> Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.F2\" title=\"Figure 2 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that TokenChain&#8217;s learning curves are consistently below the baseline for CER and WER, surpassing baseline accuracy 2&#8211;6 epochs earlier. At epoch 12, TokenChain already exceeds baseline&#8217;s final (epoch 20) correct rates by 0.2 on clean sets and 0.4 on other sets. Hence, comparable or better accuracy is achieved with approximately 40% fewer epochs (and compute), yielding substantial efficiency gains and underscoring chain feedback&#8217;s role as a capable optimization aid and regularizer.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "baseline",
                    "wer",
                    "epoch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T3\" title=\"Table 3 &#8227; 3.4 Domain Adaptation on TED-LIUM &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports final-epoch CER/WER, as the dominant TED-LIUM effect is improved generalization rather than faster convergence. Although the <math alttext=\"L_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{ASR}}</annotation></semantics></math>-only baseline already cuts WER by 52% from pre-chain, TokenChain provides further dev/test gains. The best configuration, ST-Gumbel at <math alttext=\"\\tau{=}0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}0.75</annotation></semantics></math>, attains 6.0/6.2 CER and 12.7/12.6 WER, corresponding to 55.3% and 56.4% total reductions, respectively. Argmax and annealed Gumbel are competitive but not optimal. The trend indicates that sharper discrete interfaces (smaller <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math>, approaching ST-argmax) tend to favor cross-domain transfer.</p>\n\n",
                "matched_terms": [
                    "cerwer",
                    "prechain",
                    "baseline",
                    "wer",
                    "cer",
                    "stgumbel",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">T2S results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T4\" title=\"Table 4 &#8227; 3.4 Domain Adaptation on TED-LIUM &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>: chaining yields consistent improvements over the pre-chain baseline across all criteria. Whisper-WER drops from 10.15 to 7.05&#8211;7.88, translating to <math alttext=\"22\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>22</mn><annotation encoding=\"application/x-tex\">22</annotation></semantics></math>&#8211;<math alttext=\"31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mn>31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">31\\%</annotation></semantics></math> relative gains, with best result occurring at <math alttext=\"\\tau{=}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}1.0</annotation></semantics></math>. Speaker similarity and MOS also improve by 4% on average, with ST-argmax achieving the highest SIM-O (57.22) and predicted MOS (3.03). Thus, joint training transfers well to the target domain without sacrificing naturalness.</p>\n\n",
                "matched_terms": [
                    "prechain",
                    "baseline",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced TokenChain, a machine speech chain with a fully discrete token interface. Using ST-argmax/Gumbel&#8211;Softmax with dynamic weight averaging, it enables end-to-end feedback between a semantic-token ASR and an AR T2S while keeping a NAR S2A fixed for synthesis. Empirically, TokenChain improves recognition under equal compute and converges 2&#8211;6 epochs earlier on LibriSpeech (5&#8211;13% lower equal-epoch error); under TED-LIUM adaptation it achieves substantial ASR (56%) and T2S WER (31%) reductions with limited forgetting. Ablations show annealed ST-Gumbel (<math alttext=\"\\tau\\!:\\!\\,2.0{\\to}0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">:</mo><mrow><mn>&#8201;2.0</mn><mo stretchy=\"false\">&#8594;</mo><mn>0.1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\!:\\!\\,2.0{\\to}0.1</annotation></semantics></math>) is strongest in-domain, whereas a sharper interface (<math alttext=\"\\tau{\\approx}0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8776;</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\approx}0.75</annotation></semantics></math>) favors cross-domain transfer. Future work includes adaptive learning rate scheduling, joint S2A training, scaling to larger multilingual corpora, and human evaluations.</p>\n\n",
                "matched_terms": [
                    "stgumbel",
                    "asr",
                    "wer"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "TokenChain: A Discrete Speech Chain via Semantic Token Modeling",
        "caption": "Table 2: LibriSpeech TTS: Accuracy (WER %), Speaker similarity (SIM-O), and Naturalness (Predicted MOS).",
        "body": "Model\nWER ↓\nSIM-O ↑\nPred. MOS ↑\n\n\n\nPre-chain / Baseline\n\n11.78\n64.58\n3.38\n\n\n\n\nST-Argmax\n10.41\n64.39\n3.39\n\n\nST-Gumbel Anneal\n12.73\n64.94\n3.41\n\n\nST-Gumbel 1.5\n11.37\n64.72\n3.44\n\n\nST-Gumbel 1.0\n13.40\n65.05\n3.39\n\n\nST-Gumbel 0.75\n15.52\n64.40\n3.41",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER &#8595;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SIM-O &#8593;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Pred. MOS &#8593;</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\">\n<em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Pre-chain</em><span class=\"ltx_text\" style=\"font-size:90%;\"> / Baseline</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.78</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.58</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.38</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Argmax</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">10.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.39</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel Anneal</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.73</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.94</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.41</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel 1.5</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.37</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.72</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.44</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel 1.0</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">65.05</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel 0.75</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.41</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "naturalness",
            "predicted",
            "model",
            "pred",
            "mos",
            "tts",
            "simo",
            "librispeech",
            "anneal",
            "similarity",
            "prechain",
            "baseline",
            "wer",
            "stgumbel",
            "speaker",
            "accuracy",
            "stargmax"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> evaluates T2S by calculating Whisper-WER, SIM-O, and predicted MOS on audio synthesized with a fixed S2A. Chain feedback improves content robustness while preserving perceptual quality. ST-argmax achieves the lowest Whisper-WER (-11.6% vs. baseline). SIM-O remains within <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.5 of baseline, and MOS is stable, peaking at <math alttext=\"\\tau{=}1.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>1.5</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}1.5</annotation></semantics></math> (+0.06). Sharper temperatures (<math alttext=\"\\tau{\\leq}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8804;</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\leq}1.0</annotation></semantics></math>) degrade WER with minimal change in SIM-O/MOS, indicating that over-discrete interfaces (small <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math>) impede text controllability of the T2S.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Machine Speech Chain, simulating the human perception-production loop, proves effective in jointly improving ASR and TTS. We propose TokenChain, a fully discrete speech chain coupling semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic model co-trained with ASR and a masked-generative semantic-to-acoustic model for synthesis only. End-to-end feedback across the text interface is enabled with straight-through argmax/Gumbel&#8211;Softmax and balanced with supervised ASR via dynamic weight averaging. Ablations examine optimal temperature schedules for in- and cross-domain transfer. Evaluation reveals TokenChain surpasses baseline accuracy 2&#8211;6 epochs earlier and yields 5&#8211;13% lower equal-epoch error with stable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by 31% on TED-LIUM with minimal forgetting, showing that chain learning remains effective with token interfaces and models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts",
                    "librispeech",
                    "baseline",
                    "wer",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TokenChain (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) employs SOTA-competitive architectures, coupling a semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic (T2S) model co-trained with ASR, and a masked-generative semantic-to-acoustic (S2A) module for synthesis only. The ASR&#8211;T2S interface is textual, while feedback is entirely token-based: ST-argmax/Gumbel&#8211;Softmax enable backpropagation, and a semantic-token reconstruction loss is dynamically balanced with ASR cross-entropy. In chained training, TokenChain outperforms baselines, converging 2&#8211;6 epochs earlier with 5&#8211;13% lower equal-epoch error, and under domain adaptation reduces WER by 56% for ASR and 31% for T2S with minimal forgetting, demonstrating that the speech-chain paradigm reinstantiates effectively in the discrete-token era.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all experiments, the forward pass uses a hard one-hot <math alttext=\"\\hat{\\mathbf{y}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119858;</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{y}}_{t}</annotation></semantics></math>; in the backward pass, gradients propagate through <math alttext=\"{\\mathbf{p}}_{y}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>&#119849;</mi><mi>y</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">{\\mathbf{p}}_{y}^{t}</annotation></semantics></math> for ST-argmax and through <math alttext=\"\\tilde{\\mathbf{p}}_{y}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119849;</mi><mo>~</mo></mover><mi>y</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{p}}_{y}^{t}</annotation></semantics></math> for ST-Gumbel.</p>\n\n",
                "matched_terms": [
                    "stgumbel",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics.</span>\nFor ASR, we evaluate CER and WER. For synthesized audio, we evaluate WER via Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib26\" title=\"\">26</a>]</cite>, speaker similarity (SIM-O) with WavLM&#8211;TDNN2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib27\" title=\"\">27</a>]</cite>, and speech quality using UTMOSv2 (Predicted MOS) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib28\" title=\"\">28</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "predicted",
                    "mos",
                    "simo",
                    "similarity",
                    "wer",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TokenChain Experiments.</span>\nStarting from LibriSpeech-100 pretrained ASR/T2S, enable chain feedback (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S2.SS4\" title=\"2.4 Discrete Pass-Through and Chain Feedback &#8227; 2 Methods &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>) on LibriSpeech-960 and TED-LIUM v2 for 20 epochs. As a comparison baseline, we train the same setup for 20 epochs with T2S feedback disabled. We compare ST-argmax, ST-Gumbel with temperature\n<math alttext=\"\\tau\\!\\in\\!\\{\\,\\text{anneal }2.0{\\to}0.1\\text{ in 10 epochs},\\allowbreak\\,1.5,\\,1.0,\\,0.75\\,\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8712;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mtext>anneal&#160;</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2.0</mn></mrow><mo stretchy=\"false\">&#8594;</mo><mrow><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>&#160;in 10 epochs</mtext></mrow><mo>,</mo><mn>&#8201;1.5</mn><mo>,</mo><mn>&#8201;1.0</mn><mo>,</mo><mn>&#8201;0.75</mn></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\!\\in\\!\\{\\,\\text{anneal }2.0{\\to}0.1\\text{ in 10 epochs},\\allowbreak\\,1.5,\\,1.0,\\,0.75\\,\\}</annotation></semantics></math>.\nEarly stopping halts training after 3 consecutive epochs without validation improvement.\nThe chain weight <math alttext=\"\\alpha_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mi>e</mi></msub><annotation encoding=\"application/x-tex\">\\alpha_{e}</annotation></semantics></math> follows DWA with warm-ups and capped ramp. In our runs <math alttext=\"(\\alpha_{\\mathrm{w0}},\\alpha_{\\mathrm{w1}},\\alpha_{\\max},e_{\\mathrm{ramp}},T)=(10^{-3},0.05,0.5,6,2)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>w0</mi></msub><mo>,</mo><msub><mi>&#945;</mi><mi>w1</mi></msub><mo>,</mo><msub><mi>&#945;</mi><mi>max</mi></msub><mo>,</mo><msub><mi>e</mi><mi>ramp</mi></msub><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup><mo>,</mo><mn>0.05</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">(\\alpha_{\\mathrm{w0}},\\alpha_{\\mathrm{w1}},\\alpha_{\\max},e_{\\mathrm{ramp}},T)=(10^{-3},0.05,0.5,6,2)</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "stgumbel",
                    "baseline",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T1\" title=\"Table 1 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents equal-compute CER/WER at epoch 12, when TokenChain variants have effectively converged and exceed baseline&#8217;s final figures at epoch 20. Across all splits, chain feedback outperforms the <math alttext=\"L_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{ASR}}</annotation></semantics></math>-only baseline, adding to the baseline&#8217;s roughly halved error rate from the pre-chain checkpoint. The strongest variant, ST-Gumbel Anneal, yields further 10&#8211;13% relative gains on clean sets and 5&#8211;9% on other sets. Fixed <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> around 1.5 is competitive, whereas sharper <math alttext=\"\\tau{\\leq}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8804;</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\leq}1.0</annotation></semantics></math> are less effective yet still surpass the baseline.</p>\n\n",
                "matched_terms": [
                    "stgumbel",
                    "anneal",
                    "baseline",
                    "prechain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Convergence Efficiency.</span> Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.F2\" title=\"Figure 2 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that TokenChain&#8217;s learning curves are consistently below the baseline for CER and WER, surpassing baseline accuracy 2&#8211;6 epochs earlier. At epoch 12, TokenChain already exceeds baseline&#8217;s final (epoch 20) correct rates by 0.2 on clean sets and 0.4 on other sets. Hence, comparable or better accuracy is achieved with approximately 40% fewer epochs (and compute), yielding substantial efficiency gains and underscoring chain feedback&#8217;s role as a capable optimization aid and regularizer.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "baseline",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T3\" title=\"Table 3 &#8227; 3.4 Domain Adaptation on TED-LIUM &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports final-epoch CER/WER, as the dominant TED-LIUM effect is improved generalization rather than faster convergence. Although the <math alttext=\"L_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{ASR}}</annotation></semantics></math>-only baseline already cuts WER by 52% from pre-chain, TokenChain provides further dev/test gains. The best configuration, ST-Gumbel at <math alttext=\"\\tau{=}0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}0.75</annotation></semantics></math>, attains 6.0/6.2 CER and 12.7/12.6 WER, corresponding to 55.3% and 56.4% total reductions, respectively. Argmax and annealed Gumbel are competitive but not optimal. The trend indicates that sharper discrete interfaces (smaller <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math>, approaching ST-argmax) tend to favor cross-domain transfer.</p>\n\n",
                "matched_terms": [
                    "prechain",
                    "baseline",
                    "wer",
                    "stgumbel",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">T2S results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T4\" title=\"Table 4 &#8227; 3.4 Domain Adaptation on TED-LIUM &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>: chaining yields consistent improvements over the pre-chain baseline across all criteria. Whisper-WER drops from 10.15 to 7.05&#8211;7.88, translating to <math alttext=\"22\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>22</mn><annotation encoding=\"application/x-tex\">22</annotation></semantics></math>&#8211;<math alttext=\"31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mn>31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">31\\%</annotation></semantics></math> relative gains, with best result occurring at <math alttext=\"\\tau{=}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}1.0</annotation></semantics></math>. Speaker similarity and MOS also improve by 4% on average, with ST-argmax achieving the highest SIM-O (57.22) and predicted MOS (3.03). Thus, joint training transfers well to the target domain without sacrificing naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "predicted",
                    "mos",
                    "simo",
                    "similarity",
                    "prechain",
                    "baseline",
                    "speaker",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced TokenChain, a machine speech chain with a fully discrete token interface. Using ST-argmax/Gumbel&#8211;Softmax with dynamic weight averaging, it enables end-to-end feedback between a semantic-token ASR and an AR T2S while keeping a NAR S2A fixed for synthesis. Empirically, TokenChain improves recognition under equal compute and converges 2&#8211;6 epochs earlier on LibriSpeech (5&#8211;13% lower equal-epoch error); under TED-LIUM adaptation it achieves substantial ASR (56%) and T2S WER (31%) reductions with limited forgetting. Ablations show annealed ST-Gumbel (<math alttext=\"\\tau\\!:\\!\\,2.0{\\to}0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">:</mo><mrow><mn>&#8201;2.0</mn><mo stretchy=\"false\">&#8594;</mo><mn>0.1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\!:\\!\\,2.0{\\to}0.1</annotation></semantics></math>) is strongest in-domain, whereas a sharper interface (<math alttext=\"\\tau{\\approx}0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8776;</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\approx}0.75</annotation></semantics></math>) favors cross-domain transfer. Future work includes adaptive learning rate scheduling, joint S2A training, scaling to larger multilingual corpora, and human evaluations.</p>\n\n",
                "matched_terms": [
                    "stgumbel",
                    "librispeech",
                    "wer"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "TokenChain: A Discrete Speech Chain via Semantic Token Modeling",
        "caption": "Table 3: ASR Final Epoch CER/WER (%) on TED-LIUM.",
        "body": "CER ↓\nWER ↓\n\n\nModel\ndev\ntest\ndev\ntest\n\n\n\nPre-chain (Epoch 0)\n\n13.6\n13.7\n29.0\n29.0\n\n\n\n\n\nBaseline (LASRL_{\\text{ASR}} only)\n\n6.5\n6.5\n13.8\n13.5\n\n\nST-Argmax\n6.1\n6.4\n12.8\n13.0\n\n\nST-Gumbel Anneal\n6.2\n6.2\n13.1\n12.6\n\n\nST-Gumbel 1.5\n6.2\n6.2\n13.1\n12.7\n\n\nST-Gumbel 1.0\n6.2\n6.2\n13.0\n12.6\n\n\nST-Gumbel 0.75\n6.0\n6.2\n12.7\n12.6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding:-0.25pt 4.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">CER &#8595;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER &#8595;</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">dev</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">test</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">dev</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">test</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\">\n<em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Pre-chain</em><span class=\"ltx_text\" style=\"font-size:90%;\"> (Epoch 0)</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.6</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.7</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.0</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.0</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Baseline (</span><math alttext=\"L_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">L</mi><mtext mathsize=\"0.900em\">ASR</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{ASR}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> only)</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Argmax</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel Anneal</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel 1.5</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel 1.0</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ST-Gumbel 0.75</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">12.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">12.6</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tedlium",
            "model",
            "test",
            "final",
            "cerwer",
            "epoch",
            "lasrltextasr",
            "stgumbel",
            "dev",
            "anneal",
            "prechain",
            "baseline",
            "wer",
            "cer",
            "only",
            "stargmax",
            "asr"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T3\" title=\"Table 3 &#8227; 3.4 Domain Adaptation on TED-LIUM &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports final-epoch CER/WER, as the dominant TED-LIUM effect is improved generalization rather than faster convergence. Although the <math alttext=\"L_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{ASR}}</annotation></semantics></math>-only baseline already cuts WER by 52% from pre-chain, TokenChain provides further dev/test gains. The best configuration, ST-Gumbel at <math alttext=\"\\tau{=}0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}0.75</annotation></semantics></math>, attains 6.0/6.2 CER and 12.7/12.6 WER, corresponding to 55.3% and 56.4% total reductions, respectively. Argmax and annealed Gumbel are competitive but not optimal. The trend indicates that sharper discrete interfaces (smaller <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math>, approaching ST-argmax) tend to favor cross-domain transfer.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Machine Speech Chain, simulating the human perception-production loop, proves effective in jointly improving ASR and TTS. We propose TokenChain, a fully discrete speech chain coupling semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic model co-trained with ASR and a masked-generative semantic-to-acoustic model for synthesis only. End-to-end feedback across the text interface is enabled with straight-through argmax/Gumbel&#8211;Softmax and balanced with supervised ASR via dynamic weight averaging. Ablations examine optimal temperature schedules for in- and cross-domain transfer. Evaluation reveals TokenChain surpasses baseline accuracy 2&#8211;6 epochs earlier and yields 5&#8211;13% lower equal-epoch error with stable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by 31% on TED-LIUM with minimal forgetting, showing that chain learning remains effective with token interfaces and models.</p>\n\n",
                "matched_terms": [
                    "tedlium",
                    "model",
                    "baseline",
                    "wer",
                    "only",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For ASR, scaling in self-supervised learning have driven gains, and discretizing SSL features emerged as viable inputs. Early unit-based systems required language models to match log-mel baselines <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib12\" title=\"\">12</a>]</cite>; recent work quantizes SSL features into compact codebooks to achieve competitive WER with lower storage and I/O <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib13\" title=\"\">13</a>]</cite>. These results suggest semantic token sequences are suitable carriers for recognition and, by symmetry, promising targets for text-conditioned generation.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TokenChain (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) employs SOTA-competitive architectures, coupling a semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic (T2S) model co-trained with ASR, and a masked-generative semantic-to-acoustic (S2A) module for synthesis only. The ASR&#8211;T2S interface is textual, while feedback is entirely token-based: ST-argmax/Gumbel&#8211;Softmax enable backpropagation, and a semantic-token reconstruction loss is dynamically balanced with ASR cross-entropy. In chained training, TokenChain outperforms baselines, converging 2&#8211;6 epochs earlier with 5&#8211;13% lower equal-epoch error, and under domain adaptation reduces WER by 56% for ASR and 31% for T2S with minimal forgetting, demonstrating that the speech-chain paradigm reinstantiates effectively in the discrete-token era.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "only",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all experiments, the forward pass uses a hard one-hot <math alttext=\"\\hat{\\mathbf{y}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119858;</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{y}}_{t}</annotation></semantics></math>; in the backward pass, gradients propagate through <math alttext=\"{\\mathbf{p}}_{y}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>&#119849;</mi><mi>y</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">{\\mathbf{p}}_{y}^{t}</annotation></semantics></math> for ST-argmax and through <math alttext=\"\\tilde{\\mathbf{p}}_{y}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119849;</mi><mo>~</mo></mover><mi>y</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{p}}_{y}^{t}</annotation></semantics></math> for ST-Gumbel.</p>\n\n",
                "matched_terms": [
                    "stgumbel",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics.</span>\nFor ASR, we evaluate CER and WER. For synthesized audio, we evaluate WER via Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib26\" title=\"\">26</a>]</cite>, speaker similarity (SIM-O) with WavLM&#8211;TDNN2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib27\" title=\"\">27</a>]</cite>, and speech quality using UTMOSv2 (Predicted MOS) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib28\" title=\"\">28</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TokenChain Experiments.</span>\nStarting from LibriSpeech-100 pretrained ASR/T2S, enable chain feedback (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S2.SS4\" title=\"2.4 Discrete Pass-Through and Chain Feedback &#8227; 2 Methods &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>) on LibriSpeech-960 and TED-LIUM v2 for 20 epochs. As a comparison baseline, we train the same setup for 20 epochs with T2S feedback disabled. We compare ST-argmax, ST-Gumbel with temperature\n<math alttext=\"\\tau\\!\\in\\!\\{\\,\\text{anneal }2.0{\\to}0.1\\text{ in 10 epochs},\\allowbreak\\,1.5,\\,1.0,\\,0.75\\,\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8712;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mtext>anneal&#160;</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2.0</mn></mrow><mo stretchy=\"false\">&#8594;</mo><mrow><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>&#160;in 10 epochs</mtext></mrow><mo>,</mo><mn>&#8201;1.5</mn><mo>,</mo><mn>&#8201;1.0</mn><mo>,</mo><mn>&#8201;0.75</mn></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\!\\in\\!\\{\\,\\text{anneal }2.0{\\to}0.1\\text{ in 10 epochs},\\allowbreak\\,1.5,\\,1.0,\\,0.75\\,\\}</annotation></semantics></math>.\nEarly stopping halts training after 3 consecutive epochs without validation improvement.\nThe chain weight <math alttext=\"\\alpha_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mi>e</mi></msub><annotation encoding=\"application/x-tex\">\\alpha_{e}</annotation></semantics></math> follows DWA with warm-ups and capped ramp. In our runs <math alttext=\"(\\alpha_{\\mathrm{w0}},\\alpha_{\\mathrm{w1}},\\alpha_{\\max},e_{\\mathrm{ramp}},T)=(10^{-3},0.05,0.5,6,2)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>w0</mi></msub><mo>,</mo><msub><mi>&#945;</mi><mi>w1</mi></msub><mo>,</mo><msub><mi>&#945;</mi><mi>max</mi></msub><mo>,</mo><msub><mi>e</mi><mi>ramp</mi></msub><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup><mo>,</mo><mn>0.05</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">(\\alpha_{\\mathrm{w0}},\\alpha_{\\mathrm{w1}},\\alpha_{\\max},e_{\\mathrm{ramp}},T)=(10^{-3},0.05,0.5,6,2)</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "tedlium",
                    "stgumbel",
                    "baseline",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T1\" title=\"Table 1 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents equal-compute CER/WER at epoch 12, when TokenChain variants have effectively converged and exceed baseline&#8217;s final figures at epoch 20. Across all splits, chain feedback outperforms the <math alttext=\"L_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{ASR}}</annotation></semantics></math>-only baseline, adding to the baseline&#8217;s roughly halved error rate from the pre-chain checkpoint. The strongest variant, ST-Gumbel Anneal, yields further 10&#8211;13% relative gains on clean sets and 5&#8211;9% on other sets. Fixed <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> around 1.5 is competitive, whereas sharper <math alttext=\"\\tau{\\leq}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8804;</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\leq}1.0</annotation></semantics></math> are less effective yet still surpass the baseline.</p>\n\n",
                "matched_terms": [
                    "epoch",
                    "cerwer",
                    "final",
                    "anneal",
                    "prechain",
                    "baseline",
                    "stgumbel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> evaluates T2S by calculating Whisper-WER, SIM-O, and predicted MOS on audio synthesized with a fixed S2A. Chain feedback improves content robustness while preserving perceptual quality. ST-argmax achieves the lowest Whisper-WER (-11.6% vs. baseline). SIM-O remains within <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.5 of baseline, and MOS is stable, peaking at <math alttext=\"\\tau{=}1.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>1.5</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}1.5</annotation></semantics></math> (+0.06). Sharper temperatures (<math alttext=\"\\tau{\\leq}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8804;</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\leq}1.0</annotation></semantics></math>) degrade WER with minimal change in SIM-O/MOS, indicating that over-discrete interfaces (small <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math>) impede text controllability of the T2S.</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "stargmax",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Convergence Efficiency.</span> Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.F2\" title=\"Figure 2 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that TokenChain&#8217;s learning curves are consistently below the baseline for CER and WER, surpassing baseline accuracy 2&#8211;6 epochs earlier. At epoch 12, TokenChain already exceeds baseline&#8217;s final (epoch 20) correct rates by 0.2 on clean sets and 0.4 on other sets. Hence, comparable or better accuracy is achieved with approximately 40% fewer epochs (and compute), yielding substantial efficiency gains and underscoring chain feedback&#8217;s role as a capable optimization aid and regularizer.</p>\n\n",
                "matched_terms": [
                    "epoch",
                    "final",
                    "baseline",
                    "wer",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">T2S results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T4\" title=\"Table 4 &#8227; 3.4 Domain Adaptation on TED-LIUM &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>: chaining yields consistent improvements over the pre-chain baseline across all criteria. Whisper-WER drops from 10.15 to 7.05&#8211;7.88, translating to <math alttext=\"22\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>22</mn><annotation encoding=\"application/x-tex\">22</annotation></semantics></math>&#8211;<math alttext=\"31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mn>31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">31\\%</annotation></semantics></math> relative gains, with best result occurring at <math alttext=\"\\tau{=}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}1.0</annotation></semantics></math>. Speaker similarity and MOS also improve by 4% on average, with ST-argmax achieving the highest SIM-O (57.22) and predicted MOS (3.03). Thus, joint training transfers well to the target domain without sacrificing naturalness.</p>\n\n",
                "matched_terms": [
                    "prechain",
                    "baseline",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced TokenChain, a machine speech chain with a fully discrete token interface. Using ST-argmax/Gumbel&#8211;Softmax with dynamic weight averaging, it enables end-to-end feedback between a semantic-token ASR and an AR T2S while keeping a NAR S2A fixed for synthesis. Empirically, TokenChain improves recognition under equal compute and converges 2&#8211;6 epochs earlier on LibriSpeech (5&#8211;13% lower equal-epoch error); under TED-LIUM adaptation it achieves substantial ASR (56%) and T2S WER (31%) reductions with limited forgetting. Ablations show annealed ST-Gumbel (<math alttext=\"\\tau\\!:\\!\\,2.0{\\to}0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">:</mo><mrow><mn>&#8201;2.0</mn><mo stretchy=\"false\">&#8594;</mo><mn>0.1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\!:\\!\\,2.0{\\to}0.1</annotation></semantics></math>) is strongest in-domain, whereas a sharper interface (<math alttext=\"\\tau{\\approx}0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8776;</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\approx}0.75</annotation></semantics></math>) favors cross-domain transfer. Future work includes adaptive learning rate scheduling, joint S2A training, scaling to larger multilingual corpora, and human evaluations.</p>\n\n",
                "matched_terms": [
                    "tedlium",
                    "stgumbel",
                    "asr",
                    "wer"
                ]
            }
        ]
    },
    "S3.T4": {
        "source_file": "TokenChain: A Discrete Speech Chain via Semantic Token Modeling",
        "caption": "Table 4: TED-LIUM TTS: Accuracy (WER %), Speaker similarity (SIM-O), and Naturalness (Predicted MOS).",
        "body": "Model\nWER ↓\nSIM-O ↑\nPred. MOS ↑\n\n\n\nPre-chain / Baseline\n\n10.15\n54.15\n2.89\n\n\n\n\nST-Argmax\n7.50\n57.22\n3.03\n\n\nST-Gumbel Anneal\n7.85\n56.56\n3.00\n\n\nST-Gumbel 1.5\n7.88\n56.81\n2.98\n\n\nST-Gumbel 1.0\n7.05\n56.85\n2.98\n\n\nST-Gumbel 0.75\n7.88\n56.78\n2.98",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER &#8595;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SIM-O &#8593;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Pred. MOS &#8593;</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\">\n<em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">Pre-chain</em><span class=\"ltx_text\" style=\"font-size:90%;\"> / Baseline</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.15</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">54.15</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.89</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Argmax</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">57.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.03</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel Anneal</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.85</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel 1.5</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.88</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.81</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.98</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel 1.0</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.05</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.85</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.98</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ST-Gumbel 0.75</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.98</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tedlium",
            "naturalness",
            "predicted",
            "model",
            "pred",
            "mos",
            "tts",
            "simo",
            "anneal",
            "similarity",
            "prechain",
            "baseline",
            "wer",
            "stgumbel",
            "speaker",
            "accuracy",
            "stargmax"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">T2S results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T4\" title=\"Table 4 &#8227; 3.4 Domain Adaptation on TED-LIUM &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>: chaining yields consistent improvements over the pre-chain baseline across all criteria. Whisper-WER drops from 10.15 to 7.05&#8211;7.88, translating to <math alttext=\"22\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><mn>22</mn><annotation encoding=\"application/x-tex\">22</annotation></semantics></math>&#8211;<math alttext=\"31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><mrow><mn>31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">31\\%</annotation></semantics></math> relative gains, with best result occurring at <math alttext=\"\\tau{=}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}1.0</annotation></semantics></math>. Speaker similarity and MOS also improve by 4% on average, with ST-argmax achieving the highest SIM-O (57.22) and predicted MOS (3.03). Thus, joint training transfers well to the target domain without sacrificing naturalness.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Machine Speech Chain, simulating the human perception-production loop, proves effective in jointly improving ASR and TTS. We propose TokenChain, a fully discrete speech chain coupling semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic model co-trained with ASR and a masked-generative semantic-to-acoustic model for synthesis only. End-to-end feedback across the text interface is enabled with straight-through argmax/Gumbel&#8211;Softmax and balanced with supervised ASR via dynamic weight averaging. Ablations examine optimal temperature schedules for in- and cross-domain transfer. Evaluation reveals TokenChain surpasses baseline accuracy 2&#8211;6 epochs earlier and yields 5&#8211;13% lower equal-epoch error with stable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by 31% on TED-LIUM with minimal forgetting, showing that chain learning remains effective with token interfaces and models.</p>\n\n",
                "matched_terms": [
                    "tedlium",
                    "model",
                    "tts",
                    "baseline",
                    "wer",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">TokenChain (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>) employs SOTA-competitive architectures, coupling a semantic-token ASR with a two-stage TTS: an autoregressive text-to-semantic (T2S) model co-trained with ASR, and a masked-generative semantic-to-acoustic (S2A) module for synthesis only. The ASR&#8211;T2S interface is textual, while feedback is entirely token-based: ST-argmax/Gumbel&#8211;Softmax enable backpropagation, and a semantic-token reconstruction loss is dynamically balanced with ASR cross-entropy. In chained training, TokenChain outperforms baselines, converging 2&#8211;6 epochs earlier with 5&#8211;13% lower equal-epoch error, and under domain adaptation reduces WER by 56% for ASR and 31% for T2S with minimal forgetting, demonstrating that the speech-chain paradigm reinstantiates effectively in the discrete-token era.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In all experiments, the forward pass uses a hard one-hot <math alttext=\"\\hat{\\mathbf{y}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>&#119858;</mi><mo>^</mo></mover><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{y}}_{t}</annotation></semantics></math>; in the backward pass, gradients propagate through <math alttext=\"{\\mathbf{p}}_{y}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m5\" intent=\":literal\"><semantics><msubsup><mi>&#119849;</mi><mi>y</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">{\\mathbf{p}}_{y}^{t}</annotation></semantics></math> for ST-argmax and through <math alttext=\"\\tilde{\\mathbf{p}}_{y}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS4.p1.m6\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119849;</mi><mo>~</mo></mover><mi>y</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{p}}_{y}^{t}</annotation></semantics></math> for ST-Gumbel.</p>\n\n",
                "matched_terms": [
                    "stgumbel",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics.</span>\nFor ASR, we evaluate CER and WER. For synthesized audio, we evaluate WER via Whisper-large-v3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib26\" title=\"\">26</a>]</cite>, speaker similarity (SIM-O) with WavLM&#8211;TDNN2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib27\" title=\"\">27</a>]</cite>, and speech quality using UTMOSv2 (Predicted MOS) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#bib.bib28\" title=\"\">28</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "predicted",
                    "mos",
                    "simo",
                    "similarity",
                    "wer",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TokenChain Experiments.</span>\nStarting from LibriSpeech-100 pretrained ASR/T2S, enable chain feedback (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S2.SS4\" title=\"2.4 Discrete Pass-Through and Chain Feedback &#8227; 2 Methods &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2.4</span></a>) on LibriSpeech-960 and TED-LIUM v2 for 20 epochs. As a comparison baseline, we train the same setup for 20 epochs with T2S feedback disabled. We compare ST-argmax, ST-Gumbel with temperature\n<math alttext=\"\\tau\\!\\in\\!\\{\\,\\text{anneal }2.0{\\to}0.1\\text{ in 10 epochs},\\allowbreak\\,1.5,\\,1.0,\\,0.75\\,\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8712;</mo><mrow><mo rspace=\"0.170em\" stretchy=\"false\">{</mo><mrow><mrow><mtext>anneal&#160;</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2.0</mn></mrow><mo stretchy=\"false\">&#8594;</mo><mrow><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext>&#160;in 10 epochs</mtext></mrow><mo>,</mo><mn>&#8201;1.5</mn><mo>,</mo><mn>&#8201;1.0</mn><mo>,</mo><mn>&#8201;0.75</mn></mrow></mrow><mo lspace=\"0.170em\" stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\!\\in\\!\\{\\,\\text{anneal }2.0{\\to}0.1\\text{ in 10 epochs},\\allowbreak\\,1.5,\\,1.0,\\,0.75\\,\\}</annotation></semantics></math>.\nEarly stopping halts training after 3 consecutive epochs without validation improvement.\nThe chain weight <math alttext=\"\\alpha_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><mi>e</mi></msub><annotation encoding=\"application/x-tex\">\\alpha_{e}</annotation></semantics></math> follows DWA with warm-ups and capped ramp. In our runs <math alttext=\"(\\alpha_{\\mathrm{w0}},\\alpha_{\\mathrm{w1}},\\alpha_{\\max},e_{\\mathrm{ramp}},T)=(10^{-3},0.05,0.5,6,2)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mi>w0</mi></msub><mo>,</mo><msub><mi>&#945;</mi><mi>w1</mi></msub><mo>,</mo><msub><mi>&#945;</mi><mi>max</mi></msub><mo>,</mo><msub><mi>e</mi><mi>ramp</mi></msub><mo>,</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup><mo>,</mo><mn>0.05</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>6</mn><mo>,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">(\\alpha_{\\mathrm{w0}},\\alpha_{\\mathrm{w1}},\\alpha_{\\max},e_{\\mathrm{ramp}},T)=(10^{-3},0.05,0.5,6,2)</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "tedlium",
                    "stgumbel",
                    "baseline",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T1\" title=\"Table 1 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents equal-compute CER/WER at epoch 12, when TokenChain variants have effectively converged and exceed baseline&#8217;s final figures at epoch 20. Across all splits, chain feedback outperforms the <math alttext=\"L_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{ASR}}</annotation></semantics></math>-only baseline, adding to the baseline&#8217;s roughly halved error rate from the pre-chain checkpoint. The strongest variant, ST-Gumbel Anneal, yields further 10&#8211;13% relative gains on clean sets and 5&#8211;9% on other sets. Fixed <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> around 1.5 is competitive, whereas sharper <math alttext=\"\\tau{\\leq}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8804;</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\leq}1.0</annotation></semantics></math> are less effective yet still surpass the baseline.</p>\n\n",
                "matched_terms": [
                    "stgumbel",
                    "anneal",
                    "baseline",
                    "prechain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T2\" title=\"Table 2 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> evaluates T2S by calculating Whisper-WER, SIM-O, and predicted MOS on audio synthesized with a fixed S2A. Chain feedback improves content robustness while preserving perceptual quality. ST-argmax achieves the lowest Whisper-WER (-11.6% vs. baseline). SIM-O remains within <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.5 of baseline, and MOS is stable, peaking at <math alttext=\"\\tau{=}1.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>1.5</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}1.5</annotation></semantics></math> (+0.06). Sharper temperatures (<math alttext=\"\\tau{\\leq}1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8804;</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\leq}1.0</annotation></semantics></math>) degrade WER with minimal change in SIM-O/MOS, indicating that over-discrete interfaces (small <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math>) impede text controllability of the T2S.</p>\n\n",
                "matched_terms": [
                    "predicted",
                    "mos",
                    "simo",
                    "baseline",
                    "wer",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Convergence Efficiency.</span> Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.F2\" title=\"Figure 2 &#8227; 3.3 Main Results on LibriSpeech &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that TokenChain&#8217;s learning curves are consistently below the baseline for CER and WER, surpassing baseline accuracy 2&#8211;6 epochs earlier. At epoch 12, TokenChain already exceeds baseline&#8217;s final (epoch 20) correct rates by 0.2 on clean sets and 0.4 on other sets. Hence, comparable or better accuracy is achieved with approximately 40% fewer epochs (and compute), yielding substantial efficiency gains and underscoring chain feedback&#8217;s role as a capable optimization aid and regularizer.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "baseline",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.06201v1#S3.T3\" title=\"Table 3 &#8227; 3.4 Domain Adaptation on TED-LIUM &#8227; 3 Experiments &#8227; TokenChain: A Discrete Speech Chain via Semantic Token Modeling\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports final-epoch CER/WER, as the dominant TED-LIUM effect is improved generalization rather than faster convergence. Although the <math alttext=\"L_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{ASR}}</annotation></semantics></math>-only baseline already cuts WER by 52% from pre-chain, TokenChain provides further dev/test gains. The best configuration, ST-Gumbel at <math alttext=\"\\tau{=}0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>=</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{=}0.75</annotation></semantics></math>, attains 6.0/6.2 CER and 12.7/12.6 WER, corresponding to 55.3% and 56.4% total reductions, respectively. Argmax and annealed Gumbel are competitive but not optimal. The trend indicates that sharper discrete interfaces (smaller <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math>, approaching ST-argmax) tend to favor cross-domain transfer.</p>\n\n",
                "matched_terms": [
                    "tedlium",
                    "prechain",
                    "baseline",
                    "wer",
                    "stgumbel",
                    "stargmax"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduced TokenChain, a machine speech chain with a fully discrete token interface. Using ST-argmax/Gumbel&#8211;Softmax with dynamic weight averaging, it enables end-to-end feedback between a semantic-token ASR and an AR T2S while keeping a NAR S2A fixed for synthesis. Empirically, TokenChain improves recognition under equal compute and converges 2&#8211;6 epochs earlier on LibriSpeech (5&#8211;13% lower equal-epoch error); under TED-LIUM adaptation it achieves substantial ASR (56%) and T2S WER (31%) reductions with limited forgetting. Ablations show annealed ST-Gumbel (<math alttext=\"\\tau\\!:\\!\\,2.0{\\to}0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">:</mo><mrow><mn>&#8201;2.0</mn><mo stretchy=\"false\">&#8594;</mo><mn>0.1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\!:\\!\\,2.0{\\to}0.1</annotation></semantics></math>) is strongest in-domain, whereas a sharper interface (<math alttext=\"\\tau{\\approx}0.75\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8776;</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">\\tau{\\approx}0.75</annotation></semantics></math>) favors cross-domain transfer. Future work includes adaptive learning rate scheduling, joint S2A training, scaling to larger multilingual corpora, and human evaluations.</p>\n\n",
                "matched_terms": [
                    "tedlium",
                    "stgumbel",
                    "wer"
                ]
            }
        ]
    }
}